{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"kubernetesinmyhead.net","text":"<p>These are my notes about some kubernetes related technologies</p>"},{"location":"#conventions","title":"Conventions","text":"<p>98-tips.md 99-links.md</p>"},{"location":"CI-CD/argo-events/filtering/","title":"Filters","text":"<ul> <li>Filtering in eventsources</li> </ul> <p>https://argoproj.github.io/argo-events/eventsources/filtering/</p> <ul> <li>Filtering in sensors</li> </ul> <p>https://argoproj.github.io/argo-events/sensors/filters/intro/</p>"},{"location":"CI-CD/argo-events/parameters-to-workflow/","title":"Pass parameters to workflows","text":"<p>We can trigger an argo workflow with argo events and pass parameters</p> <pre><code>    - template:\n        name: argo-workflow-trigger\n        conditions: mycondition\n        policy:\n          k8s:\n            labels:\n              workflows.argoproj.io/phase: Succeeded\n        k8s:  # I have found some problems using \"argoWorkflow:\" here\n          operation: create\n          ...\n          parameters:\n            - src:\n                dependencyName: mydependency\n                dataKey: body.fistparameter\n              dest: spec.arguments.parameters.0.value ## this will be the first parameter in the workflow\n            - src:\n                dependencyName: mydependency\n                dataKey: body.another\n              dest: spec.arguments.parameters.1.value ## this will be the second parameter in the workflow\n            - src:\n                dependencyName: mydependency\n                dataKey: body.mytitle\n              dest: metadata.annotations.workflows\\.argoproj.io\\/title # we can escape characters\n            - src:\n                dependencyName: mydependency\n                dataKey: body.mydescription\n              dest: metadata.annotations.workflows\\.argoproj\\.io\\/description # we can escape characters\n\n</code></pre>"},{"location":"CI-CD/argo-events/service-accounts/","title":"Service accounts","text":""},{"location":"CI-CD/argo-events/service-accounts/#for-eventsources","title":"For eventsources","text":"<p>The service account of an eventsource can be specified in the spec.template.serviceAccountName field of the eventsource but it does not requires special permissions. In addition to that it can be interesting to create a new one and not use the default one.</p> <p>The exception is the \"resource\" eventsource. In that case you must give that service account the list and watch permissions to that resource being watched.</p>"},{"location":"CI-CD/argo-events/service-accounts/#for-sensors","title":"For sensors","text":"<p>The service account of a sensor can be specified in the spec.template.serviceAccountName field of the eventsource but it does not requires special permissions. In addition to that it can be interesting to create a new one and not use the default one.</p> <p>The exceptions are the k8s trigger adn the argoWorkflow trigger.</p>"},{"location":"CI-CD/argo-events/service-accounts/#argo-workflow-trigger","title":"Argo workflow trigger","text":"<p>To submit a workflow, the service account of the sensor needs create and list rbac permissions. To resubmit, retry, resume or suspend a workflow, the service account of the sensor needs \"update\" and \"get\" rbac permissions.</p> <p>Here it is a non tuned rbac permssions https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/sensor-rbac.yaml</p> <p>!!! Note \"This rbac permissions are not related with what the workflow will do. Only for create, retry,... For that see service accounts in argo workflows</p>"},{"location":"CI-CD/argo-events/service-accounts/#k8s-resource-trigger","title":"K8s resource trigger","text":"<p>To create a kubernetes resource, the service account of the sensor needs \"create\" rbac permissions for that resource.</p>"},{"location":"CI-CD/argo-events/service-accounts/#links","title":"Links","text":"<ul> <li>Service Accounts in argo events https://argoproj.github.io/argo-events/service-accounts/</li> </ul>"},{"location":"CI-CD/argo-workflows/labels-annotations/","title":"Labels and annotations","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#labels","title":"Labels","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#workflow-creator","title":"Workflow creator","text":"<p>https://argo-workflows.readthedocs.io/en/stable/workflow-creator/</p>"},{"location":"CI-CD/argo-workflows/labels-annotations/#configmap","title":"Configmap","text":"<ul> <li>Executor plugin</li> </ul> <pre><code>workflows.argoproj.io/configmap-type: ExecutorPlugin\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/stable/executor_plugins/</p> <ul> <li>Memoization and cache</li> </ul> <pre><code>workflows.argoproj.io/configmap-type: Cache\n</code></pre> <ul> <li>Parameter</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/memoization/</p> <pre><code>workflows.argoproj.io/configmap-type: Parameter\n</code></pre>"},{"location":"CI-CD/argo-workflows/labels-annotations/#annotations","title":"Annotations","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#title-and-description","title":"Title and description","text":"<pre><code>workflows.argoproj.io/title: \"My title\"\n</code></pre> <p>defaults to metadata.name if not specified</p> <pre><code>workflows.argoproj.io/description: \"SuperDuperProject\"\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/stable/title-and-description/</p>"},{"location":"CI-CD/argo-workflows/retries/","title":"retryStrategy","text":"<p>retryStrategy permits to control retries and it ca be defined at 2 levels</p> <ul> <li>at workflow level (spec.retryStrategy) affects all templates in the workflow</li> <li>in every template describes how to retry a template when it fails</li> </ul> <p>In the retryStrategy we have some options</p>"},{"location":"CI-CD/argo-workflows/retries/#retrypolicy-and-expression","title":"retryPolicy and expression","text":"<p>Both are re-evaluated after each attempt. For example, if you set retryPolicy: OnFailure and your first attempt produces a failure then a retry will be attempted. If the second attempt produces an error, then another attempt will not be made. The expression result will be logical and with the retryPolicy. Both must be true to retry.</p>"},{"location":"CI-CD/argo-workflows/retries/#expression","title":"expression","text":"<p>Expression is a condition expression for when a node will be retried. If it evaluates to false, the node will not be retried and the retry strategy will be ignored. If expression evaluates to false, the step will not be retried.</p> <p>This variables are available:</p> <ul> <li>lastRetry.exitCode: The exit code of the last retry, or \"-1\" if not available</li> <li>lastRetry.status: The phase of the last retry: Error, Failed</li> <li>lastRetry.duration: The duration of the last retry, in seconds</li> <li>lastRetry.message: The message output from the last retry (available from version 3.5)</li> </ul>"},{"location":"CI-CD/argo-workflows/retries/#retrypolicy","title":"retryPolicy","text":"<p>RetryPolicy is a policy of NodePhase statuses that will be retried. Here we choose what failures type to retry.</p> <ul> <li>Always</li> </ul> <p>Retry all failed steps</p> <ul> <li>OnFailure</li> </ul> <p>Retry steps whose main container is marked as failed in Kubernetes</p> <ul> <li>OnError</li> </ul> <p>Retry steps that encounter Argo controller errors, or whose init or wait containers fail</p> <ul> <li>OnTransientError</li> </ul> <p>Retry steps that encounter errors defined as transient, or errors matching the TRANSIENT_ERROR_PATTERN environment variable. Available in version 3.0 and later.</p> <p>The retryPolicy applies even if you also specify an expression, but in version 3.5 or later the default policy means the expression makes the decision unless you explicitly specify a policy.</p> <p>About the default retryPolicy</p> <p>The default retryPolicy is OnFailure, except in version 3.5 or later when an expression is also supplied, when it is Always</p> <p></p>"},{"location":"CI-CD/argo-workflows/retries/#limit","title":"limit","text":"<p>Limit is the maximum number of retry attempts when retrying a container. It does not include the original container; the maximum number of total attempts will be <code>limit + 1</code>.</p>"},{"location":"CI-CD/argo-workflows/retries/#affinity","title":"affinity","text":"<p>Affinity prevents running workflow's step on the same host</p> <p>nodeAntiAffinity</p>"},{"location":"CI-CD/argo-workflows/retries/#backoff","title":"backoff","text":"<p>Backoff is a backoff strategy. You can configure the delay between retries with backoff. See example for usage.</p> <ul> <li> <p>duration</p> </li> <li> <p>maxDuration</p> </li> <li> <p>cap</p> </li> <li> <p>factor (int)</p> </li> </ul> <p>duration, maxDuration and cap are strings taken as unit members (by default seconds). Example: \"4\". Could also be a Duration, e.g.: \"2m\", \"6h\"</p>"},{"location":"CI-CD/argo-workflows/retries/#links","title":"Links","text":"<ul> <li>Retries</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/retries/</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/","title":"Service accounts and rbac","text":""},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#argo-workflows","title":"Argo workflows","text":"<p>All the pods in a workflow use a service account. This service account can be specified with spec.serviceAccountName in the workflow. If not, uses the \"default\" service account (not recommended).</p> <p>If using the argo cli, we can achieve that with the --serviceaccount NAME parameter</p> <pre><code>argo submit --serviceaccount myserviceaccount myworkflow\n</code></pre> <p>The minimum permissions to run workflows in newest releases (since 3.4) is:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: executor\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtaskresults\n    verbs:\n      - create\n      - patch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: executor\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: executor\nsubjects:\n  - kind: ServiceAccount\n    name: executor\n    namespace: argo\n</code></pre> <p>Depending of what the workflow has to do, it will neccesary to give that service account additional permissions via kubernetes rbac. For example, if the workflow creates pods, it will need the verb \"create\" in pods resource.</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#using-the-http-template","title":"Using the http template","text":"<p>See the http template doc</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#workflow-links","title":"Workflow links","text":"<ul> <li> <p>Argo workflows service accounts https://argo-workflows.readthedocs.io/en/stable/service-accounts/</p> </li> <li> <p>Argo workflows rbac https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#in-an-argo-events-sensor","title":"In an Argo events sensor","text":"<p>If you want to create a workflow using an argo events sensor, you can pass the name of the service account (and other arguments) to the argo cli with the field \"args\" in addition to \"source\", \"parameters\" and \"operation\".</p> <ul> <li>Argo workflows trigger spec in Argo events https://github.com/argoproj/argo-events/blob/master/api/sensor.md#argoproj.io/v1alpha1.ArgoWorkflowTrigger</li> </ul>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/","title":"Timeouts and self cleaning","text":""},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-timeout-activedeadlineseconds","title":"Workflow Timeout (activeDeadlineSeconds)","text":"<p>The maximum time allowed (timeout) for a workflow is configured with the spec.activeDeadlineSeconds fields. After this number of seconds, the workflow is terminated.</p> <p>Changing this value to zero in a running workflow will terminate it</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  activeDeadlineSeconds: 10\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-auto-deletion-ttlstrategy","title":"Workflow auto deletion (TTLStrategy)","text":"<p>We can control the self deletion of a finished workflow with some spec.TTLStrategy fields:</p> <ul> <li>secondsAfterCompletion</li> </ul> <p>Number of seconds the workflow we will be maintained after completion</p> <ul> <li>secondsAfterFailure</li> </ul> <p>Number of seconds the workflow we will be maintained after failure</p> <ul> <li>secondsAfterSuccess</li> </ul> <p>Number of seconds the workflow we will be maintained after Succeeded</p> <p>If we configure all 3, secondsAfterFailure and secondsAfterSucces have precedence</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#pod-auto-deletion-podgc","title":"Pod auto deletion (PodGC)","text":"<p>With PodGC we can configure when to delete the completed pods.</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#strategy","title":"Strategy","text":"<p>We must choose an strategy:</p> <ul> <li>\"OnPodCompletion\" deletes the pods when the pods ends (including failures)</li> <li>\"OnPodSuccess\"  deletes the pods when the pods ends successfully</li> <li>\"OnWorkflowCompletion\" deletes the pods when the workflow ends</li> <li>\"OnWorkflowSuccess\" deletes the pods when the workflow ends successfully</li> <li>No settings means no deletion will occur.</li> </ul>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#deletedelayduration","title":"deleteDelayDuration","text":"<p>spec.PodGC.deleteDelayDuration is an string field where we can specify the time to wait until the pods in the GC queue will be deleted.</p> <p>The default value is 5s. A zero (value) will delete the pods immediately</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#labelselector","title":"labelSelector","text":"<p>With labelSelector we can se filter using labels what pods will be deleted</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: pod-gc-strategy-\nspec:\n  entrypoint: pod-gc-strategy\n  podGC:\n    strategy: OnPodSuccess\n    deleteDelayDuration: 30s\n    labelSelector:\n      matchLabels:\n        should-be-deleted: \"true\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-defaults-at-controller-level","title":"Workflow defaults at controller level","text":"<p>We can configure al these the default values at controller level in the workflow-controller-configmap configMap</p> <pre><code>  workflowDefaults: |\n    spec:\n      activeDeadlineSeconds: 1200\n      ttlStrategy:\n        secondsAfterCompletion: 86400\n        secondsAfterFailure: 86400\n        secondsAfterSuccess: 28800\n      podGC:\n        strategy: OnPodCompletion\n        deleteDelayDuration: 86400s\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#cronworkflow-history-limits","title":"CronWorkflow history limits","text":"<p>We can control the number of successful jobs to mantain with spec.successfulJobsHistoryLimit</p> <p>The default value is 3</p> <p>And also the failed ones with spec.failedJobsHistoryLimit</p> <p>The default value is 1</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#template-defaults","title":"Template defaults","text":"<p>pending</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#links","title":"Links","text":"<ul> <li> <p>Cost optimization https://argo-workflows.readthedocs.io/en/stable/cost-optimisation/#limit-the-total-number-of-workflows-and-pods</p> </li> <li> <p>Timeouts https://argo-workflows.readthedocs.io/en/stable/walk-through/timeouts/</p> </li> <li> <p>Default workflow spec https://argo-workflows.readthedocs.io/en/stable/default-workflow-specs/</p> </li> <li> <p>Template defaults https://argo-workflows.readthedocs.io/en/stable/template-defaults/</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/variables-intro/","title":"Variables: Intro","text":"<p>In Argo Workflows there 2 kinds of template tag, or ways to call a variable,</p>"},{"location":"CI-CD/argo-workflows/variables-intro/#simple","title":"Simple","text":"<p>The simple way is using this format</p> <pre><code>{{variable}}\n</code></pre> <p>There is simple substitution between the variable and the value</p> <p>The recommended way is not to leave spaces between the brackets</p>"},{"location":"CI-CD/argo-workflows/variables-intro/#expression","title":"Expression","text":"<p>But we can call the variable using an expression, with this format</p> <pre><code>{{=variable}}\n</code></pre> <p>In this case the value of the variable is the result of evaluating the tag as an expression.</p> <p>There are some different things we can do using the expr language. In this example we extract data from a json</p> <pre><code>jsonpath(inputs.parameters.json, '$.some.path')\n</code></pre> <p>If we hyphens in the tag we can have unexpected error. This can be related with parameters or steps. To solve it we can rename the parameter or step, or reference them by indexing into the parameter or step map.</p> <pre><code>inputs.parameters['my-param'] or steps['my-step'].outputs.result\n</code></pre> <p>In this example we can parse a json a using the key \"password\" as a parameter</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: test\nspec:\n  entrypoint: main\n  templates:\n    - name: http-post\n      http:\n        url: \"https://{{workflow.parameters.harborUrl}}/api/v2.0/robots\"\n        method: POST\n    - name: echo\n      inputs:\n        parameters:\n          - name: password\n      container:\n        image: docker.io/alpine\n        command: [\"echo\"]\n        args: \"{{inputs.parameters.username}}\"\n    - name: main\n      steps:\n        - - name: makeapicall\n            template: http-post\n        - - name: deploy-credentials\n            template: write-secret\n            arguments:\n              parameters:\n                - name: password\n                  value: \"{{=jsonpath(steps.makeapicall.outputs.result, '$.password')}}\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/variables-intro/#links","title":"Links","text":"<ul> <li>Workflow Variables</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/variables/</p> <ul> <li>Expr Lang</li> </ul> <p>https://expr-lang.org/docs/language-definition</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/","title":"Define parameters","text":""},{"location":"CI-CD/argo-workflows/arguments/1-define/#global-parameters-arguments","title":"Global parameters (arguments)","text":"<p>We can define global parameters at workflow level</p> <ul> <li>They are located inside arguments section</li> <li>When the workflow is called fully (not calling a template) They and they are passed to the entrypoint template.</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: myworkflow\nspec:\n  arguments:\n    parameters:\n      - name: param1\n      - name: param2\n</code></pre> <p>They can be used inside the templates as variables :</p> <pre><code>\"{{workflow.parameters.param1}}\"\n\"{{workflow.parameters.param2}}\"\n</code></pre> <p>!!! note If we want to provide values to a global parameter, we must pass them there (spec.arguments.parameter.parameter.value)</p> <p>\"{{workflow.parameters.json}}\" is also a variable with all the parameters as a json string</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/#local-scoped-parameters-inputs","title":"Local scoped parameters (inputs)","text":"<p>We can also define parameters at template level as inputs. They are local scoped parameters.</p> <p>A template defines inputs which are then provided by template callers (such as steps, dag, or even a workflow).</p> <p>As inputs</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: myworkflow\nspec:\n  templates:\n    - name: mytemplate\n      inputs:\n        parameters:\n          - name: param1\n          - name: param2\n</code></pre> <p>They can be used inside the templates as:</p> <pre><code>\"{{inputs.parameters.param1}}\"\n\"{{inputs.parameters.param2}}\"\n</code></pre> <p>!!! note If we want to provide values to a local parameter, we can make it using template caller (dag, steps) level, input level and workflow level. See this</p> <p>\"{{inputs.parameters.json}}\" is also a variable with all the parameters as a json string</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/#notes-and-suggestions","title":"Notes and suggestions","text":"<ul> <li> <p>In containerset, container and script templates, inputs and outputs can only be loaded a saved from a template called main.</p> </li> <li> <p>Because there are 3 ways to call One suggestion is to define a parameter that can be used in more than one template in both places, at spec level (argument) and at template level (inputs)</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/","title":"Resolve parameters","text":"<p>When a parameter is used in a task or template, the value is resolved in the following order of precedence:</p> <ul> <li>Task/Step-Level arguments: If a parameter is passed to a task/step using arguments, this value takes precedence.</li> <li>Template inputs: If no task-level arguments are provided, the value defined in the template's inputs is used.</li> <li>Global arguments: If neither task-level arguments nor template inputs provide a value, the global arguments value is used.</li> </ul> <p>Let's see this WorkflowTemplate. It has 2 templates: main (entrypoint) and whalesay</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: test-params\nspec:\n  entrypoint: main\n  arguments:\n    parameters:\n      - name: message\n        value: \"A\" # Third in precedence, if provided and calling the main template\n  templates:\n    - name: main\n      inputs:\n        parameters:\n          - name: message\n            value: \"B\" # Second in precedence, if provided and calling the main template\n      dag:\n        tasks:\n          - name: whalesay\n            template: whalesay\n            arguments:\n              parameters:\n                - name: message\n                  value: \"C\" # First in precedence, if provided and calling the main template\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n            value: \"D\" # whalesay param\n      container:\n        image: docker/whalesay:latest\n        command: [cowsay]\n        args: [\"{{inputs.parameters.message}}\"]\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-the-whole-tamplate","title":"Calling the whole tamplate","text":"<p>If we create a workflow calling the whole template without choosing the entrypoint</p> <ul> <li>the workflow will not have entrypoint</li> <li>the printed value will be A</li> </ul> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-the-main-template","title":"Calling the main template","text":"<p>If we create a workflow calling the main template</p> <ul> <li>the entrypoint will be main</li> <li>the printed value will be C</li> </ul> <p>The order will be C &gt; B &gt; A</p> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-whalesay","title":"Calling whalesay","text":"<p>If we we create a workflow calling the whalesay template</p> <ul> <li>the entrypoint will be whalesay</li> <li>D will have preference over A. D will be the value.</li> </ul> <p>The order will be D &gt; A</p> <p>If both are not provided, the workflow will have an error</p> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/","title":"Parameters in secrets and configmaps","text":""},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#parameters-in-a-configmap","title":"Parameters in a configmap","text":"<p>If we want to get the parameters from a configmap we must label that configmap with this label</p> <pre><code>workflows.argoproj.io/configmap-type: Parameter\n</code></pre> <p>Then we can consume it using \"valueFrom\" \"configMapKeyRef\"</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: arguments-parameters-from-configmap-\nspec:\n  entrypoint: print-message-from-configmap\n  templates:\n  - name: print-message-from-configmap\n    inputs:\n      parameters:\n      - name: message\n        valueFrom:\n          configMapKeyRef:\n            name: simple-parameters\n            key: msg\n    container:\n      image: busybox\n      command: [\"echo\"]\n      args: [\"{{inputs.parameters.message}}\"]\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#parameters-in-a-secret","title":"Parameters in a secret","text":"<p>But if we want to store that parameter in a kubernetes secret we must use it:</p> <ul> <li>As an environment variable</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: print-secrets\n  templates:\n  - name: print-secrets\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: ['\n        echo \"secret from env: $MYSECRETPASSWORD\"\n      ']\n      env:\n      - name: MYSECRETPASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: my-secret\n            key: mypassword\n</code></pre> <ul> <li>As a volume</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: print-secrets\n  volumes:\n  - name: my-secret-vol\n    secret:\n      secretName: my-secret\n  templates:\n  - name: print-secrets\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: ['\n        echo \"secret from file: `cat /secret/mountpath/mypassword`\"\n      ']\n      volumeMounts:\n      - name: my-secret-vol\n        mountPath: \"/secret/mountpath\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#links","title":"Links","text":"<p>https://argo-workflows.readthedocs.io/en/stable/walk-through/secrets/</p>"},{"location":"CI-CD/argo-workflows/arguments/links/","title":"Links","text":"<ul> <li>Parameters  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/walk-through/parameters/</p> <ul> <li>Parameters spec  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/fields/#parameter</p> <ul> <li>Inputs  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/workflow-inputs/</p> <ul> <li>Intermediate parameters</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/intermediate-inputs/</p> <ul> <li>Parameters in workflow templates  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/workflow-templates/</p> <ul> <li>Workflow Variables</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/variables/</p>"},{"location":"CI-CD/argo-workflows/artifacts/key-only/","title":"Key only artifact","text":"<p>A key only artifact is an input or output artifact where we only specifiy the key, without the name of the bucket or credentials.</p> <p>With this, we can do a simpler workflow moving that settings to a an artifact repository ref</p> <ul> <li>Key-Only Artifacts</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/key-only-artifacts/</p> <ul> <li>Artifact Repository Ref</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/artifact-repository-ref/</p>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/","title":"Ways to call a (cluster)workflowtemplate","text":""},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#call-the-whole-template-with-workflowtemplateref","title":"Call the whole template with workflowTemplateRef","text":"<p>We can create a workflow specifying a (cluster)workflowtemplate to be launched using spec.workflowTemplateRef. We can also provider parameters where that they will be passed to the entrypoint.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: myworkflow-\nspec:\n  workflowTemplateRef:\n    name: myclusterworkflowtemplate\n    clusterScope: true # this calls a clusterworkflowtemplate. If false or ommited (default), it calls a workflowtemplate\n  arguments:\n    parameters:\n      - name: message\n        value: \"Hello, world!\"\n</code></pre> <p>No template inside the the (cluster)workflowtemplate is selected. The default one will be used and it will receive the parameters.</p> <p>In the UI</p> <p></p> <p>We cannot use spec.templates if we are using spec.workflowTemplateRef. This throws an error</p> <pre><code>Templates is invalid field in spec if workflow referred WorkflowTemplate reference\n</code></pre>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#from-a-task-or-step-with-templateref","title":"From a task or step with templateRef","text":"<p>We can invoke a (cluster)workflowtemplate from a task or step defined in a dag or steps template using templateRef. In this case we must choose what template will be chosen form the (cluster)workflowtemplate as entrypoint.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: myworkflow-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: main\n            templateRef:\n              name: myclusterworkflowtemplate # this calls a clusterworkflowtemplate. If false or ommited (default), it calls a workflowtemplate\n              template: whalesay # choose the desired template from the (cluster)workflowtemplate\n              clusterScope: true\n            arguments:\n              parameters:\n                - name: message\n                  value: \"Hello from task\"\n</code></pre> <p>In the UI</p> <p></p>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#precedence","title":"Precedence","text":"<p>A workflow can call a (cluster)workflowTemplate and this (cluster)workflowTemplate can call other (cluster)workflowTemplate(s).</p> <p>For example:</p> <pre><code>Workflow &gt; (cluster)workflowTemplate A &gt; (cluster)workflowTemplate B\n</code></pre> <p>Argo Workflows handles nested template calls by dynamically expanding and composing templates at runtime.</p> <ul> <li> <p>When submitting the workflow, Argo workflows resolves (cluster)workflowTemplate A and then (cluster)workflowTemplate B. This continues until all template references are resolved into concrete steps.</p> </li> <li> <p>Parameters, volumes, and other configurations cascade down through template calls. So parameters, volumes and other metadatas at higher levels take precedence with lower levels if defined in both. With ServiceAccount, the most specific serviceAccount definition wins</p> </li> <li> <p>The result is a single, expanded Workflow object with all steps defined concretely. Once expanded, the final Workflow doesn't change even if source templates are modified</p> </li> </ul> <pre><code># Workflow (highest precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"v1.0.0\"  # This wins\n    workflowTemplateRef:\n      name: deploy-template\n\n  # WorkflowTemplate (middle precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"latest\"   # Overridden by Workflow\n      - name: namespace\n        value: \"staging\"  # This wins (not defined in Workflow)\n    templates:\n    - name: main\n      templateRef:\n        name: build-template\n        template: build\n\n  # WorkflowTemplate 2 (lowest precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"dev\"      # Overridden\n      - name: namespace\n        value: \"default\"  # Overridden\n      - name: registry\n        value: \"harbor.local\" # This wins (not defined upstream)\n\n  Result: image-tag: \"v1.0.0\", namespace: \"staging\", registry: \"harbor.local\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/templates/0-templates/","title":"Templates","text":"<p>The templates are functions than can be called in argo workflows. There are several template types, and they are defined here:</p> <pre><code>spec:\n  templates:\n    - name: my-template\n      TYPEOFTEMPLATE:\n    - name: my-template2\n      TYPEOFTEMPLATE:\n    - name: my-template3\n      TYPEOFTEMPLATE:\n    - name: my-template4\n      TYPEOFTEMPLATE:\n    ...\n</code></pre> <p>There is a possible confussion with terms. A template is something like a function defined in a workflow, workflowtemplate or clusterworkflowtemplate. But a WorkflowTemplate or ClusterWorkflowTemplate is a kubernetes CRD acting like a base to create workflows. They include templates inside and other several fields.</p>"},{"location":"CI-CD/argo-workflows/templates/0-templates/#list-of-template-types","title":"List of template types","text":""},{"location":"CI-CD/argo-workflows/templates/0-templates/#template-callers","title":"Template callers","text":"<p>There are 2 special template types called template callers or template invocators. They invoke templates, workflowtemplates or clusterworkflowtemplates.</p> <ul> <li>Steps</li> </ul> <p>In the \"steps\" template caller you can define a list of tasks to be executed sequentially or in parallel. Also another options are available.</p> <ul> <li>Dag</li> </ul> <p>The \"dag\" template invocator executes other normal templates using dependencies between them</p>"},{"location":"CI-CD/argo-workflows/templates/0-templates/#other-templates","title":"Other templates","text":"<ul> <li>Container</li> </ul> <p>The most simple template type. It defines a container image with command and args like in a kubernetes pod.</p> <ul> <li>Script</li> </ul> <p>Same as container but it add a \"source\" field where you can define a script to be executed. The result is saved in an variable.</p> <ul> <li>Containerset</li> </ul> <p>It defines some containers to be executed in the same pod. An important thing is that they can share empty-dir volumes.</p> <ul> <li>Resource</li> </ul> <p>This template permits to do actions in kubernetes resources (get, create, apply, delete, replace, or patch resources on your cluster)</p> <ul> <li> <p>Http This template does a http call to an endpoint</p> </li> <li> <p>Data</p> </li> </ul> <p>This template permits to transform a source of data.</p> <ul> <li>Suspend</li> </ul> <p>Permits to suspend the execution of the workflow. It can be resumed manually or after a defined duration.</p>"},{"location":"CI-CD/argo-workflows/templates/http/","title":"Http","text":"<p>The http argo workflows template permit to do some http requests.</p>"},{"location":"CI-CD/argo-workflows/templates/http/#rbac-permissions","title":"Rbac Permissions","text":"<p>The http template uses the argo agent, not the workflow controller. When a workflow that uses the argo agent is created, a WorkflowTaskSet is created so we have to give additional permissions:</p> <ul> <li>to the service account specified in the workflow (default sa: default but change it is recommended)</li> <li>to the service account of the workflow controller (default sa: argo)</li> </ul> <p>The needed permissions are</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor-http\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets/status\n    verbs:\n      - patch\n</code></pre> <p>More info about this</p> <ul> <li>https://raw.githubusercontent.com/argoproj/argo-workflows/refs/heads/main/manifests/quick-start/base/agent-role.yaml</li> <li>https://github.com/argoproj/argo-workflows/issues/13770</li> <li>https://github.com/argoproj/argo-workflows/issues/10340</li> <li>https://argo-workflows.readthedocs.io/en/stable/upgrading/#06d4bf76f-fix-reduce-agent-permissions-fixes-7986-7987</li> </ul>"},{"location":"CI-CD/argo-workflows/templates/http/#secret","title":"Secret","text":"<p>Another requirement is to create a secret as described here</p> <p>https://github.com/argoproj/argo-workflows/issues/10340</p>"},{"location":"CI-CD/argo-workflows/templates/http/#executor-http-service-account-permissions","title":"executor-http service account permissions","text":"<p>If we create a service account called executor-http to execute the http templates, we can use this to give the minimum permissions. If you use another non http templates you will have to give that service account more permissions</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: executor-http\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor-http\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets/status\n    verbs:\n      - patch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: executor-http\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: executor-http\nsubjects:\n  - kind: ServiceAccount\n    name: executor-http\n    namespace: argo\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  annotations:\n    kubernetes.io/service-account.name: executor-http\n  name: executor-http.service-account-token\ntype: kubernetes.io/service-account-token\n</code></pre>"},{"location":"CI-CD/argo-workflows/templates/http/#options","title":"Options","text":"<p>Inside the http template we can use some configurations:</p> <ul> <li>url: to pass the url of the endpoint. With insecureSkipVerify we can ignore not known certificates</li> <li>body and bodyFrom: to pass the body</li> <li>headers: array with http headers we want to pass (name, value and valueFrom)</li> <li>method: the method of the request</li> <li>successCondition: what make the request successfull</li> <li>timeoutSeconds: timeout of the request (default 30s)</li> </ul>"},{"location":"CI-CD/argo-workflows/templates/http/#outputs","title":"Outputs","text":"<p>outputs.result in an HTTP template stores the response body HTTP templates capture the response body in the result parameter if the body is non-empty. For HTTP templates, result captures the response body. It is accessible from the outputs map: outputs.result.</p>"},{"location":"CI-CD/argo-workflows/templates/http/#variables","title":"Variables","text":"<p>Only available for successCondition</p> <pre><code>request.method: (string)\nrequest.url: (string)\nrequest.body: (string)\nrequest.headers: map\nresponse.statusCode: (int)\nresponse.body: (string)\nresponse.headers: map\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/latest/variables/#http-templates</p>"},{"location":"CI-CD/argo-workflows/templates/http/#bugs-missing-features","title":"Bugs | Missing features","text":"<ul> <li>Workflow-controller was unable to obtain node</li> </ul> <p>https://github.com/argoproj/argo-workflows/issues/13847</p> <ul> <li>Enable PodGC for the agent pod</li> </ul> <p>https://github.com/argoproj/argo-workflows/issues/12692</p>"},{"location":"CI-CD/argo-workflows/templates/http/#links","title":"Links","text":"<ul> <li>HTTP Template</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/http-template/</p> <ul> <li>HTTP spec</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/fields/#http</p>"},{"location":"CI-CD/gitlab-ci-cd/98-tips/","title":"Tips","text":""},{"location":"CI-CD/gitlab-ci-cd/98-tips/#a-cicd-variable-contains-a","title":"A CI/CD variable contains a $","text":"<p>If a gitlab cicd variable contains a $, we can escape it addin another $ in the value</p> <pre><code>asljfrower$34onamdgflg &gt; asljfrower$$34onamdgflg\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/98-tips/#credentials-to-pull-image","title":"Credentials to pull image","text":"<ul> <li> <p>create a credential with pull permissions in your registry</p> </li> <li> <p>Add a masked variable to the CI/CD with this format</p> </li> </ul> <pre><code>name: DOCKER_AUTH_CONFIG\nvalue: {\"auths\":{\"FQDN\":{\"username\":\"your-username\",\"password\":\"your-password\"}}}\n</code></pre> <p>And thats it. GitLab will automatically use these credentials to authenticate with your registry. The authentication happens at the GitLab Runner level, not within your job container.</p> <ul> <li>Run your CI/CD jobs in Docker containers</li> </ul> <p>https://docs.gitlab.com/ci/docker/using_docker_images/</p>"},{"location":"CI-CD/gitlab-ci-cd/authentication/","title":"Authentication","text":""},{"location":"CI-CD/gitlab-ci-cd/authentication/#roles","title":"Roles","text":"<p>A Gitlab role is a set of permissions a user have under a group or project. Some examples of predefined roles are:</p> <ul> <li>guest</li> <li>reporter</li> <li>developer</li> <li>mantainer</li> <li>owner</li> </ul>"},{"location":"CI-CD/gitlab-ci-cd/authentication/#scopes-of-access-tokens","title":"Scopes of access tokens","text":"<p>The scopes are the specific permissions a token has. This permissions can be</p> <ul> <li>Api permissions</li> <li>Repository permission</li> <li>Registry permissions</li> <li>Read user information</li> </ul>"},{"location":"CI-CD/gitlab-ci-cd/authentication/#scoped-tokens","title":"Scoped tokens","text":"<p>The scoped tokens are credentials that have roles assigned.</p> <p>Also they can have the following scopes:</p> <ul> <li>The GitLab API</li> <li>GitLab repositories</li> <li>The GitLab registry</li> </ul> <p>The scoped tokens can be:</p> Token Type Tied to UI Location Tied to Personal Access Tokens (PAT) Individual user Profile &gt; Access Tokens An specific user account Group Access Tokens Specific group Group Settings &gt; Access Tokens A group Project Access Tokens Specific project Project Settings &gt; Access Tokens A single project"},{"location":"CI-CD/gitlab-ci-cd/authentication/#deploy-tokens","title":"Deploy Tokens","text":"<p>The deploy tokens are more oriented to do certain operations like interacting with:</p> <ul> <li>GitLab repositories</li> <li>The GitLab registry</li> </ul> <p>Some features:</p> <ul> <li>They do not have API permissions</li> <li>They do not have roles assigned</li> <li>They can be defined at project or group level</li> </ul> Token Type Tied to UI Location Tied to Project deploy token Specific project Project Settings &gt; Repository &gt; Deploy Tokens An specific user account Group deploy token Specific group Group Settings &gt; Repository &gt; Deploy Tokens A group <p>Tip: If you create a token called \"gitlab-deploy-token\", the deploy token is automatically exposed to project CI/CD jobs as variables, where CI_DEPLOY_USER is the username and CI_DEPLOY_PASSWORD the token</p>"},{"location":"CI-CD/gitlab-ci-cd/authentication/#gitlab-cicd-job-token","title":"GitLab CI/CD job token","text":"<p>This is another special token auto generated a job is about to run and stores in the following variable: CI_JOB_TOKEN.</p> <p>Permissions:</p> <ul> <li>The token receives the same access level as the user that triggered the pipeline</li> <li>But with less permissions than a Personal Access Token</li> </ul>"},{"location":"CI-CD/gitlab-ci-cd/authentication/#links","title":"Links","text":"<ul> <li>Roles and permissions</li> </ul> <p>https://docs.gitlab.com/user/permissions/</p> <ul> <li>Personal access tokens</li> </ul> <p>https://docs.gitlab.com/user/profile/personal_access_tokens/</p> <ul> <li>Group access tokens</li> </ul> <p>https://docs.gitlab.com/user/group/settings/group_access_tokens/</p> <ul> <li>Project access tokens</li> </ul> <p>https://docs.gitlab.com/user/project/settings/project_access_tokens/</p> <ul> <li>Deploy tokens</li> </ul> <p>https://docs.gitlab.com/user/project/deploy_tokens/</p> <ul> <li>GitLab CI/CD job token</li> </ul> <p>https://docs.gitlab.com/ci/jobs/ci_job_token/</p>"},{"location":"CI-CD/gitlab-ci-cd/dynamic-child-pipelines/","title":"Dynamic child pipelines","text":"<p>Dynamic child pipelines are created dynamically from a previous job.</p> <p>We have 3 steps:</p>"},{"location":"CI-CD/gitlab-ci-cd/dynamic-child-pipelines/#pipeline-generation","title":"Pipeline generation","text":"<p>A job creates one or more pipelines in the git repository and save them in an artifact</p> <pre><code>pipeline-generation:\n  stage: build\n  script: create-pipelines.sh &gt; pipelines.yml\n  artifacts:\n    paths:\n      - pipelines.yml\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/dynamic-child-pipelines/#dynamic-child-pipelines_1","title":"Dynamic Child pipelines","text":"<p>These are the dynamic pipelines generated in the parent job</p> <p>Here $CI_PIPELINE_SOURCE always has the value of \"parent_pipeline\" so we must limit the execution of this job with it</p> <pre><code>job1:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"parent_pipeline\"\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/dynamic-child-pipelines/#child-downstream-job","title":"Child (downstream) job","text":"<p>This is the job that with a trigger that receives the Dynamic Child pipelines as artifacts from the trigger job.</p> <pre><code>child:\n  trigger:\n    include:\n      - artifact: pipelines.yml\n        job: pipeline-generation\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/dynamic-child-pipelines/#links","title":"Links","text":"<ul> <li>Dynamic child pipelines</li> </ul> <p>https://docs.gitlab.com/ci/pipelines/downstream_pipelines/#dynamic-child-pipelines</p> <ul> <li>Create child pipelines using dynamically generated configurations</li> </ul> <p>https://www.youtube.com/watch?v=nMdfus2JWHM</p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/","title":"Protect uploading secrets to main","text":""},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#protect-the-main-branch","title":"Protect the main branch","text":"<p>Go to Settings - Repository - Protected branches and configure that nobody can push to the main branch. In \"Allowed to merge\" leave the roles you permit to merge.</p> <p></p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#force-pass-the-pipeline","title":"Force pass the pipeline","text":"<p>Go to Settings - Merge requests - Merge requests and enable \"Pipelines must succeed\"</p> <p></p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#configure-the-pipeline-to-scan-the-pushes","title":"Configure the pipeline to scan the pushes","text":"<p>Configure the pipeline to scan the merge requests in the .gitlab-ci.yml file</p> <p>Example</p> <pre><code>stages:\n  - security\ntrufflehog-git:\n  stage: security\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n  image:\n    name: docker.io/trufflesecurity/trufflehog:3.82.1\n    entrypoint: [\"/bin/sh\", \"-c\"]\n  script:\n    - trufflehog --fail git \"$CI_REPOSITORY_URL\"\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#test","title":"Test","text":"<p>In order to test it, push a new branch with changes and do a merge request to main</p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#links","title":"Links","text":"<ul> <li>Merge request pipelines</li> </ul> <p>https://docs.gitlab.com/ee/ci/pipelines/merge_request_pipelines.html</p> <ul> <li>CI_PIPELINE_SOURCE predefined variable</li> </ul> <p>https://docs.gitlab.com/ee/ci/jobs/job_rules.html#ci_pipeline_source-predefined-variable</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/","title":"Stages and needs","text":"<p>The stages permit to group gitlab CI/CD jobs</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#defining-the-stages","title":"Defining the stages","text":"<p>The default stages are</p> <ul> <li>.pre</li> <li>build</li> <li>test</li> <li>deploy</li> <li>.post</li> </ul> <p>They can be redefined in the .gitlab-ci.yml file</p> <pre><code>stages:\n  - mystage1\n  - mystage2\n  - mystage3\n</code></pre> <p>If a stage is defined but no jobs use it, the stage is not visible in the pipeline</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#defining-the-stage-of-a-job","title":"Defining the stage of a job","text":"<p>We define inside a job the stage belongs to.</p> <pre><code>myjob:\n  stage: mystage1\n</code></pre> <p>If no stage is defined, the job uses the test stage by default</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#how-the-stages-run","title":"How the stages run","text":"<ul> <li>If a pipeline contains only jobs in the .pre or .post stages, it does not run. There must be at least one other job in a different stage.</li> <li>All the jobs in the same stage runs in parallel</li> <li>When all jobs in an stage succeed, the next stage jobs start.</li> <li>When all stages succeed, the pipeline is marked as passed</li> <li>If any job fails, the pipeline is marked as failed and jobs in later stages do not start. Jobs in the current stage are not stopped and continue to run.</li> </ul>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#needs","title":"Needs","text":"<p>Pending</p> <p>https://docs.gitlab.com/ci/yaml/#needs</p>"},{"location":"CI-CD/reloader/00-intro/","title":"Intro","text":"<p>Reloader permits to restart some kubernetes resources when some defined configmaps or secrets has changed.</p> <p>It Works with these kubernetes resources:</p> <ul> <li>Deployments</li> <li>Daemonsets</li> <li>Statefulsets</li> <li>DeploymentConfigs (from openshift, needs to be enabled via isOpenshift: true)</li> <li>Rollouts (from Argo rollouts, needs to be enabled via isArgoRollouts: true)</li> </ul>"},{"location":"CI-CD/reloader/00-intro/#how-it-works","title":"How it works","text":"<p>Reloader tracks the kubernetes resources configured. When a secret or configmap is updated, reloader triggers a restart of the resource.</p> <p>For this, it watches the data section of the secret</p> <p>There are 2 reload strategies here. The default one (env-vars) creates an environment variable in the restarted pods. The \"annotations\" mode add an annotation \"reloader.stakater.com/last-reloaded-from\" in the pods (via template spec)</p>"},{"location":"CI-CD/reloader/00-intro/#more-info","title":"More info","text":"<ul> <li> <p>Github https://github.com/stakater/Reloader</p> </li> <li> <p>Github docs https://github.com/stakater/Reloader/tree/master/docs</p> </li> </ul>"},{"location":"CI-CD/reloader/98-tips/","title":"Tips","text":""},{"location":"CI-CD/reloader/98-tips/#working-with-argo-rollouts","title":"Working with Argo Rollouts","text":"<p>We must enable this feature. In the helm chart:</p> <pre><code>reloader:\n  isArgoRollouts: true\n</code></pre> <p>Then we must annotate the Argo Rollout resource. See annotations</p> <p>Using workloadRef crashes the operator. See this bug: https://github.com/stakater/Reloader/issues/751</p>"},{"location":"CI-CD/reloader/annotations/","title":"Annotations","text":"<p>The method to configure how we control how reloader restarts the workloads is via resource annotations.</p> <p>There are 3 ways to the reload</p>"},{"location":"CI-CD/reloader/annotations/#automatic-reload","title":"Automatic reload","text":"<p>This is the default behaviour. We have this 3 options: if any secret inside the workload or configmap changes, or if only a configmap or secret changes.</p> <p>It is possible to use a custom annotations with a reloader controller parameter</p> Annotation Behaviour Custom annotation parameter reloader.stakater.com/auto: \"true\" reload if a secret or configmap changes --auto-annotation reloader.stakater.com/auto: \"false\" disables the reload in the workload --auto-annotation configmap.reloader.stakater.com/auto: \"true\" reload if a configmap changes --configmap-auto-annotation secret.reloader.stakater.com/auto: \"true\" reload if a secret changes --secret-auto-annotation <ul> <li>reloader.stakater.com/auto and reloader.stakater.com/search cannot be used together. the auto annotation takes precedence.</li> <li>If both configmap.reloader.stakater.com/auto and secret.reloader.stakater.com/auto are used, only one needs to be true to trigger a reload.</li> </ul> <p>Enabling --auto-reload-all in the controller makes all workloads treated as reloader.stakater.com/auto: \"true\" unless they have reloader.stakater.com/auto: \"false\"</p>"},{"location":"CI-CD/reloader/annotations/#giving-the-name-of-the-resource","title":"Giving the name of the resource","text":"<p>We can be more specific giving the name(s) of the secret(s) or configmap(s) that must trigger the reload. Multiple configmaps or secrets can be specified, comma separated</p> <p>It is possible to use a custom annotations with a reloader controller parameter</p> Annotation Behaviour Custom annotation parameter configmap.reloader.stakater.com/reload: \"NAME_OF_THE_CONFIGMAP\" reload if specified configmap changes --configmap-annotation secret.reloader.stakater.com/reload: \"NAME_OF_THE_SECRET\" reload if specified secret changes --secret-annotation"},{"location":"CI-CD/reloader/annotations/#search-and-match-restart","title":"Search and match restart","text":"<p>Another way to control the reload is using a two way annotation</p> <p>If we annotate the workload with this</p> <pre><code>reloader.stakater.com/search: \"true\"\n</code></pre> <p>... reload will trigger a reload if the configmap or secrets that the workload includes have the following annotation</p> <pre><code>reloader.stakater.com/match: \"true\"\n</code></pre> <p>It is possible the override this annotation with the --auto-search-annotation flag</p>"},{"location":"CI-CD/reloader/annotations/#other-annotations","title":"Other annotations","text":"Annotation Where Behaviour reloader.stakater.com/ignore: \"true\" CM/Secret The resource will not trigger reloads reloader.stakater.com/rollout-strategy: \"rollout\" Workload A rollout is triggered patching the template reloader.stakater.com/rollout-strategy: \"restart\" Workload The pods are deleted without patching the template deployment.reloader.stakater.com/pause-period: \"5m\" Workload Pause rollouts for a deployment for a specified duration"},{"location":"CI-CD/renovate/1-phases/","title":"Renovate phases","text":""},{"location":"CI-CD/renovate/1-phases/#configurations","title":"Configurations","text":"<p>The first step is to merge the configurations. From more important to less:</p> <pre><code>cli &gt; env &gt; file &gt; default\n</code></pre>"},{"location":"CI-CD/renovate/1-phases/#cloning","title":"Cloning","text":"<p>Then the platform module interacts with the source control platform and clones the list of configured repositories.</p> <p>There are some supported platforms we can configure, such as azure, bitbucket, github, gitlab, gitea,...</p> <p>More information about platforms here:</p> <p>https://docs.renovatebot.com/modules/platform/</p>"},{"location":"CI-CD/renovate/1-phases/#vulnerabilities","title":"Vulnerabilities","text":"<p>Next there is check for vulnerabilities.</p> <p>Is it possible to only update dependencies if vulnerabilities have been detected (security:only-security-updates)</p>"},{"location":"CI-CD/renovate/1-phases/#extract-dependencies-with-package-managers","title":"Extract dependencies with package managers","text":"<p>Then the manager module looks for files based on their name and extracts the dependencies. It assigns a datasource to each extracted package file or dependency. The datasource tells Renovate how to search for new versions.</p> <p>Example: The gitlabci manager finds a dependency: python:3.10-alpine which has the docker datasource</p> <p>Some package managers are ansible, helm-values, argocd,... and it is possible to reconfigure the file match.</p> <p>More info about the package managers module here:</p> <p>https://docs.renovatebot.com/modules/manager/</p>"},{"location":"CI-CD/renovate/1-phases/#look-up-updates","title":"Look up updates","text":"<p>Then the datasource module looks for available versions of the dependency looking up registries.</p> <p>Example: The docker datasource looks for versions and finds: [python:3.9,python:3.9-alpine,python:3.10,python:3.10-alpine,python:3.11,python:3.11-alpine]</p> <p>Some datasources are docker, github-releases, helm, ruby-gems</p> <p>More info about the datasources module here:</p> <p>https://docs.renovatebot.com/modules/datasource/</p>"},{"location":"CI-CD/renovate/1-phases/#versioning","title":"Versioning","text":"<p>Once we have located available versions, the versioning module will use a scheme to perform sorting and filtering of results.</p> <p>Example: The docker versioning returns python:3.11-alpine, because that version is compatible with python:3.10-alpine</p> <p>It is usually recommended to configure the versioning because the default way can fail in some scenarios.</p> <p>More info about versioning module here:</p> <p>https://docs.renovatebot.com/modules/versioning/</p>"},{"location":"CI-CD/renovate/1-phases/#write-updates","title":"Write updates","text":"<p>Once the updates has been chosen, the changes are pushed to the repository depending how it is configured</p>"},{"location":"CI-CD/renovate/1-phases/#links","title":"Links","text":"<ul> <li>How renovate works https://docs.renovatebot.com/key-concepts/how-renovate-works/</li> </ul>"},{"location":"CI-CD/renovate/self-hosted-configurations/host-rules-from-environment/","title":"Host rules from environment variables","text":"<p>There is a setting that permits to detect host rules from environemnt variables:</p> <pre><code>via cli: --detect-host-rules-from-env\nvia env: RENOVATE_DETECT_HOST_RULES_FROM_ENV\n</code></pre> <p>This setting by default is disabled but, enabling it, permits to configure host rules with variables.</p> <p>How that rule is detected? Renovate search for this syntax:</p> <pre><code>RENOVATE_DATASOURCENAME_DOMAIN/SUBDOMAIN_FIELD\n</code></pre> <p>for example</p> <pre><code>RENOVATE_DOCKER_DOCKER_IO_USERNAME\nRENOVATE_DOCKER_DOCKER_IO_PASSWORD\n</code></pre> <p>Notes:</p> <ul> <li>The RENOVATE_ is optional, but the documentation says it will be required in the future</li> <li>Only domains/subdomains are supported. Nothing like protocols (https://,...).</li> <li>The field name can be: TOKEN, USERNAME, PASSWORD, HTTPSPRIVATEKEY, HTTPSCERTIFICATE, HTTPSCERTIFICATEAUTHORITY</li> <li>Hyphens (-) in datasource or host name must be replaced with double underscores (__).</li> <li>Periods (.) in host names must be replaced with a single underscore (_).</li> </ul>"},{"location":"database/cloudnative-pg/98-tips/","title":"Tips","text":""},{"location":"database/cloudnative-pg/98-tips/#minimal-cluster","title":"Minimal cluster","text":"<p>This is the minimal cluster spec</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: sinbootstrap\nspec:\n  storage:\n    storageClass: standard\n    size: 1Gi\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#disable-non-ssl-connections","title":"Disable non ssl connections","text":"<p>If we want to deny all non ssl connections to the cluster, we can add this sections to the cluster</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgre\nspec:\n  postgresql:\n    pg_hba:\n      - hostssl all all all scram-sha-256\n      - hostnossl all all all reject\n</code></pre> <p>This setting add 2 rules between some fixed rules (system rules) and the default rule that permits both ssl and non ssl connections. The fist rule permit ssl connections via password and the second one denies non ssl connections.</p> <p>The postgresql documentation says \"The first record with a matching connection type, client address, requested database, and user name is used to perform authentication.\"</p> <p>Some tips:</p> <ul> <li>We can do better rules specifying users, databases and hosts</li> <li>The controller applies these rules without restarting the pods</li> <li>We can get the current rules with this</li> </ul> <pre><code>select pg_reload_conf();\ntable pg_hba_file_rules;\n</code></pre> <p>https://www.postgresql.org/docs/current/auth-pg-hba-conf.html</p>"},{"location":"database/cloudnative-pg/98-tips/#recreate-all-the-cluster-nodes","title":"Recreate all the cluster nodes","text":"<ul> <li>Destroy 2 replicas</li> </ul> <pre><code>kubectl cnpg destroy MYCLUSTER ONE-REPLICA\nkubectl cnpg destroy MYCLUSTER ANOTHER-REPLICA\n</code></pre> <p>Once they are ok, promote a replica to be primary</p> <pre><code>kubectl cnpg promote MYCLUSTER ONE-REPLICA\n</code></pre> <p>Once is prometed, destroy the older primary</p> <pre><code>kubectl cnpg destroy MYCLUSTER OLD-PRIMARY\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#info-about-primary-replicas","title":"Info about primary replicas","text":"<p>Show the nodes where the replicas are located</p> <pre><code>kubectl get pod -A -l cnpg.io/instanceRole=primary -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,NODE:.spec.nodeName\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#get-an-sql-session","title":"get an sql session","text":"<pre><code>kubectl cnpg psql mycluster\n</code></pre> <pre><code>SELECT timeline_id FROM pg_control_checkpoint();\n</code></pre>"},{"location":"database/cloudnative-pg/99-links/","title":"Links","text":"<ul> <li>CloudNativePG official site</li> </ul> <p>https://cloudnative-pg.io/</p> <ul> <li>CloudNativePG github</li> </ul> <p>https://github.com/cloudnative-pg/cloudnative-pg</p> <ul> <li>Gabriele Bartolini blog</li> </ul> <p>https://www.gabrielebartolini.it/</p> <ul> <li>Barman Cloud CNPG-I plugin</li> </ul> <p>https://cloudnative-pg.io/plugin-barman-cloud/</p> <ul> <li>Recommended architectures for PostgreSQL in Kubernetes</li> </ul> <p>https://www.cncf.io/blog/2023/09/29/recommended-architectures-for-postgresql-in-kubernetes/</p>"},{"location":"database/cloudnative-pg/errors/","title":"Errors","text":""},{"location":"database/cloudnative-pg/errors/#timeline-servers-history-checkpoint-error","title":"Timeline | server's history | checkpoint error","text":"<p>Sympthoms:</p> <p>The operator is trying to create a replica but it can't and the new instance fails with this error</p> <pre><code>...\nrequested timeline XXX is not a child of this server's history\nLatest checkpoint is at YYY on timeline XXX, but in the history of the requested timeline, the server forked off from that timeline at YYY\n</code></pre> <p>Destroying the failing replicas and forcing to create a new one does not solve the problem.</p> <p>Cause:</p> <p>There is difference between the primary instance and the backups</p> <p>Workaround:</p> <ul> <li>Do a manual backup not with the operator</li> <li>Change the cluster to 1 instance. Probably needs a manual destroy of the failing instances</li> <li>Change where the backups will be stored. We must change the backup section selecting another bucket o changing the serverName. The goal is to start an empty backup folder.</li> <li>Do a manual backup using the operator and check it is working.</li> <li>Also check the wal files are being written in the barmanObjectStore</li> <li>Change the instances to the desired number</li> </ul> <p>I have tried this using barmanObjectStore based backup.</p>"},{"location":"database/cloudnative-pg/errors/#wal-file-not-found-in-the-recovery-object-store","title":"WAL file not found in the recovery object store\"","text":"<ul> <li>See the logs in every cnpg instance</li> <li>Check the name of the failing .history file and see the .history files in the /var/lib/postgresql/wal/pg_wal/ directory of the instances</li> <li>Ensure you have the proper IRSA permissions</li> <li>Try to recreate all the instances with destroy | promote actions</li> </ul>"},{"location":"database/cloudnative-pg/errors/#the-token-included-in-the-request-has-no-service-account-role-association-for-it","title":"The token included in the request has no service account role association for it","text":"<pre><code>ERROR: Barman cloud backup delete exception: Error when retrieving credentials from container-role: Error retrieving metadata: Received non 200 response 404 from container metadata:\n...\n(ResourceNotFoundException): The token included in the request has no service account role association for it., fault: client\\n\\n\",\"error\":\"exit status 4\"\n\n</code></pre> <p>This can be caused because there were some changes in the IRSA authentication (iam role, annotation,..) To solve it, restart the cluster</p>"},{"location":"database/cloudnative-pg/errors/#error-calling-the-headbucket-operation","title":"Error calling the HeadBucket operation","text":"<pre><code>ERROR: Barman cloud WAL archiver exception: An error occurred (403) when calling the HeadBucket operation: Forbidden\"\n</code></pre> <p>This is an AWS IAM permissions issue. Probably you need to add \"s3:ListBucket\" Action permissions to the bucket itself.</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": \"arn:aws:s3:::BUCKETNAME\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"database/cloudnative-pg/errors/#http-communication-issue-error","title":"\"HTTP communication issue\" error","text":"<p>Restart the controller</p>"},{"location":"database/cloudnative-pg/errors/#a-replica-cannot-be-created","title":"A replica cannot be created","text":"<p>If we get errors like</p> <pre><code>\"requested timeline XXX is not a child of this server's history\"\n\"Latest checkpoint is at XXX on timeline XXX, but in the history of the requested timeline, the server forked off from that timeline at YYY.\"\n</code></pre> <p>and only the primary is up. We can:</p> <ul> <li>Do a manual backup via pgdump of every database</li> <li>Leave the cluster with only 1 replica and no backup section</li> <li>Rename the s3 folder or use a different serverName in the backup section.</li> <li>Enable the backup section and do a backup via the kubectl cnpg plugin</li> <li>If it works, increase the replicas to 3</li> </ul>"},{"location":"database/cloudnative-pg/errors/#using-the-csi-driver-nfs","title":"Using the csi driver NFS","text":"<ul> <li>You can probably need to give more permissions</li> <li>spec.postgresUID and spec.postgresGID (default 26) in the cluster resource definition gives you more possibilities</li> <li>Don't use the subDir parameter</li> </ul> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-postgre\nparameters:\n  mountPermissions: \"0777\"\n...\n</code></pre> <p>This avoids some permission errors and other like:</p> <pre><code>controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\n</code></pre> <pre><code>stale NFS file handle\n</code></pre> <pre><code>This is an old primary instance in a new cluster without backup\n</code></pre>"},{"location":"database/cloudnative-pg/pdb/","title":"CloudnativePG, PDB and draining nodes","text":"<p>When a node is cordoned, Kubernetes creates a taint on the node and marks it as unschedulable. This prevents new pods from being scheduled on the node.</p> <p>We can see the taint in a cordoned node</p> <pre><code>spec:\n  taints:\n  - effect: NoSchedule\n    key: node.kubernetes.io/unschedulable\n  unschedulable: true\n</code></pre> <p>By default, Cloudnative PG creates a pod disruption budget with no allowed disruptions to protect the primary instance. If the node where the primary instance is cordoned, the operator will try to find a replica in another node and then promote it to primary. Once the promotion has been done, the former primary can be evicted with a node drain.</p> <p>It is easy to check this behaviour. Simply cordon the node where the replica is, and see the logs in the controller. We can get this kind of messages.</p> <pre><code>Primary is running on an unschedulable node, will try switching over\n</code></pre> <p>If all the replicas are in not ready nodes, the pdb will continue blocking the drain operations.</p> <pre><code>Current primary is running on unschedulable node, but there are no valid candidates\n</code></pre> <p>In that situation, we must move a replica to another node an let the controller do a promotion, for example, with a restart with cnpg cli. A good practice can be to setup the anti-affinity in the cluster resource to ensure every replica in deployed in different nodes.</p>"},{"location":"database/cloudnative-pg/pdb/#disabling","title":"Disabling","text":"<p>That pdb creation can be disabled via spec.enablePDB in the cluster resource. This feature is available since the v1.23.0 release.</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: mycluster\nspec:\n  enablePDB: false\n</code></pre>"},{"location":"database/cloudnative-pg/pdb/#karpenter","title":"Karpenter","text":"<p>There is a problem with karpenter and this behaviour. Karpenter knows there is pdb in the node with no allowed disruption and the node will not be disrupted.</p> <p>You can get events like this</p> <pre><code>DisruptionBlocked\nCannot disrupt Node: pdb \"mynamespace/mycluster-primary\" prevents pod evictions\n</code></pre> <p>In order to permit a node to be disrupted by karpenter you must move the primary instaces to another nodes.</p> <p>In production environents:</p> <ul> <li>Use pod antiaffinity to have the cnpg instances in different nodes</li> <li>Rotate all the primary instances to a single node to permit a more optimized karpenter consolidation</li> <li>After the consolidation, rotate some of that primary instances to another nodes.</li> </ul> <p>See what nodes cannot be disrupted</p> <pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node | grep pdb\n</code></pre> <p>See the cluster information (needs the kubectl cnpg plugin installed)</p> <pre><code>kubectl get clusters --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t\"}{.status.currentPrimary}{\"\\n\"}{end}' | while read namespace cluster primary; do\n    node=$(kubectl get pod $primary -n $namespace -o jsonpath='{.spec.nodeName}')\n    echo \"############### CLUSTER $cluster ###############\"\n    kubectl cnpg status $cluster -n $namespace | grep -A 6 \"Instances status\"\n    nodepool=$(kubectl get node $node -o jsonpath='{.metadata.labels.karpenter\\.sh/nodepool}')\n    echo \"&gt;&gt;&gt;&gt; The cluster is in the nodepool $nodepool. Showing nodes:\"\n    kubectl get nodes -l karpenter\\.sh/nodepool=$nodepool\n    echo \"&gt;&gt;&gt;&gt; Promotion command: &lt;kubectl cnpg -n $namespace promote $cluster REPLICAID&gt;\"\n    echo \"##############################\"\ndone\n</code></pre>"},{"location":"database/cloudnative-pg/postgre-images/","title":"PostgreSQL operand Images","text":"<p>There are 2 ways to define the postgresql (operand) image we want to use in a CloudNative-PG cluster:</p> <ul> <li>using spec.imageName</li> </ul> <p>Here we configure the url of the docker image</p> <ul> <li>using spec.imageCatalogRef</li> </ul> <p>Here we can select a version from an existing ImageCatalog or ClusterImageCatalog</p>"},{"location":"database/cloudnative-pg/postgre-images/#oficial-postgresql-images","title":"Oficial postgresql images","text":"<p>CloudNative-PG builds and provides some postgresql images ready to be used</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgresql-images","title":"Postgresql images","text":"<ul> <li>Github repo</li> </ul> <p>https://github.com/cloudnative-pg/postgres-containers</p> <ul> <li>Registry</li> </ul> <p>https://github.com/cloudnative-pg/postgres-containers/pkgs/container/postgresql</p> <p>Here we can find 3 image types:</p> <ul> <li>Minimal</li> </ul> <p>They include APT PostgreSQL packages from PostgreSQL Global Development Group (PGDG).</p> <ul> <li>standard</li> </ul> <p>They include pgaudit, Postgres Failover Slots, pgvector, all locales and LLVM JIT support until 18 release</p> <ul> <li>system (Deprecated)</li> </ul> <p>They are based on standard images. Include Barman Cloud binaries for backup operations and they will be removed when in-core Barman Cloud support is phased out</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgresql-clusterimagecatalogs","title":"Postgresql ClusterImageCatalogs","text":"<p>All the current provided ClusterImageCatalogs located are here:</p> <p>https://github.com/cloudnative-pg/artifacts</p> <p>They include the standard, minimal and system images</p> <p>Also there are some legacy catalogs here:</p> <p>https://github.com/cloudnative-pg/postgres-containers/tree/main/Debian</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgis-images","title":"Postgis images","text":"<p>CloudNative-PG also builds and provides some postgis images. But the plan is to stop offering postgis image once PostgreSQL 17 reaches end of life (November 2029).</p> <p>\"Starting with PostgreSQL 18, the extension_control_path GUC will allow PostGIS to be mounted as a separate image volume, removing the need for dedicated PostGIS container images.\"</p> <ul> <li>Github repo</li> </ul> <p>https://github.com/cloudnative-pg/postgis-containers</p> <ul> <li>Registry</li> </ul> <p>https://github.com/cloudnative-pg/postgis-containers/pkgs/container/postgis</p> <p>Here we can find 2 image types:</p> <ul> <li>standard</li> </ul> <p>Without Barman Cloud</p> <ul> <li>system (Deprecated)</li> </ul> <p>with Barman Cloud</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgis-clusterimagecatalogs","title":"Postgis ClusterImageCatalogs","text":"<p>All the current provided ClusterImageCatalogs located are here:</p> <p>They include the standard and system images</p> <p>https://github.com/cloudnative-pg/postgis-containers/tree/main/image-catalogs</p>"},{"location":"database/cloudnative-pg/postgre-images/#important-notes","title":"Important notes","text":""},{"location":"database/cloudnative-pg/postgre-images/#migration-path","title":"Migration Path","text":"<ul> <li>Move to the the backup barman plugin</li> <li>Avoid using system images. Move to standard or minimal.</li> <li>Avoid using system or legacy ClusterImageCatalogs. Move to standard or minimal.</li> </ul>"},{"location":"database/cloudnative-pg/postgre-images/#default-release","title":"Default release","text":"<p>The default release of postgresql offered bye the the operator is the \"latest available minor version of the latest stable major version supported by the PostgreSQL Community\". The best practice in production is to use an specific image, better with the SHA256 digest</p>"},{"location":"database/cloudnative-pg/postgre-images/#custom-image","title":"Custom image","text":"<p>It is possible to build your own custom images</p>"},{"location":"database/cloudnative-pg/postgre-images/#operator-image","title":"Operator image","text":"<p>We can also override the container image of the operator (cloudnative-pg) changing the image of the cnpg operator deployment</p> <p>The releases can be found here:</p> <p>https://github.com/cloudnative-pg/cloudnative-pg/pkgs/container/cloudnative-pg</p> <p>the value of the OPERATOR_IMAGE_NAME will be applied in the sidecar of every instance of the cluster</p>"},{"location":"database/cloudnative-pg/production/","title":"CNPG in production","text":""},{"location":"database/cloudnative-pg/production/#deployment-and-observability","title":"Deployment and observability","text":"<ul> <li>Use gitops tools (argocd, flux,...) to control the deployment</li> <li>Use gitops tools like external-secrets operator to control the credentials</li> <li>You can enable the spec.monitoring.enablePodMonitor setting and setup a monitoring and alerting system</li> </ul>"},{"location":"database/cloudnative-pg/production/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<ul> <li>Use odd replicas (3, 5, ...)</li> <li>Leave spec.enablePDB enabled (default)</li> <li>Start using the official clusterimagecatalogs</li> <li>Configure the primaryUpdateStrategy</li> <li>Configure the affinity section to distribute the instances in nodes</li> <li>Consider to use dedicated and/or performance nodes in the the postgresql instances</li> <li>Give the postgresql pods a higher priority class</li> </ul>"},{"location":"database/cloudnative-pg/production/#configuration","title":"Configuration","text":"<ul> <li>Define the resources (requests and limits in the cluster)</li> <li>Try not to enable superuser access (spec.enableSuperuserAccess). You can create additional roles with the needed permissions.</li> <li>Always configure backup using the backup plugin</li> <li>Review backups status</li> </ul>"},{"location":"database/cloudnative-pg/production/#karpenter-and-cluster-autoescaler","title":"Karpenter and cluster autoescaler","text":"<p>Until 1.26 release, cloudnative-pg only detects a node is being drained if detects via the node.kubernetes.io/unschedulable taint</p> <p>Since 1.26 release, cloudnative-pg detects a node is being drained with these taints:</p> <ul> <li>node.kubernetes.io/unschedulable</li> <li>ToBeDeletedByClusterAutoscaler</li> <li>karpenter.sh/disrupted</li> <li>karpenter.sh/disruption</li> </ul> <p>When karpenter and cluster autoscaler taints the node, the controller knows the node will be delete and it can initiate a failover</p>"},{"location":"database/cloudnative-pg/rolling-update/","title":"Rolling update a cluster","text":""},{"location":"database/cloudnative-pg/rolling-update/#reasons","title":"Reasons","text":"<p>There are several reasons we can do some changes in a cloudnative pg cluster that requires a rolling update in the cluster, this is, recreate the postgresql pods with the new settings:</p> <ul> <li>Updates in the operator (*)</li> <li>Changes in the spec.image field or in the image catalog</li> <li>Changes in spec.resources</li> <li>Changes in the postgresql configuration</li> <li>Changes in the size of the persistent volume</li> </ul> <p>(*) There is a way to not trigger a rolling update when the operator is updated called \"In-place updates of the instance manager\". But it is not a clean way to do it.</p> <p>When a rolling update is triggered, first the operator upgrades the replicas, but we can configure how the primary instance will be updated.</p>"},{"location":"database/cloudnative-pg/rolling-update/#primaryupdatestrategy","title":"primaryUpdateStrategy","text":"<p>spec.primaryUpdateStrategy defines if we want to control the update the of primary instance.</p> <ul> <li> <p>unsupervised (default) The update is automatic based in the spec.primaryUpdateMethod field (see below)</p> </li> <li> <p>supervised This is the manual update to the primary and suspends the update of the primary. In order to continue we can manually do the switchover or the restart of the primary.</p> </li> </ul>"},{"location":"database/cloudnative-pg/rolling-update/#primaryupdatemethod","title":"primaryUpdateMethod","text":"<p>spec.primaryUpdateMethod defines how we want to update the primary instance and it is applied when the primaryUpdateStrategy is \"unsupervised\". We have 2 options here:</p> <ul> <li> <p>restart (default) This restarts the primary replica</p> </li> <li> <p>switchover</p> </li> </ul> <p>A switchover operation is triggered. In the switchover operation the former primary will be shut down. The spec.switchoverDelay can be expressed in seconds as the time to give to the primary to shutdown gracefully and archive the wal files. The default value is 3600 (1h).</p> <ul> <li>RTO (recovery time objective) is the time between the failure and when the service is up again.</li> <li>RPO (recovery point objective) is more related with the amount of data loss</li> </ul> <p>A lower spec.switchoverDelay gives priority to reduce the time (RTO) and a higher value reduces the risk of data loss (RPO).</p> <p>Then, the most aligned replica is promoted as the new primary.</p> <p>Again this value is a decision to take depending of several reasons like the environment or workload. In all cases a rolling update causes a service loss.</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/","title":"Migration to barman cloud plugin","text":""},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#install-the-barman-cloud-plugin","title":"Install the barman cloud plugin","text":"<p>Follow https://cloudnative-pg.io/plugin-barman-cloud/docs/installation/</p> <p>Requirements</p> <ul> <li>CloudNativePG version 1.26 or later. 1.27 has some improvements</li> <li>cert-manager</li> </ul>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#create-an-objectstore","title":"Create an ObjectStore","text":"<p>We must translate the spec.backup.barmanObjectStore section of the cluster to a new ObjectStore resource, under spec.configuration.</p> <p>serverName must be empty here. It must be configured in the the plugins section in the cluster</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#migration","title":"Migration","text":"<p>Here we must do some changes at once</p> <ul> <li> <p>Remove the spec.backup.barmanObjectStore section and spec.backup.retentionPolicy if it was defined. Also remove the entire spec.backup section if it is now empty</p> </li> <li> <p>Configure the spec.plugin section</p> </li> </ul> <p>parameters: barmanObjectName and serverName if needed</p> <ul> <li>Add barman-cloud.cloudnative-pg.io to the plugins list, as described in Configuring WAL archiving</li> </ul> <p>This change restarts the cluster pods</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#migration-test","title":"Migration test","text":"<ul> <li> <p>Check the wal backup is working</p> </li> <li> <p>Create a manual backup</p> </li> </ul> <pre><code>kubectl cnpg backup MYCLUSTER --method=plugin --plugin-name=barman-cloud.cloudnative-pg.io\n</code></pre>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#update-the-scheduled-backup","title":"Update the scheduled backup","text":"<p>If it works, update the scheduled backup to use the plugin</p> <p>changing</p> <pre><code>    method: barmanObjectStore\n</code></pre> <p>for</p> <pre><code>    method: plugin\n        pluginConfiguration:\n        name: barman-cloud.cloudnative-pg.io\n</code></pre>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#change-the-images","title":"Change the images","text":"<p>Finally don't use legacy or system images and migrate to minimal or standard images</p> <p>See here for more info</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#links","title":"Links","text":"<ul> <li>Migrating from Built-in CloudNativePG Backup</li> </ul> <p>https://cloudnative-pg.io/plugin-barman-cloud/docs/migration/</p> <ul> <li>[Bug]: Missing prometheus metrics</li> </ul> <p>After migrating to the barman cloud plugin, some Prometheus metrics related to backup monitoring may not be available or properly exposed. This affects monitoring dashboards that rely on backup-specific metrics to track backup success/failure rates and timing.</p> <p>https://github.com/cloudnative-pg/cloudnative-pg/issues/7812</p>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/","title":"From remote cnpg","text":"<p>We can create a new cluster from an existing cnpg cluster.</p>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#requirements","title":"Requirements","text":"<p>In the source and destination cluster we need the same:</p> <ul> <li>the same major PostgreSQL release with imageName or imageCatalogRef</li> <li>the same hardware architecture</li> <li>the same tablespaces</li> </ul> <p>We also need</p> <ul> <li>enough max_wal_senders</li> <li>network connectivity between them</li> </ul>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#importing-the-streaming_replica-creds","title":"Importing the streaming_replica creds","text":"<p>We will use the streaming_replica role in the source cluster.</p> <p>In order to get the credentials there, go to the namespace where the source cluster is located an export the secret that ends with \"-replication\"</p> <pre><code>kubectl get secret OMMITED-replication -o yaml\n</code></pre> <p>Then clean it and leave it this way</p> <pre><code>apiVersion: v1\ndata:\n  tls.crt: OMMITED\n  tls.key: OMMITED\nkind: Secret\nmetadata:\n  name: OMMITED-replication\ntype: kubernetes.io/tls\n</code></pre> <p>Go to the namespace where the new cluster will be created and import the secret</p> <pre><code>kubectl apply -f mysecret.yaml\n</code></pre>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#the-new-cluster","title":"The new cluster","text":"<p>Then create the new cluster with this basic settings</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg\nspec:\n  imageCatalogRef: # or imageName\n    apiGroup: postgresql.cnpg.io\n    kind: ClusterImageCatalog\n    major: 15 # must be the same major PostgreSQL\n    name: postgresql\n  externalClusters:\n    - name: my-remote-cluster # descriptive name\n      connectionParameters:\n        host: my-remote-host # host or ip\n        user: streaming_replica\n        sslmode: require\n      sslKey:\n        name: OMMITED-replication\n        key: tls.key\n      sslCert:\n        name: OMMITED-replication\n        key: tls.crt\n  bootstrap:\n    pg_basebackup:\n      source: my-remote-cluster\n      # Next settings if we can create a database, owner and assign credentials to that user\n      database: desired-db\n      owner: db-owner-name\n      secret:\n        name: desired-db-secret\n</code></pre>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#links","title":"Links","text":"<ul> <li>Bootstrap from a live cluster (pg_basebackup)</li> </ul> <p>https://cloudnative-pg.io/documentation/1.26/bootstrap/#requirements</p> <ul> <li>SSL Support</li> </ul> <p>https://www.postgresql.org/docs/current/libpq-ssl.html</p>"},{"location":"database/cloudnative-pg/bootstrap/recovery-backup/","title":"Recovery from a backup","text":""},{"location":"database/cloudnative-pg/bootstrap/recovery-backup/#recreate-the-cluster-from-a-backup-object","title":"Recreate the cluster from a backup object","text":"<p>If we have a backup section and working backups in our cluster, the easiest way recreate a failing cnpg cluster is using a backup object.</p> <ul> <li>Choose the desired backup</li> </ul> <p>To see our backups in our namespace</p> <pre><code>kubectl get backup\n</code></pre> <ul> <li>Delete the cluster</li> </ul> <p>Then delete the cluster</p> <pre><code>kubectl delete cluster MYCLUSTER\n</code></pre> <ul> <li>Change the cluster definition</li> </ul> <p>Configure the desired backup name changing the bootstrap section</p> <pre><code>spec:\n  bootstrap:\n    recovery:\n      backup:\n        name: MYWORKINGBACKUP\n</code></pre> <ul> <li>Change where the new backups will be stored</li> </ul> <p>Change the destination of the new backups. The recovery from backup fails is cnpg find a non empty folder. I think it is a good practice to start this new cluster storing the data in a new empty folder.</p> <pre><code>spec:\n  backup:\n    barmanObjectStore:\n      serverName: ANOTHERFOLDER\n</code></pre> <ul> <li>Apply the new cluster</li> </ul> <p>Finally apply the new cluster definition</p>"},{"location":"database/cloudnative-pg/metrics/cnpg/","title":"CNPG metrics","text":""},{"location":"database/cloudnative-pg/metrics/cnpg/#cnpg_backends_total","title":"cnpg_backends_total","text":"<p>This metric total tracks how many client connections (backend) are in a PostgreSQL database instance managed by CloudNative-PG. This includes the active, idle, idle in trasaction and other connection states</p> <p>It's derived from PostgreSQL's internal connection tracking and is essential for monitoring database connection health and It's equivalent to SELECT count(*) FROM pg_stat_activity, which counts all entries in   the activity table regardless of state.</p>"},{"location":"database/cloudnative-pg/metrics/cnpg/#cnpg_backends_waiting_total","title":"cnpg_backends_waiting_total","text":"<p>Counts the total number of backend connections that are waiting (measured in seconds).</p> <p>The metric helps identify when PostgreSQL backends are stuck waiting for resources or query completion, which can indicate database performance problems.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/","title":"Dashboard and alerts","text":""},{"location":"database/cloudnative-pg/metrics/dashboard/#official-dashboard","title":"Official dashboard","text":"<p>The official cnpg grafana dashboard is located here</p> <p>https://github.com/cloudnative-pg/grafana-dashboards/blob/main/charts/cluster/grafana-dashboard.json</p> <p>Note: This metric list is based on the current dashboard version. The required metrics may change when the dashboard is updated. Always verify against the latest dashboard version.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#required-metrics","title":"Required Metrics","text":"<p>To make the official CNPG Grafana dashboard work, you need to ensure the following metrics are available in your Prometheus instance:</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#cnpg-operator-and-cluster-metrics","title":"CNPG Operator and Cluster Metrics","text":"<p>These metrics are provided by CloudNative-PG operator and PostgreSQL clusters:</p> <pre><code>cnpg_pg_replication_streaming_replicas\ncnpg_pg_replication_is_wal_receiver_up\ncnpg_pg_replication_lag\ncnpg_pg_stat_replication_write_lag_seconds\ncnpg_pg_stat_replication_flush_lag_seconds\ncnpg_pg_stat_replication_replay_lag_seconds\ncnpg_pg_postmaster_start_time\ncnpg_pg_stat_database_xact_commit\ncnpg_pg_stat_database_xact_rollback\ncnpg_backends_total\ncnpg_pg_settings_setting\ncnpg_pg_replication_in_recovery\ncnpg_pg_stat_archiver_seconds_since_last_archival\ncnpg_collector_last_available_backup_timestamp\ncnpg_collector_postgres_version\ncnpg_pg_database_size_bytes\ncnpg_collector_first_recoverability_point\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#kube-state-metrics","title":"Kube State Metrics","text":"<p>These metrics are provided by kube-state-metrics:</p> <pre><code>kube_pod_container_resource_requests\nkube_pod_status_ready\nkube_pod_container_status_ready\nkube_pod_info\nkube_node_labels\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#kubelet-metrics","title":"Kubelet Metrics","text":"<p>These metrics are exposed by the Kubelet:</p> <pre><code>kubelet_volume_stats_available_bytes\nkubelet_volume_stats_capacity_bytes\nkubelet_volume_stats_inodes_used\nkubelet_volume_stats_inodes\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#cadvisor-metrics","title":"cAdvisor Metrics","text":"<p>These metrics are provided by cAdvisor (part of Kubelet):</p> <pre><code>container_memory_working_set_bytes\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#controller-runtime-metrics","title":"Controller Runtime Metrics","text":"<p>These metrics are provided by the controller-runtime library (used by CNPG operator):</p> <pre><code>controller_runtime_reconcile_total\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#recording-rules","title":"Recording Rules","text":"<p>The dashboard requires this recording rule to be created:</p> <pre><code>node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate\n</code></pre> <p>This rule aggregates CPU usage rates by node, namespace, pod, and container dimensions.</p> <p>Required source metric: The recording rule depends on <code>container_cpu_usage_seconds_total</code> which is provided by cAdvisor (part of Kubelet). This metric must be available in your Prometheus instance for the recording rule to work.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#alerts","title":"Alerts","text":"<ul> <li>Default provided alerts</li> </ul> <p>https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/refs/heads/main/docs/src/samples/monitoring/alerts.yaml</p>"},{"location":"database/postgresql/98-tips/","title":"Tips","text":""},{"location":"database/postgresql/98-tips/#list-all-databases","title":"List all databases","text":"<pre><code>SELECT datname FROM pg_database WHERE datistemplate = false;\n</code></pre>"},{"location":"database/postgresql/98-tips/#list-roles","title":"List roles","text":"<pre><code>SELECT rolname FROM pg_roles;\nSELECT * FROM pg_roles;\n</code></pre>"},{"location":"database/postgresql/98-tips/#list-roles-and-attributes","title":"List roles and attributes","text":"<pre><code>SELECT r.rolname, r.rolsuper, r.rolinherit,\n  r.rolcreaterole, r.rolcreatedb, r.rolcanlogin,\n  r.rolconnlimit, r.rolvaliduntil,\n  ARRAY(SELECT b.rolname\n        FROM pg_catalog.pg_auth_members m\n        JOIN pg_catalog.pg_roles b ON (m.roleid = b.oid)\n        WHERE m.member = r.oid) as memberof\n, r.rolreplication\n, r.rolbypassrls\nFROM pg_catalog.pg_roles r\nWHERE r.rolname !~ '^pg_'\nORDER BY 1;\n</code></pre>"},{"location":"database/postgresql/98-tips/#get-the-database-owner","title":"Get the database owner","text":"<pre><code>SELECT d.datname as \"Name\",\npg_catalog.pg_get_userbyid(d.datdba) as \"Owner\"\nFROM pg_catalog.pg_database d\nWHERE d.datname = 'tmb'\nORDER BY 1;\n</code></pre>"},{"location":"database/postgresql/98-tips/#create-role-with-password-and-ddbb-with-owner","title":"Create role with password and DDBB with owner","text":"<pre><code>CREATE ROLE owner WITH LOGIN PASSWORD 'whatever';\nCREATE DATABASE database WITH OWNER = 'owner';\n</code></pre>"},{"location":"database/postgresql/98-tips/#delete-roluser","title":"Delete rol/user","text":"<pre><code>REASSIGN OWNED BY grafanareader TO postgres;\nDROP OWNED BY grafanareader;\nDROP USER grafanareader;\n</code></pre>"},{"location":"database/postgresql/98-tips/#view-the-default-privileges-in-a-specific-schema","title":"View the default privileges in a specific schema","text":"<p>To view the default privileges in a specific schema in PostgreSQL, you can query the pg_default_acl system catalog. This catalog contains information about the default access control lists (ACLs) for objects created in the database.</p> <p>Query to Get Default Privileges Here is a query to retrieve the default privileges for the tmb schema:</p> <p>Explanation</p> <ul> <li>pg_default_acl: This catalog contains the default ACLs for objects created in the database.</li> <li>pg_namespace: This catalog contains information about schemas.</li> <li>pg_roles: This catalog contains information about roles.</li> <li>defaclobjtype: The type of object the default ACL applies to (e.g., r for tables, S for sequences, f for functions).</li> <li>defaclacl: The default ACLs for the specified object type.</li> </ul> <pre><code>SELECT\n    n.nspname AS schema_name,\n    r.rolname AS role_name,\n    CASE d.defaclobjtype\n        WHEN 'r' THEN 'TABLE'\n        WHEN 'S' THEN 'SEQUENCE'\n        WHEN 'f' THEN 'FUNCTION'\n        ELSE d.defaclobjtype\n    END AS object_type,\n    d.defaclacl AS default_privileges\nFROM\n    pg_default_acl d\nJOIN\n    pg_namespace n ON n.oid = d.defaclnamespace\nJOIN\n    pg_roles r ON r.oid = d.defaclrole\nWHERE\n    n.nspname = 'myschema';\n</code></pre>"},{"location":"database/postgresql/extensions/","title":"Tips: extensions","text":"<p>To determine where a PostgreSQL extension is installed, you can query the pg_extension system catalog. This catalog contains information about all the extensions installed in the current database, including the schema in which each extension is installed.</p> <p>SQL Query to Find Extension Installation Schema Here is a query to list all extensions installed in the current database along with the schema in which each extension is installed:</p> <pre><code>SELECT\n    e.extname AS extension_name,\n    n.nspname AS schema_name\nFROM\n    pg_extension e\nJOIN\n    pg_namespace n ON e.extnamespace = n.oid\nORDER BY\n    extension_name;\n</code></pre> <p>Explanation</p> <ul> <li>pg_extension: This catalog contains information about all the extensions installed in the current database.</li> <li>pg_namespace: This catalog contains information about schemas.</li> <li>extname: The name of the extension.</li> <li>extnamespace: The OID of the schema where the extension is installed.</li> <li>nspname: The name of the schema.</li> </ul>"},{"location":"deployment/resource-templating/","title":"Resource templating","text":"<p>This tools permit to group some kubernetes resources in templates and create instances with different values</p>"},{"location":"deployment/resource-templating/#list-of-tools","title":"List of tools","text":"Name How it defines Link Helm Go templates with values, charts, and helpers https://helm.sh/docs/ Kustomize Overlays, patches (strategic merge, JSON), and components https://kustomize.io/ Crossplane Composite Resource Definitions (XRDs) and Compositions https://docs.crossplane.io/ Kro ResourceGraphDefinition CRD with CEL expressions https://kro.run/docs Ytt YAML templating with overlays https://carvel.dev/ytt/docs/latest/ KCL lang Constraint-based configuration language https://www.kcl-lang.io/ Timoni CUE language with modules and bundles https://timoni.sh/quickstart/ Kubevela OAM Application CRD with components and traits https://kubevela.io/docs/ KPT Declarative configuration with functions https://kpt.dev/ Grafana Tanka Jsonnet with ksonnet libraries https://tanka.dev/ Operator SDK Custom Resource Definitions with Go/Ansible/Helm operators https://sdk.operatorframework.io/ CDK8s Object-oriented APIs in TypeScript, Python, Java, Go https://cdk8s.io/"},{"location":"deployment/resource-templating/#recommended","title":"Recommended","text":"<ul> <li>Helm: For using 3rd party applications or provide internal applications to external users</li> <li>Kustomize: for internal applications</li> <li>Ytt: for internal applications</li> <li>Crossplane: for platform engineering</li> </ul>"},{"location":"deployment/argo-rollouts/2-ways/","title":"2 Ways to create a rollout","text":"<p>There are 2 ways to create a rollout in argo rollouts</p>"},{"location":"deployment/argo-rollouts/2-ways/#single-resource","title":"Single resource","text":"<p>The first way needs to create a single rollout resource (excluding the service, ingress,..) that includes the logic of the rollout and the logic of the deployment via spec.template.</p> <p>This way does not create a deployment resource.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: rollout\n  template:\n    metadata:\n      labels:\n        app: rollout\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n  strategy:\n    blueGreen:\n      activeService: rollout\n      autoPromotionEnabled: false\n</code></pre>"},{"location":"deployment/argo-rollouts/2-ways/#separate-rollout-and-deployment","title":"Separate rollout and deployment","text":"<p>The second one is with 2 workload resources. First you create the deployment as your wish. Then you create a rollout resource without spec.template but using spec.workloadRef referencing the existing deployment.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0 # we usually want 0 replicas here. see below\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n</code></pre> <p>The point to define 0 replicas in the deployment is because the rollout has its own spec.replicas field. If we leave empty (1) or more replicas in the spec.replicas field of the deployment, this will deploy them in addition to the replicas managed by the rollout. And usually this is not a desired behaviour.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: deployment\nspec:\n  replicas: 3 # it is better to control them here\n  selector:\n    matchLabels:\n      app: deployment\n  workloadRef: \n    apiVersion: apps/v1\n    kind: Deployment\n    name: deployment\n  strategy:\n    blueGreen:\n      activeService: deployment\n      autoPromotionEnabled: false\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/","title":"Analysis vs Experiment","text":"<p>Argo Rollouts provides two mechanisms for validating deployments through metrics and progressive delivery: Analysis and Experiment. Understanding the differences between them is crucial for selecting the right approach for your deployment strategy.</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#quick-comparison","title":"Quick Comparison","text":"Aspect Analysis Experiment Purpose Evaluate metrics to make pass/fail decisions on rollout progression Run concurrent variants (baseline + canary) to compare behavior Scope Integrated into Rollout strategy steps Standalone Kubernetes resource Pod Creation Uses existing rollout replicas Creates actual pods for baseline and canary variants Duration Runs at specific points (inline, background, pre/post-promotion) Limited timeframe (e.g., 1h) Comparison Single variant analysis Side-by-side comparison of two variants Analysis Type Basic metric validation Statistical comparison (e.g., Mann-Whitney, Kayenta) Lifecycle Tied to rollout progression Independent lifecycle"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#analysis","title":"Analysis","text":""},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#what-is-an-analysis","title":"What is an Analysis?","text":"<p>An <code>Analysis</code> evaluates metrics over time to determine if a rollout should proceed, pause, or rollback. It's integrated directly into the Rollout's deployment strategy and uses the existing application replicas being rolled out.</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Metric-driven decisions: Evaluates success/failure conditions based on metrics from providers (Prometheus, Datadog, etc.)</li> <li>Multiple timing options:</li> <li>Inline: Blocks rollout progression until analysis completes</li> <li>Background: Runs concurrently with rollout steps</li> <li>Pre/Post-promotion: For BlueGreen deployments, validates before or after traffic switch</li> <li>Non-disruptive: Works with your existing rollout replicas</li> <li>Lightweight: Focuses on metric queries, not pod creation</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#analysis-resources","title":"Analysis Resources","text":"<ul> <li>AnalysisTemplate: Reusable template defining metrics and conditions (namespace-scoped)</li> <li>ClusterAnalysisTemplate: Cluster-scoped version for sharing across namespaces</li> <li>AnalysisRun: Generated instance when analysis executes</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Validate success rate during canary rollout</li> <li>Check error rates meet thresholds</li> <li>Verify response times are acceptable</li> <li>Run smoke tests before promotion (BlueGreen)</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#experiment","title":"Experiment","text":""},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#what-is-an-experiment","title":"What is an Experiment?","text":"<p>An <code>Experiment</code> creates a temporary, controlled environment where two versions of an application (baseline and canary) run concurrently for a limited duration. It generates actual pods and compares their metrics using analysis templates to determine which version performs better.</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#key-characteristics_1","title":"Key Characteristics","text":"<ul> <li>Pod-based comparison: Spins up replicas for both baseline and canary versions</li> <li>Fixed duration: Runs for a specified time window then terminates</li> <li>Standalone resource: Independent Kubernetes object, not embedded in Rollout</li> <li>Statistical analysis: Typically uses advanced comparison methods (Mann-Whitney, Kayenta)</li> <li>Isolated testing: Creates a separate environment without affecting production traffic</li> <li>Comprehensive validation: Compares both versions side-by-side before promoting</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#experiment-resources","title":"Experiment Resources","text":"<ul> <li>Experiment: Kubernetes CRD defining baseline template, canary template, duration, and analysis</li> <li>Analysis templates: Referenced for comparing the two variants</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#example-use-cases_1","title":"Example Use Cases","text":"<ul> <li>Compare v1 and v2 statistically before production rollout</li> <li>Run A/B tests in a controlled environment</li> <li>Validate major version upgrades with Kayenta analysis</li> <li>Measure performance differences before committing to rollout</li> </ul>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#when-to-use-each","title":"When to Use Each","text":""},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#use-analysis-when","title":"Use Analysis When","text":"<p>\u2705 You need lightweight metric validation \u2705 You're checking success rates, error rates, latency \u2705 You want to validate using existing rollout replicas \u2705 You need quick pass/fail decisions \u2705 You're doing canary or BlueGreen gradual progression \u2705 You want to integrate validation into your deployment steps</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#use-experiment-when","title":"Use Experiment When","text":"<p>\u2705 You need statistical comparison between two versions \u2705 You want to run A/B testing in isolation \u2705 You're comparing major version changes \u2705 You need Kayenta/Mann-Whitney statistical analysis \u2705 You want to validate without affecting production traffic \u2705 You prefer a controlled environment before production rollout</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#combined-usage","title":"Combined Usage","text":"<p>You can use both together in a staged approach:</p> <ol> <li>Pre-production validation via Experiment (statistical comparison in isolated environment)</li> <li>Production validation via Analysis (metric-driven checks during gradual rollout)</li> </ol> <p>This pattern allows you to validate changes comprehensively before committing to production traffic.</p>"},{"location":"deployment/argo-rollouts/analysis-vs-experiment/#key-metrics","title":"Key Metrics","text":"<p>Both Analysis and Experiment support multiple metric providers:</p> <ul> <li>Prometheus: Query-based metrics</li> <li>Datadog: APM and metrics</li> <li>New Relic: Application performance</li> <li>Kayenta: Statistical analysis framework</li> <li>CloudWatch: AWS metrics</li> <li>Wavefront: Time-series data</li> <li>Custom: Webhook-based metrics</li> </ul>"},{"location":"deployment/argo-rollouts/analysis/","title":"Analysis","text":"<p>Analysis is argo workflows are tests that can be launched in a kubernetes cluster, typically inside a Rollout or using Kargo promotions, but they can be used without referencing them in that applications</p> <p>That analysis make some queries to systems like prometheus, Datadog,... Also supports web queries and executing a kubernetes job.</p> <p>We can template that analysis using AnalysisTemplate or ClusterAnalysisTemplate kubernetes resources. And they are instanciated via an AnalysisRun resource.</p>"},{"location":"deployment/argo-rollouts/analysis/#analysis-spec","title":"Analysis Spec","text":"<p>When defining an AnalysisTemplate, ClusterAnalysisTemplate or AnalysisRun we have this fields</p>"},{"location":"deployment/argo-rollouts/analysis/#specmetrics","title":"spec.metrics","text":"<p>This is where we define the queries|tests|measurements via the following fields</p> <ul> <li>name</li> </ul> <p>The name we give to the test, query or measurement</p> <ul> <li>provider</li> </ul> <p>It includes the query|test itself and provider configuration. There are some supported providers like prometheus, Datadog, CloudWatch, InfluxDB, Web, Job, Graphite,...</p> <ul> <li>initialDelay</li> </ul> <p>It adds a delay to the test execution. Example: 30s, 5m,...</p> <ul> <li>count and interval</li> </ul> <p>Count is the number of times we want to repeat the test, query or measurement and interval is the time to wait between tests. Example: 15s</p>"},{"location":"deployment/argo-rollouts/analysis/#test-query-or-measurement-result","title":"Test, query or measurement result","text":"<p>The query|test|measurement itself is done against a provider, and we can consider it as successful or failed with the SuccessCondition and failureCondition settings.</p> <p>Sometimes the query|test|measurement cannot be evaluated as successful or failed and they are considered as an inconclusive result. This could happen due to missing data, timeouts, or other issues that prevent the metric from being evaluated. One example of how analysis runs could become Inconclusive, is when a metric defines no success or failure conditions. They also can</p>"},{"location":"deployment/argo-rollouts/analysis/#handling-success-results","title":"Handling Success results","text":"<p>consecutiveSuccessLimit define the required consecutive number of successes to consider the analysis to succeed</p> <p>consecutiveSuccessLimit default value is 0 (disabled) and it is available since v1.8 release</p>"},{"location":"deployment/argo-rollouts/analysis/#handling-error-results","title":"Handling Error results","text":"<ul> <li>With failureLimit we can define the maximum number of test errors we want to tolerate.</li> </ul> <p>The default value of failureLimit is 0 so no failures are tolerated. To disable we can set it to \"-1\". failureLimit has precedence over consecutiveSuccessLimit. Also failureLimit or consecutiveSuccessLimit are not reached, the test (measurement) is considered as inconclusive.</p> <ul> <li>consecutiveErrorLimit defines the maximum number of consecutive errors that are allowed for a metric before the analysis is considered to have failed.</li> </ul>"},{"location":"deployment/argo-rollouts/analysis/#handling-inconclusive-results","title":"Handling inconclusive results","text":"<p>InconclusiveLimit sets a threshold for how many inconclusive results are acceptable during an analysis. If the number of inconclusive results exceeds this limit, the analysis is marked as failed. If inconclusiveLimit is not specified, the default behavior is to allow unlimited inconclusive results, meaning the analysis will not fail due to inconclusive results</p>"},{"location":"deployment/argo-rollouts/analysis/#example","title":"Example","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisRun\nmetadata:\n  generateName: test-\n  namespace: argocd\nspec:\n  metrics:\n    - name: argocd-app-health-sync  # name of the measurement\n      initialDelay: 30s # wait 30 seconds to start doing queries\n      count: 15 # 15 times\n      interval: 10s # every 10 seconds. Mix this when the metric is updated\n      provider:\n        prometheus:\n          address: \"http://prometheus-operated.monitoring:9090\"\n          query: |\n            argocd_app_info{name=\"my-argocd-app\",health_status=\"Healthy\", sync_status=\"Synced\"}\n      successCondition: len(result) == 1 &amp;&amp; result[0] == 1  # only 1 result with value 1\n      failureCondition: len(result) == 0 || result[0] != 1 # empty array or result not 1\n      failureLimit: 3 # tolerate 3 errors max\n      consecutiveErrorLimit: 3 # tolerate 3 consecutive errors max\n      consecutiveSuccessLimit: 8 # its ok with 8 consecutiveSuccessLimit\n      inconclusiveLimit: 2 # tolerate 2 inconclusive results\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specargs","title":"spec.args","text":"<p>Inside a template we can define arguments</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: mytemplate\nspec:\n  args:\n  - name: service-name\n  - name: prometheus-port\n</code></pre> <p>And they can used later as variables in the query</p> <pre><code>{{args.service-name}}\n{{args.prometheus-port}}\"\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specttlstrategy","title":"spec.ttlStrategy","text":"<p>ttlStrategy permits to control the the lifetime of an analysis run and delete them after a period of time. If this field is unset, the analysis controller will not delete them and they must be deleted manually or via other garbage collection policies (e.g. successfulRunHistoryLimit and unsuccessfulRunHistoryLimit).</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisRun\nspec:\n  ...\n  ttlStrategy:\n    secondsAfterCompletion: 3600\n    secondsAfterSuccess: 1800\n    secondsAfterFailure: 1800\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specterminate","title":"spec.terminate","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#specmeasurementretention","title":"spec.measurementRetention","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#specdryrun","title":"spec.dryRun","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#links","title":"Links","text":"<ul> <li>Analysis &amp; Progressive Delivery</li> </ul> <p>https://argoproj.github.io/argo-rollouts/features/analysis/</p> <ul> <li>Argo Rollouts FAQ</li> </ul> <p>https://argoproj.github.io/argo-rollouts/FAQ/</p> <ul> <li>Kargo Analysis Templates Reference</li> </ul> <p>https://docs.kargo.io/user-guide/reference-docs/analysis-templates/</p> <p>AnalysisTemplate</p> <p>An AnalysisTemplate is a template spec which defines how to perform a canary analysis, such as the metrics which it should perform, its frequency, and the values which are considered successful or failed. AnalysisTemplates may be parameterized with inputs values.</p> <p>ClusterAnalysisTemplate A ClusterAnalysisTemplate is like an AnalysisTemplate, but it is not limited to its namespace. It can be used by any Rollout throughout the cluster.</p> <p>AnalysisRun An AnalysisRun is an instantiation of an AnalysisTemplate. AnalysisRuns are like Jobs in that they eventually complete. Completed runs are considered Successful, Failed, or Inconclusive, and the result of the run affect if the Rollout's update will continue, abort, or pause, respectively.</p> <p>https://argo-rollouts.readthedocs.io/en/stable/features/analysis/</p>"},{"location":"deployment/argo-rollouts/experiment/","title":"Experiments","text":"<p>An Experiment is limited run of one or more ReplicaSets for the purposes of analysis. Experiments typically run for a pre-determined duration, but can also run indefinitely until stopped. Experiments may reference an AnalysisTemplate to run during or after the experiment. The canonical use case for an Experiment is to start a baseline and canary deployment in parallel, and compare the metrics produced by the baseline and canary pods for an equal comparison.</p> <p>https://argo-rollouts.readthedocs.io/en/stable/features/experiment/</p>"},{"location":"deployment/argo-rollouts/restarting-rollouts/","title":"Restarting rollouts","text":""},{"location":"deployment/argo-rollouts/restarting-rollouts/#restart-the-rollout","title":"Restart the rollout","text":"<p>Using the kubectl plugin</p> <pre><code>kubectl argo rollouts restart -n NAMESPACE_NAME ROLLOUT_NAME\n</code></pre> <p>Argocd has an embedded action that permits to restart a rollout</p>"},{"location":"deployment/argo-rollouts/restarting-rollouts/#notes-about-the-restart","title":"Notes about the restart","text":"<ul> <li> <p>Eviction This deletes the pods of the current rollout using the eviction api. The deployment controller will replace them without creating a new replicaset. Because argo rollouts uses the eviction api, this respects the existing PodDisruptionBudgets. Also, a rollout with a single replica causes downtime.</p> </li> <li> <p>Speed The speed can be specified with spec.maxUnavailable setting in the rollout spec.</p> </li> <li> <p>Schedule It is possible to schedule a restart with the .spec.restartAt field</p> </li> </ul>"},{"location":"deployment/argo-rollouts/restarting-rollouts/#order","title":"Order","text":"<p>The restart order is:</p> <ul> <li>First the stable replica set</li> <li>Second, the current replica set</li> <li>Finally all other ReplicaSets beginning with the oldest</li> </ul>"},{"location":"deployment/argo-rollouts/restarting-rollouts/#with-stakater-reloader","title":"With stakater reloader","text":"<p>If we want to restart a rollout the same way with stakater reloader when a configmap or secret changes, we have to:</p> <ul> <li>enable argo rollouts restart with isArgoRollouts: true</li> <li>use at least the 1.1.0 release of stakater reloader</li> <li>use the single resource specification of the rollout (spec.template). Using spec.workloadRef currently crashes the controller</li> </ul>"},{"location":"deployment/argo-rollouts/restarting-rollouts/#if-using-workloadref","title":"If using workloadRef","text":"<p>If we are using spec.workloadRef instead of spec.template we can restart the rollout the same way, but we can also restart the referenced deployment with this command.</p> <pre><code>kubectl rollout restart -n NAMESPACE_NAME DEPLOYMENT NAME\n</code></pre> <p>But this generates a new replicaset and a new revision in the rollout</p> <p>The deployment name must be the deployment created by the rollout, not the deployment defined in the deployment resource itself.</p>"},{"location":"deployment/argo-rollouts/restarting-rollouts/#links","title":"Links","text":"<ul> <li> <p>Restarting Rollout Pods https://argo-rollouts.readthedocs.io/en/stable/features/restart/</p> </li> <li> <p>Stakater Reloader https://github.com/stakater/Reloader</p> </li> <li> <p>[BUG] Restarting a rollout with workloadRef crashes the operator pod #751 https://github.com/stakater/Reloader/issues/751</p> </li> <li> <p>Argocd rollout restart action https://github.com/argoproj/argo-cd/tree/master/resource_customizations/argoproj.io/Rollout/actions/restart</p> </li> </ul>"},{"location":"deployment/argo-rollouts/rollback-window-and-history/","title":"Rollback window and history limit","text":""},{"location":"deployment/argo-rollouts/rollback-window-and-history/#rollback-window","title":"Rollback window","text":"<p>spec.rollbackWindow determines the number of revisions to consider in an automatic rollback</p> <p>In this example argo rollouts will monitor the last three revisions in order to rollback automatically to the last known good revision within the rollback window.</p> <pre><code>spec:\n  rollbackWindow:\n    revisions: 3\n</code></pre> <p>The rollback window provides a way to fast track deployments to previously deployed versions (default not set)</p>"},{"location":"deployment/argo-rollouts/rollback-window-and-history/#history-limit","title":"History limit","text":"<p>spec.revisionHistoryLimit determines the number of old ReplicaSets to retain for the purpose of rollback (default 10)</p> <p>In this example, Argo Rollouts will keep the last three ReplicaSets that were created by the Rollout. If a Rollout is rolled back, it will revert to an older ReplicaSet. This does not affect the running Pods, but it does allow you to rollback to a previous version of your application if something goes wrong.</p> <pre><code>spec:\n  revisionHistoryLimit: 3\n</code></pre> <p>On clusters with thousands of rollouts memory usage for the argo-rollouts operator can be reduced significantly by changing RevisionHistoryLimit from the default of 10 to a lower number. One user of Argo Rollouts saw a 27% reduction in memory usage for a cluster with 1290 rollouts by changing RevisionHistoryLimit from 10 to 0.</p>"},{"location":"deployment/argo-rollouts/rollback-window-and-history/#links","title":"Links","text":"<p>https://argo-rollouts.readthedocs.io/en/stable/FAQ/#rollbacks https://argo-rollouts.readthedocs.io/en/stable/features/rollback/ https://argo-rollouts.readthedocs.io/en/stable/features/specification/ https://argo-rollouts.readthedocs.io/en/stable/best-practices/</p>"},{"location":"deployment/cluster-api/98-tips/","title":"Tips","text":"<ul> <li>Get cluster api related crds</li> </ul> <pre><code>kubectl api-resources  | grep -i x-k8s.io\n</code></pre> <ul> <li>Get all supported providers</li> </ul> <pre><code>clusterctl config repositories\n</code></pre>"},{"location":"deployment/cluster-api/99-links/","title":"Links","text":"<ul> <li>Root official documentation book</li> </ul> <p>https://cluster-api-operator.sigs.k8s.io/</p> <ul> <li>And source</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api-operator/tree/main/docs/book</p> <ul> <li>Github</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api/</p> <ul> <li>Cluster API Operator</li> </ul> <p>https://cluster-api-operator.sigs.k8s.io/</p>"},{"location":"deployment/cluster-api/concepts/","title":"Concepts","text":""},{"location":"deployment/cluster-api/concepts/#coreprovider","title":"CoreProvider","text":"<p>A component responsible for providing the fundamental building blocks of the Cluster API. It defines and implements the main Cluster API resources such as Clusters, Machines, and MachineSets, and manages their lifecycle. This includes:</p> <p>Defining the main Cluster API resources and their schemas. Implementing the logic for creating, updating, and deleting these resources. Managing the overall lifecycle of Clusters, Machines, and MachineSets. Providing the base upon which other providers like BootstrapProvider and InfrastructureProvider build.</p> <p>Only the cluster-api CoreProvider is available</p>"},{"location":"deployment/cluster-api/concepts/#bootstrapprovider","title":"BootstrapProvider","text":"<p>A component responsible for turning a server into a Kubernetes node as well as for:</p> <ul> <li>Generating the cluster certificates, if not otherwise specified</li> <li>Initializing the control plane, and gating the creation of other nodes until it is complete</li> <li>Joining control plane and worker nodes to the cluster</li> </ul> <p>Get all:</p> <pre><code>clusterctl config repositories | grep BootstrapProvider\n</code></pre> <p>Examples: kubeadm, talos, microk8s, rke2,...</p>"},{"location":"deployment/cluster-api/concepts/#controlplaneprovider","title":"ControlPlaneProvider","text":"<p>A component responsible for managing the control plane of a Kubernetes cluster. This includes:</p> <p>Provisioning the control plane nodes. Managing the lifecycle of the control plane, including upgrades and scaling.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep ControlPlaneProvider\n</code></pre> <p>Examples: kubeadm, talos, microk8s, rke2,...</p>"},{"location":"deployment/cluster-api/concepts/#infrastructureprovider","title":"InfrastructureProvider","text":"<p>A component responsible for the provisioning of infrastructure/computational resources required by the Cluster or by Machines (e.g. VMs, networking, etc.). For example, cloud Infrastructure Providers include AWS, Azure, and Google, and bare metal Infrastructure Providers include VMware, MAAS, and metal3.io.</p> <p>Examples: aws, azure, gcp, harvester, vsphere, metal3, vcluster, openstack, docker, byoh, ...</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep InfrastructureProvider\n</code></pre>"},{"location":"deployment/cluster-api/concepts/#ipamprovider","title":"IPAMProvider","text":"<p>A component that manages pools of IP addresses using Kubernetes resources. It serves as a reference implementation for IPAM providers, but can also be used as a simple replacement for DHCP.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep IPAMProvider\n</code></pre> <p>Examples: in-cluster, nutanix</p>"},{"location":"deployment/cluster-api/concepts/#runtimeextensionprovider","title":"RuntimeExtensionProvider","text":"<p>Get all:</p> <pre><code>clusterctl config repositories | grep RuntimeExtensionProvider\n</code></pre> <p>Examples: nutanix</p>"},{"location":"deployment/cluster-api/concepts/#addonprovider","title":"AddonProvider","text":"<p>A component that extends the functionality of Cluster API by providing a solution for managing the installation, configuration, upgrade, and deletion of Cluster add-ons using Helm charts.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep AddonProvider\n</code></pre>"},{"location":"deployment/cluster-api/deployment-operator/","title":"Deployment using operator","text":"<p>How to deploy a kubernetes cluster using cluster api operator</p>"},{"location":"deployment/cluster-api/deployment-operator/#deploy-cert-manager","title":"Deploy cert-manager","text":"<p>This is a requirement so visit https://cert-manager.io/</p>"},{"location":"deployment/cluster-api/deployment-operator/#deploy-the-operator-it-self","title":"Deploy the operator it self","text":"<p>Using the operator-components.yaml file from  https://github.com/kubernetes-sigs/cluster-api-operator/releases</p> <p>This deploys capi-operator-system namespace and the namespaced crds for the providers</p> <ul> <li>coreprovider</li> <li>bootstrap provider</li> <li>control plane provider</li> <li>infraestucture provider</li> <li>...</li> </ul>"},{"location":"deployment/cluster-api/resource-table/","title":"Resource Table","text":"<p>This table shows the gateway api resource and how it can be created using provider specific resources</p> <p>Cluster</p> AWS EKS AWS Self AWS ROSA Vsphere Kubeadm Talos spec.infrastructureRef AWSManagedCluster AWSCluster ROSACluster VSphereCluster spec.controlPlaneRef AWSManagedControlPlane ROSAControlPlane KubeadmControlPlane TalosControlPlane <p>MachinePool</p> AWS EKS AWS Self AWS ROSA Vsphere Kubeadm Talos infrastructureRef AWSManagedMachinePool AWSMachinePool ROSAMachinePool bootstrap EKSConfig KubeadmConfig TalosConfig <p>MachineDeployment</p> AWS EKS AWS Self AWS ROSA Vsphere Kubeadm Talos <p>Machine</p> AWS EKS AWS Self AWS ROSA Vsphere Kubeadm Talos VSphereMachine"},{"location":"deployment/cluster-api/providers/98-links/","title":"Links","text":"<ul> <li>Providers list</li> </ul> <p>https://cluster-api.sigs.k8s.io/reference/providers</p>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/","title":"Bootstrap","text":"<p>If we want to deploy the Kubernetes Cluster API Provider AWS (CAPA) using the cluster api operator we have the following steps</p> <ul> <li>Create all the necessary IAM resources needed for CAPA to manage AWS infrastructure</li> <li>Deploy the CAPA infraestructure provider</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#create-the-necessary-iam-resources","title":"Create the necessary IAM resources","text":"<p>We will use the clusterawsadm binary with a AWSIAMConfiguration configuration file. This will create a CloudFormation stack named cluster-api-provider-aws-sigs-k8s-io (by default) and includes:</p> <ul> <li>iam roles</li> <li>iam policies</li> <li>instance profiles</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#authentication","title":"Authentication","text":"<p>First of all we need to authenticate clusterawsadm with an administrative user using the following environment variables:</p> <ul> <li>AWS_REGION</li> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> <li>AWS_SESSION_TOKEN (if using Multi-factor authentication)</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#awsiamconfiguration-file","title":"AWSIAMConfiguration file","text":"<p>Then we can configure clusterawsadm with a AWSIAMConfiguration file.</p> <p>Some of the settings are:</p> <ul> <li>Name and tags for the Cloudformation stack</li> <li>The region to create it (if not priovided via environment variable or cli parameter)</li> <li>Dedicated IAM user (spec.bootstrapUser)</li> </ul> <p>It creates a dedicated IAM user and group with proper permissions for bootstrapping and managing Cluster API AWS Provider resources. This avoid using personal AWS credentials and permit multitenancy scenarios.</p> <p>Later we can generate access keys for this user and use them when running clusterawsadm bootstrap credentials encode-as-profile</p> <p>We can also add prefixes or suffixes to the roles, users and policies that will be created</p> <pre><code>Final IAM User Name: {namePrefix}{userName}{nameSuffix}\nFinal IAM Group Name: {namePrefix}{groupName}{nameSuffix}\n</code></pre> <p>The default created user and group is bootstrapper.cluster-api-provider-aws.sigs.k8s.io user</p> <ul> <li>EKS settings (spec.eks)</li> </ul> <p>Eks is enabled by default, but here we can enable separate roles per EKS cluster (iamRoleCreation), enable support for machinepool resources or fargate profiles so on</p> <p>More settings and info</p> <ul> <li>Documentation https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites</li> <li>AWSIAMConfiguration CRD https://cluster-api-aws.sigs.k8s.io/crd/</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#create-cloudformation-stack","title":"Create CloudFormation stack","text":"<p>Now we can create the CloudFormation stack with our AWSIAMConfiguration file</p> <pre><code>aws cloudformation list-stacks\nclusterawsadm bootstrap iam create-cloudformation-stack --config bootstrap.yaml \n</code></pre> <p>This generates resources:</p> <ul> <li>The bootstrap user and group</li> <li>Intance profiles (control-plane, controllers and nodes)</li> <li>Some ManagedPolicies</li> <li>Other Roles</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#deploy-the-capa-infraestructure-provider","title":"Deploy the CAPA infraestructure provider","text":""},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#controller-authentication-aws-profile","title":"Controller Authentication aws profile","text":"<p>The controller deployment needs baseline AWS authentication to the CAPA controller in order to be deployed.</p> <p>If we have created a bootstrapUser, we must create an Access key and save the to an aws profile for it</p> <pre><code>aws configure --profile=bootstrap-capa\n</code></pre> <p>Then we must translate this credentials to the kubernetes cluster where we will deploy the CAPA provider. We assume it will be in a aws-bootstrap secret in the capa-system namespace.</p> <pre><code>export AWS_PROFILE=bootstrap-capa\nexport AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile) # need to pass region?\necho $AWS_B64ENCODED_CREDENTIALS | base64 -d # check the data. \nkubectl create secret generic aws-bootstrap --from-literal=AWS_B64ENCODED_CREDENTIALS=\"${AWS_B64ENCODED_CREDENTIALS}\" --namespace capa-system\n</code></pre> <p>They act as default credentials. Later, it is typical to use identityRef as per cluster credentials (spec.identityRef in an AWSCluster resource).</p>"},{"location":"deployment/cluster-api/providers/aws/10-bootstrap/#deploy-capa-using-capi-operator","title":"Deploy CAPA using Capi Operator","text":"<p>We can deploy the aws infraestructure provider this way</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: InfrastructureProvider\nmetadata:\n  name: aws\n  namespace: capa-system\nspec:\n  version: v2.10.0\n  configSecret:\n    name: aws-bootstrap\n  # some optional features\n  manager:\n    featureGates:\n      MachinePool: true\n</code></pre> <p>See more here about optional features</p>"},{"location":"deployment/cluster-api/providers/aws/11-features/","title":"Features","text":""},{"location":"deployment/cluster-api/providers/aws/11-features/#enable-separate-roles-per-eks-cluster","title":"Enable separate roles per EKS cluster","text":"<p>By default, all the EKS cluster share the same IAM roles. See this if we want to have separate roles per EKS cluster</p>"},{"location":"deployment/cluster-api/providers/aws/11-features/#enable-machinepool-support","title":"Enable MachinePool support","text":"<p>Cluster api has a resource called MachinePool. In CAPA, in order to manage MachinePool resources we need to enable the MachinePool feature when deploying the aws provider</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: InfrastructureProvider\nmetadata:\n  name: aws\nspec:\n  manager:\n    featureGates:\n      MachinePool: true\n</code></pre> <p>But CAPA supports 2 machine pool resources:</p> <ul> <li>AWSMachinePool</li> </ul> <p>These are aws autoescaling groups in order to orquestrate ec2 machines</p> <ul> <li>AWSManagedMachinePool</li> </ul> <p>These are EKS managed node groups. In this case we need to give permissions to create the default role for managed machine pools in the AWSIAMConfiguration file</p> <pre><code>apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1\nkind: AWSIAMConfiguration\nspec:\n  eks:\n    managedMachinePool:\n      disable: false \n</code></pre> <p>The MachinePoolMachines features also enables the creation of Machine and AWSMachine objects for nodes created by a AWSMachinePool.</p> <p>See more here https://cluster-api-aws.sigs.k8s.io/topics/machinepools</p>"},{"location":"deployment/cluster-api/providers/aws/11-features/#add-more-control-plane-roles","title":"Add more control plane roles","text":""},{"location":"deployment/cluster-api/providers/aws/11-features/#enable-fargate-profiles-in-eks","title":"Enable Fargate profiles in EKS","text":"<p>To use Fargate profiles in EKS we need to create the default role for the fargate profiles in the AWSIAMConfiguration</p> <pre><code>apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1\nkind: AWSIAMConfiguration\nspec:\n  eks:\n    fargate:\n      disable: false\n</code></pre> <p>And enable that feature in the provider</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: InfrastructureProvider\nmetadata:\n  name: aws\nspec:\n  manager:\n    featureGates:\n      EKSFargate: true\n</code></pre>"},{"location":"deployment/cluster-api/providers/aws/11-features/#links","title":"Links","text":"<ul> <li>Using clusterawsadm to fulfill prerequisites</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites</p> <ul> <li>Enabling EKS support</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/eks/enabling</p> <ul> <li>Provider features</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/reference/reference.html</p>"},{"location":"deployment/cluster-api/providers/aws/12-authentication/","title":"Authentication","text":""},{"location":"deployment/cluster-api/providers/aws/12-authentication/#cloudformation-stack-user","title":"Cloudformation Stack user","text":"<p>The user that creates the Cloudformation Stack must have be an administrative user in an AWS account. It is authenticated via the following environment variables</p> <pre><code>AWS_REGION\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN (MFA)\n</code></pre> <p>Then it will do a clusterawsadm bootstrap iam create-cloudformation-stack --config myconfig.yaml</p> <p>This command also updates the current Cloudformation Stack settings</p> <p>More info here</p> <p>https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites</p>"},{"location":"deployment/cluster-api/providers/aws/12-authentication/#capa-controller-credentials","title":"CAPA Controller credentials","text":"<p>We need to give credentials to the CAPA controller in order to manage aws resources. It can be a good practice to create an specific user/role different from the user that created the Cloudformation Stack.</p> <p>We can achieve that via spec.bootstrapUser in our AWSIAMConfiguration file</p> <pre><code>apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1\nkind: AWSIAMConfiguration\nspec:\n  bootstrapUser:\n    enable: true\n    userName: capa-manager-user # Defaults to \u201cbootstrapper.cluster-api-provider-aws.sigs.k8s.io\u201d\n    groupName:  capa-manager-group # Defaults to \u201cbootstrapper.cluster-api-provider-aws.sigs.k8s.io\u201d\n</code></pre> <p>We can also provide a prefix and suffix via spec.namePrefix and spec.nameSuffix. This affects to roles, users and policies.</p> <p>We must translate the credentials for this user/role to the CAPA controller. For example:</p> <ul> <li>Create an access key for the user and load in an aws profile</li> <li>Load the aws profile and create a secret in the kubernetes cluster where the controller will be deployed.</li> <li>Configure the provider to use that secret</li> </ul> <p>See this for other controller authentication like IRSA, Kiam and kube2iam</p> <ul> <li>IAM roles to deploy management clusters instead of AWS credentials</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/using-iam-roles-in-mgmt-cluster.</p> <ul> <li>Specifying the IAM Role to use for Management Components</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/specify-management-iam-role.html</p> <ul> <li>See this for AWSManagedControlPlane credentials and multitenancy</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/12-authentication/#separate-eks-roles-per-cluster","title":"Separate EKS roles per cluster","text":"<p>When deploying EKS clusters, by default they share the same IAM roles. If we want different roles per cluster, we must permit the EKS controller to create IAM roles per EKS cluster</p> <pre><code>apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1\nkind: AWSIAMConfiguration\nspec:\n  eks:\n    iamRoleCreation: true\n</code></pre> <p>... and enable the EKSEnableIAM feature in the provider, that enables the automatic creation of unique IAM roles for each individual EKS cluster.</p> <p>```yaml apiVersion: operator.cluster.x-k8s.io/v1alpha2 kind: InfrastructureProvider metadata:   name: aws spec:   manager:     featureGates:       EKSEnableIAM: true</p>"},{"location":"deployment/cluster-api/providers/aws/20-cluster/","title":"AWS Cluster","text":"<p>When deploying a kubernetes cluster under AWS must define the following sections under the cluster resource</p>"},{"location":"deployment/cluster-api/providers/aws/20-cluster/#kubernetes-cluster-definition-specinfrastructureref","title":"Kubernetes cluster definition (spec.infrastructureRef)","text":"<p>In spec.infrastructureRef we can use a self managed cluster (AWSCluster) or eks cluster (AWSManagedCluster). This table shows the aws provider resources that can be called here</p> EKS Self managed Resource AWSManagedCluster AWSCluster Template AWSManagedClusterTemplate AWSClusterTemplate <p>It is a lightweight wrapper that delegates most configuration to AWSManagedControlPlane.</p>"},{"location":"deployment/cluster-api/providers/aws/20-cluster/#control-plane-definition-speccontrolplaneref","title":"Control plane definition (spec.controlPlaneRef)","text":"<ul> <li> <p>In EKS we must use a AWSManagedControlPlane resource. Its template is AWSManagedControlPlaneTemplate</p> </li> <li> <p>When we want a a self managed control plane, the resource will depend of the chosen controlplane provider, for example, a KubeadmControlPlane resource if using kubeadm provider.</p> </li> </ul> <p>This is a more complex resource that permits to configure:</p> <ul> <li>credentials to authenticate against aws</li> <li>basic stuff like cluster name, kubernetes version, aws region</li> <li>network: vpc, subnets</li> <li>api access: endpoint access, authentication mode,...</li> <li>k8s settings: upgrade policy, addons, imagelookup</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/","title":"AWSManagedControlPlane","text":"<p>This resource configures the eks cluster and it has several sections that translate the eks api to the resource</p>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/#specidentityref","title":"spec.identityRef","text":"<p>This field is the credentials to use when reconciling the managed control plane.</p> <p>If no identity is specified, the default identity for this controller will be used.</p> <p>Here we can use 3 identity types (spec.identityRef.kind)</p>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/#awsclustercontrolleridentity","title":"AWSClusterControllerIdentity","text":"<p>This authentication method uses the controller credentials. The AWSClusterControllerIdentity resource permits to filter what namespaces can call it.</p> <p>Currently only the name \"default\" is valid (CAPA controllers use only one credentials).</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AWSClusterControllerIdentity\nmetadata:\n  name: default\nspec:\n  allowedNamespaces:\n    list:\n      - demo-capi\n</code></pre>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/#awsclusterstaticidentity","title":"AWSClusterStaticIdentity","text":"<p>This authentication method uses static credentials stored in a kubernetes secret that contains the following keys:</p> <ul> <li>AccessKeyID</li> <li>SecretAccessKey</li> </ul> <p>It can also be filtered by permitted namespace</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: \"test-account\"\nspec:\n  secretRef:\n  allowedNamespaces:\n    list:\n      - demo-capi\n</code></pre>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/#awsclusterroleidentity","title":"AWSClusterRoleIdentity","text":"<p>This authentication method uses the STS::AssumeRole API</p>"},{"location":"deployment/cluster-api/providers/aws/21-awsmanagedcontrolplane/#links","title":"Links","text":"<ul> <li>Multi-tenancy</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/multitenancy</p> <ul> <li>Multitenancy setup with EKS and Service Account</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/full-multitenancy-implementation</p>"},{"location":"deployment/cluster-api/providers/aws/31-eks-node-groups/","title":"EKS node groups","text":"<p>In order to deploy an EKS node group we need to create a cluster api resource called  MachinePool that loads 2 Kubernetes Cluster API Provider AWS resources:</p> <ul> <li> <p>AWSMachinePool, that configures the nodes</p> </li> <li> <p>EKSConfig, that configure the node bootstrappin</p> </li> </ul> <pre><code>apiVersion: cluster.x-k8s.io/v1beta2\nkind: MachinePool\nmetadata:\n  name: bottlerocket\nspec:\n  template:\n    spec:\n      clusterName: demo-capi\n      infrastructureRef:\n        apiGroup: infrastructure.cluster.x-k8s.io\n        kind: AWSManagedMachinePool\n        name: bottlerocket\n      bootstrap:\n        configRef:\n          apiGroup: infrastructure.cluster.x-k8s.io\n          kind: EKSConfig\n          name: demo-capi-bootstrap\n</code></pre> <p>Both CRDs are installed EKSConfig when deploying the Kubernetes Cluster API Provider AWS (CAPA)</p>"},{"location":"deployment/cluster-api/providers/aws/31-eks-node-groups/#awsmachinepool","title":"AWSMachinePool","text":""},{"location":"deployment/cluster-api/providers/aws/31-eks-node-groups/#eksconfig","title":"EKSConfig","text":""},{"location":"deployment/cluster-api/providers/aws/31-eks-node-groups/#links","title":"Links","text":"<ul> <li>Cluster api MachinePool</li> </ul> <p>https://cluster-api.sigs.k8s.io/tasks/experimental-features/machine-pools</p> <ul> <li>AWSMachinePool</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/topics/machinepools</p>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/","title":"Observed limitations","text":""},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#eks-auto-mode","title":"EKS auto mode","text":"<p>EKS auto mode is not supported yet</p> <ul> <li>https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/5278</li> <li>https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5642</li> </ul>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#vpc-resource-is-missing-in-aws","title":"VPC resource is missing in AWS","text":"<pre><code>.spec.vpc.id is set but VPC resource is missing in AWS; failed to describe VPC resources. (might be in creation process): failed to query ec2 for VPCs: operation error EC2: DescribeVpcs, exceeded maximum number of attempts, 3, Probable clock skew error\n</code></pre> <p>Permissions?</p> <p>See https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/4191</p>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#nat-gateways","title":"Nat gateways","text":"<p>It is not possible to configure only 1 Nat gateway en a 3 zone eks cluster</p> <p>https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/1323 https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/1484</p>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#coredns-enabled","title":"Coredns enabled","text":"<p>With empty addons coredns is deployed via deployment. Default eks behaviour. Is it related with bootstrapSelfManagedAddons?</p>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#machinepools-and-awsmanagedmachinepool-maturity","title":"MachinePools and AWSManagedMachinePool maturity","text":"<p>Is it mature?</p>"},{"location":"deployment/cluster-api/providers/aws/95-observed-limitations/#auth-with-empty-clusters","title":"Auth with empty clusters","text":"<p>It is neccesary to configure authentication in the controller when no kubernetes clusters are deployed yet</p>"},{"location":"deployment/cluster-api/providers/aws/98-links/","title":"Links","text":"<ul> <li>Github</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api-provider-aws</p> <ul> <li>Book</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/</p> <ul> <li>Kubernetes Cluster API Provider AWS CRDs</li> </ul> <p>https://cluster-api-aws.sigs.k8s.io/crd/</p>"},{"location":"deployment/cluster-api/providers/azure/azure/","title":"Azure (CAPZ)","text":"<p>Github</p> <ul> <li>https://github.com/kubernetes-sigs/cluster-api-provider-azure</li> </ul> <p>Documentation</p> <ul> <li>https://capz.sigs.k8s.io/</li> </ul>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/","title":"Kubeadm (CABPK)","text":"<p>The full name for CABPK is Cluster Api Bootstrap Provider Kubeadm and it is configured via a KubeadmConfig resource</p> <p>This bootstrap provider translates the KubeadmConfig resource into a cloud-init/ignition scripts that will convert a virtual machine into a kubernetes node</p>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#deploying-the-provider","title":"Deploying the provider","text":"<pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: BootstrapProvider\n\n\nmetadata:\n  name: kubeadm\nspec:\n  version: v1.11.2\n</code></pre> <p>The deployed manifest is bootstrap-components.yaml and includes the following CRDs:</p>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#kubeadmconfig","title":"KubeadmConfig","text":"<ul> <li>Immutable resource tied to a specific Machine</li> <li>Created automatically from a KubeadmConfigTemplate when a Machine is provisioned</li> <li>Contains the actual kubeadm configuration for that single machine instance</li> <li>Cannot be reused across multiple machines</li> </ul>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#kubeadmconfigtemplate","title":"KubeadmConfigTemplate","text":"<ul> <li>Template resource defining the configuration pattern</li> <li>Referenced by MachineDeployment, MachineSet, or MachinePool</li> <li>Used to generate individual KubeadmConfig objects for each machine</li> <li>Reusable across multiple machines with the same configuration needs</li> </ul>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#using-kubeadmconfig-or-kubeadmconfigtemplate","title":"Using KubeadmConfig or KubeadmConfigTemplate","text":"<p>Manual KubeadmConfig Creation Usage</p> <ul> <li>Control plane machines - Often created manually or via KubeadmControlPlane</li> <li>One-off machines - Single machines that don't need templating</li> <li>Testing/debugging - Direct configuration without template layer</li> </ul> <p>KubeadmConfigTemplate Usage</p> <ul> <li>Worker node pools - Multiple machines with identical configuration</li> <li>MachineDeployments - Scalable machine groups</li> <li>MachineSets/MachinePools - Any scenario requiring multiple machines from a template</li> </ul>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#kubeadmconfig-fields","title":"KubeadmConfig fields","text":"<p>We can configure the following settings with this provider</p> <p>Boot method</p> <ul> <li>spec.format permits to choose between cloud-init or ignition</li> <li>spec.files permits to pass files to the user data</li> <li>spec.ignition permits to pass ignition configuration</li> </ul> <p>Kubeadm settings</p> <ul> <li>the joinConfiguration</li> <li>the initConfiguration</li> <li>the ClusterConfiguration</li> <li>kubeadm verbosity</li> </ul> <p>Commands</p> <ul> <li>spec.bootCommands run very early in the cloud init boot process</li> <li>spec.preKubeadmCommands run before kubeadm runs</li> <li>spec.postKubeadmCommands run after kubeadm</li> </ul> <p>Node</p> <ul> <li>spec.diskSetup to configure partitions and filesystems</li> <li>spec.mounts to configure mounts</li> <li>spec.ntp to configure ntp settings</li> <li>spec.users to add additional users</li> </ul>"},{"location":"deployment/cluster-api/providers/bootstrap/kubeadm/#links","title":"Links","text":"<p>https://cluster-api.sigs.k8s.io/tasks/bootstrap/kubeadm-bootstrap/</p>"},{"location":"deployment/cluster-api/providers/controlplane/kubeadm/","title":"Kubeadm","text":"<p>Deploy the provider</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: ControlPlaneProvider\nmetadata:\n  name: kubeadm\nspec:\n  version: v1.11.2\n</code></pre> <p>The deployed manifest is control-plane-components.yaml and includes the following CRDs</p> <ul> <li>KubeadmControlPlane</li> <li>KubeadmControlPlaneTemplate</li> </ul>"},{"location":"deployment/cluster-api/providers/controlplane/kubeadm/#settings","title":"Settings","text":"<p>We can configure the following settings</p> <ul> <li>The kubernetes version</li> <li>How to replace the control plane machines with new oness</li> <li>The number of control plane nodes</li> <li>How to initialize and join the control plane nodes, with spec.kubeadmConfigSpec</li> <li>A Infraestructure Provider template, with spec.machineTemplate (AWSMachineTemplate, VSphereMachineTemplate...)</li> </ul>"},{"location":"deployment/cluster-api/providers/controlplane/kubeadm/#links","title":"Links","text":"<p>https://cluster-api.sigs.k8s.io/tasks/bootstrap/kubeadm-bootstrap/kubelet-config https://cluster-api.sigs.k8s.io/tasks/control-plane/kubeadm-control-plane</p>"},{"location":"deployment/cluster-api/providers/core/cluster-api/","title":"Cluster api core provider","text":"<p>Deploy the provider</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: CoreProvider\nmetadata:\n  name: cluster-api\nspec:\n  version: v1.11.2 # Choose the release from &lt;https://github.com/kubernetes-sigs/cluster-api/releases&gt;\n</code></pre> <p>The deployed manifest name is core-components.yaml and includes the capi-system namespace and the following resources</p> <ul> <li>Cluster</li> <li>ClusterClass</li> <li>ClusterResourceSet</li> <li>ClusterResourceSetBinding</li> <li>ExtensionConfig</li> <li>IPAddress</li> <li>IPAddressClaim</li> <li>Machine</li> <li>MachineDeployment</li> <li>MachineDrainRule</li> <li>MachineHealthCheck</li> <li>MachinePool</li> <li>MachineSet</li> </ul>"},{"location":"deployment/cluster-api/providers/google/google-gcp/","title":"Google GCP","text":"<ul> <li>Github</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api-provider-gcp</p>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/","title":"Non-Cloud Infrastructure Providers","text":"<p>Cluster API provides several infrastructure providers designed for environments outside of major cloud platforms. This document explores bare metal, virtualization, and nested cluster providers that enable Kubernetes deployment without dependency on AWS, Azure, GCP, or similar cloud services.</p>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#overview","title":"Overview","text":"<p>Non-cloud infrastructure providers enable Kubernetes cluster provisioning across diverse environments:</p> <ul> <li>Bare Metal: Physical servers without virtualization layers</li> <li>Virtualization: Virtual machines on platforms like KubeVirt, vSphere</li> <li>Nested/Virtual Clusters: Kubernetes clusters running within existing Kubernetes clusters</li> </ul> <p>These providers handle infrastructure lifecycle management, OS provisioning (when applicable), and Kubernetes node bootstrapping.</p>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#virtualization-and-nested-cluster-providers","title":"Virtualization and Nested Cluster Providers","text":""},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#kubevirt","title":"KubeVirt","text":"<p>Run VMs as Kubernetes resources alongside containerized workloads.</p> <p>Key Features:</p> <ul> <li>Native Kubernetes API for VM management</li> <li>VMs run as pods with KVM virtualization</li> <li>Live migration support</li> <li>Integration with Kubernetes networking and storage</li> <li>PCI passthrough support</li> <li>CNCF Status: Incubating (since April 2022)</li> </ul> <p>Links:</p> <ul> <li>https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt</li> <li>https://kubevirt.io/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#vcluster","title":"vcluster","text":"<p>Virtual Kubernetes clusters running inside existing Kubernetes clusters.</p> <p>Key Features:</p> <ul> <li>Lightweight virtual clusters</li> <li>Full Kubernetes API compatibility</li> <li>Certified Kubernetes distribution (100% conformance tests)</li> <li>Resource isolation and multi-tenancy</li> <li>Syncs only necessary resources to host cluster</li> <li>No performance overhead</li> <li>CNCF Status: Not a CNCF project (as of 2025)</li> </ul> <p>Links:</p> <ul> <li>https://github.com/loft-sh/cluster-api-provider-vcluster</li> <li>https://www.vcluster.com/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#harvester","title":"Harvester","text":"<p>Open source hyperconverged infrastructure (HCI) built on Kubernetes.</p> <p>Key Features:</p> <ul> <li>Built on Kubernetes, KubeVirt, and Longhorn</li> <li>Hyperconverged infrastructure (compute, storage, networking)</li> <li>VM lifecycle management</li> <li>Web UI and API</li> <li>Rancher integration</li> <li>Live migration and backup</li> <li>CNCF Status: Not a CNCF project (built on CNCF projects)</li> </ul> <p>Links:</p> <ul> <li>https://github.com/harvester/harvester</li> <li>https://harvesterhci.io/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#dedicated-bare-metal-providers","title":"Dedicated Bare Metal Providers","text":""},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#metal3","title":"Metal3","text":"<p>The most mature and widely adopted bare metal provider for Cluster API.</p> <p>Key Features:</p> <ul> <li>Uses OpenStack Ironic for bare metal provisioning</li> <li>Supports hardware introspection and discovery</li> <li>Handles firmware and BIOS configuration</li> <li>Provides power management capabilities (IPMI, Redfish)</li> <li>Network boot support (PXE, iPXE)</li> <li>CNCF Status: Incubating (since August 2025)</li> </ul> <p>Links:</p> <ul> <li>https://github.com/metal3-io/cluster-api-provider-metal3</li> <li>https://metal3.io/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#tinkerbell","title":"Tinkerbell","text":"<p>A bare metal provisioning framework designed for scalability and flexibility.</p> <p>Key Features:</p> <ul> <li>Workflow-based provisioning system</li> <li>Microservices architecture</li> <li>Hardware data management</li> <li>DHCP and TFTP services included</li> <li>Custom workflow definitions</li> <li>CNCF Status: Sandbox (since November 2020)</li> </ul> <p>Links:</p> <ul> <li>https://github.com/tinkerbell/cluster-api-provider-tinkerbell</li> <li>https://tinkerbell.org/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#sidero-community-maintained","title":"Sidero (Community Maintained)","text":"<p>Note: Sidero Labs is no longer actively developing Sidero Metal. The project is now community-maintained. For new deployments, Sidero Labs recommends using Omni as the alternative.</p> <p>A bare metal provider built specifically for Talos Linux.</p> <p>Key Features:</p> <ul> <li>Talos Linux native integration</li> <li>API-driven bare metal management</li> <li>Automatic hardware discovery</li> <li>Secure boot support</li> <li>Immutable infrastructure approach</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Status:</p> <ul> <li>No longer actively developed by Sidero Labs</li> <li>Community-maintained (Slack support available)</li> <li>Supports Kubernetes v1.34 and Talos v1.11</li> <li>Recommended to use Omni for new deployments</li> </ul> <p>Links:</p> <ul> <li>https://github.com/siderolabs/sidero</li> <li>https://www.sidero.dev/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#omni-sidero-labs-alternative","title":"Omni (Sidero Labs Alternative)","text":"<p>The successor to Sidero from Sidero Labs, providing a SaaS platform for Talos Linux cluster management.</p> <p>Key Features:</p> <ul> <li>Managed Talos Linux platform</li> <li>Multi-cluster management</li> <li>Works with bare metal, cloud, and edge</li> <li>Web-based UI and API</li> <li>Secure by default (WireGuard-based)</li> <li>GitOps integration</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Note: Omni is not a traditional Cluster API provider but rather a complete cluster management platform that can work alongside or instead of Cluster API.</p> <p>Links:</p> <ul> <li>https://github.com/siderolabs/omni</li> <li>https://omni.siderolabs.com/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#hivelocity","title":"Hivelocity","text":"<p>A commercial bare metal cloud provider with Cluster API integration.</p> <p>Key Features:</p> <ul> <li>Managed bare metal infrastructure</li> <li>Global data center presence</li> <li>API-driven provisioning</li> <li>Network configuration management</li> <li>Storage options</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Links:</p> <ul> <li>https://github.com/hivelocity/cluster-api-provider-hivelocity</li> <li>https://www.hivelocity.net/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#flexiblehybrid-providers","title":"Flexible/Hybrid Providers","text":""},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#byoh-bring-your-own-host","title":"BYOH (Bring Your Own Host)","text":"<p>Note: The original VMware Tanzu BYOH provider is no longer actively maintained. An actively maintained fork is available from Platform9.</p> <p>The most flexible option for existing bare metal infrastructure.</p> <p>Key Features:</p> <ul> <li>SSH-based host registration</li> <li>No special hardware requirements</li> <li>Works with Ubuntu 20.04 and 22.04</li> <li>Agent-based approach</li> <li>Supports existing infrastructure</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Status:</p> <ul> <li>Alpha stage (not production-ready)</li> <li>Platform9 fork actively maintained</li> <li>Supports Kubernetes v1.31.*</li> <li>No backwards-compatibility guarantee</li> </ul> <p>Links:</p> <ul> <li>https://github.com/platform9/cluster-api-provider-bringyourownhost (Active fork)</li> <li>https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost (Original, unmaintained)</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#maas-metal-as-a-service","title":"MAAS (Metal as a Service)","text":"<p>Canonical's bare metal provisioning solution.</p> <p>Key Features:</p> <ul> <li>Comprehensive hardware management</li> <li>Network configuration automation</li> <li>Storage configuration</li> <li>Integration with Juju</li> <li>Web UI and API</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Links:</p> <ul> <li>https://github.com/spectrocloud/cluster-api-provider-maas</li> <li>https://maas.io/</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#k0smotron-remotemachine","title":"k0smotron RemoteMachine","text":"<p>SSH-based remote machine provisioning.</p> <p>Key Features:</p> <ul> <li>SSH connectivity</li> <li>k0s distribution focus</li> <li>Simple deployment model</li> <li>No agent required on target</li> <li>CNCF Status: Not a CNCF project</li> </ul> <p>Links:</p> <ul> <li>https://github.com/k0sproject/k0smotron</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#provider-comparison","title":"Provider Comparison","text":""},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#virtualization-and-nested-providers","title":"Virtualization and Nested Providers","text":"Provider Maturity Complexity CNCF Status Infrastructure Type Special Requirements KubeVirt High Medium Incubating VM in K8s Existing K8s cluster vcluster Medium Low Not CNCF Virtual K8s in K8s Existing K8s cluster Harvester Medium Medium Not CNCF (uses CNCF projects) HCI on bare metal Bare metal servers"},{"location":"deployment/cluster-api/providers/infra/non-cloud-providers/#bare-metal-and-hybrid-providers","title":"Bare Metal and Hybrid Providers","text":"Provider Maturity Complexity Hardware Discovery Boot Method Self-Hosted CNCF Status Special Requirements Metal3 High Medium-High Yes PXE/iPXE Yes (BM) Incubating IPMI/Redfish Tinkerbell Medium Medium Yes PXE Yes (BM) Sandbox DHCP/TFTP infrastructure Sidero Medium (Community) Medium Yes PXE Yes (BM) Not CNCF Talos Linux (not actively developed) Omni Medium Low No Agent-based Optional Not CNCF Talos Linux, Omni account Hivelocity Medium Low N/A Managed No Not CNCF Hivelocity account BYOH Low (Alpha) Low No N/A Yes (Both) Not CNCF SSH access (Platform9 fork) MAAS High Medium-High Yes PXE Yes (BM) Not CNCF MAAS server k0smotron Low-Medium Low No N/A Yes (Both) Not CNCF SSH access <p>Legend:</p> <ul> <li>Self-Hosted: Yes (BM) = Bare metal only, Yes (Both) = Bare metal and cloud, Optional = SaaS or self-hosted, No = Managed service only</li> <li>CNCF Status: Incubating/Sandbox = Official CNCF project level, Not CNCF = Independent project, Part of CAPI = Kubernetes subproject</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/vsphere/","title":"VMware Vsphere (CAPV)","text":""},{"location":"deployment/cluster-api/providers/infra/vsphere/#requirements","title":"Requirements","text":"<ul> <li>It needs DHCP in the primary VM Network</li> <li>Configure one resource pool across the hosts onto which the workload clusters will be provisioned</li> <li>Every host in the resource pool will need access to shared storage, such as VSAN in order to be able to make use of MachineDeployments and high-availability control planes</li> <li>To use persistence storage we need the vsphere csi driver</li> <li>We can use virtual machines published in the main README or build them. The machines must have cloudinit/Ignition, kubeadm and a container runtime.</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/vsphere/#notes","title":"Notes","text":"<ul> <li>the default and recomended cloneMode for vsphereMachines is linked clone</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/vsphere/#deployment","title":"Deployment","text":"<p>We need a vsphere-settings secret with credentials and settings. See the release and configuration options in https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: InfrastructureProvider\nmetadata:\n  name: vsphere\nspec:\n  version: v1.14.0\n  configSecret:\n    name: vsphere-settings\n</code></pre> <p>This adds the following cluster scoped crds</p> <ul> <li>VSphereClusterIdentity</li> <li>VSphereDeploymentZone</li> <li>VSphereFailureDomain</li> </ul> <p>and namespace scoped crds</p> <ul> <li>VSphereCluster</li> <li>VSphereClusterTemplate</li> <li>VSphereMachine</li> <li>VSphereMachineTemplate</li> <li>VSphereVM</li> </ul>"},{"location":"deployment/cluster-api/providers/infra/vsphere/#links","title":"Links","text":"<p>Github repo</p> <ul> <li>https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/</li> </ul> <p>Getting started</p> <ul> <li> <p>https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/main/docs/getting_started.md</p> </li> <li> <p>Vsphere CSI Driver</p> </li> </ul> <p>https://github.com/kubernetes-sigs/vsphere-csi-driver</p> <ul> <li>Vsphere Cloud Provider</li> </ul> <p>https://github.com/kubernetes/cloud-provider-vsphere</p>"},{"location":"deployment/cluster-api/resources/","title":"Cluster API Resources","text":""},{"location":"deployment/cluster-api/resources/#operator-crds","title":"Operator CRDs","text":"<p>This resources are created when deploying the operator and uses the operator.cluster.x-k8s.io/v1alpha2 API Group</p> Kind Description AddonProvider Manages addon provider installations BootstrapProvider Manages bootstrap provider installations ControlPlaneProvider Manages control plane provider installations CoreProvider Manages core Cluster API provider installations InfrastructureProvider Manages infrastructure provider installations IPAMProvider Manages IP address management provider installations RuntimeExtensionProvider Manages runtime extension provider installations"},{"location":"deployment/cluster-api/resources/#providers","title":"Providers","text":"<p>Once we have the cluster api operator installed, we need to the deploy the chosen providers. Here is the provider list: https://cluster-api.sigs.k8s.io/reference/providers</p>"},{"location":"deployment/cluster-api/resources/#core-provider-crds","title":"Core Provider CRDs","text":"<p>This resources are created when deploying the cluster-api Core Provider</p> Kind API Group Maturity Description Cluster cluster.x-k8s.io/v1beta2 Stable Represents a Kubernetes cluster managed by Cluster API ClusterClass cluster.x-k8s.io/v1beta2 Beta Defines reusable cluster templates and configurations MachineDeployment cluster.x-k8s.io/v1beta2 Stable Manages declarative updates for machines MachineDrainRule cluster.x-k8s.io/v1beta2 Stable Defines rules for draining machines before deletion MachineHealthCheck cluster.x-k8s.io/v1beta2 Stable Monitors and remediates unhealthy machines MachinePool cluster.x-k8s.io/v1beta2 Beta Manages groups of machines with identical configuration Machine cluster.x-k8s.io/v1beta2 Stable Represents a single machine in the cluster MachineSet cluster.x-k8s.io/v1beta2 Stable Ensures a specified number of machines are running ClusterResourceSetBinding addons.cluster.x-k8s.io/v1beta2 GA Binds ClusterResourceSets to specific clusters ClusterResourceSet addons.cluster.x-k8s.io/v1beta2 GA Defines resources to be applied to matching clusters IPAddressClaim ipam.cluster.x-k8s.io/v1beta2 Stable Claims an IP address from an IPAM provider IPAddress ipam.cluster.x-k8s.io/v1beta2 Stable Represents an allocated IP address ExtensionConfig runtime.cluster.x-k8s.io/v1beta2 Alpha Configures runtime extensions for lifecycle hook customizations"},{"location":"deployment/cluster-api/resources/#provider-crds","title":"Provider CRDs","text":"<p>When deploying other providers than the Core Provider, every provider installs its own additional CRDs but they all have the same apiVersion, regarding the provider. It only changes the name of the CRD.</p> Provider Type API Group Bootstrap bootstrap.cluster.x-k8s.io ControlPlane controlplane.cluster.x-k8s.io Infrastructure infrastructure.cluster.x-k8s.io IPAM ipam.cluster.x-k8s.io Addon addons.cluster.x-k8s.io Runtime runtime.cluster.x-k8s.io <p>To see all installed provider CRDs filtered by provider type we can do a kubectl api-resources  | grep API_GROUP command</p> <p>The provider CRDs permits to configure the different cluster api resources deployed by the core provider.</p> <p>This tables exclude IPAM, addon and IPAM resources</p> Resource Bootstrap Provider Infrastructure Provider ControlPlane Provider Cluster - Cluster ControlPlane ClusterClass ConfigTemplate (workers) ClusterTemplate, MachineTemplate, MachinePoolTemplate ControlPlaneTemplate Machine Config Machine - MachineSet ConfigTemplate MachineTemplate - MachineDeployment ConfigTemplate MachineTemplate - MachinePool ConfigTemplate MachinePoolTemplate -"},{"location":"deployment/cluster-api/resources/#provider-crd-reference-examples","title":"Provider CRD Reference Examples","text":"<p>Cluster resource references:</p> <ul> <li>Infrastructure Provider: <code>AWSCluster</code>, <code>AzureCluster</code>, <code>GCPCluster</code>, <code>VSphereCluster</code></li> <li>ControlPlane Provider: <code>KubeadmControlPlane</code>, <code>AWSManagedControlPlane</code>, <code>AzureManagedControlPlane</code>, <code>MicroK8sControlPlane</code></li> </ul> <p>Machine resource references:</p> <ul> <li>Bootstrap Provider: <code>KubeadmConfig</code>, <code>MicroK8sConfig</code></li> <li>Infrastructure Provider: <code>AWSMachine</code>, <code>AzureMachine</code>, <code>GCPMachine</code>, <code>VSphereMachine</code></li> </ul> <p>MachineSet, MachineDeployment resources reference templates:</p> <ul> <li>Bootstrap Provider: <code>KubeadmConfigTemplate</code>, <code>MicroK8sConfigTemplate</code></li> <li>Infrastructure Provider: <code>AWSMachineTemplate</code>, <code>AzureMachineTemplate</code>, <code>GCPMachineTemplate</code>, <code>VSphereMachineTemplate</code></li> </ul> <p>MachinePool resource references templates:</p> <ul> <li>Bootstrap Provider: <code>KubeadmConfigTemplate</code></li> <li>Infrastructure Provider: <code>AWSMachinePool</code>, <code>AzureMachinePool</code>, <code>GCPMachinePool</code>, <code>VSphereMachinePool</code></li> </ul> <p>ClusterClass resource references multiple templates:</p> <ul> <li>Infrastructure Provider: <code>AWSClusterTemplate</code>, <code>AzureClusterTemplate</code>, <code>GCPClusterTemplate</code>, <code>VSphereClusterTemplate</code> (cluster), and corresponding <code>MachineTemplate</code> resources</li> <li>ControlPlane Provider: <code>KubeadmControlPlaneTemplate</code>, <code>AWSManagedControlPlaneTemplate</code></li> <li>Bootstrap Provider: <code>KubeadmConfigTemplate</code> (for workers)</li> </ul>"},{"location":"deployment/cluster-api/resources/cluster/","title":"Cluster","text":"<p>This is the goal of cluster api.</p> <p>When creating a cluster we need to define:</p> <ul> <li>The control plane configuration   no This is done with a resource provided by a control plane provider</li> </ul> <p>Examples: <code>KubeadmControlPlane</code>, <code>AWSManagedControlPlane</code>, <code>AzureManagedControlPlane</code>, <code>MicroK8sControlPlane</code></p> <ul> <li>The kubernetes cluster configuration</li> </ul> <p>This is done with a resource provided by an infraestructure provider</p> <p>Examples: <code>AWSCluster</code>, <code>AzureCluster</code>, <code>GCPCluster</code>, <code>VSphereCluster</code></p>"},{"location":"deployment/crossplane/providers-and-mrap/","title":"Crossplane Providers and Managed Resource Activation Policies","text":""},{"location":"deployment/crossplane/providers-and-mrap/#overview","title":"Overview","text":"<p>This guide explains the relationship between Providers and Managed Resource Activation Policies (MRAP) in Crossplane v2, and how to configure them effectively.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#understanding-providers","title":"Understanding Providers","text":""},{"location":"deployment/crossplane/providers-and-mrap/#what-are-providers","title":"What Are Providers?","text":"<p>Providers are packages that extend Crossplane with the ability to manage external resources (AWS, GCP, Azure, Kubernetes, etc.). Each provider:</p> <ul> <li>Installs Custom Resource Definitions (CRDs) for managed resources</li> <li>Runs a controller that reconciles those resources</li> <li>Manages the lifecycle of cloud infrastructure</li> </ul>"},{"location":"deployment/crossplane/providers-and-mrap/#provider-types","title":"Provider Types","text":"<p>Monolithic Providers (legacy):</p> <ul> <li>Single package containing all resources for a cloud provider</li> <li>Example: <code>provider-aws</code> (includes S3, EC2, RDS, etc.)</li> <li>Larger footprint, slower updates</li> </ul> <p>Family Providers (modular):</p> <ul> <li>Split into smaller, focused packages</li> <li>Example: <code>provider-aws-s3</code>, <code>provider-aws-ec2</code>, <code>provider-aws-rds</code></li> <li>Install only what you need</li> <li>Better performance and faster updates</li> </ul>"},{"location":"deployment/crossplane/providers-and-mrap/#installing-providers","title":"Installing Providers","text":"<p>Providers are installed using the <code>Provider</code> resource:</p> <pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-aws-s3\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws-s3:v2.0.0\n</code></pre> <p>When a provider is installed:</p> <ol> <li>Crossplane downloads the package</li> <li>CRDs are created in the cluster</li> <li>Provider controller is deployed</li> <li>Provider becomes available for use</li> </ol> <p>Check provider status:</p> <pre><code>kubectl get providers\nkubectl get providerrevisions\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#understanding-managed-resource-activation-policies-mrap","title":"Understanding Managed Resource Activation Policies (MRAP)","text":""},{"location":"deployment/crossplane/providers-and-mrap/#what-are-mraps","title":"What Are MRAPs?","text":"<p>MRAPs are Crossplane v2 resources that control which managed resource types Crossplane actively reconciles.</p> <p>Key concept: Installing a provider creates CRDs, but MRAP determines if Crossplane watches and reconciles resources of those types.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#why-mraps-exist","title":"Why MRAPs Exist","text":"<p>Performance: Limit reconciliation to only the resources you actually use Security: Explicit control over what resource types can be managed Multi-tenancy: Different namespaces can have different activation policies Scalability: Reduce overhead in clusters with many providers installed</p>"},{"location":"deployment/crossplane/providers-and-mrap/#the-default-catch-all-mrap","title":"The Default Catch-All MRAP","text":"<p>After upgrading to Crossplane v2, a default catch-all MRAP may be created that activates all managed resource types.</p> <p>Problem with catch-all:</p> <ul> <li>Reconciles every CRD, even unused ones</li> <li>Poor performance at scale</li> <li>Less explicit control</li> <li>Higher resource consumption</li> </ul> <p>Best practice: Delete the default catch-all and create specific MRAPs.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#creating-specific-mraps","title":"Creating Specific MRAPs","text":"<p>Create targeted activation policies for only the resource types you use:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-storage-resources\nspec:\n  activate:\n    - buckets.s3.aws.upbound.io          # v1 cluster-scoped\n    - buckets.s3.aws.m.upbound.io        # v2 namespaced\n    - bucketpolicies.s3.aws.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#provider-mrap-workflow","title":"Provider + MRAP Workflow","text":"<p>The complete flow for using managed resources:</p> <pre><code>1. Install Provider\n   \u2514\u2500&gt; Creates CRDs (e.g., buckets.s3.aws.upbound.io)\n\n2. Create MRAP\n   \u2514\u2500&gt; Activates reconciliation for specific CRD types\n\n3. Deploy ProviderConfig\n   \u2514\u2500&gt; Configures credentials for the provider\n\n4. Create Managed Resources\n   \u2514\u2500&gt; Crossplane reconciles them (because MRAP activates them)\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#installation-example","title":"Installation Example","text":""},{"location":"deployment/crossplane/providers-and-mrap/#step-1-install-the-provider","title":"Step 1: Install the Provider","text":"<pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-aws-s3\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws-s3:v2.0.0\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-2-create-mrap","title":"Step 2: Create MRAP","text":"<pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-s3-activation\nspec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-3-configure-provider-credentials","title":"Step 3: Configure Provider Credentials","text":"<pre><code>apiVersion: aws.m.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-config\n  namespace: production\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      key: credentials\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-4-create-managed-resource","title":"Step 4: Create Managed Resource","text":"<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  name: my-bucket\n  namespace: production\nspec:\n  forProvider:\n    region: us-east-1\n  providerConfigRef:\n    name: aws-config\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#mrap-best-practices","title":"MRAP Best Practices","text":""},{"location":"deployment/crossplane/providers-and-mrap/#1-be-specific","title":"1. Be Specific","text":"<p>Only activate resource types you actually use:</p> <p>Bad (activates everything):</p> <pre><code>spec:\n  activate:\n    - \"*\"  # Avoid this\n</code></pre> <p>Good (explicit list):</p> <pre><code>spec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - instances.ec2.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#2-include-both-v1-and-v2-during-migration","title":"2. Include Both v1 and v2 During Migration","text":"<p>During the v1 to v2 migration period, include both cluster-scoped and namespaced versions:</p> <pre><code>spec:\n  activate:\n    - buckets.s3.aws.upbound.io      # v1 (cluster-scoped)\n    - buckets.s3.aws.m.upbound.io    # v2 (namespaced)\n</code></pre> <p>Once migration is complete, remove v1 entries.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#3-organize-by-function","title":"3. Organize by Function","text":"<p>Create separate MRAPs for different resource groups:</p> <pre><code>---\napiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: storage-resources\nspec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n---\napiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: compute-resources\nspec:\n  activate:\n    - instances.ec2.aws.m.upbound.io\n    - securitygroups.ec2.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#4-document-your-activation-policies","title":"4. Document Your Activation Policies","text":"<p>Maintain clear documentation of which MRAPs exist and why, so teams know what resources are available.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#managing-multiple-providers","title":"Managing Multiple Providers","text":"<p>When using multiple providers (AWS, GCP, Azure), create organized MRAPs:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: multi-cloud-storage\nspec:\n  activate:\n    # AWS S3\n    - buckets.s3.aws.m.upbound.io\n    # GCP Storage\n    - buckets.storage.gcp.m.upbound.io\n    # Azure Storage\n    - accounts.storage.azure.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/crossplane/providers-and-mrap/#resources-not-reconciling","title":"Resources Not Reconciling","text":"<p>Problem: Created a managed resource but it's not being reconciled</p> <p>Solutions:</p> <ol> <li>Check if provider is installed and healthy: <code>kubectl get providers</code></li> <li>Verify MRAP includes the resource type:    <code>kubectl get managedresourceactivationpolicy -o yaml</code></li> <li>Check provider logs for errors</li> </ol>"},{"location":"deployment/crossplane/providers-and-mrap/#mrap-not-found","title":"MRAP Not Found","text":"<p>Problem: Error about missing MRAP after upgrade</p> <p>Solution: Create an MRAP for the resource types you're using. In v2, MRAPs are required.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#too-many-resources-activated","title":"Too Many Resources Activated","text":"<p>Problem: Performance issues, high memory usage</p> <p>Solution: Review MRAPs and remove unused resource types from activation lists.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#checking-active-resources","title":"Checking Active Resources","text":"<p>To see which resource types are activated:</p> <pre><code>kubectl get managedresourceactivationpolicy -o yaml\n</code></pre> <p>To list all CRDs installed by providers:</p> <pre><code>kubectl get crd | grep -E '(aws|gcp|azure).*upbound'\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#migration-considerations","title":"Migration Considerations","text":"<p>When migrating from v1 to v2:</p> <ol> <li>Providers continue working - Existing providers don't need    immediate changes</li> <li>MRAPs are new - You must create MRAPs in v2 for reconciliation    to work</li> <li>Both versions coexist - Include both v1 and v2 resource types    in MRAPs during migration</li> <li>Cleanup after migration - Remove v1 resource types from MRAPs    once migration is complete</li> </ol>"},{"location":"deployment/crossplane/providers-and-mrap/#summary","title":"Summary","text":"Component Purpose Required? Provider Installs CRDs and controllers Yes MRAP Activates reconciliation for resource types Yes (v2) ProviderConfig Configures credentials Yes Managed Resource Actual infrastructure to create Yes <p>Key takeaway: Providers install capabilities, MRAPs activate them, ProviderConfigs configure them, and Managed Resources use them.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#references","title":"References","text":"<ul> <li>Crossplane Provider Documentation</li> <li>Managed Resource Activation Policies</li> <li>Upbound Marketplace</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/","title":"Upgrading Crossplane from v1 to v2","text":""},{"location":"deployment/crossplane/upgrading-to-v2/#overview","title":"Overview","text":"<p>This guide covers the complete upgrade process from Crossplane v1 to v2, with a focus on environments using standalone managed resources (not Compositions).</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#prerequisites","title":"Prerequisites","text":"<p>Before upgrading to Crossplane v2, ensure:</p> <ol> <li>Running Crossplane v1.20 - Upgrade to v1.20 first if on an earlier version</li> <li>Remove deprecated features:</li> <li>Native patch and transform compositions (replaced by composition functions)</li> <li>ControllerConfig type (replaced by DeploymentRuntimeConfig)</li> <li>External secret stores (no longer supported)</li> <li>Default registry flags (use fully qualified image names)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#whats-new-in-v2","title":"What's New in v2","text":"<ul> <li>Namespaced managed resources - Resources are now namespace-scoped instead of cluster-scoped</li> <li>Managed Resource Activation Policies (MRAP) - Control which resources Crossplane reconciles</li> <li>Composition functions - More flexible composition approach</li> <li>Improved multi-tenancy - Better isolation with namespace-scoped resources</li> <li>Breaking changes - Some v1 features removed</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#upgrade-process","title":"Upgrade Process","text":""},{"location":"deployment/crossplane/upgrading-to-v2/#step-1-pre-upgrade-assessment","title":"Step 1: Pre-Upgrade Assessment","text":"<ol> <li>Verify Crossplane version (check that you're running v1.20)</li> <li>Inventory your resources (list all managed resources and providers)</li> <li>Check for deprecated features:</li> <li>Review compositions for native patch and transform</li> <li>Check for ControllerConfig resources</li> <li>Verify no external secret stores are configured</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-2-upgrade-crossplane-core","title":"Step 2: Upgrade Crossplane Core","text":"<ol> <li>Upgrade to v2</li> <li>Verify the upgrade (check deployment and logs)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-3-upgrade-provider-packages","title":"Step 3: Upgrade Provider Packages","text":"<ol> <li>Update provider manifests</li> </ol> <p>Ensure providers use fully qualified image names and v2-compatible versions:</p> <p><code>yaml    apiVersion: pkg.crossplane.io/v1    kind: Provider    metadata:      name: provider-aws-s3    spec:      package: xpkg.upbound.io/upbound/provider-aws-s3:v2</code></p> <ol> <li>Deploy updated providers</li> </ol> <p>Apply the updated provider manifests.</p> <ol> <li>Verify provider health</li> </ol> <p>Check that providers are healthy and running:</p> <p><code>bash    kubectl get providers    kubectl get providerrevisions</code></p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-4-verify-and-test","title":"Step 4: Verify and Test","text":"<ol> <li>Check that existing cluster-scoped managed resources continue working</li> <li>Verify providers are healthy and running</li> <li>Test that existing infrastructure remains functional</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#complete-upgrade-checklist","title":"Complete Upgrade Checklist","text":"<ul> <li>[ ] Verify running Crossplane v1.20</li> <li>[ ] Inventory all managed resources and providers</li> <li>[ ] Remove deprecated features (ControllerConfig, external secret stores, etc.)</li> <li>[ ] Upgrade Crossplane core to v2</li> <li>[ ] Upgrade provider packages to v2-compatible versions</li> <li>[ ] Verify all existing resources continue working</li> <li>[ ] Test existing infrastructure functionality</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#best-practices","title":"Best Practices","text":"<ol> <li>Test in staging first - Perform the upgrade in a non-production environment</li> <li>Gradual migration - Migrate resources incrementally, not all at once</li> <li>Keep both versions during transition - Maintain both cluster-scoped and namespaced resources during migration</li> <li>Use orphan deletion policy - Set <code>deletionPolicy: Orphan</code> for safety during migration</li> <li>Monitor closely - Watch logs and metrics during and after the upgrade</li> <li>Document your process - Keep detailed notes of your specific migration steps</li> <li>Backup configurations - Export all resource definitions before upgrading</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#day-2-operations","title":"Day 2 Operations","text":"<p>After successfully upgrading Crossplane to v2, you can begin adopting v2 features at your own pace. Existing v1 cluster-scoped resources continue working indefinitely alongside new v2 namespaced resources.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#understanding-coexistence","title":"Understanding Coexistence","text":"<p>Crossplane v2 supports both cluster-scoped (v1) and namespaced (v2) resources simultaneously. This allows you to:</p> <ul> <li>Run v2 without immediately migrating existing resources</li> <li>Test namespaced resources while keeping production on cluster-scoped resources</li> <li>Migrate resources gradually over time</li> <li>Maintain both resource types indefinitely if needed</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#configure-managed-resource-activation-policies-mrap","title":"Configure Managed Resource Activation Policies (MRAP)","text":"<p>In Crossplane v2, you must explicitly specify which managed resources should be reconciled using MRAPs.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-1-review-default-mrap","title":"Step 1: Review Default MRAP","text":"<p>By default, Crossplane v2 may create a catch-all MRAP that activates all resource types. Review existing policies:</p> <pre><code>kubectl get managedresourceactivationpolicy -o yaml\n</code></pre>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-2-create-targeted-mraps","title":"Step 2: Create Targeted MRAPs","text":"<p>Create activation policies for your specific resource types:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-resources\nspec:\n  activate:\n    - buckets.s3.aws.upbound.io        # v1 cluster-scoped\n    - buckets.s3.aws.m.upbound.io      # v2 namespaced\n    - instances.ec2.aws.upbound.io\n    - instances.ec2.aws.m.upbound.io\n</code></pre> <p>Important: Include both cluster-scoped (<code>.aws.upbound.io</code>) and namespaced (<code>.aws.m.upbound.io</code>) resource types during the migration period.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-3-delete-default-catch-all-optional","title":"Step 3: Delete Default Catch-All (Optional)","text":"<p>For better performance and security, delete the default catch-all MRAP after creating specific policies.</p> <p>See the Providers and MRAP guide for more details.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#migrating-to-namespaced-managed-resources","title":"Migrating to Namespaced Managed Resources","text":"<p>When ready, you can migrate existing cluster-scoped resources to namespaced versions.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#understanding-the-resource-changes","title":"Understanding the Resource Changes","text":"<ul> <li>v1 (cluster-scoped): <code>s3.aws.upbound.io/v1beta2</code></li> <li>v2 (namespaced): <code>s3.aws.m.upbound.io/v1beta1</code></li> </ul> <p>The <code>.m.</code> indicates a namespaced managed resource.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#migration-strategies","title":"Migration Strategies","text":"<p>Choose based on your resource type and tolerance for downtime:</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#option-a-create-first-migration","title":"Option A: Create-First Migration","text":"<p>Use when duplicate resources are acceptable or downtime is tolerable.</p> <ol> <li>Deploy the new namespaced resource</li> <li>Wait for the resource to become ready</li> <li>Update application references (ConfigMaps, Secrets, etc.)</li> <li>Delete the old cluster-scoped resource</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#option-b-orphan-and-adopt-migration","title":"Option B: Orphan-and-Adopt Migration","text":"<p>Use for resources with globally unique names (like S3 buckets) or zero-downtime requirements.</p> <ol> <li>Set <code>deletionPolicy: Orphan</code> on the old resource</li> <li>Delete the old Crossplane resource (cloud resource remains)</li> <li>Deploy the new namespaced resource with the same cloud resource name</li> <li>Verify the resource was adopted (check events and status)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#migrate-providerconfigs","title":"Migrate ProviderConfigs","text":"<p>ProviderConfigs also need to be namespaced:</p> <p>Before (v1)</p> <pre><code>apiVersion: aws.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-provider\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      namespace: crossplane-system\n      key: credentials\n</code></pre> <p>After (v2)</p> <pre><code>apiVersion: aws.m.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-provider\n  namespace: production-infrastructure\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      key: credentials\n</code></pre> <p>Note: The secret reference no longer needs a namespace when ProviderConfig is namespaced (it uses the same namespace).</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#create-namespace-structure","title":"Create Namespace Structure","text":"<p>Plan and create namespaces for your managed resources:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production-infrastructure\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: staging-infrastructure\n</code></pre>"},{"location":"deployment/crossplane/upgrading-to-v2/#exploring-other-v2-features","title":"Exploring Other v2 Features","text":"<p>Beyond namespaced resources, consider exploring:</p> <ol> <li>Composition functions - More powerful than patch and transform</li> <li>Multi-tenancy - Use namespaces to isolate teams/environments</li> <li>Refined MRAP policies - Optimize which resources are actively reconciled</li> <li>Updated monitoring - Adjust dashboards for namespace-scoped resources</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#references","title":"References","text":"<ul> <li>Crossplane v2 Upgrade Guide</li> <li>Managed Resource Activation Policies</li> <li>Upbound Provider Documentation</li> </ul>"},{"location":"git/detached-head/","title":"Detached head","text":"<p>A detached HEAD is a Git state where HEAD points directly to a specific commit instead of pointing to a branch reference.</p> <ul> <li>Normal state</li> </ul> <pre><code>git checkout branch\n</code></pre> <pre><code>HEAD -&gt; main -&gt; commit abc123\nHEAD points to main, which points to a commit.\n</code></pre> <ul> <li>Detached HEAD:</li> </ul> <pre><code>git checkout commitid\n</code></pre> <pre><code>HEAD -&gt; commit abc123\nHEAD points directly to a commit, not through a branch.\n</code></pre>"},{"location":"git/detached-head/#notes","title":"Notes","text":"<ul> <li> <p>CI/CD pipelines often checkout specific commits (like GitLab does with git checkout 35ff590b)</p> </li> <li> <p>If you want to push the branch, you can get this error</p> </li> </ul> <pre><code>error: src refspec MYBRANCH does not match any\nerror: failed to push some refs to 'MYREPO'\n</code></pre> <p>This is because git looks for a local branch named MYBRANCH to push. In detached HEAD state, there's no local main branch reference, so it fails.</p> <p>Solution: Using git push origin HEAD:MYBRANCH pushes whatever HEAD points to (the commit) directly to the remote main branch</p>"},{"location":"gitops/get-raw-yaml/","title":"Ways to get a raw yaml","text":""},{"location":"gitops/get-raw-yaml/#from-kustomize","title":"From kustomize","text":"<p>If using kustomize based specifications, we can use</p> <pre><code>kustomize build\nkubectl kustomize\n</code></pre>"},{"location":"gitops/get-raw-yaml/#from-helm","title":"From helm","text":"<p>If using native helm, we can use</p> <pre><code>helm template\n</code></pre> <p>If using helmCharts inside a kustomization yaml, we wan use</p> <pre><code>kustomize build --enable-helm\nkubectl kustomize --enable-helm\n</code></pre> <p>https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/helmcharts/</p> <p>It has some limitations and it will be deprecated in favour of KRM functions</p> <p>https://github.com/kubernetes-sigs/kustomize/issues/4401</p>"},{"location":"gitops/get-raw-yaml/#using-krm-kpt-functions","title":"Using KRM (kpt) functions","text":"<p>KRM functions are pluggable, reusable scripts or containers that automate the transformation, validation, and generation of Kubernetes YAML resources, enabling powerful and flexible configuration workflows.</p> <p>However, support in kustomize and kubectl kustomize is still experimental or alpha, and may not be recommended for production use yet. The ecosystem is growing, but you should check the documentation and maturity of the specific tool and function you plan to use.</p> <p>https://kpt.dev/book/02-concepts/03-functions</p>"},{"location":"gitops/get-raw-yaml/#gitops","title":"Gitops","text":""},{"location":"gitops/get-raw-yaml/#from-an-argocd-applcation","title":"From an argocd applcation","text":"<p>We can use</p> <pre><code>argocd app manifest &lt;app-name&gt;\n</code></pre>"},{"location":"gitops/get-raw-yaml/#fluxcd","title":"Fluxcd","text":"<p>FluxCD\u2019s source-controller can fetch and output raw manifests from Git, Helm, or OCI sources.</p>"},{"location":"gitops/raw-manifests-vs-helm/","title":"Raw Manifests vs Helm in GitOps","text":"<p>Trade-offs between raw YAML manifests and Helm charts in a GitOps workflow, covering transparency, tooling complexity, and operational concerns.</p>"},{"location":"gitops/raw-manifests-vs-helm/#raw-manifests","title":"Raw Manifests","text":"<p>Raw manifests are plain Kubernetes YAML files committed directly to Git. Tools like ArgoCD and FluxCD can sync them to a cluster without additional processing.</p>"},{"location":"gitops/raw-manifests-vs-helm/#advantages-of-raw-manifests","title":"Advantages of Raw Manifests","text":"<ul> <li>No additional tooling required beyond <code>kubectl</code></li> <li>Full transparency: what you see in Git is exactly what gets applied</li> <li>Simple debugging: no templating layer to reason about</li> <li>Works natively with Kustomize for environment-specific overlays</li> <li>Easier to review in pull requests</li> </ul>"},{"location":"gitops/raw-manifests-vs-helm/#disadvantages-of-raw-manifests","title":"Disadvantages of Raw Manifests","text":"<ul> <li>Repetition across environments (dev, staging, production)</li> <li>Managing environment-specific differences requires Kustomize overlays</li> <li>Risk of configuration drift between environments if not carefully managed</li> <li>Harder to reuse across multiple projects or teams</li> </ul>"},{"location":"gitops/raw-manifests-vs-helm/#helm-charts","title":"Helm Charts","text":"<p>Helm is a package manager for Kubernetes that uses templates to generate manifests dynamically. Charts bundle all resources for an application and expose configurable values.</p>"},{"location":"gitops/raw-manifests-vs-helm/#advantages-of-helm-charts","title":"Advantages of Helm Charts","text":"<ul> <li>Parameterized configuration through <code>values.yaml</code></li> <li>Single chart can target multiple environments with different values files</li> <li>Large ecosystem of community charts (e.g., Artifact Hub)</li> <li>Atomic upgrades and rollbacks via Helm release history</li> <li>Encapsulates application complexity behind a clean interface</li> </ul>"},{"location":"gitops/raw-manifests-vs-helm/#disadvantages-of-helm-charts","title":"Disadvantages of Helm Charts","text":"<ul> <li>Not declarative: <code>helm install/upgrade/rollback</code> are imperative commands</li> <li>Templating adds complexity: Go templates can be hard to read and debug</li> <li><code>helm template</code> output must be inspected to understand what is applied</li> <li>Debugging requires understanding both the chart logic and rendered output</li> <li>Upstream chart changes may introduce unexpected modifications</li> <li>Chart dependencies can create version conflicts</li> </ul>"},{"location":"gitops/raw-manifests-vs-helm/#comparison-table","title":"Comparison Table","text":"Aspect Raw Manifests Helm Charts Declarative Yes No Complexity Low Medium to High Reusability Low High Transparency High Medium Environment handling Kustomize overlays Values files Rollback Git revert <code>helm rollback</code> Community ecosystem Limited Large Debugging ease High Medium"},{"location":"gitops/raw-manifests-vs-helm/#gitops-tool-integration","title":"GitOps Tool Integration","text":""},{"location":"gitops/raw-manifests-vs-helm/#argocd","title":"ArgoCD","text":"<p>ArgoCD syncs plain manifests from Git declaratively. Point the Application source to the directory containing the rendered YAML files.</p> <pre><code>spec:\n  source:\n    repoURL: https://github.com/my-org/my-repo\n    path: manifests/my-app\n    targetRevision: HEAD\n</code></pre>"},{"location":"gitops/raw-manifests-vs-helm/#manifest-generate-paths-annotation","title":"manifest-generate-paths annotation","text":"<p>The <code>argocd.argoproj.io/manifest-generate-paths</code> annotation on an Application tells ArgoCD which repository paths should trigger a manifest regeneration when a webhook event fires. Without it, every push to the repo triggers a refresh for all applications, regardless of what changed.</p> <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/manifest-generate-paths: /manifests/my-app\n</code></pre> <p>Multiple paths are separated by <code>;</code>. With raw manifests the path is explicit and predictable (the directory containing the YAMLs). With Helm, the scope is harder to narrow down because values files and chart dependencies may live across multiple locations.</p> <p>This annotation is one of the strongest performance arguments for raw manifests in large monorepos with many ArgoCD Applications.</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/high_availability/#webhook-and-manifest-paths-annotation</p>"},{"location":"gitops/raw-manifests-vs-helm/#fluxcd","title":"FluxCD","text":"<p>FluxCD uses dedicated CRDs for each approach:</p> <ul> <li><code>Kustomization</code> CR for raw manifests (with optional Kustomize processing)</li> <li><code>HelmRelease</code> CR for Helm charts managed by the helm-controller</li> </ul>"},{"location":"gitops/raw-manifests-vs-helm/#rendered-manifest-pattern","title":"Rendered Manifest Pattern","text":"<p>The recommended GitOps approach: render all templates in CI and commit the resulting plain YAML to Git. The GitOps operator syncs those static files \u2014 no rendering happens at deploy time.</p> <pre><code># CI pipeline: render once, commit the output\nhelm template my-release my-chart -f values-prod.yaml &gt; manifests/my-app.yaml\n# or\nkustomize build overlays/production &gt; manifests/my-app.yaml\n</code></pre> <ul> <li>Helm and Kustomize act as CI rendering tools, not as deploy tools</li> <li><code>manifest-generate-paths</code> paths are trivially precise: the rendered folder</li> <li>Pull request diffs show exact manifest changes, not template diffs</li> <li>Rollback is a <code>git revert</code> on the rendered files \u2014 no Helm state involved</li> </ul> <p>See get-raw-yaml for rendering commands.</p>"},{"location":"gitops/raw-manifests-vs-helm/#recommendations","title":"Recommendations","text":"<ul> <li>Use the rendered manifest pattern as the primary GitOps approach</li> <li>Use Helm or Kustomize as CI rendering tools, committing plain YAML to Git</li> <li>Avoid <code>kubectl apply</code> or <code>helm install</code> in a GitOps workflow \u2014 these are   imperative and bypass Git as the source of truth</li> </ul>"},{"location":"gitops/argocd/98-production-tips/","title":"Production tips","text":""},{"location":"gitops/argocd/98-production-tips/#installation-id","title":"Installation Id","text":"<p>Settings location: argocd-cm Configmap</p> <p>Optional installation id. Allows to have multiple installations of Argo CD in the same cluster.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  installationID: \"my-unique-id\"\n</code></pre> <p>If you are managing one cluster using multiple Argo CD instances, you will need to set <code>installationID</code> in the Argo CD ConfigMap. This will prevent conflicts between the different Argo CD instances:</p> <ul> <li>Each managed resource will have the annotation <code>argocd.argoproj.io/installation-id: &lt;installation-id&gt;</code></li> <li>It is possible to have applications with the same name in Argo CD instances without causing conflicts.</li> </ul>"},{"location":"gitops/argocd/argocd-secret-eso/","title":"argocd-secret via extenalsecret","text":""},{"location":"gitops/argocd/argocd-secret-eso/#intro","title":"Intro","text":"<p>The argocd-secret kubernetes secret stores some information self managed by argocd like:</p> <pre><code>admin.password\nadmin.passwordMtime\nserver.secretkey\ntls.crt\ntls.key\n</code></pre> <p>It also supports other settings related with the webhooks token</p> <pre><code>webhook.github.secret: \nwebhook.gitlab.secret: \nwebhook.bitbucket.uuid: \nwebhook.bitbucketserver.secret: \nwebhook.gogs.secret: \n</code></pre> <p>and also additional users/accounts related info</p> <p>We can get that info with</p> <pre><code>kubectl get secret argocd-secret -o yaml --show-managed-fields\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#the-problem","title":"The problem","text":"<p>The problem with this is that argocd is the owner of those fields. If we want to patch this content, mixing that selfmanaged fields with other controllers, like external-secrets-operator, we can get some unexpected results, like the deletion of the managed fields</p> <p></p> <p>If the deletion occurs, those fields can be recreated restarting the argocd-server server</p> <pre><code>kubectl rollout restart deployment argocd-server -n argocd\n</code></pre> <p>Also,the annotation argocd.argoproj.io/tracking-id can be change all the time between:</p> <pre><code>myapp:external-secrets.io/ExternalSecret:argocd/argocd-secret\nmyapp:/Secret:argocd/argocd-secret\n</code></pre> <p>Making the owner of that field different at every change</p> <ul> <li>argocd-controller</li> <li>externalsecrets.external-secrets.io/argocd-secret</li> </ul>"},{"location":"gitops/argocd/argocd-secret-eso/#solution-1-via-externalsecret","title":"Solution 1: Via ExternalSecret","text":"<ul> <li>argocd.argoproj.io/compare-options: IgnoreExtraneous</li> </ul> <p>Because the owner changes all the time, this ignores that frequent out of sync only.</p> <ul> <li> <p>argocd.argoproj.io/sync-options: Prune=false</p> </li> <li> <p>creationPolicy: Merge</p> </li> </ul> <p>The external secrets operator does not create the secret. It only merges the values with an existing secret</p> <ul> <li>deletionPolicy: Merge</li> </ul> <p>If the secret is deleted from the provider, external secrets operator simply removes the keys from the secret, not the secret itself</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: argocd-secret\nspec:\n  data:\n    - remoteRef:\n        key: secret/argocd-webhook-token\n      secretKey: webhook.github.secret\n  target:\n    creationPolicy: Merge\n    deletionPolicy: Merge\n    template:\n      metadata:\n        annotations:\n          argocd.argoproj.io/sync-options: Prune=false\n          argocd.argoproj.io/compare-options: IgnoreExtraneous\n...\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#solution-2-move-our-setting-to-another-externalsecret","title":"Solution 2: Move our setting to another (external)secret","text":"<p>Argocd permits to put the value of a configmap or secret in our own custom secret. This permits to let external secrets operator to manage that external secret without touch the original one.</p> <ul> <li>Create our custom secret</li> </ul> <p>First we have to create an (external)secret with our value</p> <p>It must have the app.kubernetes.io/part-of: argocd label</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: argocd-custom-secret\nspec:\n  data:\n    - remoteRef:\n        key: secret/argocd-webhook-token \n      secretKey: webhook.github.secret\n  target:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/part-of: argocd\n...\n</code></pre> <ul> <li>Link the original place with the custom secret</li> </ul> <p>Then we go to the original place where that value is configured an make a link to our new secret with the following format:</p> <pre><code>KEY: $CUSTOM-SECRET-NAME:KEY-IN-CUSTOM-SECRET\n</code></pre> <p>For example</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-secret\ntype: Opaque\nstringData:\nwebhook.github.secret: $argocd-custom-secret:webhook.github.secret\n</code></pre> <ul> <li>Restart workload</li> </ul> <p>Depending of what workload and setting is, we can probably need to restart the workload. For example</p> <pre><code>kubectl rollout restart deployment argocd-server -n argocd\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#notes","title":"Notes","text":"<ul> <li> <p>This solution puts $argocd-custom-secret:webhook.github.secret as the value in the beginning. The restart changes it with the value in the custom secret.</p> </li> <li> <p>If we are doing the link with a kustomize patch and it is not applied, we must probably delete the secret first. This removes the self managed data including admin password and certificates, and the argocd needs to be restarted to recreate them</p> </li> </ul>"},{"location":"gitops/argocd/argocd-source/","title":".argocd-source.yaml","text":"<p>The <code>.argocd-source.yaml</code> file is a special file placed in the source application directory inside the Git repository. ArgoCD reads it during manifest generation and merges its content with the application source fields defined in the <code>Application</code> manifest.</p> <p>!!! warning \"GitOps purity and vendor lock-in\"     The ArgoCD documentation itself flags parameter overrides as \"an anti-pattern to GitOps\", because the effective source of truth becomes a union of the Git repository and the override state \u2014 not Git alone. Additionally, this file is an ArgoCD-specific convention: a plain <code>kustomize build</code> or any other toolchain will silently ignore it, coupling the repository to ArgoCD internals.</p> <pre><code>The most GitOps-aligned use is the **write-back** pattern with argocd-image-updater, where automated tooling commits updated values back to Git through this file. Avoid using it as a general-purpose override mechanism in production.\n</code></pre>"},{"location":"gitops/argocd/argocd-source/#use-cases","title":"Use Cases","text":"<p>It solves two main problems:</p> <ul> <li>Write-back: Provides a unified Git-native way to override application parameters, enabling tools like argocd-image-updater to write updated image tags back to the repository without modifying the <code>Application</code> manifest directly.</li> <li>Application discovery: Supports discovering applications in Git repositories for projects like ApplicationSet (see git files generator).</li> </ul>"},{"location":"gitops/argocd/argocd-source/#file-naming","title":"File Naming","text":"Scenario Filename Shared overrides for all apps at a path <code>.argocd-source.yaml</code> Overrides for a specific app <code>.argocd-source-&lt;appname&gt;.yaml</code> Specific app + <code>apps-in-any-namespace</code> feature <code>.argocd-source-&lt;namespace&gt;_&lt;appname&gt;.yaml</code> <p>When both a generic <code>.argocd-source.yaml</code> and an app-specific file exist, the generic file is merged first, then the app-specific file is merged on top (allowing per-app overrides).</p>"},{"location":"gitops/argocd/argocd-source/#content","title":"Content","text":"<p>The file supports the same fields as the <code>source</code> section of an <code>Application</code> manifest. Only the tool-specific override fields are relevant here.</p>"},{"location":"gitops/argocd/argocd-source/#kustomize","title":"Kustomize","text":"<p>Each entry in <code>images</code> is a string following the <code>kustomize edit set image</code> syntax. ArgoCD matches images by the left-hand side (the original image name) and applies the override. Multiple images from the same app can be listed, one per entry:</p> Format Effect <code>image:newTag</code> Override only the tag <code>image=newImage</code> Rename the image, keep original tag <code>image=newImage:newTag</code> Rename and override the tag <code>image=newImage@digest</code> Rename and pin to a digest <pre><code>kustomize:\n  images:\n    - nginx:1.27.0                              # override tag only\n    - postgres=my-registry/postgres:15-alpine   # rename + tag\n    - myapp=myapp@sha256:abc123def456           # rename + pin digest\n    - sidecar=quay.io/myorg/sidecar:v2.0.0     # different registry + tag\n</code></pre> <p>A deployment with containers <code>nginx</code>, <code>postgres</code>, <code>myapp</code>, and <code>sidecar</code> would have each image matched and replaced independently.</p> <p>Override the Kustomize version (ArgoCD v3.2+):</p> <pre><code>kustomize:\n  version: v4.5.7\n</code></pre>"},{"location":"gitops/argocd/argocd-source/#helm","title":"Helm","text":"<p>Override Helm parameters:</p> <pre><code>helm:\n  parameters:\n    - name: image.tag\n      value: v1.2.3\n    - name: replicaCount\n      value: \"3\"\n</code></pre> <p>Override values inline:</p> <pre><code>helm:\n  values: |\n    image:\n      tag: v1.2.3\n    replicaCount: 3\n</code></pre> <p>Override value files:</p> <pre><code>helm:\n  valueFiles:\n    - values-prod.yaml\n</code></pre>"},{"location":"gitops/argocd/argocd-source/#merge-behavior","title":"Merge Behavior","text":"<p>ArgoCD merges the content of <code>.argocd-source.yaml</code> into the <code>Application</code> spec at render time. Fields present in the file override the corresponding fields in the <code>Application</code> manifest. Fields absent in the file remain as defined in the manifest.</p>"},{"location":"gitops/argocd/argocd-source/#links","title":"Links","text":"<ul> <li>Parameter overrides documentation   https://argo-cd.readthedocs.io/en/stable/user-guide/parameters/#store-overrides-in-git</li> <li>argocd-image-updater write-back   https://argocd-image-updater.readthedocs.io/en/stable/basics/update-methods/</li> </ul>"},{"location":"gitops/argocd/settings-stable/","title":"Settings table","text":""},{"location":"gitops/argocd/settings-stable/#list-of-argocd-settings","title":"List of argocd settings","text":"<pre><code>AppC = application controller\nServer = argocd server\nAppSC = applicationset controller\nRepo = repo server\n</code></pre> Variable Setting in the configmap configmap AppC Server AppSC Repo REDIS_PASSWORD argocd-redis x x x ARGOCD_CONTROLLER_REPLICAS hardcoded x ARGOCD_RECONCILIATION_TIMEOUT timeout.reconciliation argocd-cm x x ARGOCD_HARD_RECONCILIATION_TIMEOUT timeout.hard.reconciliation argocd-cm x ARGOCD_RECONCILIATION_JITTER timeout.reconciliation.jitter argocd-cm x ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS controller.repo.error.grace.period.seconds argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH argocd-cmd-params-cm x ARGOCD_APP_STATE_CACHE_EXPIRATION argocd-cmd-params-cm x REDIS_SERVER argocd-cmd-params-cm x x x REDIS_COMPRESSION argocd-cmd-params-cm x x x REDISDB argocd-cmd-params-cm x x x ARGOCD_DEFAULT_CACHE_EXPIRATION argocd-cmd-params-cm x x x ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS argocd-cmd-params-cm x ARGOCD_APPLICATION_NAMESPACES argocd-cmd-params-cm x ARGOCD_CONTROLLER_SHARDING_ALGORITHM argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT argocd-cmd-params-cm x ARGOCD_K8SCLIENT_RETRY_MAX argocd-cmd-params-cm x x ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF argocd-cmd-params-cm x x ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF argocd-cmd-params-cm x ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT argocd-cmd-params-cm x ARGOCD_HYDRATOR_ENABLED argocd-cmd-params-cm x x ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING argocd-cmd-params-cm x ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL argocd-cmd-params-cm x KUBECACHEDIR hardcoded x ARGOCD_SERVER_INSECURE argocd-cmd-params-cm x ARGOCD_SERVER_BASEHREF argocd-cmd-params-cm x ARGOCD_SERVER_ROOTPATH argocd-cmd-params-cm x ARGOCD_SERVER_LOGFORMAT argocd-cmd-params-cm x ARGOCD_SERVER_LOG_LEVEL argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER argocd-cmd-params-cm x ARGOCD_SERVER_DISABLE_AUTH argocd-cmd-params-cm x ARGOCD_SERVER_ENABLE_GZIP argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_SERVER_X_FRAME_OPTIONS argocd-cmd-params-cm x ARGOCD_SERVER_CONTENT_SECURITY_POLICY argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_TLS_MIN_VERSION argocd-cmd-params-cm x x ARGOCD_TLS_MAX_VERSION argocd-cmd-params-cm x x ARGOCD_TLS_CIPHERS argocd-cmd-params-cm x x ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_OIDC_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_LOGIN_ATTEMPTS_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_STATIC_ASSETS argocd-cmd-params-cm x ARGOCD_APP_STATE_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_MAX_COOKIE_NUMBER argocd-cmd-params-cm x ARGOCD_SERVER_LISTEN_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_METRICS_LISTEN_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_INSECURE argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_HEADERS argocd-cmd-params-cm x ARGOCD_APPLICATION_NAMESPACES argocd-cmd-params-cm x ARGOCD_SERVER_ENABLE_PROXY_EXTENSION argocd-cmd-params-cm x ARGOCD_API_CONTENT_TYPES argocd-cmd-params-cm x ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS argocd-cmd-params-cm x"},{"location":"gitops/argocd/applicationset/templating/","title":"Templating","text":"<p>There are 2 ways to define the application template that an applicationset will create:</p> <ul> <li>Fast template</li> <li>Go template + Sprig function library</li> </ul>"},{"location":"gitops/argocd/applicationset/templating/#fast-template","title":"Fast template","text":"<p>By default, an argocd applicationset uses fastemplate as template engine to define an application.</p> <p>According the argocd documentation, it will be deprecated. https://github.com/argoproj/argo-cd/issues/10858</p>"},{"location":"gitops/argocd/applicationset/templating/#go-text-template","title":"Go text template","text":"<p>It will replace fast template and it must be activated in the applicationset definition with goTemplate: true. The go text template is more powerful than the fast template and provides some functions</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: my appset\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n</code></pre> <p>In addition to the go template, also the Sprig function library can be used with goTemplate enabled</p>"},{"location":"gitops/argocd/applicationset/templating/#gotemplateoptions","title":"goTemplateOptions","text":"<p>It is possible to add some goTemplateOptions The recommendeed goTemplateOptions is [\"missingkey=error\"]. This makes the applicationset fail is a parameter defined in the template does not exist.</p>"},{"location":"gitops/argocd/applicationset/templating/#template-patch","title":"Template Patch","text":"<p>pending</p>"},{"location":"gitops/argocd/applicationset/templating/#links","title":"Links","text":"<ul> <li>Argocd Templates</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Template/</p> <ul> <li>Argocd Go Template</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/GoTemplate/</p> <ul> <li>fasttemplate</li> </ul> <p>https://github.com/valyala/fasttemplate</p> <ul> <li>Go text template</li> </ul> <p>https://pkg.go.dev/text/template</p> <ul> <li>Sprig function library</li> </ul> <p>https://masterminds.github.io/sprig/</p>"},{"location":"gitops/argocd/clusters/add-a-cluster/","title":"Add a cluster","text":"<p>We want to update the credentials for a kubernetes cluster added to argocd.</p> <p>This has been tested with a kubeadm cluster (not a eks cluster or similar)</p> <p>We will have</p> <ul> <li>The kubeconfig file of the argocd main instance that will receive the new cluster</li> <li>The kubeconfig file of the argocd instance we want to add to the main instance</li> </ul>"},{"location":"gitops/argocd/clusters/add-a-cluster/#login-to-the-argocd-main-instance","title":"Login to the argocd main instance","text":"<p>For that we need:</p> <pre><code>argocd login --grpc-web MY-ARGOCD-SERVER # Log in our argocd server\nargocd login --sso --grpc-web MY-ARGOCD-SERVER # or via sso\n</code></pre>"},{"location":"gitops/argocd/clusters/add-a-cluster/#context","title":"Context","text":"<p>Get the context for the cluster we want to add</p> <pre><code>KUBECONFIG=PATH_TO_KUBECONFIG kubectl config get-contexts\n</code></pre>"},{"location":"gitops/argocd/clusters/add-a-cluster/#add-a-temporary-cluster","title":"Add a temporary cluster","text":"<p>Then add the cluster with a temporary name</p> <pre><code>argocd cluster add MYCONTEXT --kubeconfig PATH_TO_KUBECONFIG --name CLUSTER_NAME --project MY_PROJECT --cluster-endpoint kubeconfig --grpc-web \n</code></pre> <p>This creates</p> <ul> <li>a ServiceAccount \"argocd-manager\"</li> <li>a ClusterRole \"argocd-manager-role\"</li> <li>a ClusterRoleBinding \"argocd-manager-role-binding\"</li> <li>a Created bearer token secret for ServiceAccount \"argocd-manager\"</li> </ul>"},{"location":"gitops/argocd/clusters/add-a-cluster/#notes","title":"Notes","text":"<p>The temporary name permits to explore the new secret created in the argocd namespace and get the config key that stores the cluster configuration</p> <p>Copying that config to an existing cluster fixes the following error:</p> <pre><code>the server has asked for the client to provide credentials\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/","title":"Add an EKS Cluster to ArgoCD","text":""},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#overview","title":"Overview","text":"<p>Two methods to register an EKS cluster:</p> <ul> <li>CLI (<code>argocd cluster add</code>): bootstraps a cluster Secret automatically. Useful for the initial setup.</li> <li>Declarative Secret: a <code>v1/Secret</code> with label <code>argocd.argoproj.io/secret-type: cluster</code>.   Preferred for GitOps.</li> </ul> Scenario ArgoCD location Auth method Same account EKS in the same AWS account IRSA or Pod Identity Cross-account EKS in a different AWS account IRSA or Pod Identity (cross-account assumption) Outside AWS On-prem / other cloud Bearer token or AWS profile"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#prerequisites","title":"Prerequisites","text":"<p>EKS authentication modes \u2014 two ways to map IAM identities to Kubernetes RBAC:</p> <ul> <li>ConfigMap (<code>aws-auth</code>): legacy mode, edit <code>kube-system/aws-auth</code>.</li> <li>EKS API (access entries): newer mode, must be enabled on the cluster. The IAM principal that created   the cluster gets an <code>AmazonEKSClusterAdminPolicy</code> access entry by default.</li> </ul> <p>OIDC provider \u2014 required for IRSA only. Each EKS cluster has an OIDC URL (EKS console \u2192 Overview \u2192 Details). Verify it is registered in IAM \u2192 Identity Providers. If missing, create it: provider type <code>OpenID Connect</code>, audience <code>sts.amazonaws.com</code>.</p> <p>IRSA (IAM Roles for Service Accounts) \u2014 lets Kubernetes service accounts assume IAM roles via OIDC federation. Requires OIDC provider setup and a service account annotation.</p> <p>Pod Identity \u2014 newer alternative to IRSA. Uses the EKS Pod Identity Agent add-on (a DaemonSet) instead of OIDC federation. No OIDC provider setup and no service account annotation needed. Requires ArgoCD to run on EKS.</p> <p>IAM roles carry temporary credentials and are assumable by any authorized principal. An ARN uniquely identifies every AWS resource.</p>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#scenarios-1-2-same-account-and-cross-account","title":"Scenarios 1 &amp; 2: Same Account and Cross-Account","text":"<p>See irsa-setup.md for the full walkthrough covering both IRSA and Pod Identity: management role, cluster role, access entries, and the cluster Secret YAML.</p>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#scenario-3-argocd-outside-aws","title":"Scenario 3: ArgoCD Outside AWS","text":"<p>Option A \u2014 Bearer token (CLI): use <code>argocd cluster add</code> with a kubeconfig that has valid AWS credentials. See CLI Registration.</p> <p>Option B \u2014 AWS profile (v2.10+): store AWS credentials in a Kubernetes Secret and reference a named profile:</p> <pre><code>config: |\n  {\n    \"awsAuthConfig\": { \"clusterName\": \"&lt;CLUSTER_NAME&gt;\", \"profile\": \"&lt;AWS_PROFILE_NAME&gt;\" },\n    \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"&lt;BASE64_CA_DATA&gt;\" }\n  }\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#cli-registration","title":"CLI Registration","text":"<pre><code># 1. Get the kubeconfig of the target cluster\naws configure list-profiles\nexport AWS_PROFILE=&lt;TARGET_PROFILE&gt;\naws eks update-kubeconfig --kubeconfig /path/to/target.yaml --name &lt;CLUSTER_NAME&gt;\n\n# 2. Identify the context name\nexport KUBECONFIG=/path/to/target.yaml\nkubectl config get-contexts\n\n# 3. Log in to ArgoCD (pick one)\nargocd login --grpc-web &lt;ARGOCD_FQDN&gt;\nargocd login --sso &lt;ARGOCD_FQDN&gt;\n# In-cluster (core mode):\nexport KUBECONFIG=/path/to/argocd-manager.yaml &amp;&amp; argocd login --core\n\n# 4. Register the cluster\nargocd cluster add &lt;CONTEXT_NAME&gt; \\\n  --name &lt;DESIRED_CLUSTER_NAME&gt; \\\n  --kubeconfig /path/to/target.yaml \\\n  --cluster-endpoint kubeconfig\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#final-steps","title":"Final Steps","text":"<p>After CLI registration, ArgoCD creates a Secret with a long auto-generated name. Export and manage it declaratively:</p> <pre><code>kubectl get secret                          # locate the generated secret\nkubectl get secret &lt;GENERATED_NAME&gt; -o yaml &gt; /path/to/clusters/&lt;CLUSTER&gt;.yaml\n</code></pre> <p>Edit the exported file: remove <code>creationTimestamp</code>, <code>resourceVersion</code>, <code>uid</code>, and managed-fields annotations. Set a meaningful <code>name</code>. Add the file to <code>kustomization.yaml</code>, commit and push, sync, verify the cluster is healthy, then delete the original Secret.</p>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#common-errors","title":"Common Errors","text":"<p><code>NoCredentialProviders</code> \u2014 ArgoCD pod cannot find AWS credentials. For IRSA: verify the service account annotation and OIDC provider registration. For Pod Identity: verify the add-on is installed and the pod identity associations exist.</p> <p><code>argocd-k8s-auth failed with exit code 20</code> \u2014 cluster API is unreachable (network / security group issue) or the assumed role has no access entry in the target cluster.</p>"},{"location":"gitops/argocd/clusters/eks/add-eks-cluster/#links","title":"Links","text":"<ul> <li>ArgoCD declarative setup \u2014 EKS</li> <li>EKS cluster authentication</li> <li>EKS access entries</li> <li>IRSA \u2014 IAM roles for service accounts</li> <li>IAM roles</li> <li>IAM roles \u2014 terms and concepts</li> <li>ArgoCD on AWS with multiple clusters</li> </ul>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/","title":"EKS + ArgoCD: IAM Authentication Setup","text":"<p>Two options when ArgoCD and the target EKS cluster run in AWS: IRSA (requires OIDC provider, service account annotation) and Pod Identity (requires EKS Pod Identity Agent add-on, no annotation needed). The cluster Secret format and target cluster role are identical for both \u2014 only the management role trust policy and ArgoCD service account setup differ.</p>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#scenario-1-same-account","title":"Scenario 1: Same Account","text":""},{"location":"gitops/argocd/clusters/eks/irsa-setup/#argocd-management-iam-role","title":"ArgoCD management IAM role","text":"<p>Create a role that both <code>argocd-application-controller</code> and <code>argocd-server</code> will assume. Trust policy:</p> <pre><code>{\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"Federated\": \"arn:aws:iam::&lt;ARGOCD_ACCOUNT_ID&gt;:oidc-provider/oidc.eks.&lt;REGION&gt;.amazonaws.com/id/&lt;OIDC_ID&gt;\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n      \"StringEquals\": {\n        \"oidc.eks.&lt;REGION&gt;.amazonaws.com/id/&lt;OIDC_ID&gt;:sub\": [\n          \"system:serviceaccount:argocd:argocd-application-controller\",\n          \"system:serviceaccount:argocd:argocd-server\"\n        ]\n      }\n    }\n  }]\n}\n</code></pre> <p>Permissions policy: allow <code>sts:AssumeRole</code> on <code>*</code> (or scoped to specific cluster role ARNs).</p> <p>Annotate both ArgoCD service accounts with the management role ARN:</p> <pre><code>eks.amazonaws.com/role-arn: arn:aws:iam::&lt;ARGOCD_ACCOUNT_ID&gt;:role/&lt;ARGOCD_MANAGEMENT_ROLE&gt;\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#target-cluster-iam-role","title":"Target cluster IAM role","text":"<p>Create a role in the target cluster's account. Trust policy \u2014 allow the management role to assume it:</p> <pre><code>{\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n      \"AWS\": \"arn:aws:iam::&lt;ARGOCD_ACCOUNT_ID&gt;:role/&lt;ARGOCD_MANAGEMENT_ROLE&gt;\"\n    },\n    \"Action\": \"sts:AssumeRole\"\n  }]\n}\n</code></pre> <p>Grant cluster access: add the role to <code>aws-auth</code> (ConfigMap mode) or create an access entry with <code>AmazonEKSClusterAdminPolicy</code> (EKS API mode).</p>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#cluster-secret","title":"Cluster Secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    argocd.argoproj.io/secret-type: cluster\n  name: &lt;CLUSTER-NAME&gt;\ntype: Opaque\nstringData:\n  name: \"arn:aws:eks:&lt;REGION&gt;:&lt;TARGET_ACCOUNT_ID&gt;:cluster/&lt;CLUSTER_NAME&gt;\"\n  server: \"https://&lt;CLUSTER_ID&gt;.gr7.&lt;REGION&gt;.eks.amazonaws.com\"\n  config: |\n    {\n      \"awsAuthConfig\": {\n        \"clusterName\": \"&lt;CLUSTER_NAME&gt;\",\n        \"roleARN\": \"arn:aws:iam::&lt;TARGET_ACCOUNT_ID&gt;:role/&lt;TARGET_CLUSTER_ROLE&gt;\"\n      },\n      \"tlsClientConfig\": { \"insecure\": false, \"caData\": \"&lt;BASE64_CA_DATA&gt;\" }\n    }\n</code></pre> <p>Retrieve endpoint and CA data:</p> <pre><code>aws eks describe-cluster --name &lt;CLUSTER_NAME&gt; --query \"cluster.endpoint\" --output text\naws eks describe-cluster --name &lt;CLUSTER_NAME&gt; --query \"cluster.certificateAuthority.data\" --output text\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#scenario-2-cross-account","title":"Scenario 2: Cross-Account","text":"<p>Identical to Scenario 1. The management role trust policy and cluster Secret format are unchanged \u2014 only the account IDs differ. The <code>roleARN</code> in the cluster Secret config points to a role in a different AWS account.</p>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#alternative-eks-pod-identity","title":"Alternative: EKS Pod Identity","text":"<p>Pod Identity replaces IRSA on the ArgoCD cluster side. The target cluster role and cluster Secret are unchanged. Requires the EKS Pod Identity Agent add-on installed on the ArgoCD cluster.</p> Aspect IRSA Pod Identity Trust policy principal OIDC federated identity <code>pods.eks.amazonaws.com</code> service Service account annotation Required Not needed OIDC provider setup Required Not required ArgoCD must run on EKS No Yes"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#management-role-trust-policy","title":"Management role trust policy","text":"<p>Replace the OIDC-based trust with:</p> <pre><code>{\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" },\n    \"Action\": [\"sts:AssumeRole\", \"sts:TagSession\"]\n  }]\n}\n</code></pre>"},{"location":"gitops/argocd/clusters/eks/irsa-setup/#pod-identity-associations","title":"Pod identity associations","text":"<p>No service account annotation. Create an association for each ArgoCD service account instead:</p> <pre><code>aws eks create-pod-identity-association \\\n  --cluster-name &lt;ARGOCD_CLUSTER_NAME&gt; \\\n  --namespace argocd \\\n  --service-account argocd-application-controller \\\n  --role-arn arn:aws:iam::&lt;ARGOCD_ACCOUNT_ID&gt;:role/&lt;ARGOCD_MANAGEMENT_ROLE&gt;\n\naws eks create-pod-identity-association \\\n  --cluster-name &lt;ARGOCD_CLUSTER_NAME&gt; \\\n  --namespace argocd \\\n  --service-account argocd-server \\\n  --role-arn arn:aws:iam::&lt;ARGOCD_ACCOUNT_ID&gt;:role/&lt;ARGOCD_MANAGEMENT_ROLE&gt;\n</code></pre>"},{"location":"gitops/argocd/deletion/00-index/","title":"Deletion","text":"<p>There are several ways to disable automatic deletion of things in argocd</p>"},{"location":"gitops/argocd/deletion/00-index/#settings-table","title":"Settings table","text":"Setting Level Goal finalizer ApplicationSet Permits to enable foreground/background deletion of Application spec.syncPolicy.applicationsSync ApplicationSet / Controller Prevent an/all ApplicationSet delete applications spec.syncPolicy.preserveResourcesOnDeletion ApplicationSet Does not delete Application resources whe the Applicationset is deleted finalizer Application Non-cascade does not delete Application resources when the Application is deleted spec.syncPolicy.automated Application If ommited, disables applying new changes spec.syncPolicy.automated.prune Application \"False\" disables pruning resources not defined in the target state dry-run mode Controller Prevent all ApplicationSet doing actions argocd.argoproj.io/sync-options: Delete=false Resource argocd.argoproj.io/sync-options: Prune=false Resource"},{"location":"gitops/argocd/deletion/10-application/","title":"Application deletion","text":""},{"location":"gitops/argocd/deletion/10-application/#finalizers","title":"Finalizers","text":"<p>A kubernetes finalizer gives the responsability to a controller to prevent resource deletion. In argocd we can specify a finalizer in an Application via metadata.finalizers field.</p> <p>In this case, the  finalizer configures how the kubernetes resources defined in the application will be deleted.</p>"},{"location":"gitops/argocd/deletion/10-application/#cascade-deletion","title":"Cascade deletion","text":"<p>Configuring a finalizer in an Application enables the cascade deletion. The application is not deleted is not deleted inmediately. Kubernetes marks the resource for deletion, but delegates the deletion of the child resources to the Argocd controller, that performs cleanup before allowing the resource to be deleted.</p> <p>There are 2 ways to perform a cascade deletion, also called propagation policies:</p>"},{"location":"gitops/argocd/deletion/10-application/#foreground-cascade-propagation-policy","title":"foreground cascade propagation policy","text":"<p>This is a synchronous deletion. The deletion of the Application is locked until all child resources are successfully deleted, when the controller removes the finalizer. Slower, but this does a clean an ordered deletion of child resources.</p> <p>The foreground cascade deletion finalizer is resources-finalizer.argocd.argoproj.io</p> <pre><code>metadata:\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\n</code></pre>"},{"location":"gitops/argocd/deletion/10-application/#background-cascade-propagation-policy","title":"background cascade propagation policy","text":"<p>This is an asynchronous deletion. The Argocd controller initiates the deletion of the child resources in the background, but the deletion of the Application is not locked. It is a faster but may leave orphaned resources if deletion fails.</p> <p>The background cascade deletion finalizer is resources-finalizer.argocd.argoproj.io/background</p> <pre><code>metadata:\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io/background\n</code></pre>"},{"location":"gitops/argocd/deletion/10-application/#non-cascading-orphan-deletion","title":"Non cascading (orphan) deletion","text":"<p>When deleting Application with no finalizer, no resources will be deleted, only the Application</p> <p>This can be useful:</p> <ul> <li>when we want to keep resources running but manage them differently</li> <li>moving resources between Application or ApplicationSet without downtime</li> <li>remove the Application but don't want to risk deleting critical resources</li> </ul> <p>The default finalizer for an Application is foreground cascade deletion</p> <ul> <li>The ApplicationSet has no default finalizer</li> </ul>"},{"location":"gitops/argocd/deletion/10-application/#application-manual-deletion","title":"Application manual deletion","text":"<p>Via argocd binary</p> <pre><code># foreground\nargocd app delete APPNAME \nargocd app delete APPNAME --cascade # or\nargocd app delete APPNAME --cascade --propagation-policy foreground # or\n# background\nargocd app delete APPNAME --cascade --propagation-policy background # or\n# orphan\nargocd app delete APPNAME --cascade=false\n</code></pre> <p>Via kubectl</p> <pre><code># foreground\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io\"]}}' --type merge\nkubectl delete app APPNAME\n# background\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io/background\"]}}' --type merge\nkubectl delete app APPNAME\n# orphan\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": null}}' --type merge\nkubectl delete app APPNAME\n</code></pre>"},{"location":"gitops/argocd/deletion/20-applicationset/","title":"ApplicationSet","text":""},{"location":"gitops/argocd/deletion/20-applicationset/#relation-between-the-the-applicationset-and-generated-applications","title":"Relation between the the ApplicationSet and generated Applications","text":"<p>The generated Applications by an ApplicationSet have:</p> <ul> <li>In .metadata.ownerReferences, a reference to the ApplicationSet as owner</li> <li>In .metadata.finalizers a resources-finalizer.argocd.argoproj.io finalizer if the ApplicationSet has .syncPolicy.preserveResourcesOnDeletion as false</li> <li>By default they have the resources-finalizer.argocd.argoproj.io finalizer</li> </ul>"},{"location":"gitops/argocd/deletion/20-applicationset/#deleting-an-applicationset","title":"Deleting an ApplicationSet","text":"<p>When an ApplicationSet is deleted, this will occur in order</p> <ul> <li> <p>the ApplicationSet is deleted</p> </li> <li> <p>the generated Applications are deleted (because of the owner reference)</p> </li> <li> <p>the deployed resources created in that Application are deleted</p> </li> </ul> <p>There are 3 ways to control how this deletion is done via finalizers</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#default-no-finalizer","title":"Default (no finalizer)","text":"<p>By default an ApplicationSet has not finalizer. This means the argocd applicationset controller will not manage the deletion of the ApplicationSet. It will be done using kubernetes garbage collector.</p> <ul> <li>Nothing blocks or delays its deletion. The ApplicationSet is deleted inmediately</li> <li>This performs a cascade deletion of the Applications and resources, because of the owner reference</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/</p> <p>If we want to delete an ApplicationSet resource, while preventing Applications (and their deployed resources) from being deleted, we can use a non-cascading delete:</p> <pre><code>kubectl delete ApplicationSet (NAME) --cascade=orphan\n</code></pre>"},{"location":"gitops/argocd/deletion/20-applicationset/#using-argocd-finalizer","title":"Using argocd finalizer","text":"<p>We can add a finalizer an ApplicationSet. This makes the applicationset controller responsible to manage how the Applications and resources are deleted</p> <ul> <li>Foreground</li> </ul> <p>The foreground finalizer blocks deletion until all Applications are deleted and ensures complete and ordered cleanup.</p> <ul> <li>Background</li> </ul> <p>The background finalizer initiates the deletion in the background. Faster, but may leave resources if deletion of child resources fails.</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#preserve-applications-resources","title":"Preserve Application's resources","text":"<p>If we want to preserve the deletion of the Application's resources we can enable spec.syncPolicy.preserveResourcesOnDeletion in the ApplicationSet.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  syncPolicy:\n    preserveResourcesOnDeletion: true\n</code></pre> <p>This removes the resources-finalizer.argocd.argoproj.io finalizer in the generated Applications so a non cascading (orphan) deletion is performed in the Application</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#applicationset-permissions","title":"ApplicationSet permissions","text":"<p>We can configure if an ApplicationSet can create, update and delete its discovered applications.</p> <p>This is different than when an ApplicationSet is deleted.</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#dry-run","title":"Dry run","text":"<p>At controller level we can disable all modifications that the ApplicationSets can do in Application in an argocd instance.</p> <p>This is done enabling the dryrun mode (--dryrun parameter) via the data.ApplicationSetcontroller.dryrun key in the argocd-cmd-params-cm ConfigMap</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#policy-at-applicationset-level","title":"Policy at ApplicationSet level","text":"<p>Also we can control the individual actions an ApplicationSet can to in its applications via the spec.syncPolicy.applicationsSync setting with the following values:</p> Action create update delete sync (default) \u2611 \u2611 \u2611 create-only \u2611 \u2612 \u2612 create-delete \u2611 \u2612 \u2611 create-update \u2611 \u2611 \u2612 dry run mode \u2612 \u2612 \u2612 <p>So a to prevent that deletion we can use, for example</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  syncPolicy:\n    applicationsSync: create-update\n</code></pre> <p>This also can be configured at ApplicationSet controller level via --policy parameter. This setting exists in the data.ApplicationSetcontroller.policy key in the argocd-cmd-params-cm ConfigMap. This setting takes precedence over all ApplicationSets configuration although we can change this behaviour via data.ApplicationSetcontroller.enable.policy.override in the argocd-cmd-params-cm ConfigMap</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/","title":"Safe changes to an Applicationset","text":"<p>Changing an Applicationset can be dangerous because by default it can create and remove applications. So this is a workaround to avoid disgusting situations</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-disable-autosync-in-parent-application","title":"Preparation: Disable autosync in parent Application","text":"<p>!!! danger \"This is critical\"</p> <pre><code>If the Applicationset itself is deployed using gitops, disable autosync (at least, the prune option) in the application that manages our Applicationset.\n</code></pre>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-disable-autosync-in-generated-applications","title":"Preparation: Disable autosync in generated Applications","text":"<p>This is optional, depending of the changes. We can disable autoSync in the generated Applications (spec.template.spec.syncPolicy)</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-prevent-applications-being-deleted","title":"Preparation: Prevent applications being deleted","text":"<p>If your have modified things in the the generators, this can potentially delete applications.</p> <p>We can control this adding temporarily the spec.syncPolicy.applicationsSync: create-update setting to our Applicationset. This prevents our Applicationset from deleting applications.</p> <p>But we have a problem here. How to identify what applications will be deleted when we remove the create-update setting from the Applicationset?. We can add a version label to the generated applications in the current (old) Applicationset.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Applicationset\nmetadata:\n  name: myappset\nspec:\n  syncPolicy:\n    applicationsSync: create-update\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/version: v0.0.1\n</code></pre> <p>Commit and sync this changes to the current Applicationset</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#add-your-changes","title":"Add your changes","text":"<p>Next step is to add the changes:</p> <ul> <li>The changes you really want to do to the Applicationset</li> <li>Increase the version label to v0.0.2</li> </ul> <p>Commit and push the changes. The changes to the Applicationset will not be applied because we have disable autoSync</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#check-name","title":"Check name","text":"<p>With our Applicationset with pending changes:</p> <ul> <li>Explore the differences between both versions</li> <li>Check the Applicationset's name does not change and the old one will not be deleted (pruned).</li> </ul> <p>!!! danger \"\"</p> <pre><code>The default behaviour when deleting an Applicationset is delete the Applicationset itself and all the generated applications and resources.\n</code></pre>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#sync-your-changes","title":"Sync your changes","text":"<p>If you are ok with the changes sync the Applicationset resource</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#check-if-it-will-delete-applications","title":"Check if it will delete Applications","text":"<p>The new changes are propagated to the Applications. We must check the labels in the Applications generated by our Applicationset. The Applications with v0.0.2 are ok, but if any has v0.0.1, they will be delete when removing applicationsSync</p> <p>!!! warning \"\"     Remember to check the label only in the applications that this Applicationset generates</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#remove-protections","title":"Remove protections","text":"<p>If all generated Applications are ok and with v0.0.2, we can</p> <ul> <li>remove the applicationsSync option</li> <li>enable autoSyn in the application that manages our Applicationset</li> </ul>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/","title":"Safe changes to Resources","text":""},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#remove-from-argocd-not-from-the-cluster","title":"Remove from argocd, not from the cluster","text":"<p>If we want to remove a kubernetes resources from an argocd Application we can make it in different ways.</p>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#with-the-tracking-method","title":"With the tracking method","text":"<p>When argocd manages a kubernetes marks it to know it is being managed.</p> <p>Since argocd 3.0 the default tracking method is adding an annotation to the resource</p> <pre><code>argocd.argoproj.io/tracking-id\n</code></pre> <p>Until 3.0 the default tracking method was adding the app.kubernetes.io/instance label, and that can cause some errors because it is a well known label included in some applications. In this case it is a good practice to change the tracking method or changing the label.</p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/resource_tracking/</p> <p>STEPS</p> <ul> <li> <p>First of all the more secure starting point is to disable autoSync in the Application or, at least, selfHeal to false</p> </li> <li> <p>Then we only need to delete that tracking method from the resource, for example the annotation, but autosync and self heal will be add it. So we need</p> </li> <li> <p>Next we can edit that resource and remove the tracking annotation. The resource will be shown out of sync. This can be done via kubectl or argocd web interface.</p> </li> <li> <p>Then we push the changes to git where we declare we dont want the resource in that Application. Refresh the Application and the resource will dissapear but it will not be deleted from the cluster</p> </li> </ul>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#with-orphan-deletion","title":"With orphan deletion","text":"<p>This is faster but it is also potentially less secure because implies a deletion.</p> <ul> <li> <p>First of all disable autosync in the application</p> </li> <li> <p>Then remove the resource from the argocd ui using Non-cascading (Orphan) Delete. The resource will be shown out of sync.</p> </li> </ul> <p> </p> <ul> <li>Then we push the changes to git where we declare we dont want the resource in that Application. Refresh the Application and the resource will dissapear but it will not be deleted from the cluster</li> </ul>"},{"location":"gitops/argocd/deletion/99-links/","title":"Links","text":"<ul> <li>Controlling if/when the ApplicationSet controller modifies Application resources</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/ApplicationSet/Controlling-Resource-Modification/</p> <ul> <li>Application Pruning &amp; Resource Deletion</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/ApplicationSet/Application-Deletion/</p> <ul> <li>Automated Sync Policy</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p> <ul> <li>Sync Options</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/</p> <ul> <li>App Deletion</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/app_deletion/</p> <ul> <li>Everything You Ever Wanted to Know About Deletion and Argo CD Finalizers but Were Afraid to Ask</li> </ul> <p>https://codefresh.io/blog/argocd-application-deletion-finalizers/</p>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/","title":"ApplicationSet Not Found UI Error","text":""},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#problem-description","title":"Problem Description","text":"<p>After upgrading ArgoCD from 2.x to 3.x, non admin users with project-scoped permissions see this error in the UI:</p> <pre><code>Unable to load data: ApplicationSet XXX not found in any namespace\n</code></pre>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#root-cause","title":"Root Cause","text":"<p>In argocd 3.0 a RBAC change was introduced.</p> <p>Prior to argocd v3.0, policies granting update and delete to applications also applied to sub-resources (like ApplicationSet Starting with v3.0, update and delete actions only apply to the application itself. ArgoCD 3.0+ disabled permission inheritance by default, requiring explicit permissions for each resource type.</p> <p>The ArgoCD UI tries to load ApplicationSet information when displaying applications (for context/breadcrumbs), but users lack the necessary permissions.</p> <p>Notes</p> <ul> <li>This is a UI display issue, not a functional problem</li> <li>Admin users are not affected (they have global permissions)</li> </ul>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#solutions","title":"Solutions","text":"<ul> <li>Restore old behaviour</li> </ul> <p>It is possible to enable the previous behaviour with this setting in the argocd-cm configmap</p> <pre><code>server.rbac.disableApplicationFineGrainedRBACInheritance: \"false\"\n</code></pre> <p>\u26a0\ufe0f Warning: This affects all RBAC inheritance behavior, not just ApplicationSets.</p> <ul> <li>Add Explicit ApplicationSet Permissions (Recommended)</li> </ul> <p>Another option is to give minimal ApplicationSet permissions</p> <pre><code>p, proj:MYPROJECT:MYROLE, applicationsets, get, PROJECT/*, allow\n</code></pre> <p>Testing</p> <pre><code>argocd account can-i get applicationsets 'PROJECT/*'\n</code></pre>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#related-issues","title":"Related Issues","text":"<ul> <li>GitHub Issue #23571: ApplicationSet not found Toast</li> <li>ArgoCD 3.0 Migration Guide</li> <li>ApplicationSet Security Documentation</li> </ul>"},{"location":"gitops/argocd/errors/cache-key-missing/","title":"cache: key is missing","text":"<p>After an update we get this error in the web interface</p> <pre><code>error getting cached app managed resources: cache: key is missing\n</code></pre> <p>It seems the solution is restarting the argocd-application-controller</p> <pre><code>kubectl rollout restart statefulset -n argocd argocd-application-controller\n</code></pre>"},{"location":"gitops/argocd/errors/update-to-2.12/","title":"Update to 2.12 breaks appset","text":"<p>Until argocd 2.11 there should be a relation between the repository url defined in the repository secret and the repoURL configured in the application and applicationset.</p> <p>Since the 2.12 release there is another restriction. If you have a repository with a project configured, only the applications that belongs to that project can use the repository. So</p> <p>But there is another problem related with the Git generator in applicationsets. The applicationset cannot belong to a project, so all the repositories used by that applicationsets must not have a project defined, or they will fail</p> <p>Links</p> <ul> <li>v2.11 to 2.12</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/upgrading/2.11-2.12/</p> <ul> <li>2.12.x throws could not read Username for 'https://gitlab.com' from the UI</li> </ul> <p>https://github.com/argoproj/argo-cd/issues/19585</p> <ul> <li>rpc error: code = Internal desc</li> </ul> <p>https://github.com/argoproj/argo-cd/issues/19174</p> <p>If you delete the project from the repositories and the errors in the appset don't dissapear, you can force a restart  with kubectl -n argocd rollout restart sts,deploy</p>"},{"location":"gitops/argocd/errors/update-to-2.12/#cache-key-is-missing","title":"cache: key is missing","text":"<p>After an update we get this error in the web interface</p> <pre><code>error getting cached app managed resources: cache: key is missing\n</code></pre> <p>It seems the solution is restarting the argocd-application-controller</p> <pre><code>kubectl rollout restart statefulset -n argocd argocd-application-controller\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/","title":"Change admin user","text":"<p>Argocd documentation recommends to disable the admin user.</p>"},{"location":"gitops/argocd/login-credentials/disable-admin/#new-user","title":"New user","text":"<p>First we will create a new user that will have adminpermissions</p>"},{"location":"gitops/argocd/login-credentials/disable-admin/#create-the-new-user","title":"Create the new user","text":"<p>To add the user we must add it in the argocd-cm ConfigMap. The user will have login permissions.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  accounts.myuser: login\n  accounts.myuser.enabled: \"true\"\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#make-it-administrator","title":"Make it administrator","text":"<p>There is a predefined role called admin that our user will receive. Will will only remove all default permissions.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.default: \"\"\n  policy.csv: |\n    g, myuser, role:admin\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#assign-password","title":"Assign password","text":"<p>Get the value of the default admin password</p> <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 --decode; echo\n</code></pre> <p>Login to our argocd instance</p> <pre><code>argocd login \"fqdn of the server\"\n</code></pre> <p>And change give the new user a password</p> <pre><code>argocd account update-password --account myuser --grpc-web\n</code></pre> <p>Test login with the new user</p> <pre><code>argocd logout \"fqdn of the server\"\nargocd login \"fqdn of the server\"\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#disable-admin-user","title":"Disable admin user","text":"<p>Change the default admin password (optional)</p> <pre><code>argocd account update-password --account admin --grpc-web\n</code></pre> <p>Disable the admin user</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  admin.enabled: \"false\"\n</code></pre> <p>And delete delete the original secret</p> <p>```shell kubectl -n argocd delete secret argocd-initial-admin-secret</p>"},{"location":"gitops/argocd/notifications/0-index/","title":"Argocd Notifications","text":""},{"location":"gitops/argocd/notifications/0-index/#notification-services","title":"Notification services","text":"<p>Argocd supports several notification services:</p> <ul> <li>Alertmanager</li> <li>AWS SQS</li> <li>Email</li> <li>GitHub</li> <li>Google Chat</li> <li>Grafana</li> <li>Mattermost</li> <li>NewRelic</li> <li>Opsgenie</li> <li>Overview</li> <li>PagerDuty</li> <li>PagerDuty V2</li> <li>Pushover</li> <li>Rocket.Chat</li> <li>Slack</li> <li>Teams</li> <li>Telegram</li> <li>Webex Teams</li> <li>Webhook</li> </ul>"},{"location":"gitops/argocd/notifications/0-index/#install-the-notifications-catalog","title":"Install the notifications catalog","text":"<p>We can customize the notifications catalog, but we can use the official provided one. The stable url of this manifest is located here:</p> <pre><code>https://raw.githubusercontent.com/argoproj/argo-cd/stable/notifications_catalog/install.yaml\n</code></pre>"},{"location":"gitops/argocd/notifications/0-index/#setup-subscriptions","title":"Setup subscriptions","text":"<p>There are 2 places where to setup what notifications to send</p>"},{"location":"gitops/argocd/notifications/0-index/#default-subscriptions","title":"Default subscriptions","text":"<p>The default subscriptions are defined in in the argocd-notifications-cm configMap in the data.subscriptions field.</p> <p>This settings sends notifications when all the applications have the health degraded to the \"MYRECIPIENT\" destination.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  subscriptions: |\n    - recipients:\n      - MYRECIPIENT\n      triggers:\n      - on-health-degraded\n</code></pre> <p>In this configmap we also configure that recipient for the notification service we want to use</p>"},{"location":"gitops/argocd/notifications/0-index/#at-application-level","title":"At application level","text":"<p>At application level we can choose what events we want to receive adding an annotation to the application using this format</p> <pre><code>notifications.argoproj.io/subscribe.&lt;trigger&gt;.&lt;service&gt;: &lt;recipient&gt;\n</code></pre> <p>The trigger is the event. We have this default triggers</p> <ul> <li>on-created</li> <li>on-deleted</li> <li>on-deployed</li> <li>on-health-degraded</li> <li>on-sync-failed</li> <li>on-sync-running</li> <li>on-sync-status-unknown</li> <li>on-sync-succeeded</li> </ul> <p>The service is the notification service or provider.</p> <p>Finally the recipient is configured in the argocd-notifications-cm configMap. We can separate more than one recipient separated with \";\"</p> <p>An example to send when the application is has the unknown status via telegram to the sre and developers recipient is adding this annotation to the application:</p> <pre><code>notifications.argoproj.io/subscribe.on-sync-status-unknown.telegram: sre;developers\n</code></pre>"},{"location":"gitops/argocd/notifications/0-index/#links","title":"Links","text":"<ul> <li>Notifications Overview</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/</p> <ul> <li>Notification subscriptions</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/subscriptions/</p> <ul> <li>Triggers</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/triggers/</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/","title":"Send notifications to Microsoft Teams","text":""},{"location":"gitops/argocd/notifications/1-microsoft-teams/#create-the-webhoook","title":"Create the webhoook","text":"<p>First we need to create a webhook in Microsoft Teams.</p> <p>Choose the desired channel and go to \"Manage channel\" &gt; \"Conectors\" &gt; \"Edit\" and add an incoming webhook. At the end we get and Url we must copy and configure in argocd</p> <p>We can configure a custom image for this webhook</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#configure-argocd-notifications-cm-configmap","title":"Configure argocd-notifications-cm configmap","text":"<p>In this configmap we can setup the recipients, the destinations. If we want to add a chanel called argocdNotify, the configmap can be</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  service.teams: |\n    recipientUrls:\n      argocdNotify: $channel-teams-url\n</code></pre> <p>Do not change $channel-teams-url with the former url</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#configure-argocd-notifications-cm-secret","title":"Configure argocd-notifications-cm secret","text":"<p>Argocd will search the url in the argocd-notifications-cm secret</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-notifications-cm\nstringData:\n  channel-teams-url: MYWEBHOOKURL\n</code></pre> <p>Don't forget to setup subscriptions</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#links","title":"Links","text":"<ul> <li>Teams</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/teams/</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/","title":"Reconciliation","text":""},{"location":"gitops/argocd/reconciliation/00-reconciliation/#application-sources","title":"Application sources","text":"<p>Argocd permits to define 3 repository types (Application sources) and we can use them to define the desired state of the cluster:</p> <ul> <li>Git repositories</li> <li>Helm repositories</li> <li>Oci registries</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#what-is-the-reconciliation-process","title":"What is the reconciliation process","text":"<p>The reconcilation is a discovery process where some tasks are done:</p> <ul> <li>argocd-repo-server checks if there are changes in the application sources (and update the stored cache)</li> <li>argocd-repo-server generates the final manifests (desired state)</li> <li>argocd-application-controller checks if there are differences with the live state (drift detection)</li> </ul> <p>If there are any Git changes, Argo CD will only update applications with the auto-sync setting enabled</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#argocd-repo-server-operations","title":"argocd-repo-server operations","text":""},{"location":"gitops/argocd/reconciliation/00-reconciliation/#recheck-repository-and-store-in-cache","title":"Recheck repository and store in cache","text":"<p>argocd-repo-server checks every certain time if there are changes in the Application source (repository) and stores the most recent one in the redis cache</p> <p>The frequency is controlled this way:</p> <pre><code>CLI parameter: --revision-cache-expiration\nDefault: 3m\nSetting: timeout.reconciliation in argocd-cm ConfigMAp\nENV: ARGOCD_RECONCILIATION_TIMEOUT\n</code></pre> <ul> <li>In a git source checks if the latest commit SHA for the desired branch/tag has changed</li> <li>In a helm source, checks for the index.yaml file</li> <li>In an OCi repository checks the tag list</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#render-manifests-and-store-in-cache","title":"Render manifests and store in cache","text":"<p>Using that Application source cache, the argocd-repo-server:</p> <ul> <li>Generates the final manifests that represents the desired (target) state using the  the proper tool (kustomize, helm template, ...)</li> <li>Stores them in the redis cache.</li> </ul> <p>This is done for every application and returns the generated manifests to the application-controller.</p> <p>It is possible to control the expiration of that cache with this:</p> <pre><code>CLI parameter: --repo-cache-expiration  \nDefault: 24h\nSetting: reposerver.repo.cache.expiration  in argocd-cmd-params-cm ConfigMAp\nENV: ARGOCD_REPO_CACHE_EXPIRATION\n</code></pre> <p>There is an additional  per application tunning that makes a new commit be ignored and consider our cache as valid, so no new manifests generation will be launched. This is done via argocd.argoproj.io/manifest-generate-paths annotation, that tells argocd the paths in the git repo must change to trigger a new manifest generation in our application.</p> <ul> <li>Repo Server Cache Code</li> </ul> <p>https://github.com/argoproj/argo-cd/blob/master/reposerver/cache/cache.go</p> <ul> <li>Command line</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/server-commands/argocd-repo-server/</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#drift-detection-application-controller","title":"Drift detection (application controller)","text":"<p>The application-controller compares that final manifests (desired/target state) with the live (real) state in the cluster in order to detect drift.</p> <ul> <li>If a drift is detected, the application is consideres \"Out of Sync\"</li> <li>Applying the desired state is part of the Sync process, not the reconciliation process and it can triggered automatically enabl AutoSync</li> </ul> <p>In the application controller, the ARGOCD_RECONCILIATION_TIMEOUT (timeout.reconciliation in the argocd-cm ConfigMap) controls how often the application-controller checks all applications for drift, regardless of whether any changes occurred. This is:</p> <ul> <li>If no timeout.reconciliation is configured, the default value is 120s</li> <li>Valid values are duration strings (5m, 3h, 5d,..)</li> <li>A zero value disables this reconciliation operation</li> <li>The argocd-repo-server and argocd-application-controller must be restarted to apply this setting</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#jitter","title":"Jitter","text":"<p>There are some situations where the reconciliation operation can be delayed because of a large number of applications. We can give more extra time to the refresh operation via the timeout.reconciliation.jitter setting.</p> <ul> <li>Valid values are duration strings (5m, 3h, 5d,..)</li> <li>A zero value disables the jitter</li> <li>The default value is 60 seconds</li> </ul> <p>Example: if the sync timeout is 3 minutes and the jitter is 1 minute, then the actual timeout will be between 3 and 4 minutes. The default timeout.reconciliation and timeout.reconciliation.jitter values makes the maximum period to be 3 minutes</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#hard-refresh","title":"Hard refresh","text":"<p>It is possible to ignore the cached application source when doing a refresh of an application. This is the hard refresh and it forces the repo server to regenerate the application source cache again.</p> <p>This can be useful in some situations:</p> <ul> <li>Helm charts with code changes but same version</li> <li>External secrets (Vault, etc.) that don't trigger Git changes</li> <li>Registry updates where chart digest changes but version stays same</li> <li>Cache corruption or stuck sync states</li> </ul> <p>Hard refresh is an expensive operation as it rebuilds everything from scratch.</p> <p>By default argocd doesn't do a hard refresh but we can trigger a hard refresh manually.</p> <ul> <li>Via argocd ui (refresh + hard button)</li> <li>Using the argocd cli (argocd app get --hard-refresh)</li> <li>Via argocd.argoproj.io/refresh=hard annotation to the application</li> <li>Using the api</li> </ul> <p>We can also configure a periodic hard refresh. This is done at controller level with the timeout.hard.reconciliation setting in the argocd-cm Configmap.</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#other-ways-to-trigger-an-application-refresh","title":"Other ways to trigger an Application refresh","text":"<ul> <li>Manually via argocd ui (refresh button)</li> <li>Using the argocd cli (argocd app get --refresh)</li> <li>Via a webhook (a change in the source notifies a change to argocd)</li> <li>Via argocd.argoproj.io/refresh=normal annotation to the application</li> <li>Using the api</li> <li>Undocummented behaviour accessing applications the web UI.</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#event-driven-automatic-refresh","title":"Event driven automatic refresh","text":"<p>There is another mecanism where argocd application controller triggers a refresh.</p> <p>Application-controller watches Kubernetes resources using watch APIs. When a resource's resourceVersion changes, triggers immediate reconciliation. We can see in the logs strings like \"Requesting app refresh caused by object update\". This can cause high CPU with frequently-changing resources and it can be avoid enabling resource.ignoreResourceUpdatesEnabled in the argocd-cm ConfigMap, enabled by default since argocd 3.0</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#refresh-settings-table","title":"Refresh settings table","text":"Concept Default Environment variable argocd-cm ConfigMap Binary Normal refresh 120s ARGOCD_RECONCILIATION_TIMEOUT timeout.reconciliation --app-resync Jitter 60s ARGOCD_RECONCILIATION_JITTER timeout.reconciliation.jitter --app-resync-jitter Hard refresh ARGOCD_HARD_RECONCILIATION_TIMEOUT timeout.hard.reconciliation --app-hard-resync"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#application-controller-references","title":"Application controller references","text":"<ul> <li>Application Controller Code</li> </ul> <p>https://github.com/argoproj/argo-cd/blob/master/cmd/argocd-application-controller/commands/argocd_application_controller.go</p> <ul> <li>Command line</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/server-commands/argocd-application-controller/</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/","title":"Tune reconciliation","text":""},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#timeoutreconciliation","title":"timeout.reconciliation","text":"<p>When deploying many Applications, the reconciliation default values can cause high cpu / memory consuption. To avoid it we can increase the timeout.reconciliation value to a greater value (12h, 24h) or disabling it (0)</p> <p>Changing this value needs to restart the Application controller and the repo server</p> <p>If you set timeout.reconciliation to 0 or a big value, then Argo CD will stop polling Git repositories automatically at every 3 minutes so it is recommended to use alternative methods such as webhooks, for example, when a new commit is pushed to a git repository to tell the reposerver to recheck if there are changes</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#setting-the-application-webhook","title":"Setting the Application webhook","text":"<p>If the git repository does not trigger Applications using Applicationsets, expose only the argocd api (argocd server) and create a webhook for https://myargocd/api/webhook</p> <pre><code>service: argocd-server\nport: 443\nname: https\n</code></pre> <p>What this Application webhook does:</p> <ul> <li>It triggers a normal refresh to the Applications</li> <li>Uses intelligent filtering: Only triggers refresh if the webhook event matches the application's source repository and if the changed files match the application's refresh paths (if configured). See manifest-generate-paths for more tunning.</li> <li>This is defined in webhook.go and in application_annotations.go.</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#setting-the-applicationset-webhook","title":"Setting the Applicationset webhook","text":"<p>If the git repository triggers Applications using Applicationsets, also expose the Applicationset controller and create a webhook for https://Applicationseturl/api/webhook</p> <pre><code>service: argocd-applicationset-controller\nport: 7000\nname: webhook\n</code></pre> <p>What this Applicationset webhook does:</p> <ul> <li>It makes an annotation argocd.argoproj.io/application-set-refresh to the Applicationset.</li> <li>This is defined in webhook.go and common.go</li> <li>This annotation makes the applicationset re-evaluate generators (like Git files generator).</li> </ul> <p>Links</p> <ul> <li>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/webhook/</li> <li>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git/#webhook-configuration</li> <li>See more info here</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#manifest-generate-paths","title":"manifest-generate-paths","text":"<p>Another great setting is using manifest-generate-paths. We can tell argocd Applications to only trigger a refresh is the changes are detected in one or more paths of the github repository.</p> <p>This can be done adding the argocd.argoproj.io/manifest-generate-paths label to the Applications</p> <pre><code>argocd.argoproj.io/manifest-generate-paths: .\n</code></pre> <p>This only will trigger a refresh when the changes are detected in the path the Application has been declared. We can configure more paths comma separated.</p> <p>This does not work for helm registries</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#selective-sync-applyoutofsynconly","title":"Selective Sync (ApplyOutOfSyncOnly)","text":"<p>It is possible to reduce the calls to the kubernetes api server made by argocd syncing only the changed objects. This is called Selective Sync</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\n...\nspec:\n  syncPolicy:\n    syncOptions:\n    - ApplyOutOfSyncOnly=true\n</code></pre> <p>But this has some counterparts</p> <ul> <li>The sync operation is not recorded in the history</li> <li>The rollback is not possible</li> <li>The hooks (not the git webhooks) dont run</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#links","title":"Links","text":"<ul> <li>High Availability</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/high_availability/</p> <ul> <li>Reconcile Optimization</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/reconcile/</p> <ul> <li>Selective Sync</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/selective_sync/</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/","title":"Gitlab webhook","text":"<p>Now we will argocd make a refresh when a push event is received in the gitlab repository. This is very useful when increasing or disabling timeout.reconciliation</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#gitlab-side","title":"Gitlab side","text":"<p>Go to your repository in Settings -  Webhooks and create a webhook with this options.</p> <pre><code>URL: https://YOUR_ARGOCD_INSTANCE_URL/api/webhook  (we need it exposed via ingress)\nSecret Token: use a password generator to define a token\nTrigger. Choose push events and configure the branch where your argocd manifests are located\nSSL verification: If your argocd instace has a known certificate, let it enabled\n</code></pre> <p></p> <p></p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#argocd-side-custom-secret","title":"Argocd side (custom secret)","text":"<p>By defaut argocd-secret is used, but we will use a custom secret to store this setting. If you don't want to create a new secret, it is posibble to directly configure the value of the token in the webhook.gitlab.secret key of the argocd-secret Secret. I propose this way because of some problems merging an externalsecret with the existing argocd-secret Secret</p> <ul> <li>Configure the argocd-secret to tell the webhook.gitlab.secret will be in another secret</li> </ul> <pre><code>apiVersion: v1\nstringData:\n  webhook.gitlab.secret: $argocd-custom-secrets:webhook.gitlab.secret\nkind: Secret\nmetadata:\n  name: argocd-secret\ntype: Opaque\n</code></pre> <p>And the create a secret:</p> <ul> <li>called argocd-custom-secrets</li> <li>label it with app.kubernetes.io/part-of: argocd</li> <li>include the webhook.gitlab.secret key with the token</li> </ul> <p>Now will link the webhook.gitlab.secret with the recently created secret. This is done setting this line in the argocd-secret Secret</p> <p>Finally test the hook in the the place where we created the webhook instance with a push test</p> <p></p> <p>It must have connectivity between gitlab and argocd</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#same-for-the-applicationset-controller","title":"Same for the Applicationset controller","text":"<p>We can do the same for the applicationsets, specially relevant if we:</p> <ul> <li>have increased the default 3m value of applicationsetcontroller.requeue.after setting in the argocd-cmd-params-cm ConfigMap</li> <li>make it at Applicationset level with requeueAfterSeconds</li> </ul> <p>Steps</p> <ul> <li>Create a new ingress that exposes the webhook port of the argocd-applicationset-controller pod</li> <li>Create a new webhook in gitlab the same way, now with the url https://THE_NEW_INGRESS/api/webhook</li> </ul>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#links","title":"Links","text":"<ul> <li>Git Webhook Configuration</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/webhook/</p> <ul> <li>Git Generator</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git/</p>"},{"location":"gitops/argocd/repositories/repositories/","title":"Repositories","text":"<p>Repositories tell ArgoCD where to read the content used to deploy applications.</p> <p>ArgoCD supports two types of repositories:</p> <ul> <li>Git \u2014 plain manifests, Kustomize, or Helm values stored in Git</li> <li>Helm \u2014 classic HTTP/S chart repositories or OCI registries</li> </ul> <p>A Git repo can reference external Helm chart repositories without registering them as a Secret \u2014 unless authentication is required.</p>"},{"location":"gitops/argocd/repositories/repositories/#declarative-definition","title":"Declarative definition","text":"<p>Repositories are Kubernetes Secrets labelled with <code>argocd.argoproj.io/secret-type: repository</code>. ArgoCD watches for that label and registers the repository automatically.</p> <p>Supported authentication methods per type:</p> <ul> <li>Git HTTPS \u2014 <code>username</code> + <code>password</code> (or personal access token)</li> <li>Git SSH \u2014 <code>sshPrivateKey</code></li> <li>Helm HTTP/S \u2014 <code>username</code> + <code>password</code>, optional TLS client cert fields</li> </ul>"},{"location":"gitops/argocd/repositories/repositories/#credential-templates","title":"Credential templates","text":"<p>When many repositories share the same credentials, use a credential template instead of repeating secrets per repo. The label changes to <code>repo-creds</code> and the <code>url</code> acts as a prefix matcher.</p> <p>A Secret with its own credentials will not inherit from a matching template.</p>"},{"location":"gitops/argocd/repositories/repositories/#project-scoped-repositories","title":"Project-scoped repositories","text":"<p>A repository can be bound to an ArgoCD project by adding the <code>project</code> field.</p>"},{"location":"gitops/argocd/repositories/repositories/#with-project-field-vs-without","title":"With project field vs without","text":"Global (no <code>project</code> field) Project-scoped (<code>project</code> field set) Visibility Available to all projects Only the named project can use it <code>sourceRepos</code> required Yes \u2014 must be listed in the AppProject No \u2014 implicitly allowed for that project Credentials Listed in Secret or via <code>repo-creds</code> template Same, but the repo is invisible to other projects Self-service Admin adds it once, all projects can reference it Developers can manage their own repos (with RBAC) ApplicationSet Usable from any project Only by AppSets whose <code>project</code> matches; templated <code>project</code> fields (e.g. <code>{{ project }}</code>) require a global repo <p>When to use project-scoped repos:</p> <ul> <li>The repository belongs to a single team and must not be accessible to others.</li> <li>Delegate repo management to developers via RBAC (<code>create/update/delete</code> on   <code>repositories</code> scoped to the project namespace).</li> </ul> <p>When to keep repos global:</p> <ul> <li>Shared Helm chart repositories referenced by multiple projects.</li> <li>ApplicationSets that template the <code>project</code> field dynamically \u2014 these can   only consume non-scoped repositories.</li> <li>Credential templates (<code>repo-creds</code>) are always global; there is no   project-scoped credential template concept.</li> </ul>"},{"location":"gitops/argocd/repositories/repositories/#links","title":"Links","text":"<ul> <li>Private repositories</li> <li>Repository security</li> <li>Project-scoped repositories and clusters</li> <li>Declarative setup \u2014 repositories</li> <li>Repository credential templates</li> <li>Repository examples YAML</li> <li>Repo-creds examples YAML</li> </ul>"},{"location":"gitops/argocd/sync/00-sync/","title":"Sync","text":"<p>Argocd in their documentation defines 2 states for an argocd application:</p> <ul> <li>The target state, as the desired state of the application.</li> </ul> <p>We define this state via files in git repositories</p> <ul> <li>The live state</li> </ul> <p>The real state is how the application is the kubernetes cluster</p> <p>Sync is the argocd process that applies the manifests in the cluster.</p>"},{"location":"gitops/argocd/sync/00-sync/#sync-status","title":"Sync status","text":"<p>We have 3 possible sync statuses</p> <ul> <li>Synced</li> </ul> <p>The target state and the live state are the same. Everything is ok.</p> <p></p> <ul> <li>OutOfSync</li> </ul> <p>There are differences between the target state and the live state. Sometimes a Sync is needed to move this state to Synced</p> <p></p> <ul> <li>Unknown</li> </ul> <p>There is a problem with the Sync process</p>"},{"location":"gitops/argocd/sync/00-sync/#sync-process","title":"Sync process","text":"<p>The sync process or sync stage is the operation that applies the manifests of an application to the kubernetes cluster</p> <p>It cant be executed:</p> <ul> <li> <p>Using the web interface</p> </li> <li> <p>Using the argocd cli</p> </li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/commands/argocd_app_sync/</p> <ul> <li>Using kubectl</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-kubectl/</p> <ul> <li>In an automated way</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p> <p>See the other links in this section for more information</p>"},{"location":"gitops/argocd/sync/98-tips-external-secret/","title":"External-secret OutOfSync","text":"<p>Sometimes we can get an argocd Application OutOfSync because an external-secret is OutOfSync with some differences in this places:</p> <ul> <li>conversionStrategy</li> <li>decodingStrategy</li> <li>decodingStrategy</li> </ul> <p>... and we did not define that values.</p> <p>The root cause is that External Secrets Operator acts as a mutating admission controller, modifying resources after they're applied. It adds default values for conversionStrategy, decodingStrategy, and metadataPolicy fields when they're not explicitly specified in the ExternalSecret manifest. This creates a drift between your Git source (without these fields) and the live cluster state (with default values added), causing ArgoCD to show the resources as OutOfSync</p>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-1-ignoredifferences","title":"Solution 1: ignoreDifferences","text":"<p>We can ignore that fields at controller level. This is not the best option because we are ignoring some fields in the resource.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  resource.customizations.ignoreDifferences.external-secrets.io_ExternalSecret: |\n      jqPathExpressions:\n      - '.spec.data[]?.remoteRef.conversionStrategy'\n      - '.spec.data[]?.remoteRef.decodingStrategy'\n      - '.spec.data[]?.remoteRef.metadataPolicy'\n      - '.spec.dataFrom[]?.extract.conversionStrategy'\n      - '.spec.dataFrom[]?.extract.decodingStrategy'\n      - '.spec.dataFrom[]?.extract.metadataPolicy'\n      - '.spec.dataFrom[]?.find.conversionStrategy'\n      - '.spec.dataFrom[]?.find.decodingStrategy'\n      - '.spec.dataFrom[]?.find.metadataPolicy'\n</code></pre> <p>This can also be configured at Application level with spec.ignoreDifferences. It must be configured in all drifted Applications</p> <p>We also neeed to ignore that differences when syncing</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-options: RespectIgnoreDifferences=true\n</code></pre>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-2-add-default-values","title":"Solution 2: add default values","text":"<p>Another option is to add to the external-secret the default values the external secret operator adds. For example</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nspec:\n  data:\n  - secretKey: mykey\n    remoteRef:\n      key: mykey\n      conversionStrategy: Default\n      decodingStrategy: None\n      metadataPolicy: None\n</code></pre>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-3-use-server-side-apply","title":"Solution 3: use Server Side Apply","text":"<p>Another option is to use Server Side Apply. With this:</p> <ul> <li>ArgoCD declares ownership of only the fields it manages</li> <li>Other controllers can own and modify their own fields (like external secrets operator)</li> <li>Kubernetes merges changes from multiple sources without conflicts</li> </ul> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-options: ServerSideApply=true\nspec:\n  ...\n</code></pre> <p>Note: I did not make this work for now.</p>"},{"location":"gitops/argocd/sync/98-tips/","title":"Argocd Tips","text":""},{"location":"gitops/argocd/sync/98-tips/#treat-something-as-string-in-jsonpointers","title":"Treat some/thing as string in jsonPointers","text":"<p>We want to exclude a path like this using ignoreDifferences and jsonPointers</p> <pre><code>/spec/template/metadata/annotations/checksum/secret-jobservice\n</code></pre> <p>But the key to exclude is checksum/secret-jobservice and not secret-jobservice inside checksum</p> <p>For this we must escape th \"/\" with \"~1\"</p> <pre><code>      ignoreDifferences:\n        - group: apps\n          kind: Deployment\n          name: harbor-core\n          jsonPointers:\n            - /spec/template/metadata/annotations/checksum~1secret-jobservice\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/","title":"ServerSideApply","text":"<p>There are 2 ways to apply manifests in kubernetes. Client-Side and Server-Side</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#client-side-apply-in-kubernetes","title":"Client-Side Apply in kubernetes","text":"<p>By default kubectl apply uses the traditional client-side way.</p> <p>The full desired state is stored in the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation on the object. ArgoCD uses this annotation to compute diffs and detect drift. This approach has a hard limit: annotations cannot exceed 262144 bytes (256KB). Large CRDs with complex specs will hit this limit and fail to apply.</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#server-side-apply-in-kubernetes","title":"Server-Side Apply in kubernetes","text":"<p>It became GA in kubernetes 1.22 as a new object merge algorithm, as well as tracking of field ownership, running on the Kubernetes API server.</p> <p>Server-Side Apply allows multiple \"managers\" (e.g., ArgoCD, kubectl, other controllers) to declaratively manage different parts of a resource's configuration. Each manager owns specific fields in the resource's manifest. The Kubernetes API server tracks which manager owns which fields in the resource's spec. This is called field ownership. When a manager applies changes, only the fields it owns are updated, leaving fields owned by other managers untouched. If two managers attempt to modify the same field, the API server detects the conflict and rejects the change unless explicitly forced.</p> <p>To see what \"manager\" controls what fields we can use this:</p> <pre><code>kubectl get RESOURCE RESOURCENAME --show-managed-fields -o yaml\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#serversideapply-in-argocd","title":"ServerSideApply in argocd","text":"<ul> <li>Resource exceeds the 262144 bytes annotation limit (common with large CRDs).</li> <li>Patching resources not fully managed by ArgoCD.</li> <li>More declarative field-ownership tracking instead of last-applied-state tracking.</li> <li>Improved merge behavior: changes by other controllers (e.g. HPA) are not overwritten by ArgoCD unless it owns those fields.</li> <li>Conflict detection: ArgoCD reports conflicts when another manager modifies fields it is also trying to manage.</li> <li>Declarative ownership: explicit per-field responsibility makes multi-tool management auditable.</li> <li>CRD compatibility: required for certain advanced controllers that rely on field ownership.</li> <li>Conflict risk: multiple tools modifying the same fields can cause conflicts if not carefully managed.</li> <li>API server load shift: merging moves from ArgoCD to the API server.</li> </ul>"},{"location":"gitops/argocd/sync/ServerSideApply/#enable-serversideapply-in-argocd","title":"Enable ServerSideApply in argocd","text":""},{"location":"gitops/argocd/sync/ServerSideApply/#at-application-controller-level","title":"At Application controller level","text":"<p>We can enable server side apply with the param --server-side-diff-enabled in the argocd-application-controller, or with the following setting in the argocd-cmd-params-cm configMap</p> <pre><code>controller.diff.server.side: true # false by default\n</code></pre> <p>Enables the server side diff feature at the application controller level. Diff calculation will be done by running a server side apply dryrun (when diff cache is unavailable)</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#at-application-level","title":"At Application level","text":"<p>SSA can be enabled per ArgoCD Application using <code>syncOptions</code>:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nspec:\n  syncPolicy:\n    syncOptions:\n      - ServerSideApply=true\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#at-resource-level","title":"At resource level","text":"<p>The most surgical approach: annotate only the specific resources that need SSA, leaving all others on client-side apply. This is the recommended pattern when SSA is needed only because of large CRDs hitting the annotation limit.</p> <p>Note \u2014 force-conflicts is automatic: ArgoCD unconditionally runs <code>kubectl apply --server-side --force-conflicts</code> whenever SSA is enabled, whether at Application level or via resource annotation. Field ownership conflicts from previous client-side apply managers are resolved automatically on the first SSA sync \u2014 no manual <code>kubectl</code> intervention needed.</p> <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-options: ServerSideApply=true\n</code></pre> <p>To apply SSA only to CRDs when deploying manifests that include them (e.g. via kustomize), use a patch so the rest of the resources continue using client-side apply:</p> <pre><code>patches:\n  - patch: |-\n      apiVersion: apiextensions.k8s.io/v1\n      kind: CustomResourceDefinition\n      metadata:\n        name: placeholder\n        annotations:\n          argocd.argoproj.io/sync-options: ServerSideApply=true\n    target:\n      kind: CustomResourceDefinition\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#diff-noise-on-non-declared-fields","title":"Diff noise on non-declared fields","text":"<p>Enabling SSA \u2014 whether globally or per resource \u2014 can surface diffs on fields that were never declared in Git.</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#root-cause","title":"Root cause","text":"<p>With SSA, the Kubernetes API server or mutating admission webhooks may inject default values into fields not declared in the manifest (e.g. <code>weight: 1</code> on a single-backend Istio route). These fields are owned by a different field manager (the webhook or the API server), not by ArgoCD. ArgoCD sees the live object has a value that is absent from the desired state and reports drift. This was less visible with client-side apply because the diff was based solely on the last-applied annotation, which only contained what ArgoCD had explicitly set.</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#solutions","title":"Solutions","text":""},{"location":"gitops/argocd/sync/ServerSideApply/#option-1-declare-the-field-explicitly-cleanest","title":"Option 1 \u2014 Declare the field explicitly (cleanest)","text":"<p>Add the defaulted value to your manifest so the desired and live states match:</p> <pre><code>    weight: 1\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#option-2-ignoredifferences-with-a-json-pointer","title":"Option 2 \u2014 ignoreDifferences with a JSON pointer","text":"<pre><code>spec:\n  ignoreDifferences:\n    - group: networking.istio.io\n      kind: VirtualService\n      jsonPointers:\n        - /spec/http/*/route/*/weight\n</code></pre> <p>Option 3 \u2014 ignoreDifferences scoped to a specific field manager (ArgoCD 2.7+)</p> <pre><code>spec:\n  ignoreDifferences:\n    - group: networking.istio.io\n      kind: VirtualService\n      managedFieldsManagers:\n        - istio-pilot\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#typical-progression-when-adopting-ssa-for-large-crds","title":"Typical progression when adopting SSA for large CRDs","text":"<pre><code>Large CRD hits the 262144 annotation limit\n        \u2193\nEnable SSA only on the affected resources (annotation or kustomize patch)\n        \u2193\nGain: large CRDs apply successfully\nLoss: diff noise appears on fields defaulted by webhooks or the API server\n        \u2193\nAdd ignoreDifferences rules or declare the fields explicitly to compensate\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#links","title":"Links","text":"<ul> <li>Server-Side Apply</li> <li>Server-Side Apply support for ArgoCD (proposal)</li> <li>ServerSideApply argocd sync option</li> </ul>"},{"location":"gitops/argocd/sync/ordering-resources/","title":"Ordering resources during a sync operation","text":"<p>To order the synchronization of resources, ArgoCD takes several factors into account. Listed from highest to lowest priority:</p> <ul> <li>Resource phase</li> <li>Resource wave</li> <li>Resource type</li> <li>Resource name</li> </ul> <p>Before starting the actual synchronization, ArgoCD performs a dry-run.</p>"},{"location":"gitops/argocd/sync/ordering-resources/#resource-phases","title":"Resource Phases","text":"<p>ArgoCD has 3 execution phases:</p> <ul> <li>Pre-Sync</li> <li>Sync</li> <li>Post-Sync</li> </ul> <p>By default, all objects use the \"Sync\" phase, but it can be specified on the object using one of these annotations:</p> <ul> <li>argocd.argoproj.io/hook: PreSync</li> <li>argocd.argoproj.io/hook: Sync</li> <li>argocd.argoproj.io/hook: PostSync</li> </ul> <p>The PreSync and PostSync phases are used to specify hooks, intended for running tasks before or after the synchronization.</p>"},{"location":"gitops/argocd/sync/ordering-resources/#resource-waves","title":"Resource Waves","text":"<p>Waves are a way to order resources within the same phase. By default, the wave of a resource is 0, but it can be changed using the annotation:</p> <pre><code>argocd.argoproj.io/sync-wave: \"wave-number\"\n</code></pre> <p>The wave number can be positive or negative, where negative numbers are applied first. This is an example of wave ordering:</p> <pre><code>-4\n-1\n0 (default)\n1\n3\n</code></pre>"},{"location":"gitops/argocd/sync/ordering-resources/#resource-type","title":"Resource Type","text":"<p>Within the same wave, ArgoCD takes the resource type into account for ordering. The applied order can be seen here:</p> <p>https://github.com/argoproj/gitops-engine/blob/master/pkg/sync/sync_tasks.go</p> <p>Custom resources (i.e., object types added via custom resource definitions) are not included in this order and will be applied at the end of the wave.</p>"},{"location":"gitops/argocd/sync/ordering-resources/#resource-name","title":"Resource Name","text":"<p>Within the same wave and resource type, resources are ordered alphabetically by name.</p>"},{"location":"gitops/argocd/sync/ordering-resources/#final-order","title":"Final Order","text":"<ul> <li> <p>Execution of the Pre-Sync phase, ordering resources by wave number.   Within each wave, ordered by resource type.   If resources of the same type exist within that wave, they are ordered by   name.</p> </li> <li> <p>Execution of the Sync phase, ordering resources by wave number.   Within each wave, ordered by resource type.   If resources of the same type exist within that wave, they are ordered by   name.</p> </li> <li> <p>Execution of the PostSync phase, ordering resources by wave number.   Within each wave, ordered by resource type.   If resources of the same type exist within that wave, they are ordered by   name.</p> </li> </ul>"},{"location":"gitops/argocd/sync/ordering-resources/#considerations","title":"Considerations","text":"<ul> <li>Regarding resource health:   ArgoCD does not advance to the next step in the order until the previous one   has completed successfully (health check).   ArgoCD knows how to check health for base Kubernetes resources (deployments,   jobs, services, etc.) and some additional CRDs.   For CRDs it does not know how to check, custom health checks can be added via   Lua scripts, though the process is quite cumbersome.</li> </ul>"},{"location":"gitops/argocd/sync/ordering-resources/#links","title":"Links","text":"<ul> <li> <p>Sync and waves   https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/</p> </li> <li> <p>Hooks   https://argo-cd.readthedocs.io/en/stable/user-guide/resource_hooks/</p> </li> <li> <p>Mastering Argo CD Sync Waves: A Deep Dive into Effective GitOps   Synchronization Strategies   https://www.youtube.com/watch?v=LKuRtOTvlXk</p> </li> <li> <p>Resource health   https://argo-cd.readthedocs.io/en/stable/operator-manual/health/</p> </li> <li> <p>Resource health added for custom resource definitions   https://github.com/argoproj/argo-cd/tree/master/resource_customizations</p> </li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/","title":"Resource Hooks","text":"<p>Resource hooks are ways to execute some actions at certain moments of the sync operation. They are typically used in resources like Pods, Jobs and Workflows (from Argo workflows)</p> <p>For this we only need to add the following anotation to the resource:</p> <pre><code>argocd.argoproj.io/hook: HOOK\n</code></pre> <p>The HOOK can be:</p> <ul> <li>PreSync</li> </ul> <p>It will run in the PreSync phase</p> <ul> <li>Sync</li> </ul> <p>It will run in the Sync phase</p> <ul> <li>PostSync</li> </ul> <p>It will run in the PostSync phase</p> <ul> <li>SyncFail</li> </ul> <p>It will run if a sync operation fails</p> <ul> <li>PostDelete</li> </ul> <p>It will run  after all Application resources are deleted</p> <ul> <li>Skip</li> </ul> <p>Argocd will skip the application of the manifest</p>"},{"location":"gitops/argocd/sync/resource-hooks/#important-notes","title":"Important notes","text":"<ul> <li> <p>Hooks are not run during selective sync</p> </li> <li> <p>Multiple hooks can be specified as a comma separated list</p> </li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#deletion-policy","title":"Deletion policy","text":"<p>We can also define when to delete the hook with the following annotation:</p> <pre><code>argocd.argoproj.io/hook-delete-policy: POLICY\n</code></pre> <p>Where the policy can be:</p> <ul> <li>HookSucceeded</li> </ul> <p>The hook is deleted after the hook ends ok</p> <ul> <li>HookFailed</li> </ul> <p>The hook is deleted is it fails</p> <ul> <li>BeforeHookCreation (default)</li> </ul> <p>The hook is deleted before the new one is created. It exits to be used with named hooks</p> <p>Multiple hook delete policies can be specified as a comma separated list.</p>"},{"location":"gitops/argocd/sync/resource-hooks/#named-hooks","title":"Named hooks","text":"<p>If the Resource hook have a name, it is considered a named hook and it will be only created once. If we want to create it at every sync, we have 2 options:</p> <ul> <li>use generateName instead of name</li> <li>use BeforeHookCreation as deletion policy</li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#jobs-and-workflows-from-argo-workflows","title":"Jobs and workflows (from Argo workflows)","text":"<p>A job has the field ttlSecondsAfterFinished and a workflow (from Argo Workflows) have ttlStrategy. Both properties offer autoclean after some time.</p> <p>Using this fields can cause an OutOfSync state when the deletion comes. Using deletion hooks instead of ttlSecondsAfterFinished and ttlStrategy avoids this situation.</p>"},{"location":"gitops/argocd/sync/resource-hooks/#argocd-cannot-delete-a-resource-hook","title":"Argocd cannot delete a resource hook","text":"<p>In some situations I have detected argocd cannot delete a resource hook in a job with</p> <pre><code>argocd.argoproj.io/hook: Sync\nargocd.argoproj.io/hook-delete-policy: BeforeHookCreation\n</code></pre> <p>In the application controller</p> <pre><code>deleted resource batch/Job ... reason=ResourceDeleted type=Normal\n</code></pre> <p>But the resource hooks keeps in a Pending Deletion state</p> <p>There are some entries that suggest there is a bug here</p> <ul> <li>https://github.com/argoproj/argo-cd/issues/14929</li> <li>https://github.com/argoproj/gitops-engine/pull/461</li> <li>https://github.com/argoproj/argo-cd/issues/16446</li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#links","title":"Links","text":"<ul> <li>Resource Hooks</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/resource_hooks/</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/","title":"Control retries and timeouts","text":""},{"location":"gitops/argocd/sync/syncPolicy-retry/#control-retries-in-syncpolicy","title":"Control retries in syncPolicy","text":"<p>We can control the retries when a sync operation starts. We can configure it with spec.syncPolicy.retry at application definition level or as a parameter with the argocd binary</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#retry-limit","title":"Retry limit","text":"<p>The number of failed retries is configured with the retry limit option. If a sync attempt fails, ArgoCD will automatically retry the sync up to the number of times defined by the retry.limit If the value in less than 0, ArgoCD will retry indefinitely until the sync succeeds or is manually stopped.</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#backoff","title":"Backoff","text":"<p>With retry.backoff we can configure the delay between sync attempts with the possibility to increase the time between every failed attempt to avoid overwhelming the system or external resources.</p> <ul> <li> <p>duration is the delay between attempts (default 5s)</p> </li> <li> <p>factor multiplies the delay (duration) after each failed retry (default 2)</p> </li> <li> <p>maxDuration is the max delay between retry attempts (default 3m0s)</p> </li> </ul>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#example-in-the-application-spec","title":"Example in the application spec","text":"<pre><code>3 retries\n1st retry delay: 10 seconds\n2st retry delay: 30 seconds\n3st retry delay: 60 seconds\n</code></pre> <p>... (but never exceeding 5 minutes between retries)</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\nspec:\n  syncPolicy:\n    retry:\n      limit: 3\n      backoff:\n        duration: 10\n        factor: 3 \n        maxDuration: 5m\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#related-argocd-app-sync-parameters","title":"Related argocd app sync parameters","text":"<pre><code>--retry-limit\n--retry-backoff-duration\n--retry-backoff-factor\n--retry-backoff-max-duration\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#setup-a-timeout-for-app-sync","title":"Setup a timeout for app sync","text":"<p>It is possible, and probably recommended to configure a timeout in seconds when an application starts a sync operation.</p> <p>It is configured at controller level, and sets the maximum time allowed for a single sync operation to complete before it is considered failed.</p> <p>This is done with the controller.sync.timeout.seconds setting in the argocd-cmd-params-cm configmap</p> <pre><code>controller.sync.timeout.seconds: \"1800\" # 30 minutes\n</code></pre> <p>The default value is \"0\", no timeout</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/","title":"Sync and compare options","text":""},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#sync-options","title":"Sync options","text":"<p>Argocd offer several sync options to configure the Sync process.</p> <p>This options can be configured at application level, resource level, or both</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#at-application-level","title":"At application level","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\nspec:\n  syncPolicy:\n    syncOptions:\n    - OPTION=VALUE\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#at-resource-level","title":"At resource level","text":"<p>At resource level is made using an annotation in the resource. Multiple options can be configured comma separated.</p> <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-options: OPTION1=VALUE,OPTION2=VALUE\n</code></pre> <p>Multiple Sync Options can be configured</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#list-of-sync-options","title":"List of sync options","text":"Explanation ApplyOutOfSyncOnly Only sync OutOfSync resources. Specially relevant in big applications or low resource api servers CreateNamespace Create the spec.destination.namespace namespace. Useful with spec.syncPolicy.managedNamespaceMetadata Delete Configures how a resource will be deleted after the application is deleted FailOnSharedResource Makes the sync process fail if a resource is managed by other argocd application Force Force=true,Replace=true deletes and recreate a resource at every sync Prune Prune resources not declared in the target state but they exist in the application PruneLast Moves the prune process at the end of the sync operation PrunePropagationPolicy Permits to choose the garbage collection (foreground,background or orphan) Replace Converts kubectl apply operation in a kubectl replace or kubectl create operation RespectIgnoreDifferences Used spec.ignoreDifferences also in the sync process. Don't sync/correct that fields ServerSideApply With true, we pass --server-side=true parameter to kubectl SkipDryRunOnMissingResource Ignores if a resource to be created does not have its CRD Validate With false, we pass --validate=false to kubectl"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#notes","title":"Notes","text":"<ul> <li>Replace=true takes precedence over ServerSideApply=true.</li> <li>Prune=false Prevents a resource from being pruned</li> <li>Prune=confirm Requires manual confirmation before pruning</li> <li>Delete=false Don't delete the resource from the cluster during app deletion</li> <li>Delete=confirm Requires manual confirmation before deletion</li> <li>RespectIgnoreDifferences sync option is only effective when the resource is already created in the cluster. If the Application is being created and no live state exists, the desired state is applied as-is</li> </ul>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#links","title":"Links","text":"<ul> <li>Sync Options</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/</p> <ul> <li>Garbage Collection</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/","title":"Automated sync","text":"<p>By default, after a reconciliation/refresh, an Application does not make and automatic sync when it detects differences between the target and the live state. So the differences between them are not applied in an automated way. For that we need to enable the autosync feature in the Application (or Applicationset)</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#enabling-autosync","title":"Enabling autosync","text":"<p>We can enable autosync with this setting in the Application resource</p> <pre><code>spec:\n  syncPolicy:\n    automated: {}\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#autosync-options","title":"autosync options","text":"<p>There some options here to configure the automated sync.</p> <ul> <li>prune</li> </ul> <p>Enables automatic deletion of resources that they are not defined in the Application but they were. The default value is false.</p> <ul> <li>allowEmpty</li> </ul> <p>This setting is disabled by default and prevents a pruning operation can remove all the resources in the Application.</p> <ul> <li>selfHeal</li> </ul> <p>When a change in detected in the kubernetes cluster that generates a drift, argocd by default ignores it. Enabling selfHeal triggers a new sync.</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#toggling-autosync","title":"Toggling autosync","text":"<p>Since argocd 3.1 we can also enable/disable for an Application resource using a new feature:</p> <pre><code>spec:\n  syncPolicy:\n    automated:\n      enabled: true # or false\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#some-notes-about-this-new-feature","title":"Some notes about this new feature","text":"<ul> <li> <p>Setting this value to false, disables autosync but permits to configure prune, allowEmpty and selfHeal</p> </li> <li> <p>This setting is currently not working because of a bug https://github.com/argoproj/argo-cd/issues/24171</p> </li> <li> <p>This setting has no effect in Applications managed by ApplicationSets.</p> </li> </ul> <p>To enable/disable autosync in an Application managed by ApplicationSet without changing the setting in all generated Applications, we have some options. For example we can use templatePatch or ignoring that field in the ApplicationSet and then changing manually the desired value in our Application</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  ignoreApplicationDifferences:\n    - jsonPointers:\n        - /spec/syncPolicy/automated/enabled\n</code></pre> <p>This permits enabling and disabling the autosync manually and directly in the Application resource but loosing gitops control of desired state.</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#links","title":"Links","text":"<p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/","title":"GitRepository","text":"<p>It produces an artifact for a git repository revision. This is a tarball artifact with the fetched data.</p> <p>When defining a GitRepository, we can specify some options</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#repo-settings","title":"Repo settings","text":"<p>We can configure</p> <ul> <li>the url of the repository (spec.url)</li> <li>the proxy, if needed (spec.proxySecretRef)</li> <li>the credentials, if needed (spec.secretRef)</li> <li>the provider. It can be generic(default), github or azure (spec.provider)</li> </ul>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#what-to-include-in-the-tarball","title":"What to include in the tarball","text":"<ul> <li>sparseCheckout</li> </ul> <p>With spec.sparseCheckout we can specify a list of directories we want to be the only included in the tarball</p> <ul> <li>ignore</li> </ul> <p>With spec.ignore we can we can specify a list of directories that they will be excluded in the tarball Use with caution because there is default exclusion list</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#checkout-specref","title":"Checkout (spec.ref)","text":"<ul> <li>the commit sha</li> <li>the name of the reference</li> <li>a SemVer tag expression</li> <li>a tag</li> <li>a branch (default master)</li> </ul> <p>What takes precedecens follows this order</p> <pre><code>commit &gt; name &gt; semver &gt; tag &gt; branch\n</code></pre>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#repo-operations","title":"Repo operations","text":"<ul> <li>the checkout frequency (spec.interval)</li> <li>the timeout (spec.timeout)</li> <li>if we want to stop the reconciliation (spec.suspend)</li> <li>if we want to verify the commit (spec.verify)</li> <li>if we want to initialize and fetch all Git submodules recursively when cloning the repository (spec.recurseSubmodules)</li> <li>if we want to include artifacts from another GitRepository (spec.include)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/gitrepositories/</p> <p>and the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.GitRepository</p>"},{"location":"gitops/fluxcd/source-controller/helmchart/","title":"HelmChart","text":"<p>A helm chart downloads a helm chart and packages as a tarball</p>"},{"location":"gitops/fluxcd/source-controller/helmchart/#configuration","title":"Configuration","text":""},{"location":"gitops/fluxcd/source-controller/helmchart/#source","title":"Source","text":"<ul> <li>We can define the source of the chart with spec.sourceRef</li> <li>The chart name or path of the sourceRef (spec, chart)</li> <li>The list of value files, as a relative path in the sourceRef (spec.valuesFiles)</li> <li>If some declared value files don't exist, we can ignore the error (spec.ignoreMissingValuesFiles)</li> </ul> <p>If the sourceRef is a HelmRepository</p> <ul> <li>The version (spec.version)</li> </ul>"},{"location":"gitops/fluxcd/source-controller/helmchart/#operations","title":"Operations","text":"<ul> <li>The frequency to check the spec.sourceRef for updates (spec.interval)</li> <li>If we want to create a new version with a new ChartVersion or Revision (spec.reconcileStrategy)</li> </ul> <p>for a HelmRepository, it will be ChartVersion for a GitRepository or bucket, it will be Revision</p> <ul> <li>If we want to suspend the reconciliation (spec.suspend)</li> <li>If we want to verify the chart (spec.verify)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/helmcharts/</p> <p>And the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.HelmChart</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/","title":"HelmRepository","text":"<p>There are 2 HelmRepository types</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#helm-https-repository","title":"Helm HTTP/S repository","text":"<p>Defines a Source to produce an Artifact for a Helm repository index YAML (index.yaml)</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#oci-helm-repository","title":"OCI Helm repository","text":"<p>Defines a source that does not produce an Artifact. It\u2019s a data container to store the information about the OCI repository that can be used by HelmChart to access OCI Helm charts.</p> <p>Because of that, an oci helm repository dont have ready or status</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#configuration","title":"Configuration","text":"<p>We can configure some options about the repository</p> <ul> <li>the url (spec.url) and type between default or oci (spec.type)</li> <li>the credentials, if needed (spec.secretRef and spec.passCredentials)</li> <li>The client certificates (spec.certSecretRef)</li> </ul> <p>For default repositories</p> <ul> <li>the checkout frequency (spec.interval)</li> </ul> <p>For oci repositories</p> <ul> <li>If we want permit non-tls connections to an oci repository (spec.insecure)</li> <li>The oci provider, between aws, azure, gcp or generic (spec.provider)</li> </ul> <p>And about the operations</p> <ul> <li>the timeout (spec.timeout)</li> <li>if we want to stop the reconciliation (spec.suspend)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/helmrepositories/</p> <p>and the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.HelmRepository</p>"},{"location":"k8s-resource-handling/","title":"Kubernetes Resource Handling","text":"<p>Tools and frameworks for templating, composing, and managing Kubernetes resources.</p>"},{"location":"k8s-resource-handling/#templating-and-manifest-generation","title":"Templating and manifest generation","text":""},{"location":"k8s-resource-handling/#helm","title":"Helm","text":"<p>The most widely adopted Kubernetes package manager. Helm uses Go templates to generate Kubernetes manifests from reusable charts, supporting versioned releases and rollbacks.</p>"},{"location":"k8s-resource-handling/#kustomize","title":"Kustomize","text":"<p>A template-free approach to Kubernetes manifest customization. Kustomize uses overlays and patches to transform base manifests without modifying the originals, and is built into <code>kubectl</code>.</p>"},{"location":"k8s-resource-handling/#ytt","title":"ytt","text":"<p>A YAML-aware templating tool from the Carvel project. ytt uses Starlark (a Python dialect) for overlay logic while keeping YAML structure intact, avoiding the whitespace pitfalls of text-based templating.</p>"},{"location":"k8s-resource-handling/#jsonnet-tanka","title":"Jsonnet / Tanka","text":"<p>Grafana Tanka uses Jsonnet, a data templating language, to generate Kubernetes manifests. More expressive than YAML but with a steeper learning curve.</p>"},{"location":"k8s-resource-handling/#cdk8s","title":"cdk8s","text":"<p>AWS CDK for Kubernetes. Define manifests using TypeScript, Python, Go, or Java. Useful if you prefer general-purpose languages over YAML.</p>"},{"location":"k8s-resource-handling/#cue-timoni","title":"CUE / Timoni","text":"<p>CUE is a data validation and configuration language that can generate, validate, and constrain Kubernetes manifests with strong type checking. Timoni builds on CUE as a package manager for Kubernetes from the Flux ecosystem, similar to Helm but with CUE instead of Go templates.</p>"},{"location":"k8s-resource-handling/#kcl","title":"KCL","text":"<p>Kusion Configuration Language. A CNCF project for writing and validating Kubernetes configs with a purpose-built language.</p>"},{"location":"k8s-resource-handling/#dhall","title":"Dhall","text":"<p>A programmable, typed configuration language that can target Kubernetes YAML with built-in imports and guarantees about termination.</p>"},{"location":"k8s-resource-handling/#resource-composition-and-abstraction","title":"Resource composition and abstraction","text":""},{"location":"k8s-resource-handling/#crossplane-composite-resources","title":"Crossplane Composite Resources","text":"<p>Crossplane XRDs and Compositions allow platform teams to define higher-level abstractions that provision both Kubernetes and cloud-provider resources through a single API object.</p>"},{"location":"k8s-resource-handling/#kro","title":"Kro","text":"<p>Kro (Kubernetes Resource Orchestrator) lets you define custom groups of Kubernetes resources as a single logical unit using ResourceGroups, simplifying complex multi-resource deployments.</p>"},{"location":"k8s-resource-handling/#metacontroller","title":"Metacontroller","text":"<p>A lightweight framework for writing custom controllers using simple webhooks instead of full operator SDKs. Enables resource composition with minimal boilerplate.</p>"},{"location":"k8s-resource-handling/#yaml-and-json-processing","title":"YAML and JSON processing","text":""},{"location":"k8s-resource-handling/#yq","title":"yq","text":"<p>A lightweight command-line YAML processor for reading, writing, and manipulating YAML files. Useful for scripting Kubernetes manifest transformations and extracting values from resource definitions.</p>"},{"location":"k8s-resource-handling/#jq","title":"jq","text":"<p>The JSON equivalent of yq. Often used with <code>kubectl -o json</code> output for querying and transforming Kubernetes resource data in scripts.</p>"},{"location":"k8s-resource-handling/helm/98-tips/","title":"Tips","text":""},{"location":"k8s-resource-handling/helm/98-tips/#keep-a-resource","title":"Keep a resource","text":"<p>Add the following annotation to keep a resource when a helm uninstall, upgrade or rollback operation is done</p> <pre><code>  annotations:\n    helm.sh/resource-policy: keep\n</code></pre>"},{"location":"k8s-resource-handling/helm/98-tips/#nil-pointer-evaluating-interface-with-range-loop-no-accede-al-values","title":"nil pointer evaluating interface with range loop, no accede al values","text":"<p>https://stackoverflow.com/questions/57475521/ingress-yaml-template-returns-error-in-renderring-nil-pointer-evaluating-int</p>"},{"location":"k8s-resource-handling/helm/98-tips/#iterate-over-range","title":"Iterate over range","text":"<p>https://stackoverflow.com/questions/56224527/helm-iterate-over-range</p>"},{"location":"k8s-resource-handling/helm/98-tips/#links","title":"Links","text":"<ul> <li>Chart Development Tips and Tricks</li> </ul> <p>https://helm.sh/docs/howto/charts_tips_and_tricks/</p>"},{"location":"k8s-resource-handling/helm/syntax/","title":"Syntax","text":"<p>https://helm.sh/docs/chart_template_guide/function_list/ https://helm.sh/docs/chart_template_guide/control_structures/ if/else with range define template block</p> <p>https://helm.sh/docs/chart_template_guide/yaml_techniques/ https://helm.sh/docs/chart_template_guide/data_types/</p> <p>https://helm.sh/docs/chart_best_practices/templates/</p>"},{"location":"k8s-resource-handling/helm/syntax/#-and-","title":"{{-  and -}}","text":"<p>{{- (with the dash and space added) indicates that whitespace should be chomped left -}} means whitespace to the right should be consumed. Be careful! Newlines are whitespace!</p>"},{"location":"k8s-resource-handling/helm/syntax/#_1","title":":=","text":"<p>In Helm charts, the := operator is used in the Go templating language to assign a value to a variable. {{- $myVar := .Values.myValue -}} you can use $myVar in your template to refer to the value of .Values.myValue.</p> <p>Please note that the scope of variables in Helm templates can be complex, and the := operator creates a variable that is only available in the scope where it is defined.</p>"},{"location":"k8s-resource-handling/helm/variables/","title":"Helm variables","text":"<p>Helm provides a set of built-in variables that you can use in your Helm templates. These variables are automatically populated by Helm and can be used to customize your Kubernetes manifests. Here is a list of some common Helm variables:</p>"},{"location":"k8s-resource-handling/helm/variables/#release-information","title":"Release Information","text":"<pre><code>{{ .Release.Name }}: The name of the release.\n{{ .Release.Namespace }}: The namespace to which the release is deployed.\n{{ .Release.IsUpgrade }}: True if the current operation is an upgrade.\n{{ .Release.IsInstall }}: True if the current operation is an install.\n{{ .Release.Revision }}: The revision number of the release.\n{{ .Release.Service }}: The service rendering the template. (usually helm)\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#chart-information","title":"Chart Information","text":"<pre><code>{{ .Chart.Name }}: The name of the chart.\n{{ .Chart.Version }}: The version of the chart.\n{{ .Chart.AppVersion }}: The app version of the chart.\n{{ .Chart.Description }}: The description of the chart.\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#values","title":"Values","text":"<pre><code>{{ .Values }}: The values passed into the chart.\n{{ .Values.&lt;key&gt; }}: Access a specific value from the values file.\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#files","title":"Files","text":"<pre><code>{{ .Files }}: Access non-template files in the chart.\n{{ .Files.Get \"&lt;file&gt;\" }}: Get the contents of a file.\n{{ .Files.GetBytes \"&lt;file&gt;\" }}: Get the contents of a file as bytes.\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#capabilities","title":"Capabilities","text":"<pre><code>{{ .Capabilities.APIVersions.Has \"batch/v1\" }}: Check if a specific API version is available.\n{{ .Capabilities.KubeVersion.GitVersion }}: The Kubernetes version.\n{{ .Capabilities.KubeVersion.Major }}: The major version of Kubernetes.\n{{ .Capabilities.KubeVersion.Minor }}: The minor version of Kubernetes.\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#template-information","title":"Template Information","text":"<pre><code>\n{{ .Template.Name }}: The name of the template file being rendered.\n</code></pre>"},{"location":"k8s-resource-handling/helm/variables/#links","title":"Links","text":"<ul> <li>Built-in Objects</li> </ul> <p>https://helm.sh/docs/chart_template_guide/builtin_objects/</p>"},{"location":"k8s-resource-handling/kustomize/10-patches/","title":"Patches (overlays)","text":"<p>Patches is a kustomize feature that can add, modify or override fields on resources. It is the new way to modify existing and declared manifests</p> <p>Since kustomize 5.0.0, it makes obsolete the following features:</p> <ul> <li>patchesJson6902</li> <li>patchesStrategicMerge</li> </ul>"},{"location":"k8s-resource-handling/kustomize/10-patches/#ways-to-patch","title":"Ways to patch","text":"<p>There are 2 ways to patch a resource using kustomize:</p> <ul> <li>using JSON6902</li> <li>using Strategic Merge</li> </ul> <p>The way to declare the patch can be:</p> <ul> <li> <p>Using a file as reference (with the \"path\" key) In a strategic merge patch, the file will be a yaml file. In a json6902 will be a json file</p> </li> <li> <p>With an inline patch, writing the content in the kustomization file</p> </li> <li> <p>The target resource can be a single resource or multiple resources</p> </li> </ul>"},{"location":"k8s-resource-handling/kustomize/10-patches/#notes-about-patching-specific-array-elements","title":"Notes about patching specific array elements","text":"<p>Kustomize doesn't support fine-grained patching of specific array elements. The standard way to modify an array element while maintaining the others, is to overwrite the whole resource.</p>"},{"location":"k8s-resource-handling/kustomize/10-patches/#links","title":"Links","text":"<ul> <li>Kustomize patches</li> </ul> <p>https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/patches/</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/","title":"Jsonpatch (RFC 6902)","text":"<p>A jsonpatch is a json file or json file content that includes the desired changes to apply to a kubernetes resource</p> <p>A jsonpatch can do almost everything a patchStrategicMerge can do, but with a briefer syntax</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#using-a-file-path","title":"Using a file (path)","text":"<p>ingress_patch.json</p> <pre><code>[\n  {\"op\": \"replace\",\n   \"path\": \"/spec/rules/0/host\",\n   \"value\": \"foo.bar.io\"},\n\n  {\"op\": \"replace\",\n   \"path\": \"/spec/rules/0/http/paths/0/backend/servicePort\",\n   \"value\": 80},\n\n  {\"op\": \"add\",\n   \"path\": \"/spec/rules/0/http/paths/1\",\n   \"value\": { \"path\": \"/healthz\", \"backend\": {\"servicePort\":7700} }}\n]\n</code></pre> <p>kustomization.yaml</p> <pre><code>patches:\n- path: ingress_patch.json\n  target:\n    group: networking.k8s.io\n    version: v1beta1\n    kind: Ingress\n    name: my-ingress\n</code></pre>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#using-inline-patch","title":"Using inline patch","text":"<p>If we want to use the inline option in a json6902 patch, we must provide it this way.</p> <p>\"op\" can be \"add\", \"replace\", or \"remove\".</p> <pre><code>patches:\n  - patch: |-\n      - op: replace\n        path: /spec/template/spec/containers/0/image\n        value: nginx:1.21.0      \n</code></pre>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#special-characters","title":"Special characters","text":"<p>The ~ character is used as an escape character</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#0","title":"~0","text":"<p>In Kustomize, \"~0\" typically refers to a special path indicating the current directory where the kustomization.yaml file is located. This is a shorthand for referencing files within the same directory as the Kustomization configuration. It's commonly used for defining paths to base Kubernetes manifests or patches that Kustomize will use to customize and deploy resources</p> <p>~0 represents a ~ character itself.</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#1","title":"~1","text":"<p>In Kustomize, \"~1\" represents a backslash, which is used to escape characters in a string. When used in a path, it means to include a forward slash (\"/\"). So, \"~1\" translates to \"/\" in Kustomize paths.</p> <pre><code>patches:\n  - patch: |-\n      - op: add\n        path: /metadata/labels/app.kubernetes.io~1version\n        value: 1.21.0    \n</code></pre> <p>https://github.com/kubernetes-sigs/kustomize/issues/1256 https://github.com/kubernetes-sigs/kustomize/issues/907</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#notes","title":"Notes","text":"<p>In the standard JSON merge patch, JSON objects are always merged but lists are always replaced</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#links","title":"Links","text":"<ul> <li>jsonpatch.com</li> </ul> <p>https://jsonpatch.com/</p> <ul> <li>JavaScript Object Notation (JSON) Patch  RFC 6902</li> </ul> <p>https://datatracker.ietf.org/doc/html/rfc6902</p>"},{"location":"k8s-resource-handling/kustomize/11-patches-jsonpatch/#json6902","title":"JSON6902","text":""},{"location":"k8s-resource-handling/kustomize/12-patches-sm/","title":"Strategic Merge Patch (SMP)","text":"<p>An strategic merge patch is a yaml file or yaml file content that includes the group/version/kind/name of the resource to patch and the desired values</p> <p>An strategic merge patch is a kubernetes k8s resource that needs to include:</p> <ul> <li>the group</li> <li>the version</li> <li>the kind</li> <li>the metadata name</li> </ul>"},{"location":"k8s-resource-handling/kustomize/12-patches-sm/#using-a-file-path","title":"Using a file (path)","text":"<p>add-label.patch.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dummy-app\n  labels:\n    app.kubernetes.io/version: 1.21.0\n</code></pre> <p>kustomization.yaml</p> <pre><code>patches:\n  - path: add-label.patch.yaml\n\n</code></pre>"},{"location":"k8s-resource-handling/kustomize/12-patches-sm/#path-without-target","title":"path without target","text":"<pre><code>resources:\n  - original.yaml\npatches:\n  - path: mypatch.yaml\n</code></pre> <p>Using patches without target section, 4 fields must match between the original manifiest and the patch</p> <ul> <li>apiVersion</li> <li>kind</li> <li>metadata.name</li> <li>metadata.namespace, if it is specified in the original manifiest</li> </ul> <p>If this not matches, we will get the find unique target for patch error</p>"},{"location":"k8s-resource-handling/kustomize/12-patches-sm/#using-inline-patch","title":"Using inline patch","text":"<p>If we want to use the inline option in a SMP, we must provide the content as is</p> <pre><code>...\npatches:\n  - patch: |-\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: dummy-app\n        labels:\n          app.kubernetes.io/version: 1.21.0\n...\n</code></pre>"},{"location":"k8s-resource-handling/kustomize/12-patches-sm/#links","title":"Links","text":"<ul> <li>patchStrategicMerge</li> </ul> <p>https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md</p>"},{"location":"k8s-resource-handling/kustomize/98-tips/","title":"Tips","text":""},{"location":"k8s-resource-handling/kustomize/98-tips/#extract-crds","title":"Extract crds","text":"<p>This tool extracts all CRDs from a kubernetes cluster and it writes the discovered json schemas in a local directory</p> <p>https://github.com/datreeio/CRDs-catalog/releases/latest/download/crd-extractor.zip</p>"},{"location":"k8s-resource-handling/kustomize/98-tips/#tools-to-validate-manifests","title":"Tools to validate manifests","text":"<ul> <li>kubeconform</li> </ul> <pre><code>kubectl kustomize . | kubeconform -schema-location default -schema-location '&lt;https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json&gt;' -verbose\n</code></pre> <ul> <li>datree</li> </ul> <pre><code>datree test /path/to/file\n</code></pre> <ul> <li>kubeval</li> </ul> <pre><code>kubeval --additional-schema-locations file:\"/home/jorge.phiguera@bosonit.local/.datree/crdSchemas\" /path/to/file\n</code></pre>"},{"location":"k8s-resource-handling/kustomize/99-links/","title":"Links","text":"<ul> <li>Declarative Management of Kubernetes Objects Using Kustomize</li> </ul> <p>https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/</p> <ul> <li>Kustomize reference</li> </ul> <p>https://kubectl.docs.kubernetes.io/references/kustomize/</p> <ul> <li>Kustomize sig</li> </ul> <p>https://github.com/kubernetes-sigs/kustomize/</p>"},{"location":"k8s-resource-handling/yq/98-tips/","title":"Basic usage","text":""},{"location":"k8s-resource-handling/yq/98-tips/#changing-a-single-value","title":"Changing a single value","text":"<pre><code>yq -i '.field.subfield.key = \"myvalue\"' MYFILE.yaml\n</code></pre>"},{"location":"k8s-resource-handling/yq/99-links/","title":"Links","text":""},{"location":"k8s-resource-handling/yq/99-links/#mike-farah","title":"Mike Farah","text":"<ul> <li>Github</li> </ul> <p>https://github.com/mikefarah/yq</p> <ul> <li>Documentation</li> </ul> <p>https://mikefarah.gitbook.io/yq</p> <ul> <li>Containers</li> </ul> <p>https://hub.docker.com/r/mikefarah/yq</p>"},{"location":"k8s-resource-handling/yq/99-links/#kislyuk","title":"Kislyuk","text":"<ul> <li>Github</li> </ul> <p>https://github.com/kislyuk/yq</p> <ul> <li>Documentation</li> </ul> <p>https://kislyuk.github.io/yq/</p>"},{"location":"k8s-resource-handling/yq/styles/","title":"Tips","text":""},{"location":"k8s-resource-handling/yq/styles/#style-writting-keys","title":"Style writting keys","text":"<p>If the written key does not exists, i do not uses double quotes If it exists, maintains the existing style</p>"},{"location":"k8s-resource-handling/ytt/00-intro/","title":"Intro","text":""},{"location":"k8s-resource-handling/ytt/00-intro/#how-it-works","title":"How it works","text":"<p>Ytt is a command line tool useful for template an patch yaml files. For that purpose, it uses some diferent document types</p>"},{"location":"k8s-resource-handling/ytt/00-intro/#templated","title":"Templated","text":"<p>A templated document in ytt is any YAML file that uses ytt's templating syntax (#@) to create dynamic, configurable Kubernetes manifests or other YAML output. Here is where we define the final yaml file.</p> <p>They do not start with an specific anotation, but they have template logic inside.</p>"},{"location":"k8s-resource-handling/ytt/00-intro/#data-value","title":"Data value","text":"<p>A data value a ytt document that contains the values that the templates can use.</p> <p>It is very similar to a values.yaml files in helm but in this case can be validated against a schema</p> <ul> <li>https://carvel.dev/ytt/docs/latest/how-to-use-data-values/</li> <li>https://carvel.dev/ytt/docs/latest/ytt-data-values/</li> </ul>"},{"location":"k8s-resource-handling/ytt/00-intro/#data-value-schema","title":"Data Value Schema","text":"<p>A data value schema is a ytt document where we define things for the data values documents:</p> <ul> <li>the expected structure. the shape of complex data structures</li> <li>the default values for optional fields</li> <li>types (string, int, bool, etc)</li> <li>validation rules</li> </ul> <p>When providing data values, ytt validates them against this schema to see if the values provided are correct.</p> <ul> <li>https://carvel.dev/ytt/docs/latest/lang-ref-ytt-schema/</li> <li>https://carvel.dev/ytt/docs/latest/how-to-write-validations/</li> <li>https://carvel.dev/ytt/docs/latest/schema-validations-cheat-sheet/</li> </ul>"},{"location":"k8s-resource-handling/ytt/00-intro/#overlay","title":"Overlay","text":"<p>An Overlay Document in ytt is a document that modifies or patches existing YAML documents by applying transformations, additions, deletions, or replacements. It's marked with the #@overlay/match annotation and allows you to make targeted changes to other documents.</p> <ul> <li>https://carvel.dev/ytt/docs/latest/ytt-overlays/</li> <li>https://carvel.dev/ytt/docs/latest/lang-ref-ytt-overlay/</li> </ul>"},{"location":"k8s-resource-handling/ytt/00-intro/#table-and-annotations","title":"Table and annotations","text":"Type Annotation Purpose Templated contains lines with #@ Data Values begins with #@data/values provide values Data Value Schema begins with #@data/values-schema data value validation Overlay begins with @overlay/match Plain No annotations"},{"location":"k8s-resource-handling/ytt/00-intro/#conclusions","title":"Conclusions","text":"<p>In ytt we define template documents with the estructure and logic of the final yaml files we want.</p> <p>We give them values using data value documents, that can be validated against data value schema documents.</p> <p>We can use overlay documents to modify final output without modifying original templates, for example, adding or removing sections.</p>"},{"location":"k8s-resource-handling/ytt/00-intro/#links","title":"Links","text":"<ul> <li>Data Values vs Overlays</li> </ul> <p>https://carvel.dev/ytt/docs/latest/data-values-vs-overlays/</p>"},{"location":"k8s-resource-handling/ytt/99-links/","title":"Links","text":""},{"location":"k8s-resource-handling/ytt/99-links/#ytt-site","title":"YTT site","text":"<p>https://carvel.dev/ytt/</p>"},{"location":"k8s-resource-handling/ytt/99-links/#examples","title":"Examples","text":"<ul> <li> <p>https://github.com/carvel-dev/ytt/tree/develop/examples</p> </li> <li> <p>https://github.com/topics/ytt</p> </li> </ul>"},{"location":"k8s-resource-handling/ytt/data-values/","title":"Data values","text":"<p>If we want to create a template or schema that can be called via data values, we can start our schema file this</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n</code></pre> <p>This imports the the \"data\" struct from the @ytt:data module</p>"},{"location":"k8s-resource-handling/ytt/data-values/#declaring-data-values","title":"Declaring data values","text":"<p>A data value declaration has 3 parts: a name, a default value, and a type.</p> <pre><code>this-is-the-name: this-is-the-default-value\nmap:\n  integer: 45323\n  boolean: true\narray:\n- default-empty-string: \"\"\n  boolean: false\n</code></pre> <p>Or it can be called via data values</p> <pre><code>name: #@ data.values.name\n</code></pre>"},{"location":"k8s-resource-handling/ytt/data-values/#schema-validations","title":"Schema validations","text":""},{"location":"k8s-resource-handling/ytt/overlay/","title":"Overlay","text":"<p>An Overlay Document in ytt is a document that modifies or patches existing YAML documents by applying transformations, additions, deletions, or replacements. It's marked with the #@overlay/match annotation and allows you to make targeted changes to other documents.</p> <p>Purpose:</p> <ul> <li>Modify existing documents: Change specific fields or sections</li> <li>Add new content: Insert additional fields, containers, or resources</li> <li>Remove content: Delete unwanted fields or sections</li> <li>Replace values: Override specific values in targeted documents</li> <li>Conditional modifications: Apply changes based on conditions</li> </ul> <pre><code>#@overlay/match by=overlay.subset({\"kind\": \"Deployment\"})\n---\nspec:\n  #@overlay/match missing_ok=True\n  template:\n    spec:\n      containers:\n      #@overlay/match by=\"name\"\n      - name: app\n        #@overlay/replace\n        image: nginx:1.22\n        #@overlay/insert\n        env:\n        - name: DEBUG\n          value: \"true\"\n</code></pre> <p>Common overlay operations:</p> <ul> <li>@overlay/match: Select which documents/fields to modify</li> <li>@overlay/replace: Replace the entire value</li> <li>@overlay/insert: Add new fields or array items</li> <li>@overlay/remove: Delete fields or array items</li> <li>@overlay/append: Add to the end of arrays</li> <li>@overlay/merge: Merge objects together</li> </ul> <p>Use cases:</p> <ul> <li>Environment-specific changes: Different configs for dev/staging/prod</li> <li>Feature toggles: Enable/disable features conditionally</li> <li>Security patches: Add security contexts or policies</li> <li>Resource adjustments: Modify CPU/memory limits per environment</li> <li>Multi-tenancy: Customize base templates per tenant</li> </ul> <p>How it works:</p> <ul> <li>Target selection: Uses match criteria to find documents to modify</li> <li>Transformation: Applies the specified changes (replace, insert, remove, etc.)</li> <li>Output: Produces the modified YAML with changes applied</li> </ul> <p>In summary: An overlay document is ytt's way of applying targeted patches or modifications to existing YAML documents, enabling flexible customization without duplicating entire templates.</p>"},{"location":"kubernetes/98-tips/","title":"Tips","text":""},{"location":"kubernetes/98-tips/#how-to-use-an-environment-variable-as-an-arguement","title":"How to use an environment variable as an arguement","text":"<p>We must use this format</p> <pre><code>$(VAR_NAME)\n</code></pre> <p>For example</p> <pre><code>args:\n- echo \n- $(VAR_NAME)\nenv:\n- name: VAR_NAME\n  value: myvalue\n</code></pre>"},{"location":"kubernetes/99-links/","title":"Links","text":"<ul> <li>Official website</li> </ul> <p>https://kubernetes.io/</p> <ul> <li>Kubernetes Failure Stories</li> </ul> <p>https://k8s.af/</p>"},{"location":"kubernetes/compat/","title":"Kubernetes compatibilities and requirements","text":""},{"location":"kubernetes/compat/#keywords","title":"Keywords","text":"<ul> <li>Compatibility Matrix</li> <li>System Requirements</li> <li>Supported Releases</li> </ul>"},{"location":"kubernetes/compat/#cilium","title":"Cilium","text":"<p>Relation with kubernetes releases, linux distributions, linux kernel and others</p> <p>https://docs.cilium.io/en/stable/network/kubernetes/compatibility/ https://docs.cilium.io/en/stable/operations/system_requirements/</p>"},{"location":"kubernetes/compat/#karpenter","title":"Karpenter","text":"<p>https://karpenter.sh/docs/upgrading/compatibility/</p>"},{"location":"kubernetes/compat/#kyverno","title":"Kyverno","text":"<p>Relation with kubernetes releases</p> <p>https://kyverno.io/docs/installation/</p>"},{"location":"kubernetes/compat/#argocd","title":"Argocd","text":"<p>Tested kubernetes versions</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/</p>"},{"location":"kubernetes/compat/#jaeger-operator","title":"Jaeger Operator","text":"<p>https://github.com/jaegertracing/jaeger-operator/blob/main/COMPATIBILITY.md</p>"},{"location":"kubernetes/dashboards/","title":"Kubernetes Dashboard Alternatives","text":"<p>The official Kubernetes Dashboard was archived in January 2026 and no longer receives security updates. Below are open-source alternatives for cluster visualization and management.</p>"},{"location":"kubernetes/dashboards/#comparison-table","title":"Comparison Table","text":"Project CNCF Status Multi-Cluster Auth Focus Headlamp Sandbox / SIG UI Yes OIDC, RBAC General-purpose dashboard Skooner Sandbox No OIDC, SA token Lightweight real-time UI Meshery Sandbox Yes OIDC, LDAP Cloud native manager Karpor KusionStack Yes RBAC AI-powered search &amp; insight Cyclops - No RBAC Developer self-service UI Devtron - Yes RBAC, SSO CI/CD + dashboard platform <p>All projects listed are Apache 2.0 licensed.</p>"},{"location":"kubernetes/dashboards/#project-maturity","title":"Project Maturity","text":"<p>Based on GitHub activity, release frequency, and community engagement:</p> <p>\ud83d\udfe2 Highly Active - Recommended for production:</p> <ul> <li>Meshery (9.9k stars, 315 contributors, v0.8.205 Feb 2026)</li> <li>Headlamp (5.7k stars, 170 contributors, v0.40.1 Feb 2026)</li> <li>Devtron (5.4k stars, 101 contributors, v2.0.0 Dec 2025)</li> </ul> <p>\ud83d\udfe1 Moderately Active - Suitable but slower release cycle:</p> <ul> <li>Cyclops (3.3k stars, last commit Jan 2026)</li> <li>Karpor (1.7k stars, last commit Dec 2025)</li> </ul> <p>\ud83d\udd34 Stale - Use with caution:</p> <ul> <li>Skooner (1.4k stars, last commit Jun 2024 - 19 months ago)</li> </ul>"},{"location":"kubernetes/dashboards/#headlamp","title":"Headlamp","text":"<ul> <li>Repository: kubernetes-sigs/headlamp</li> <li>CNCF Sandbox project, part of kubernetes-sigs</li> <li>Official recommended migration path from Kubernetes Dashboard</li> <li>Web UI and desktop app (Electron)</li> <li>Plugin ecosystem for extending functionality</li> <li>Multi-cluster support with OIDC and RBAC authentication</li> <li>Dynamic and themeable interface</li> </ul>"},{"location":"kubernetes/dashboards/#skooner","title":"Skooner","text":"<ul> <li>Repository: skooner-k8s/skooner</li> <li>CNCF Sandbox project (formerly k8dash)</li> <li>Lightweight, real-time dashboard using WebSockets (no polling)</li> <li>Built with React + Express.js, single service (no DB or cache needed)</li> <li>Authentication via OIDC, ServiceAccount token, or NodePort</li> <li>Responsive UI, mobile-friendly</li> <li>Requires <code>metrics-server</code> for resource usage views</li> </ul>"},{"location":"kubernetes/dashboards/#meshery","title":"Meshery","text":"<ul> <li>Repository: meshery/meshery</li> <li>CNCF Sandbox project, broader scope than a simple dashboard</li> <li>Cloud native manager with multi-cluster management</li> <li>GitOps-centric design with REST and GraphQL APIs</li> <li>Pluggable adapters for CNCF projects and service meshes</li> <li>Visual topology and performance management</li> <li>AI-assisted operations via natural language</li> </ul>"},{"location":"kubernetes/dashboards/#karpor","title":"Karpor","text":"<ul> <li>Repository: KusionStack/karpor</li> <li>AI-powered Kubernetes explorer from the KusionStack ecosystem</li> <li>Multi-cluster resource search with natural language queries</li> <li>Compliance governance and security posture insights</li> <li>Resource topology visualization across clusters</li> <li>Built-in risk scanning and audit capabilities</li> </ul>"},{"location":"kubernetes/dashboards/#cyclops","title":"Cyclops","text":"<ul> <li>Repository: cyclops-ui/cyclops</li> <li>Developer-friendly UI framework for Kubernetes</li> <li>Renders dynamic forms from Helm charts (no YAML editing required)</li> <li>Kubernetes operator with a <code>Module</code> CRD</li> <li>GitOps support and Backstage plugin available</li> <li>Focused on developer self-service and Internal Developer Platforms (IDP)</li> </ul>"},{"location":"kubernetes/dashboards/#devtron","title":"Devtron","text":"<ul> <li>Repository: devtron-labs/devtron</li> <li>Full DevOps platform with dashboard + integrated CI/CD pipelines</li> <li>Multi-cluster management with centralized visibility</li> <li>RBAC + 7 SSO providers (OAuth2, SAML, LDAP, etc.)</li> <li>Live manifest editing and integrated terminal for debugging</li> <li>AI-assisted debugging and no-code CI/CD workflows with Helm</li> <li>GitOps integration and ArgoCD compatibility</li> </ul>"},{"location":"kubernetes/dashboards/#migration-from-kubernetes-dashboard","title":"Migration from Kubernetes Dashboard","text":"<p>When migrating from the archived Kubernetes Dashboard, prioritize actively maintained projects:</p> <ol> <li>Best match (production-ready): Headlamp \u2014 closest UX parity with official dashboard</li> <li>Multi-cluster focus: Meshery (full platform), Headlamp, or Devtron</li> <li>CI/CD integration: Devtron combines dashboard with pipelines</li> <li>Developer platforms: Cyclops for Helm-based self-service</li> <li>\u26a0\ufe0f Avoid for production: Skooner is unmaintained (last commit Jun 2024)</li> </ol>"},{"location":"kubernetes/dashboards/#links","title":"Links","text":"<ul> <li>Kubernetes Dashboard Archive Notice</li> <li>Headlamp Documentation</li> <li>Skooner Documentation</li> <li>Meshery Documentation</li> <li>Karpor Documentation</li> <li>Cyclops Documentation</li> <li>Devtron Documentation</li> </ul>"},{"location":"kubernetes/hack-dns/","title":"Hack dns","text":""},{"location":"kubernetes/hack-dns/#redirect-internal-service-to-external-fqdn","title":"Redirect internal service to external fqdn","text":"<p>We can redirect the dns resolution of an internal service to an external fqdn</p> <p>We can achieve this using an external name service. With the following example, \"my-service\" will return \"my.fqdn.url\"</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  type: ExternalName\n  externalName: my.fqdn.url\n</code></pre>"},{"location":"kubernetes/hack-dns/#redirect-external-fqdn-to-internal-service","title":"Redirect external fqdn to internal service","text":"<p>We can do that changing coredns configuration. For example this line redirects \"my.fqdn.url\" to \"my-service.default.svc.cluster.local\"</p> <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        rewrite name my.fqdn.url my-service.default.svc.cluster.local\n        ...\n    }\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n</code></pre> <p>This needs to restart coredns deployment</p> <p>https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/</p>"},{"location":"kubernetes/hack-dns/#redirect-an-domain-to-a-custom-dns-server","title":"Redirect an domain to a custom dns server","text":"<p>We can redirect all domain resolutions to a custom dns server</p> <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        forward graphenus.internal my.dnsserver.ip\n        ...\n    }\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n</code></pre> <p>https://coredns.io/manual/setups/</p>"},{"location":"kubernetes/hack-dns/#custom-dns-entries-in-a-pod","title":"Custom dns entries in a pod","text":"<p>In pod spec.hostAliases we can set a list of entries that contains a hostname and an ip to give custom resolutions to a pod. This adds a /etc/hosts entry</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test\nspec:\n  hostAliases:\n  - ip: \"127.0.0.1\"\n    hostnames:\n    - \"foo.local\"\n    - \"bar.local\"\n  - ip: \"10.1.2.3\"\n    hostnames:\n    - \"foo.remote\"\n    - \"bar.remote\"\n  ...\n</code></pre> <p>https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/</p>"},{"location":"kubernetes/hack-dns/#core-dns-in-eks","title":"Core dns in EKS","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html</p> <p>https://repost.aws/knowledge-center/eks-conditional-forwarder-coredns</p>"},{"location":"kubernetes/autoescaling-finops/autoescaling-nodes/","title":"Autoescaling nodes","text":""},{"location":"kubernetes/autoescaling-finops/autoescaling-nodes/#solutions","title":"Solutions","text":"<ul> <li> <p>Cluster Autoescaler https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler</p> </li> <li> <p>Karpenter https://karpenter.sh/</p> </li> <li> <p>Google GKE Autopilot https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</p> </li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/","title":"Cloud Cost Solutions","text":"<p>Open-source solutions for monitoring and analyzing cloud costs in Kubernetes environments.</p>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#opencost","title":"OpenCost","text":"<p>OpenCost is a vendor-neutral open-source project for measuring and allocating infrastructure and container costs in real-time. It's a CNCF Incubating project that provides cost visibility for Kubernetes workloads.</p> <ul> <li>GitHub Stars: 6.1k</li> <li>Contributors: 140</li> <li>Company Behind: Originally developed by Kubecost (Stackwatch Inc.), now owned by IBM (since September 2024)</li> <li>CNCF Status: Incubating (accepted as Sandbox in June 2022, promoted to Incubating in October 2024)</li> <li>CNCF Member: IBM is a founding member of CNCF (2015)</li> <li>Contributing Organizations: Kubecost, RedHat, AWS, Adobe, SUSE, Armory, Google Cloud, Pixie, Mindcurv, D2IQ, New Relic</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#opencost-references","title":"OpenCost References","text":"<ul> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#kubecost","title":"Kubecost","text":"<p>Kubecost provides real-time cost visibility and insights for Kubernetes clusters. While it offers commercial enterprise features, the core product (formerly known as Kubecost Free) is available as open-source and provides comprehensive cost monitoring capabilities.</p> <ul> <li>GitHub Stars: 577</li> <li>Contributors: 150+</li> <li>Company Behind: Kubecost (Stackwatch Inc.), acquired by IBM in September 2024</li> <li>CNCF Relation: Not a CNCF project (commercial product), but created and donated OpenCost to CNCF</li> <li>CNCF Member: Kubecost was a CNCF member; now part of IBM (CNCF founding member)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#kubecost-references","title":"Kubecost References","text":"<ul> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Kubecost Blog</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#grafana-cloud-cost-exporter","title":"Grafana Cloud Cost Exporter","text":"<p>Grafana Cloud Cost Exporter is an open-source Prometheus exporter that collects cloud provider cost metrics and makes them available for visualization in Grafana dashboards. It focuses on exporting billing data as Prometheus metrics rather than providing a complete cost allocation platform.</p> <ul> <li>GitHub Stars: 102</li> <li>Contributors: 18+</li> <li>Company Behind: Grafana Labs</li> <li>CNCF Relation: Not a CNCF project, but Grafana Labs contributes to OpenCost and other CNCF projects</li> <li>CNCF Member: Grafana Labs is a CNCF Platinum member (upgraded from Silver in July 2021)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#grafana-cloud-cost-exporter-references","title":"Grafana Cloud Cost Exporter References","text":"<ul> <li>GitHub Repository</li> <li>Grafana Cloud Cost Dashboards</li> <li>AWS Cost Explorer API</li> <li>Azure Cost Management API</li> <li>GCP Billing Export</li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/","title":"Horizontal pod autoescaler","text":""},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#definir-que-queremos-escalar","title":"Definir que queremos escalar","text":"<p>Se hace mediante spec.scaleTargetRef</p> <pre><code>    scaleTargetRef:\n      apiVersion: apps/v1\n      kind: Deployment\n      name: mideployment\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#replicas-deseadas","title":"Replicas deseadas","text":"<p>Se hace mediante:</p> <ul> <li>spec.minReplicas</li> <li>spec.maxReplicas</li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#donde-buscar-la-metrica","title":"Donde buscar la metrica","text":"<p>Al definir una metrica de escalado, debemos definir mediante \"type\" donde mirar</p> <ul> <li> <p>Resource: Con Resource se mira el uso de requests de cpu o memoria de los contenedores de un pod</p> </li> <li> <p>ContainerResource Con ContainerResource, estable desde kubernetes 1.30 mirara el uso de requests de cpu o memoria de un contenedor individual dentro de un pod</p> </li> <li> <p>Pods Pods permite utilizar custom metrics</p> </li> <li> <p>External Para usar metricas externas a kubernetes</p> </li> <li> <p>Object Object mirara en un objeto de kubernetes</p> </li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#escalados","title":"Escalados","text":""},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-al-porcentaje-de-uso-de-requests","title":"En base al porcentaje de uso de requests","text":"<p>Con target.type: Utilization y averageUtilization podemos especificar un numero que sera un porcentaje del uso de resource requests (cpu y/o memoria). HPA intentara mantener ese porcentaje mediante el escalado.</p> <p>Solo esta soportado para ContainerResource y Resource</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: memory\n      target:\n        averageUtilization: 70 # media de un 70% de los requests de memoria\n        type: Utilization\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-a-un-promedio","title":"En base a un promedio","text":"<p>Con target.type: AverageValue + averageValue podemos especificar un quantity como una media de un valor de la metrica. HPA intentara mantener ese valor promedio mediante el escalado.</p> <p>En el caso de recursos de pods, sera bytes para memoria y milicores para CPU.</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: cpu\n      target:\n        averageValue: 500 # media de 500 milicores\n        type: AverageValue\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-a-un-valor-absoluto","title":"En base a un valor absoluto","text":"<p>Con target.type: Value + value  definimos un un quantity como un valor absoluto de la metrica. HPA intentara mantener ese valor mediante el escalado.</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: cpu\n      target:\n        value: 2 # uso de 2 cpus sumando los pods\n        type: Value\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#behaviour","title":"Behaviour","text":"<p>Con spec.behaviour se puede controlar la frecuencia y velocidad de escalado y desescalado</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#stabilizationwindowseconds","title":"stabilizationWindowSeconds","text":"<p>Con stabilizationWindowSeconds se puede reducir la frecuencia de escalado o desescalado cuando las metricas cambian. Con este valor, se definen una ventana de valores que se tendran en cuenta a la hora del escalado o desescalado.</p> <p>El valor puede ser desde 0 (no hay estabilzacion) hasta 3600 (un dia). Por defecto, al escalar hacia arriba el valor es 0 (no hay estabilizacion) y al escalar hacia abajo 300.</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#policies","title":"Policies","text":"<p>Con las policies se pueden definir una serie de posibles politicas de escalado.</p> <pre><code>behavior:\n  scaleDown:\n    policies:\n    - type: Pods # this permits to scale down 3 replicas in 2 minutes\n      value: 3\n      periodSeconds: 120\n    - type: Percent # this permits at most 20% of the current replicas to be scaled down in 3 minutes\n      value: 20\n      periodSeconds: 180\n</code></pre> <p>Si hay varias politicas definidas, por defecto se aplica la que permite mas cambios. Con selectPolicy se puede cambiar este comportamiento y elegir que politica usar.</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#links","title":"Links","text":"<ul> <li>Horizontal Pod Autoscaling</li> </ul> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</p> <ul> <li>HorizontalPodAutoscaler Walkthrough</li> </ul> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</p> <ul> <li>autoscaling/v2 API</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/","title":"Scale to 0","text":"<p>It can be useful to scale to 0 when desired some workloads in order to reduce resource utilizacion. In addition to this, downscaling nodes will make a cheaper deployment.</p> <p>So, scale to 0 the deployments you need and the let karpenter o another tool to consolidate and delete nodes.</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#note-about-gitops-tools","title":"Note about gitops tools","text":"<p>If you are using a gitops tool like argocd or flux, take care about the interactions with the original manifests. Argocd recommends to not specify the replicas in your manifests and let them to be managed by horizontal pod autoescaler or these tools.</p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#keda","title":"Keda","text":"<p>Keda has a scaler called \"cron\" that permits to scale to 0 the desired workloads when needed. It alson contains a lot of ways to scale our workloads.</p> <p>https://keda.sh/docs/latest/scalers/cron/</p> <p>Keda recommends not to use horizontal pod autoescaler with keda</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#kube-green","title":"Kube-green","text":"<p>Kube green permit sto \"sleep\" your pods</p> <p>https://kube-green.dev/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#snorlax","title":"Snorlax","text":"<p>Snorlax is a tool that also permits to sleep your workloads</p> <p>https://github.com/moonbeam-nyc/snorlax</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#sleepcycles","title":"Sleepcycles","text":"<p>Another tool. They say the argocd self healing must be disabled</p> <p>https://github.com/rekuberate-io/sleepcycles</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/","title":"Vertical por autoescaling","text":"<p>In the context of Vertical Pod Autoscaling (VPA) in Kubernetes, the terms lower bound, target, uncapped target, and upper bound refer to the recommendations provided by the VPA for setting the resource requests for pods. These recommendations are calculated based on historical usage data, current resource requests, and limits defined in the pod's specification. Here's what each term means:</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#recommendations","title":"Recommendations","text":""},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#lower-bound","title":"Lower Bound","text":"<p>This is the minimum amount of resources the VPA recommends for the pod to function correctly based on its observed resource usage. Setting the resource requests below this value might lead to insufficient resources for the pod, potentially causing performance issues or even failures</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#target","title":"Target","text":"<p>This is the VPA's recommendation for the resource requests that should be set for the pod to ensure optimal performance under normal workload conditions. The target value is calculated based on the pod's historical resource usage, aiming to balance resource efficiency with sufficient headroom for typical workload variations</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#uncapped-target","title":"Uncapped Target","text":"<p>The uncapped target is similar to the target but without considering the resource limits set on the pod. This value represents what the VPA calculates the pod needs based on its usage, without being constrained by the current resource limits. If the uncapped target is higher than the upper bound (i.e., the current limit), it indicates that the pod's performance might be constrained by its resource limits</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#upper-bound","title":"Upper Bound","text":"<p>This is the maximum amount of resources the VPA believes the pod might need under peak conditions observed during the analysis period. Setting the resource requests or limits at or above this value should ensure that the pod has enough resources to handle spikes in workload without being throttled or running out of resources</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/","title":"Disruption","text":"<p>Karpenter's disruption is the process that makes Karpenter terminates nodes in the Kubernetes cluster.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#planning-phase-disruption-controller","title":"Planning phase (disruption controller)","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#search-candidates","title":"Search candidates","text":"<p>The disruption controller is continuously discovering nodes that can be disrupted because of this reasons:</p> <ul> <li>drift</li> <li>consolidation (empty)</li> <li>consolidation (underutilized)</li> </ul> <p>The number of candidates per reason is located in the karpenter_voluntary_disruption_eligible_nodes prometheus metric</p> <p>First, the disruption controller starts searching for candidates for a drift disruption and it gives them priorities. If a node in that list has pods that cannot be evicted from the node, the node is ignored for now.</p> <p>The disruption can be blocked here because of:</p> <ul> <li>Pod disruption budgets</li> </ul> <p>Pods in the node affected by disruption buckets with 0 ALLOWED DISRUPTIONS</p> <ul> <li>karpenter.sh/do-not-disrupt</li> </ul> <p>Pods in the node with karpenter.sh/do-not-disrupt: \"true\" annotation. If the nodeclaim has terminationGracePeriod configured, it will still be eligible for disruption via drift.</p> <p>We can see that blocked nodes with</p> <pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node | grep DisruptionBlocked\n</code></pre> <p>If no nodes cannot be disrupted, the same process will start with the consolidation disruption.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#evaluate-candidates","title":"Evaluate candidates","text":"<ul> <li>NodePool\u2019s disruption budget</li> </ul> <p>The next step is to check the if the node respect the NodePool\u2019s disruption budget, a mechanism to control the speed of the disruption process.</p> <ul> <li>Evaluate if new nodes are needed</li> </ul> <p>Then the disruption controller does a simulation to estimate if any replacement nodes are needed.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#taint-the-nodes","title":"Taint the nodes","text":"<p>Next, the chosen node(s) are tainted with karpenter.sh/disrupted:NoSchedule to prevent new pods being scheduled there.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#deploy-replacement-nodes","title":"Deploy replacement nodes","text":"<p>If new replacement nodes are needed, the disruption controller triggers their deployment and wait until they are deployed. If the deployment fails, the node(s) is(are) untainted and the whole process starts again.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#node-deletion","title":"Node deletion","text":"<p>Here the disruption controller deletes the node. All the Nodes and NodeClaims deployed via Karpenter have a kubernetes finalizer karpenter.sh/termination.  So the the deletion is blocked leaves that task to the termination controller.</p> <p>When the termination controller terminates the node, the whole process starts again.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#execution-phase-termination-controller","title":"Execution phase (termination controller)","text":"<p>The termination controller is responsible to finally delete the node. The deletion if blocked by the finalizer. This deletion can be triggered by:</p> <ul> <li>the disruption controller</li> <li>a user using manual disruption</li> <li>an external system that deletes the node resource</li> </ul> <p>The APIServer has added the DeletionTimestamp on the node</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#taint","title":"Taint","text":"<p>The chosen node(s) is(are) tainted with karpenter.sh/disrupted:NoSchedule to prevent new pods being scheduled there. Depending of the disruption method, that taint can exist.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#eviction","title":"Eviction","text":"<p>The termination controller starts evicting the pods using the Kubernetes Eviction API.</p> <ul> <li>This respects Pod disruption budgets</li> <li>Static pods, pods tolerating the karpenter.sh/disrupted:NoSchedule taint, and succeeded/failed pods are ignored</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#cleaning","title":"Cleaning","text":"<ul> <li>When the node is drained, the NodeClaim is deleted</li> <li>Finally the finalizer is removed from the node so the APIServer can remove it</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#forceful-deletion","title":"Forceful deletion","text":"<p>In expiration and interruption methods the disruption controller immediately triggers tainting and draining as soon as the event is detected (interruption signal or expireAfter).</p> <ul> <li>That methods do not respect NodePool\u2019s disruption budget.</li> <li>Pod disruption budgets can be used to control the disruption speed at application level.</li> <li>That methods they do not wait for a replacement node to be healthy</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/","title":"Disruption methods","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#manual-deletion","title":"Manual deletion","text":"<ul> <li>Node</li> </ul> <p>A node can be manually deleted by a user or by an external system. When a node without this finalizer is deleted (via kubectl or api call), the instance will not be deleted from AWS EC2. It is only unregistered from the kubernetes cluster. All the containing pods will deleted by a gargage collection and they will start in another node. This karpenter finalizer improves the deletion process.</p> <ul> <li>Nodeclaim</li> </ul> <p>The karpenter nodes are associated to a nodeclain. Deleting the nodeclain also deletes the node.</p> <ul> <li>Nodepool</li> </ul> <p>The nodepools are the owners of the nodeclaims (ownerReferences). If a nodepool is deleted, the associated nodeclaims and nodes will be deleted.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automatic-graceful-methods-consolidation","title":"Automatic graceful methods: Consolidation","text":"<p>With this method karpenter tries to reduce the cluster cost deleting nodes or replacing them.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#consolidationpolicy","title":"consolidationPolicy","text":"<p>Karpenter can delete nodes:</p> <ul> <li>when the node is empty. This is when it has no daemonset related pods running. (deletion mechanism)</li> <li>when the workloads can run in another nodes. (deletion mechanism)</li> <li>when the nodes can be replaced with cheaper variants. (replace mechanism)</li> </ul> <p>There are 2 consolidation policies: WhenEmptyOrUnderutilized (default) and WhenEmpty and it can be specified in spec.disruption.consolidationPolicy of the nodepool.</p> <p>The order of the actions that the consolidation tries to do is:</p> <ul> <li>delete all the empty nodes in parallel</li> <li>delete 2 or more nodes and possibly creating a new one if this is a cheaper solution</li> <li>delete a single node and possibly creating a new one if this is a cheaper solution</li> </ul> <p>Nodes with fewer pods, or with upcoming expiration or with lower priority pods will be better candidates to be consolidated</p> <p>Things like the anti-affinity, pod disruption budgets or topology spreads affects the effectiveness of the consolidation</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#spot-to-spot-consolidation","title":"Spot to spot consolidation","text":"<p>The spot nodes are consolidated by default with the deletion mechanism.</p> <p>It is possible to enable the replace one through the SpotToSpotConsolidation feature flag karpenter considers another things in addition to the cheapest price. It also needs a minimum of 15 instance types to work and possibility to be interrupted is also observed.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#consolidateafter","title":"consolidateAfter","text":"<p>When a pod is added or deleted from a node, karpenter starts to calculate if the node is consolidatable when the value specified in spec.disruption.consolidateAfter is reached. With this we can tell karpenter to be more cautious or aggressive in terms of consolidation.</p> <p>We can disable the consolidation with the \"Never\" value here</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automatic-graceful-methods-drift","title":"Automatic graceful methods: Drift","text":"<p>The drift method tries to reconciliate the desired state of the nodepools and ec2nodeclasses with the actual one. In order to check if there is a drift, karpenter compares some fields in that resources. It also maintains some hashes in the resources.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automated-forceful-methods-expiration","title":"Automated forceful methods: Expiration","text":"<p>It is possible to expire nodes with the spec.template.spec.expireAfter field. The default vale is 720 hours (30 days)</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automated-forceful-methods-interruption","title":"Automated forceful methods: Interruption","text":"<p>When this methods karpenter watch some events that can cause involuntary interruptions.</p> <ul> <li>AWS will reclaim an spot instance</li> <li>Maintenance tasks</li> <li>Instance deletion events</li> <li>Instance stopping events</li> </ul> <p>Then karpenter sends a drain, taint and deletion of the node.</p> <p>With the spot interruption warnings, there are 2 minutes to solve the situation. In order to get the events we need to configure an sqs queue and a some EventBridge rules. Also, by default, karpenter does not manage the Spot Rebalance Recommendations.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/","title":"Disruption speed","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/#nodepool-terminationgraceperiod","title":"Nodepool TerminationGracePeriod","text":"<p>The spec.template.spec.terminationGracePeriod in a nodepool sets the time a node can be in a draining state before being forcibly deleted. Changing this value in the nodepool will drift the nodeclaims.</p> <p>During this time, the pods are being deleted based on the terminationGracePeriodSeconds pods setting until terminationGracePeriod in the nodepool reaches, so it can be a good practice to set the terminationGracePeriod with a greater value than the higher terminationGracePeriodSeconds. With this command we can get the 5 higher terminationGracePeriod setting in all the pods of the cluster:</p> <pre><code>kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.terminationGracePeriodSeconds}{\"\\n\"}{end}' | sort -k 2,2n | tail -5\n</code></pre> <p>Warning. This command does not makes a relation between the pod and the nodepool where it is deployed</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/#nodepool-disruption-budgets","title":"Nodepool Disruption Budgets","text":"<p>With spec.disruption.budgets we can control the speed of the disruption defining budgets. In a budget we can set some filters and settings:</p> <ul> <li>nodes:</li> </ul> <p>Limit the maximum nodes of the nodepool that can be deleted at the same time. It can be an number or a percentage. A \"0\" value disables the disruption for this budget.</p> <ul> <li>reasons:</li> </ul> <p>This budget applies to nodes that they are being disrupted for the specified reasons. The possible values are Empty, Drifted and Underutilized</p> <ul> <li>schedule:</li> </ul> <p>Cron like schedule when this budget applies</p> <ul> <li>duration:</li> </ul> <p>It is a needed setting when using schedule. It defines the time this budget is active when the schedule starts.</p> <p>By default, there is only a budget with nodes: 10%</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/","title":"Scheduling","text":"<p>In kubernetes we can define some settings to limit where a pod can be scheduled, and this affects karpenter.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#resources-requests-and-limits","title":"Resources (requests and limits)","text":"<p>It is a kubernetes best practice to define in our workload correct values for CPU/Memory requests and limits. In Karpenter scenarios this particularly important.</p> <p>In terms of karpenter scheduling, the important setting here is the CPU/Memory request value, because if a pod does not have a node where to be deployed, it will be in \"Pending\" state, triggering a new nodeclaim.</p> <p>But it is a karpenter best practice to give the memory request and limit the same value to avoid (OOM) conditions in situations where some pods can exceed their requests and the same time. For example, during a consolidation or a simply drain.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#restrict-nodes","title":"Restrict nodes","text":"<p>There are some ways to restrict the nodes where the pods can be scheduled.</p> <ul> <li> <p>nodeSelector is the simplest way, using node labels in the definition of the pods.</p> </li> <li> <p>affinity is for more complex situations we can use affinity in pods. Here we can use nodeAffinity, podAffinity and podAntiAffinity.</p> </li> <li> <p>taints and tolerations. Here we define taints in the nodes. Then we define tolerations at pod level to permit them.</p> </li> <li> <p>topologySpreadConstraints permits to control de distribution of pods in your cluster using.</p> </li> </ul> <p>Karpenter supports the following topologyKey(s):  </p> <ul> <li>topology.kubernetes.io/zone  </li> <li>kubernetes.io/hostname  </li> <li>karpenter.sh/capacity-type  </li> </ul> <p>For more information visit the folling links</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#links","title":"Links","text":"<ul> <li>Scheduling</li> </ul> <p>https://karpenter.sh/docs/concepts/scheduling/</p> <ul> <li>Karpenter Best practices</li> </ul> <p>https://docs.aws.amazon.com/eks/latest/best-practices/karpenter.html</p> <ul> <li>Pod Scheduling API</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/","title":"spotToSpotConsolidation","text":"<p>spotToSpotConsolidation is a karpenter feature (alpha since v0.34.x) that permits to replace existing Spot Instances with new Spot Instances that are more cost-effective or better suited to your workloads.</p> <p>spotToSpotConsolidation is disabled by default because increases the risk of workload interruptions because it can increase the termination and creation of spot instances. By keeping it disabled by default, Karpenter ensures a more stable and predictable environment.</p> <p>It is an aggresive way to optimize costs and be more resource effective</p> <p>In 1 node to 1 node consolidations, Karpenter requires at least 15 different instance types  a minimum instance type flexibility of 15 candidate instance types. flexibility won\u2019t lead to \u201crace to the bottom\u201d scenarios.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#recommended-environments","title":"Recommended environments","text":"<p>This situations make spotToSpotConsolidation a good option</p> <ul> <li>Clusters with high tolerance to interruptions</li> <li>Non production clusters</li> <li>Environments where Spot Instance prices fluctuate frequently</li> </ul> <p>To make a cluster more interruption tolerant, there are features like Pod Disruption Budgets or graceful termination hooks</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#non-recommended-environments","title":"Non recommended environments","text":"<ul> <li>Production clusters</li> </ul> <p>Test it in non production clusters first</p> <ul> <li>The workloads are stateful</li> </ul> <p>Stateful applications (e.g., databases, message queues) are not ideal for frequent Spot Instance replacements. Moving them to on demand nodes can be a good practice</p> <ul> <li>Ha apps</li> </ul> <p>If your workloads require consistent availability and cannot tolerate interruptions, this feature may introduce unnecessary risk.</p> <ul> <li>Spot Market Is Unstable</li> </ul> <p>In regions or zones where Spot Instance availability is highly volatile, frequent consolidations may lead to excessive disruptions.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#how-to-enable","title":"How to enable","text":"<p>It is a feature flag / gate. To enable it via helm, we need this</p> <pre><code>settings:\n  featureGates:\n    spotToSpotConsolidation: true\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#links","title":"Links","text":"<ul> <li>Disruption</li> </ul> <p>https://karpenter.sh/docs/concepts/disruption/</p> <ul> <li>Applying Spot-to-Spot consolidation best practices with Karpenter  </li> </ul> <p>https://aws.amazon.com/blogs/compute/applying-spot-to-spot-consolidation-best-practices-with-karpenter/</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/","title":"Tips","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#show-karpenter-nodes","title":"Show karpenter nodes","text":"<pre><code>eks-node-viewer -disable-pricing -node-selector karpenter.sh/registered=true\n</code></pre> <pre><code>kubectl get node -o yaml | grep -A 1 finalizer\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#get-the-node-events","title":"Get the node events","text":"<pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#cannot-disrupt-node","title":"Cannot disrupt Node","text":"<pre><code>Cannot disrupt Node: state node doesn't contain both a node and a nodeclaim\n</code></pre> <p>This can tell you this node is not managed by karpenter</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#spottospotconsolidation-is-disabled","title":"SpotToSpotConsolidation is disabled","text":"<pre><code>SpotToSpotConsolidation is disabled, can't replace a spot node with a spot node\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#cant-replace-with-a-cheaper-node","title":"Can't replace with a cheaper node","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#pdb-xxx-prevents-pod-evictions","title":"Pdb XXX prevents pod evictions","text":"<pre><code>Pdb XXX prevents pod evictions\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/99-links/","title":"Links","text":"<ul> <li>Official Karpenter Docs  </li> </ul> <p>https://karpenter.sh/docs/</p> <ul> <li>Karpenter Best Practices  </li> </ul> <p>https://aws.github.io/aws-eks-best-practices/karpenter/</p> <ul> <li>Karpenter Blueprints for Amazon EKS  </li> </ul> <p>https://github.com/aws-samples/karpenter-blueprints</p> <ul> <li>Optimizing Karpenter on EKS: A Guide to Efficient NodePool Configuration Strategies</li> </ul> <p>https://builder.aws.com/content/2z6RjwBGRVcPg7IIpYf9tk0XpzQ/optimizing-karpenter-on-eks-a-guide-to-efficient-nodepool-configuration-strategies</p> <ul> <li>Escalamiento de nodos de Amazon EKS con Karpenter  </li> </ul> <p>https://aws.amazon.com/es/blogs/aws-spanish/escalamiento-de-nodos-de-amazon-eks-con-karpenter/</p> <ul> <li>Reinvent 2023</li> </ul> <p>https://youtu.be/lkg_9ETHeks?feature=shared</p> <ul> <li>Amazon EC2 Instance Selector</li> </ul> <p>https://github.com/aws/amazon-ec2-instance-selector</p> <ul> <li>Introducing the price-capacity-optimized allocation strategy for EC2 Spot Instances</li> </ul> <p>https://aws.amazon.com/blogs/compute/introducing-price-capacity-optimized-allocation-strategy-for-ec2-spot-instances/</p>"},{"location":"kubernetes/autoescaling-finops/keda/98-tips/","title":"Tips","text":""},{"location":"kubernetes/autoescaling-finops/keda/98-tips/#multiple-triggers","title":"Multiple triggers","text":"<p>KEDA allows you to use multiple triggers as part of the same ScaledObject or ScaledJob.</p> <p>By doing this, your autoscaling becomes better:</p> <ul> <li>All your autoscaling rules are in one place</li> <li>You will not have multiple ScaledObject\u2019s or ScaledJob\u2019s interfering with each other</li> </ul> <p>KEDA will start scaling as soon as when one of the triggers meets the criteria. Horizontal Pod Autoscaler (HPA) will calculate metrics for every scaler and use the highest desired replica count to scale the workload to.</p>"},{"location":"kubernetes/autoescaling-finops/keda/98-tips/#hpa-and-keda","title":"HPA and keda","text":"<p>We recommend not to combine using KEDA\u2019s ScaledObject with a Horizontal Pod Autoscaler (HPA) to scale the same workload.</p> <p>They will compete with each other resulting given KEDA uses Horizontal Pod Autoscaler (HPA) under the hood and will result in odd scaling behavior.</p> <p>If you are using a Horizontal Pod Autoscaler (HPA) to scale on CPU and/or memory, we recommend using the CPU scaler &amp; Memory scaler scalers instead.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/","title":"Scaled Object","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specscaletargetref","title":"spec.scaleTargetRef","text":"<p>pending</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#spectriggers","title":"spec.triggers","text":"<p>Here we can define one or more triggers that can activate the autoescaling</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#replicas-and-fallback","title":"Replicas and fallback","text":"<p>We can control the replicas we want with this settings</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specminreplicacount","title":"spec.minReplicaCount","text":"<p>The desired min replicas.</p> <p>If the scaledobject has only cpu/memory triggers, minReplicaCount 0 is not permitted</p> <p>The default value is 0</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specmaxreplicacount","title":"spec.maxReplicaCount","text":"<p>This will be spec.maxReplicas in the hpa and it is the maximum number of replicas of the target resource</p> <p>The default value is 100</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specidlereplicacount","title":"spec.idleReplicaCount","text":"<p>With this setting we can define the number of replicas to scale the target resource to when there is no activity detected by the triggers, but before scaling all the way down to zero.</p> <p>Allows you to keep a minimum number of \"idle\" replicas running when there is no activity, instead of scaling directly to zero.</p> <p>This is an optional setting.</p> <ul> <li>If not specified, it is ignored</li> <li>The only supported value is 0 (https://github.com/kedacore/keda/issues/2314) so it effectively behaves the same as scaling to zero.</li> <li>Must be less than minReplicaCount</li> </ul>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specfallback","title":"spec.fallback","text":"<p>This is an optional setting that permits to configure a default behaviour when an scaler fails getting metrics from the source.</p> <p>It only supports scalers whose target is an AverageValue metric (cpu/memory not supported) and ScaledObjects (not ScaledJobs)</p> <p>Here we can define:</p> <ul> <li>spec.fallback.failureThreshold</li> </ul> <p>As the number of failures needed to start the fallback</p> <ul> <li>spec.fallback.replicas</li> </ul> <p>The desired replicas in fallback</p> <ul> <li>spec.fallback.behavior</li> </ul> <p>The desired behaviour. There are some options here (static, currentReplicas, currentReplicasIfHigher and currentReplicasIfLower)</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#behaviour","title":"Behaviour","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specpollinginterval","title":"spec.pollingInterval","text":"<p>This field defines the interval to check each trigger, in seconds, to determine if scaling actions are needed.</p> <p>The default value (if not specified) is 30</p> <p>When the deployment or other workload is at 0 replicas, KEDA will check the triggers every seconds defined in spec.pollingInterval. The polling interval is controlled by KEDA.</p> <p>When the deployment is running (with replicas), the HPA</p> <p>A lower value makes KEDA checks more frequently and react faster, but increases API calls/load A higher value makes KEDA checks less frequently with an slower reaction, but less load</p> <p>When scaling from 0 to 1, KEDA will poll for a metric value every pollingInterval seconds while the number of replicas is 0</p> <p>While scaling from 1 to N, this value is controlled by the horizontal-pod-autoscaler-sync-period parameter in the kube-controller-manager</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#cooldown","title":"Cooldown","text":"<ul> <li>spec.cooldownPeriod</li> </ul> <p>It applies when scaling to 0 and when scaling to the minReplicaCount. KEDA waits for the cooldownPeriod after the last trigger reported active before scaling the resource down.</p> <p>Prevents rapid scale-downs by introducing a delay after the last scaling activity. Useful for workloads that may have intermittent spikes, ensuring that resources are not scaled down too quickly after a burst of activity.</p> <p>It is an optional setting with default value 300 (5 minutes)</p> <ul> <li>spec.initialCooldownPeriod</li> </ul> <p>This is the time to wait to scale after the initial creation of the ScaledObject.</p> <p>Prevents immediate scale-downs right after deployment or KEDA startup, giving the workload time to stabilize and process initial events. Useful for workloads that need a warm-up period or to avoid premature scaling down due to delayed metrics or triggers.</p> <p>It is an optional setting with default value 0, so no wait.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#advanced","title":"Advanced","text":"<p>spec.advanced.scalingModifiers spec.advanced.restoreToOriginalReplicaCount spec.advanced.horizontalPodAutoscalerConfig</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#pause-an-scaled-object","title":"Pause an scaled object","text":"<p>It is possible to pause the autoescaling adding this 2 annotations to an ScaledObject.</p> <pre><code>autoscaling.keda.sh/paused: \"true\"\nautoscaling.keda.sh/paused-replicas: \"0\"\n</code></pre> <p>If paused-replicas = 0, the hpa is deleted If paused-replicas = NUMBER, the hpa has minReplicas and maxReplicas = NUMBER</p> <p>To restart autoescaling, we only need to remove the annotations.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#links","title":"Links","text":"<ul> <li>Scaling Deployments, StatefulSets &amp; Custom Resources</li> </ul> <p>https://keda.sh/docs/2.17/concepts/scaling-deployments/</p> <ul> <li>ScaledObject specification</li> </ul> <p>https://keda.sh/docs/latest/reference/scaledobject-spec/</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/","title":"Autoscaling","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaling/#what-kuberenetes-resources-can-keda-scale","title":"What kuberenetes resources can keda scale","text":"<p>Keda can scale reources like:</p> <ul> <li> <p>Deployments</p> </li> <li> <p>Statefulsets</p> </li> <li> <p>Custom Resources via ScaleTargetRef</p> </li> </ul> <p>KEDA can scale any Kubernetes resource that implements the /scale subresource such as Rollout, from Argo Rollouts.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/#caching-metrics","title":"Caching metrics","text":"<p>pending</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/#autoscaling-phases","title":"Autoscaling Phases","text":"<ul> <li>Activation/Desactivation phase</li> </ul> <p>Here, the KEDA operator decides if the workload need to be scaled from zero to 1 or from 1 to zero. This phase defines where the sacler is active.</p> <p>If spec.minReplicaCount is &gt;= 1, the scaler is always active and the activation value will be ignored.</p> <ul> <li>Scaling phase</li> </ul> <p>This phase scales from 1 to N or from N to 1. It defines the final Horizontal Pod Autoscaler to be created with the proper settings.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/","title":"AWS Billing","text":"<p>Cost and Usage Report (CUR) or Cost Explorer API</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#summary","title":"Summary","text":"<p>Amazon Athena is a serverless, interactive query service that allows you to analyze data directly in Amazon S3 using standard SQL. We will do this:</p> <ul> <li>Export the billing data to a s3 bucket</li> <li>Configure Athena to analyze that information</li> <li>Configure Opencost to use that Athena settings</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#buckets","title":"Buckets","text":"<p>Create 2 s3 buckets</p> <ul> <li>One for the billing data</li> <li>Another required for Athena results</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#create-data-export","title":"Create data export","text":"<p>Using the AWS Console, go to Billing and Cost Management &gt; Cost &amp; Usage Analysis &gt; Data exports and create an export</p> <p>Choose the export name and the s3 bucket and prefix. Also:</p> <pre><code>Type: Standard data export\nName: Choose one, for example opencost-myekscluster\nData table content settings: CUR 2.0\nAdditional export content: Check \"Include resource IDs\" # https://github.com/opencost/opencost/issues/3076\nTime granularity: Hourly (recommended)\nCompression type and file format: Parquet (optimized for Athena)\nS3 bucket: Choose the s3bucket created for the billing data\nS3 path prefix: Choose a prefix\n</code></pre> <p>First export can take some hours to appear in S3. Wait for initial data generation exploring the s3 bucket some hours later.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#configure-athena","title":"Configure Athena","text":"<p>Using the AWS Console, go to AWS Athena</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#workgroup","title":"Workgroup","text":"<p>Create an Athena workgroup for opencost:</p> <ul> <li>Query result configuration: customer-managed</li> <li>Location of query result: associate the created bucket for athena queries</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#create-database-and-billing-table","title":"Create database and billing table","text":"<p>Under athena query editor, select the workgroup opencost and create the database (glue)</p> <pre><code>CREATE DATABASE IF NOT EXISTS MYDATABASE;\n</code></pre> <p>Then select in the dropdown menu the created database and create the table using the following sql, changing the LOCATION</p> <ul> <li>Create table file</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#add-partitions-required","title":"Add partitions (REQUIRED)","text":"<p>After creating the table, you must add partitions for the billing periods. Without partitions, OpenCost will fail with SQL errors.</p> <p>Check your S3 bucket structure first to see available billing periods, then choose one of these approaches:</p> <p>Option 1: Add partitions manually (change the billing period and bucket path):</p> <pre><code>ALTER TABLE billing ADD IF NOT EXISTS\nPARTITION (billing_period='2025-10')\nLOCATION 's3://YOUR-BUCKET/path-to-data/BILLING_PERIOD=2025-10/';\n</code></pre> <p>Option 2: Auto-discover all partitions (recommended):</p> <pre><code>MSCK REPAIR TABLE billing;\n\n</code></pre> <p>The MSCK REPAIR TABLE command will scan your S3 bucket structure and automatically discover all the partition folders (like BILLING_PERIOD=2024-10/, BILLING_PERIOD=2024-09/, etc.) and add them to the table metadata.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#tests-change-the-billing-period","title":"Tests (change the billing period)","text":"<p>Verify partitions were added:</p> <pre><code>DESCRIBE billing;\nSHOW PARTITIONS billing;\n</code></pre> <p>Query</p> <pre><code>SELECT\n  line_item_usage_start_date,\n  line_item_product_code,\n  SUM(line_item_unblended_cost) as cost\nFROM billing\nWHERE billing_period = '2025-09'\nGROUP BY line_item_usage_start_date, line_item_product_code\nLIMIT 10;\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost","title":"Opencost","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-iam-permissions","title":"Opencost Iam permissions","text":"<p>OpenCost supports four AWS authorizer types:</p> <ol> <li>AccessKey - Direct AWS access key and secret authentication</li> <li>ServiceAccount - Kubernetes service account with pod annotations</li> <li>AssumeRole - IAM role assumption using another authorizer</li> <li>WebIdentity - Web identity token authentication (supports Google as identity provider)</li> </ol> <p>The Service Account authorizer leverages Kubernetes service account annotations and works with AWS pod identity mechanisms like:</p> <ul> <li>EKS Pod Identity</li> <li>IAM Roles for Service Accounts (IRSA)</li> </ul> <p>A very open policy can be this</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-settings","title":"Opencost settings","text":"<ul> <li>For Pod Identity Access</li> <li>Creating a secret called cloud-costs using externalsecrets operator. The target of the external-secret can be this</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-deployment","title":"Opencost deployment","text":"<p>Configure opencost using helm chart for aws billing, choosing our secret</p> <pre><code>opencost:\n  cloudIntegrationSecret: \"cloud-costs\"\n  cloudCost:\n    enabled: true\n  exporter: # this fixed a bug mounting the secret?\n    extraVolumeMounts:\n      - mountPath: \"/var/configs\"\n        name: cloud-integration\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#links","title":"Links","text":"<ul> <li>Installing on Amazon Web Services (AWS)</li> </ul> <p>https://opencost.io/docs/configuration/aws/</p> <ul> <li>Creating reports</li> </ul> <p>https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html</p> <ul> <li>IAM</li> </ul> <p>https://github.com/opencost/opencost/issues/3056 https://github.com/opencost/opencost/issues/1217 https://github.com/opencost/opencost/issues/3204 https://github.com/opencost/opencost/issues/2869</p> <p>https://medium.com/@prassonmishra330/cloud-cost-integration-aws-with-opencost-8b9557448e3a</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-in-cluster/","title":"AWS Regular OpenCost (in-cluster costs)","text":"<p>Opencost gets the following information</p> <ul> <li>Kubernetes Resource Usage Metrics (via Prometheus)</li> </ul> <p>OpenCost queries Prometheus for historical usage data so you need metrics from kubelet, node-exporter or kube-state-metrics)</p> <ul> <li>AWS Instance Information (from Kubernetes Node Metadata)</li> </ul> <p>It also get aws instance information like instance type, AWS region, Availability zone, Provider ID</p> <ul> <li>AWS Pricing Data (from Public AWS Pricing API)</li> </ul> <p>It gets EC2 on-demand instance pricing (per instance type, per region), CPU pricing (per vCPU-hour), Memory pricing (per GB-hour), EBS storage pricing, Network egress pricing (inter-AZ and internet)</p> <p>It automatically detects AWS as the cloud provider and fetches pricing from AWS public pricing endpoint:</p> <p>https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/${node_region}/index.json</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-in-cluster/#limitations-without-cur","title":"Limitations Without CUR","text":"<ul> <li>Uses list prices - not your actual negotiated AWS rates</li> <li>No reserved instance discounts - cannot track RI or Savings Plans</li> <li>Spot pricing is estimated - not actual billed spot prices</li> <li>No other AWS services - only EC2/EBS visible (no S3, RDS, etc.)</li> <li>No reconciliation - costs are estimates until CUR data is integrated</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/","title":"AWS Spot Instance Data Feed","text":"<p>It provides OpenCost accurate cost allocation and historical pricing data for spot instances.</p> <p>Spot instance prices fluctuate constantly. The Data Feed provides the actual prices paid at hourly granularity, which is more accurate than using current spot prices for historical costs.</p> <p>To make it work we must:</p> <ul> <li>Configure an Spot Instance Data Feed</li> <li>Give opencost permissions to access to the spot feed bucket via IRSA or PIA</li> </ul> <p>We can use the opencost cloud cost feature to get out-of-cluster costs</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#regional-availability","title":"Regional Availability","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#supported-regions","title":"Supported Regions","text":"<p>Spot Data Feed subscription is available in all AWS regions except:</p> <ul> <li>China (Beijing)</li> <li>China (Ningxia)</li> <li>AWS GovCloud (US)</li> <li>Regions that are disabled by default (opt-in regions)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#opt-in-regions","title":"Opt-in Regions","text":"<p>Regions introduced after March 20, 2019 are disabled by default and require manual enablement:</p> <ul> <li>Africa (Cape Town) - af-south-1</li> <li>Asia Pacific (Hong Kong) - ap-east-1</li> <li>Europe (Milan) - eu-south-1</li> <li>Europe (Spain) - eu-south-2</li> <li>Middle East (Bahrain) - me-south-1</li> <li>Middle East (UAE) - me-central-1</li> <li>Asia Pacific (Hyderabad) - ap-south-2</li> <li>Asia Pacific (Jakarta) - ap-southeast-3</li> <li>Asia Pacific (Melbourne) - ap-southeast-4</li> <li>Canada (Central) - ca-west-1</li> <li>Europe (Zurich) - eu-central-2</li> <li>Israel (Tel Aviv) - il-central-1</li> </ul> <p>Important: Even if an opt-in region is enabled, Spot Data Feed subscription may not be available. For example, eu-south-2 returns <code>UnsupportedOperation</code> error even when the region is enabled.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#troubleshooting-regional-issues","title":"Troubleshooting Regional Issues","text":"<p>If you encounter this error:</p> <pre><code>An error occurred (UnsupportedOperation) when calling the DescribeSpotDatafeedSubscription operation: The functionality you requested is not available in this region.\n</code></pre> <p>Solution: Use an alternative region where the service is fully supported, such as:</p> <ul> <li>eu-west-1 (Ireland)</li> <li>eu-central-1 (Frankfurt)</li> <li>us-east-1 (N. Virginia)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#create-spot-data-feed-subscription","title":"Create Spot Data Feed Subscription","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#aws-cli-command","title":"AWS CLI Command","text":"<pre><code>aws ec2 create-spot-datafeed-subscription \\\n    --dry-run \\\n    --bucket your-spot-datafeed-bucket \\\n    --prefix spot-datafeed/ \\\n    --region eu-south-2\n</code></pre> <pre><code>aws ec2 create-spot-datafeed-subscription \\\n    --bucket your-spot-datafeed-bucket \\\n    --prefix spot-datafeed/ \\\n    --region us-east-1\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#parameters","title":"Parameters","text":"<ul> <li><code>--bucket</code>: S3 bucket name where spot pricing data will be stored</li> <li><code>--prefix</code>: Optional prefix for the data files (recommended for organization)</li> <li><code>--region</code>: AWS region where the subscription should be created</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#verify-subscription","title":"Verify Subscription","text":"<pre><code>aws ec2 describe-spot-datafeed-subscription\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#links","title":"Links","text":"<ul> <li>Track your Spot Instance costs using the Spot Instance data feed</li> </ul> <p>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-data-feeds.html</p>"},{"location":"kubernetes/distributions/eks/authentication-mode/","title":"EKS Authentication Mode","text":"<p>EKS clusters use an authentication mode to control how IAM principals (users and roles) are granted access to Kubernetes APIs and objects within the cluster.</p>"},{"location":"kubernetes/distributions/eks/authentication-mode/#available-modes","title":"Available Modes","text":"Mode Value Description ConfigMap only <code>CONFIG_MAP</code> Uses only the <code>aws-auth</code> ConfigMap (legacy, deprecated) API and ConfigMap <code>API_AND_CONFIG_MAP</code> Supports both access entries via EKS API and <code>aws-auth</code> ConfigMap API only <code>API</code> Uses only access entries via EKS API (recommended)"},{"location":"kubernetes/distributions/eks/authentication-mode/#how-it-works","title":"How It Works","text":"<p>The authentication mode determines which methods can be used to grant IAM principals access to the cluster:</p>"},{"location":"kubernetes/distributions/eks/authentication-mode/#configmap-method-deprecated","title":"ConfigMap Method (Deprecated)","text":"<ul> <li>Edit the <code>aws-auth</code> ConfigMap inside the cluster</li> <li>Maps IAM roles/users to Kubernetes RBAC groups</li> <li>Managed using kubectl within the cluster</li> <li>Cannot be enabled after cluster creation if not initially enabled</li> <li>The IAM principal that created the cluster has implicit <code>system:masters</code> permissions</li> </ul>"},{"location":"kubernetes/distributions/eks/authentication-mode/#access-entries-method-recommended","title":"Access Entries Method (Recommended)","text":"<ul> <li>Manage access using EKS API, AWS CLI, SDKs, CloudFormation, or Console</li> <li>Create access entries outside the cluster</li> <li>Use access policies for preconfigured permissions or Kubernetes RBAC for custom permissions</li> <li>Cannot be disabled once enabled</li> <li>All principals are visible and manageable via the EKS API</li> </ul>"},{"location":"kubernetes/distributions/eks/authentication-mode/#comparison","title":"Comparison","text":"Aspect ConfigMap Access Entries Management Location Inside cluster (kubectl) Outside cluster (AWS API) Visibility Limited (cluster creator invisible) Full (all entries visible) Tooling kubectl, manual YAML editing AWS CLI, Console, SDKs, IaC tools Status Deprecated Recommended Required for EKS Auto Mode No Yes Required for Hybrid Nodes Optional Yes"},{"location":"kubernetes/distributions/eks/authentication-mode/#migration-path","title":"Migration Path","text":"<p>The <code>API_AND_CONFIG_MAP</code> mode allows both methods to coexist during migration from the deprecated ConfigMap approach to access entries. Each method maintains separate entries.</p>"},{"location":"kubernetes/distributions/eks/authentication-mode/#migration-steps","title":"Migration Steps","text":"<ol> <li>Change cluster authentication mode from <code>CONFIG_MAP</code> to <code>API_AND_CONFIG_MAP</code></li> <li>Create access entries for existing <code>aws-auth</code> ConfigMap entries</li> <li>Test access with new access entries</li> <li>Remove entries from <code>aws-auth</code> ConfigMap</li> <li>Optionally change mode to <code>API</code> to disable ConfigMap method permanently</li> </ol>"},{"location":"kubernetes/distributions/eks/authentication-mode/#important-considerations","title":"Important Considerations","text":"<ul> <li>The <code>aws-auth</code> ConfigMap method is deprecated by AWS</li> <li>Once access entries are enabled, they cannot be disabled</li> <li>If ConfigMap method is not enabled during cluster creation, it cannot be enabled later</li> <li>All clusters created before access entries were introduced have ConfigMap enabled by default</li> <li>Amazon EKS Auto Mode requires access entries</li> <li>Hybrid nodes require <code>API</code> or <code>API_AND_CONFIG_MAP</code> modes</li> <li>Platform version requirements apply for access entries support</li> </ul>"},{"location":"kubernetes/distributions/eks/authentication-mode/#platform-version-requirements","title":"Platform Version Requirements","text":"<p>To use access entries, the cluster must have a platform version equal to or later than:</p> Kubernetes Version Minimum Platform Version 1.30 eks.2 1.29 eks.1 1.28 eks.6 Earlier versions All supported"},{"location":"kubernetes/distributions/eks/authentication-mode/#recommendation","title":"Recommendation","text":"<ul> <li>New clusters: Use <code>API</code> mode to leverage access entries from the start</li> <li>Existing clusters with ConfigMap: Use <code>API_AND_CONFIG_MAP</code> during migration, then transition to <code>API</code> after migrating all entries</li> <li>Clusters with hybrid nodes: Must use <code>API</code> or <code>API_AND_CONFIG_MAP</code></li> </ul>"},{"location":"kubernetes/distributions/eks/authentication-mode/#references","title":"References","text":"<ul> <li>AWS EKS - Change authentication mode to use access entries</li> <li>AWS EKS - Grant IAM users and roles access to Kubernetes APIs</li> <li>AWS EKS - Grant IAM users access to Kubernetes with a ConfigMap</li> <li>AWS EKS - Migrating existing aws-auth ConfigMap entries to access entries</li> </ul>"},{"location":"kubernetes/distributions/eks/auto-mode/","title":"Auto Mode","text":""},{"location":"kubernetes/distributions/eks/auto-mode/#eks-addons-deployed-by-auto-mode","title":"EKS Addons Deployed by Auto Mode","text":"<p>When you enable EKS Auto Mode, AWS automatically deploys and manages the following essential cluster capabilities:</p> <ul> <li>Pod networking - Amazon VPC CNI plugin for Kubernetes networking</li> <li>Service networking - kube-proxy for Kubernetes service discovery</li> <li>Cluster DNS - CoreDNS for internal cluster DNS resolution</li> <li>Autoscaling - Karpenter for compute autoscaling</li> <li>Block storage - Amazon EBS CSI Driver for persistent volume support</li> <li>Load balancer controller - AWS Load Balancer Controller for ALB/NLB integration</li> <li>Pod Identity agent - EKS Pod Identity Agent for IAM role management</li> <li>Node monitoring agent - CloudWatch agent for node metrics</li> </ul>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/","title":"Eks Pod identity agent","text":"<p>Eks pod identity is a feature in Amazon EKS that simplifies the process to give permissions to a kubernetes service accounts inside an eks cluster.</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#prepare-the-system","title":"Prepare the system","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/pod-id-agent-setup.html</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#policy-to-the-nodes","title":"Policy to the nodes","text":"<p>Ensure the AmazonEKSWorkerNodePolicy policy is added to the node role</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#install-the-agent-addon","title":"Install the agent addon","text":"<p>Install the Amazon EKS Pod Identity Agent addon to EKS</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#prepare-iam","title":"Prepare IAM","text":""},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#create-the-policy","title":"Create the Policy","text":"<p>Create a policy with the desired permissions to the kubernetes application</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#create-the-role-role","title":"Create the role Role","text":"<p>Create a role with that policy and this trust relationship</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#add-the-association-in-eks","title":"Add the association in EKS","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/pod-id-association.html</p> <p>In our eks cluster - Access tab, create a new Pod Identity association</p> <ul> <li>choose the created iam role</li> <li>choose the namespace</li> <li>choose an existing service account inside that namespace</li> </ul> <p>And that's it!!</p> <p>The service account or the application usually don't need additional settings (no arn, no annotation,..). But check the documentation or forums for every application in order to use Pod Identity Agent</p>"},{"location":"kubernetes/distributions/eks/eks-pod-identity-agent/#links","title":"Links","text":"<ul> <li> <p>EKS Pod Identities https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html</p> </li> <li> <p>Github https://github.com/aws/eks-pod-identity-agent</p> </li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/","title":"Pod Authentication Methods for AWS API","text":"<p>This document describes the various methods available to authenticate Kubernetes pods against AWS APIs.</p>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#overview","title":"Overview","text":"<p>When running applications in Kubernetes that need to interact with AWS services, you must provide AWS credentials. The method you choose impacts security, complexity, and operational overhead.</p>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#authentication-methods","title":"Authentication Methods","text":""},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#1-irsa-iam-roles-for-service-accounts","title":"1. IRSA (IAM Roles for Service Accounts)","text":"<p>IRSA uses an OIDC identity provider to establish trust between Kubernetes ServiceAccounts and AWS IAM roles. The pod receives a JWT token that AWS STS exchanges for temporary AWS credentials.</p> <p>How it works:</p> <ul> <li>Cluster OIDC provider is registered with AWS IAM</li> <li>ServiceAccount is annotated with IAM role ARN</li> <li>AWS SDK automatically retrieves credentials via environment variables and mounted tokens</li> <li>No interception or DaemonSet required</li> </ul> <p>Security characteristics:</p> <ul> <li>Fine-grained permissions per ServiceAccount</li> <li>Short-lived credentials with automatic rotation</li> <li>No privileged containers needed</li> <li>Excellent audit trail via CloudTrail</li> </ul> <p>Operational considerations:</p> <ul> <li>Configuration lives in Kubernetes manifests (GitOps friendly)</li> <li>Requires OIDC provider setup per cluster</li> <li>Cross-account access requires additional trust policy configuration</li> </ul> <p>Use cases:</p> <ul> <li>Standard choice for EKS workloads</li> <li>GitOps-centric workflows</li> <li>When you need configuration in version control</li> </ul> <p>See IRSA documentation for implementation details.</p>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#2-eks-pod-identity","title":"2. EKS Pod Identity","text":"<p>EKS Pod Identity is AWS's newest solution that simplifies IAM role assignment without requiring OIDC provider configuration. An AWS-managed agent handles credential distribution.</p> <p>How it works:</p> <ul> <li>EKS add-on installs an agent DaemonSet</li> <li>Pod Identity associations are created via AWS API</li> <li>Agent intercepts SDK calls and provides credentials</li> <li>ServiceAccounts require no annotations</li> </ul> <p>Security characteristics:</p> <ul> <li>Pod-specific permissions</li> <li>Temporary credentials</li> <li>Managed by AWS</li> <li>Simplified cross-account access</li> </ul> <p>Operational considerations:</p> <ul> <li>Simpler initial setup than IRSA</li> <li>Associations managed outside Kubernetes manifests</li> <li>Less GitOps-friendly (external API configuration)</li> <li>Requires newer AWS SDK versions</li> </ul> <p>Use cases:</p> <ul> <li>New EKS clusters</li> <li>When centralized IAM management is preferred</li> <li>Cross-account access patterns</li> <li>Infrastructure-as-Code managed associations</li> </ul> <p>See EKS Pod Identity Agent documentation for implementation details.</p>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#3-node-iam-role-ec2-instance-profile","title":"3. Node IAM Role (EC2 Instance Profile)","text":"<p>EC2 instances running Kubernetes nodes have IAM roles attached via instance profiles. All pods on a node inherit the node's IAM permissions.</p> <p>How it works:</p> <ul> <li>IAM role is attached to EC2 instances</li> <li>AWS metadata service provides credentials</li> <li>All pods automatically use node credentials</li> <li>No pod-specific configuration needed</li> </ul> <p>Security characteristics:</p> <ul> <li>Coarse-grained permissions (all pods share access)</li> <li>Automatic credential rotation</li> <li>Follows principle of least privilege poorly</li> </ul> <p>Operational considerations:</p> <ul> <li>Zero configuration in pod manifests</li> <li>Simple to implement</li> <li>Works on any Kubernetes distribution with EC2 nodes</li> </ul> <p>Use cases:</p> <ul> <li>Single-tenant clusters with uniform permissions</li> <li>Quick prototypes</li> <li>When fine-grained access control isn't required</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#4-static-iam-credentials","title":"4. Static IAM Credentials","text":"<p>Long-lived AWS access keys stored as Kubernetes Secrets and exposed to pods via environment variables.</p> <p>How it works:</p> <ul> <li>IAM user access keys created in AWS</li> <li>Keys stored in Kubernetes Secrets</li> <li>Environment variables injected into pods</li> </ul> <p>Security characteristics:</p> <ul> <li>Long-lived credentials</li> <li>Manual rotation required</li> <li>Risk of credential leakage</li> <li>No automatic expiry</li> </ul> <p>Operational considerations:</p> <ul> <li>Works in any environment (not EKS-specific)</li> <li>Simple to understand</li> <li>Rotation is operationally intensive</li> </ul> <p>Use cases:</p> <ul> <li>Development and testing environments only</li> <li>Non-EKS clusters without better alternatives</li> <li>Last resort when other methods aren't available</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#5-external-secrets-operator","title":"5. External Secrets Operator","text":"<p>External Secrets Operator synchronizes secrets from external secret management systems (AWS Secrets Manager, Parameter Store) into Kubernetes Secrets.</p> <p>How it works:</p> <ul> <li>Operator runs in cluster with IRSA or Pod Identity</li> <li>Fetches secrets from AWS Secrets Manager or Parameter Store</li> <li>Creates and updates Kubernetes Secrets automatically</li> <li>Applications consume standard Kubernetes Secrets</li> </ul> <p>Security characteristics:</p> <ul> <li>Centralized secret management</li> <li>Automatic synchronization</li> <li>Audit trail in AWS</li> <li>Still distributes static credentials (but managed)</li> </ul> <p>Operational considerations:</p> <ul> <li>Additional operator to maintain</li> <li>Requires IRSA/Pod Identity for operator authentication</li> <li>Supports automatic secret rotation</li> </ul> <p>Use cases:</p> <ul> <li>Managing static credentials from external AWS accounts</li> <li>Centralized secret lifecycle management</li> <li>When secrets need to be shared across multiple applications</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#6-aws-secrets-and-configuration-provider-ascp","title":"6. AWS Secrets and Configuration Provider (ASCP)","text":"<p>ASCP uses the Secrets Store CSI driver to mount secrets from AWS Secrets Manager or Parameter Store as volumes in pods.</p> <p>How it works:</p> <ul> <li>CSI driver retrieves secrets at pod start</li> <li>Secrets mounted as files in pod filesystem</li> <li>Uses IRSA or Pod Identity for authentication</li> </ul> <p>Security characteristics:</p> <ul> <li>Secrets not exposed as environment variables</li> <li>Automatic rotation support</li> <li>Secrets fetched only when pod starts</li> </ul> <p>Operational considerations:</p> <ul> <li>Requires CSI driver installation</li> <li>Secrets only refreshed on pod restart by default</li> <li>More complex than environment variables</li> </ul> <p>Use cases:</p> <ul> <li>When secrets must be files rather than environment variables</li> <li>Applications that watch filesystem for secret changes</li> <li>Enhanced security posture over environment variables</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#7-assumed-roles-via-aws-sts","title":"7. Assumed Roles via AWS STS","text":"<p>Applications explicitly assume additional IAM roles using AWS STS AssumeRole API after initial authentication via IRSA or Pod Identity.</p> <p>How it works:</p> <ul> <li>Pod authenticates with IRSA or Pod Identity initially</li> <li>Application code calls STS AssumeRole for additional roles</li> <li>Returns temporary credentials for assumed role</li> </ul> <p>Security characteristics:</p> <ul> <li>Dynamic role assumption</li> <li>Short-lived credentials</li> <li>Chain of trust</li> <li>Fine-grained access control</li> </ul> <p>Operational considerations:</p> <ul> <li>Application must implement assume-role logic</li> <li>Requires trust relationships between roles</li> <li>Enables complex permission scenarios</li> </ul> <p>Use cases:</p> <ul> <li>Multi-account access patterns</li> <li>Dynamic permission escalation</li> <li>Service-to-service authentication across accounts</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#8-legacy-solutions-kube2iam-kiam","title":"8. Legacy Solutions (kube2iam, KIAM)","text":"<p>Both kube2iam and KIAM intercept AWS metadata service calls to provide pod-specific IAM roles.</p> <p>How they work:</p> <ul> <li>DaemonSet runs on each node</li> <li>iptables rules redirect metadata requests</li> <li>Pod annotations specify desired IAM role</li> </ul> <p>Status:</p> <ul> <li>kube2iam: Still maintained but no longer recommended by AWS. AWS officially recommends using IRSA for EKS clusters</li> <li>KIAM: Archived on March 5, 2024 and no longer maintained</li> <li>Both have security concerns with privileged DaemonSets</li> <li>Both superseded by IRSA and EKS Pod Identity</li> </ul> <p>Do not use for new deployments.</p>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#comparison-matrix","title":"Comparison Matrix","text":"Method Security Complexity EKS-Specific GitOps Friendly Status IRSA Excellent Medium Yes Very Active EKS Pod Identity Excellent Low Yes Moderate Active Node IAM Role Poor Low No Yes Active Static Credentials Poor Low No Yes Active External Secrets Good Medium No Moderate Active ASCP Good Medium No Moderate Active Assumed Roles Excellent High No Yes Active kube2iam/KIAM Poor Medium No Yes Deprecated"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#security-ranking","title":"Security Ranking","text":"<p>From most to least secure:</p> <ol> <li>IRSA / EKS Pod Identity - Pod-specific, short-lived credentials</li> <li>Assumed Roles via STS - Dynamic, temporary credentials</li> <li>ASCP / External Secrets - Managed static credentials</li> <li>Node IAM Role - Shared permissions, automatic rotation</li> <li>Static Credentials - Manual management required</li> <li>Hardcoded Credentials - Never use</li> </ol>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#decision-guide","title":"Decision Guide","text":""},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#for-eks-clusters","title":"For EKS Clusters","text":"<p>When you need per-pod IAM permissions:</p> <ul> <li>Use IRSA if you prefer GitOps workflows and manifest-based configuration</li> <li>Use EKS Pod Identity if you prefer infrastructure-as-code for IAM associations</li> </ul> <p>When all pods need the same permissions:</p> <ul> <li>Use Node IAM Role for simplicity</li> </ul> <p>When you need static credentials from external accounts:</p> <ul> <li>Use External Secrets Operator with IRSA or Pod Identity</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#for-non-eks-clusters","title":"For Non-EKS Clusters","text":"<p>With EC2 nodes:</p> <ul> <li>Use Node IAM Role if shared permissions are acceptable</li> <li>Use Static Credentials as last resort</li> </ul> <p>Without EC2 nodes (on-premises, other clouds):</p> <ul> <li>Use Static Credentials with robust rotation practices</li> <li>Consider External Secrets Operator for centralized management</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#for-multi-account-scenarios","title":"For Multi-Account Scenarios","text":"<p>Complex cross-account access:</p> <ul> <li>Use IRSA or Pod Identity with Assumed Roles via STS</li> <li>Configure trust relationships between accounts</li> </ul> <p>Simple cross-account secrets:</p> <ul> <li>Use External Secrets Operator</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#best-practices","title":"Best Practices","text":""},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#general-recommendations","title":"General Recommendations","text":"<ul> <li>Avoid static credentials whenever possible</li> <li>Use short-lived credentials with automatic rotation</li> <li>Follow principle of least privilege per pod</li> <li>Implement proper IAM role trust policies</li> <li>Monitor credential usage via CloudTrail</li> <li>Never commit credentials to version control</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#for-production-environments","title":"For Production Environments","text":"<ul> <li>Use IRSA or EKS Pod Identity as default choice</li> <li>Implement pod security policies to prevent credential access</li> <li>Regular audit of IAM role assignments</li> <li>Automated detection of credential leakage</li> <li>Rotate static credentials regularly if they must be used</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#for-development-environments","title":"For Development Environments","text":"<ul> <li>Still prefer IRSA/Pod Identity when possible</li> <li>Static credentials acceptable for local development</li> <li>Use separate AWS accounts for development</li> <li>Implement credential cleanup automation</li> </ul>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#migration-path","title":"Migration Path","text":"<p>From legacy solutions to modern approaches:</p> <ol> <li>From kube2iam/KIAM: Migrate to IRSA or EKS Pod Identity</li> <li>From Static Credentials: Migrate to IRSA or EKS Pod Identity</li> <li>From Node IAM Role: Migrate to IRSA or EKS Pod Identity for fine-grained control</li> <li>Between IRSA and Pod Identity: Both are valid, choose based on operational preferences</li> </ol>"},{"location":"kubernetes/distributions/eks/pod-authentication-methods/#links","title":"Links","text":"<ul> <li>AWS EKS IAM Roles for Service Accounts</li> <li>AWS EKS Pod Identities</li> <li>External Secrets Operator</li> <li>AWS Secrets and Configuration Provider</li> <li>AWS STS AssumeRole</li> </ul>"},{"location":"kubernetes/distributions/kubeadm/98-tips/","title":"Kubeadm tips","text":""},{"location":"kubernetes/distributions/kubeadm/98-tips/#auto-clean-kubeadm-backups","title":"Auto clean kubeadm backups","text":"<p>This command configures tmp systemd-tmpfiles-clean to delete kubeadm backups older than 6 months</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/tmpfiles.d/kubeadm-tmp.conf\ne    /etc/kubernetes/tmp/kubeadm*        -    -    -    6M\nEOF\n</code></pre> <p>And execute it with</p> <pre><code>systemd-tmpfiles --clean /etc/tmpfiles.d/kubeadm-tmp.conf\n</code></pre> <p>Or restart the service</p> <pre><code>systemctl restart systemd-tmpfiles-clean.service\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/certificates/","title":"Certificates","text":""},{"location":"kubernetes/distributions/kubeadm/certificates/#server-certificates","title":"Server certificates","text":"<p>They can be checked with</p> <pre><code>kubeadm certs check-expiration\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/certificates/#admin-and-super-admin-certificates-kubeconfig","title":"admin and super-admin certificates (kubeconfig)","text":"<p>There are 2 kubeconfigs in /etc/kubernetes: admin.conf and super-admin.conf (since kubeadm 1.29).</p> <p>admin.conf should be the admin kubeconfig to be used. super-admin.conf is designed for emergency scenarios when:</p> <ul> <li>Regular admin access is broken or compromised</li> <li>You need to recover from certificate issues</li> <li>RBAC configuration is broken</li> <li>You need to bypass admission controllers or other security policies</li> </ul> <p>Both uses system:masters RBAC group but super-admin.conf bypasses admission controllers and it is created for Emergency/recovery only. It should be highly restricted</p> <p>Both can be renewed via</p> <pre><code>kubeadm init phase kubeconfig admin\nkubeadm init phase kubeconfig super-admin\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/certificates/#the-server-has-asked-for-the-client-to-provide-credentials","title":"the server has asked for the client to provide credentials","text":"<p>This is a typical error showing the admin client certificate expired and it should be renewed via kubeadm</p> <pre><code>couldn't get current server API group list: the server has asked for the client to provide credentials\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/certificates/#kubernetes-control-plane-certificates","title":"kubernetes control plane certificates","text":"<p>controller-manager, scheduler and kubeconfig related certificates can be updated via kubeadm too</p>"},{"location":"kubernetes/distributions/kubeadm/expose-etcd/","title":"Change listen address in etcd","text":"<p>In a non ha deployment, the etcd metrics listens in 127.0.0.1:2381 by default.</p> <p>If we want to change to 0.0.0.0:2381 we need to change the kubeadm-config ConfigMap</p> <pre><code>kubectl edit cm kubeadm-config -n kube-system\n</code></pre> <p>And leave the etcd section this way</p> <pre><code>apiVersion: v1\ndata:\n  ClusterConfiguration: |\n    etcd:\n      local:\n        dataDir: /var/lib/etcd\n        extraArgs: \n          listen-metrics-urls: http://0.0.0.0:2381\nkind: ConfigMap\nmetadata:\n  name: kubeadm-config\n  namespace: kube-system\n</code></pre> <p>And upgrade all the master nodes</p> <pre><code>kubeadm upgrade node --dry-run\nkubeadm upgrade node\n</code></pre> <p>Check with</p> <pre><code>kubectl describe pod -l component=etcd\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/","title":"Static control plane pods","text":"<p>Kubeadm deploy some static control plane pods in using the /etc/kubernetes/manifests directory.</p> <ul> <li> <p>They are managed directly by the kubelet, not by the Kubernetes API server or controllers and they are not rescheduled to other nodes</p> </li> <li> <p>They are not evicted by normal Kubernetes eviction mechanisms (such as those triggered by resource pressure or node taints).</p> </li> <li> <p>If killed, kubelet always restarts them</p> </li> <li> <p>When draining a node, they are ignored by default and they will remain running unless you use the --force flag</p> </li> <li> <p>In this case, kubelet uses the requests values only for internal resource management (e.g., for eviction thresholds and reporting), not for scheduling decisions.</p> </li> <li> <p>If a control plane container exceeds its CPU or memory limit, it may be throttled (CPU) or killed (memory), just like any other pod.</p> </li> </ul> <p>Best Practice: Set appropriate requests and limits to protect both the control plane and other workloads on the node.</p>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#configure-requests-and-limits","title":"Configure requests and limits","text":"<p>We can use the patches feature</p> <pre><code>sudo mkdir /etc/kubernetes/patches\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#api-server","title":"Api server","text":"<p>sudo vi /etc/kubernetes/patches/kube-apiserver.yaml</p> <pre><code>spec:\n  containers:\n    - name: kube-apiserver\n      resources:\n        requests:\n          cpu: 1000m\n        limits:\n          memory: 8000Mi\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#controller-manager","title":"Controller manager","text":"<p>sudo vi /etc/kubernetes/patches/kube-controller-manager.yaml</p> <pre><code>spec:\n  containers:\n  - name: kube-controller-manager\n    resources:\n      requests:\n        cpu: 200m\n      limits:\n        memory: 500Mi\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#scheduler","title":"Scheduler","text":"<p>Edit the kube scheduler manifest</p> <pre><code>sudo vi /etc/kubernetes/patches/kube-scheduler.yaml\n</code></pre> <pre><code>spec:\n  containers:\n  - name: kube-scheduler\n    resources:\n      requests:\n        cpu: 100m\n      limits:\n        memory: 300Mi\n</code></pre> <pre><code>sudo kubeadm upgrade node --patches /etc/kubernetes/patches/ --dry-run\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#etcd","title":"Etcd","text":"<pre><code>sudo vi /etc/kubernetes/patches/etcd.yaml\n</code></pre> <pre><code>spec:\n  containers:\n  - name: etcd\n    resources:\n      requests:\n        cpu: 200m\n      limits:\n        memory: 500Mi\n</code></pre> <p>Remember pass this param in all kubeadm upgrades</p> <p>Other way is editing the configuration</p> <pre><code>kubectl edit cm -n kube-system kubeadm-config\n</code></pre>"},{"location":"kubernetes/distributions/kubeadm/static-control-plane-pods/#links","title":"Links","text":"<ul> <li>Reconfiguring a kubeadm cluster</li> </ul> <p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/</p> <ul> <li>kubeadm Configuration (v1beta4)</li> </ul> <p>https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</p> <p>https://serverfault.com/questions/1089688/setting-resource-limits-on-kube-apiserver</p>"},{"location":"kubernetes/kubectl/98-tips/","title":"Tips","text":""},{"location":"kubernetes/kubectl/98-tips/#delete-all-evicted-pods","title":"Delete all evicted pods","text":"<pre><code>kubectl get pods --all-namespaces | awk '/Evicted/ {print \"kubectl delete po -n \",$1,$2}'|bash -x  \n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-the-current-cluster","title":"Get the current cluster","text":"<pre><code>kubectl config view --minify -o jsonpath='{.clusters[].cluster.server}'\n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-all-images","title":"get all images","text":"<pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{.items[*].spec.containers[*].image}\" |\n    tr -s '[[:space:]]' '\\n' |\n    sort |\n    uniq -c\n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-etcd-status","title":"Get etcd status","text":"<pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table endpoint health\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table endpoint status\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table member list\n</code></pre>"},{"location":"kubernetes/storage/99-links/","title":"Links","text":"<ul> <li>Storage</li> </ul> <p>https://kubernetes.io/docs/concepts/storage/</p> <ul> <li>storageclass.info</li> </ul> <p>https://storageclass.info/</p> <ul> <li>CSI github</li> </ul> <p>https://github.com/kubernetes-csi</p> <ul> <li>CSI doc</li> </ul> <p>https://kubernetes-csi.github.io/docs</p>"},{"location":"kubernetes/storage/errors/","title":"Errors","text":""},{"location":"kubernetes/storage/errors/#only-dynamically-provisioned-pvc","title":"Only dynamically provisioned pvc","text":"<p>only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize</p> <p>Solution: If storageclass supports resize, add allowVolumeExpansion: true to the storageclass</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/","title":"Increase the pvc size in a statefulset","text":"<p>In order to increase the desired size of a volumeClaimTemplates in a kubernetes statefulset, if we only change the size, we wil get this error:</p> <pre><code>recreating StatefulSet because the update operation wasn't possible\n...\nForbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden\"\n</code></pre> <p>This is because that field is inmutable.</p> <p>If we can lose data, we can simply delete the statefulset and create it again with the desired size. But if we want to maintain the data the steps can be:</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#disable-autosync","title":"Disable autosync","text":"<p>If using autosync in argocd or similar tools, disable autosync</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#increase-the-size-in-the-pvc","title":"Increase the size in the PVC","text":"<p>Edit the PVC manually and change the size with</p> <pre><code>kubectl edit pvc NAME-OF-THE-PVC\n</code></pre> <p>And the wait until it has been resized.</p> <pre><code>kubectl get pvc NAME-OF-THE-PVC -w\n</code></pre> <p>This step needs to have an storageclass with allowVolumeExpansion supported by the storagebackend and enabled in our storageclass definition</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#do-an-orphan-delete-of-the-statefulset","title":"Do an orphan delete of the statefulset","text":"<p>Next we will delete the statefulset via</p> <pre><code>kubectl delete sts --cascade=orphan STATEFULSET\n</code></pre> <p>This will not delete the pods and PVC</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#reapply-the-statefulset-with-the-new-size","title":"Reapply the statefulset with the new size","text":"<p>Finally apply the manifest with the new size</p> <pre><code>kubectl apply -f STATEFULSET\n</code></pre> <p>If using argocd or similar tool, change the size in the manifest, and do a syn of the statefulset. Enable autosync again</p>"},{"location":"kubernetes/storage/node-disk-protection/","title":"Node disk protection","text":"<p>Kubernetes has 2 mechanisms to protect a node because of disk usage problems</p>"},{"location":"kubernetes/storage/node-disk-protection/#image-garbage-collector","title":"Image Garbage Collector","text":"<ul> <li>Container runtime</li> </ul> <p>Imagefs is the storage space used by the container runtime (containerd/cri-o,docker) for its operations. Containerd tracks and reports usage of its own storage directories, this is /var/lib/containerd/</p> <pre><code>/var/lib/containerd/\n\u251c\u2500\u2500 io.containerd.content.v1.content/     # Image blobs. This is what Image GC primarily targets for removal\n\u251c\u2500\u2500 io.containerd.snapshotter.v1.overlayfs/ # Layer snapshots. NOT cleaned by Image GC - requires container restart/cleanup\n\u251c\u2500\u2500 io.containerd.metadata.v1.bolt/       # Metadata database\n\u2514\u2500\u2500 other containerd directories...\n</code></pre> <ul> <li>Kubelet</li> </ul> <p>Then kubelet queries containerd for storage stats via CRI and receives imageFs data</p> <ul> <li>Image Garbage Collector</li> </ul> <p>Then kubelet removes unused images when some thresholds are reaches.</p> <p>When the imageFS usage reaches the HighThresholdPercent setting, kubelet starts to delete container images ordered by last time they were used until the LowThresholdPercent is reached</p> <p>Since kubernetes 1.30 (beta) we can configure a imageMaximumGCAge as the maximum time a local image can be unused for</p> <ul> <li>Metrics</li> </ul> <p>This metrics are exposed under /proxy/stats/summary API</p> <p>You can see this in the kubelet metrics:</p> <pre><code>kubectl get --raw /api/v1/nodes/&lt;node&gt;/proxy/stats/summary | jq '.node.runtime.imageFs'\n</code></pre>"},{"location":"kubernetes/storage/node-disk-protection/#pod-eviction","title":"Pod eviction","text":"<p>Node-pressure eviction can remove pods because a threshold has been reached, and there are 3 filesystem identifiers that can be used with eviction signals:</p> <ul> <li>nodefs</li> </ul> <p>Is the directory path defined under --root-dir kubelet setting. The default is /var/lib/kubelet</p> <p>nodefs.available is calculated via node.stats.fs.available</p> <ul> <li>imagefs</li> </ul> <p>Here we have the container images. In containerd this is located in /var/lib/containerd/images/</p> <p>imagefs.available eviction signal is calculated via node.stats.runtime.imagefs.available</p> <ul> <li>containerfs</li> </ul> <p>Here we have the writeable layers and logs. In containerd this is located in /var/lib/containerd/containers/</p> <p>containerfs.available eviction signal is calculated via node.stats.runtime.containerfs.available</p> <p>We can get this 3 data for a node with:</p> <pre><code>kubectl get --raw /api/v1/nodes/NODE/proxy/stats/summary | jq '.node.fs'\nkubectl get --raw /api/v1/nodes/NODE/proxy/stats/summary | jq '.node.runtime'\n</code></pre> <p>In BottleRocket OS or Flatcar all paths are under the same / overlay partition so the result is the same.</p>"},{"location":"kubernetes/storage/node-disk-protection/#soft-and-hard-eviction","title":"Soft and hard eviction","text":"<ul> <li>The soft eviction has a grace period until kubelet start to evict pods.</li> <li>The hard eviction has no grace period</li> <li>By default, only hard evictions are configured: imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10% This can be a good situation for spot instances, stateless workloads or environments with constant pod creation/deletion</li> <li>For production environments, define eviction soft settings with higher values and trigger some automatic and proactive cleanup during grace period</li> <li>Setup soft and hard prometheus alerts</li> </ul> <pre><code>evictionHard: # default\n    nodefs.available: \"10%\" \n    imagefs.available: \"15%\"\nevictionSoft:\n    nodefs.available: \"20%\"    # 5% buffer before hard eviction\n    imagefs.available: \"25%\"   # 10% buffer before hard eviction\nevictionSoftGracePeriod:\n    nodefs.available: \"2m\"     # Allow 2 minutes for cleanup/migration\n    imagefs.available: \"2m\"\n</code></pre>"},{"location":"kubernetes/storage/node-disk-protection/#links","title":"Links","text":"<ul> <li>Node-pressure Eviction</li> </ul> <p>https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/</p> <ul> <li>Garbage collection of unused containers and images</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/#containers-images</p> <ul> <li>Local ephemeral storage</li> </ul> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/","title":"CPU requests and limits","text":""},{"location":"kubernetes/tuning/cpu-requests-limits/#units","title":"Units","text":"<p>The CPU requests and limits can be specified in some formats:</p> <ul> <li>Using a number, with or without decimals</li> <li>Using milicores / milicpu (a cpu capacity / 1000)</li> </ul> <p>For values less that 1 CPU, it is recommended to use the milicpu format and not the decimal format</p> <pre><code>resources:\n    requests:\n    cpu: 2 # 2 CPU\n</code></pre> <pre><code>resources:\n    requests:\n    cpu: 0.5 # Half CPU\n</code></pre> <pre><code>resources:\n    requests:\n    cpu: \"200m\" # 200 milicores = 0.2\n</code></pre> <p>If we define a cpu limit but not a CPU request, kubernetes gives both the same value.</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#cpu-request","title":"CPU request","text":"<p>Giving a CPU request to a kubernetes container has some implications:</p> <ul> <li>Scheduling:</li> </ul> <p>Kube scheduler will assign that pod to a node that has that CPU resources as available. If no node has that resources the pod will be in Pending state until they new resources are available.</p> <ul> <li>Guarantee:</li> </ul> <p>Once the container is up in the node, that CPU resources will be guaranteed.</p> <ul> <li>Weight in CPU contention:</li> </ul> <p>The requested CPU will be used during CPU contention situations in the node. Containers with higher CPU requests will have higher priority accesing the node CPU.</p> <p>Este maximo de cpu que puede lograr vendra dado por la capacidad del nodo o, si el container tiene cpu limit, por dicho limite.</p> <ul> <li>HPA</li> </ul> <p>That value will be used in the CPU horizontal pod autoescaler</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#cpu-limit","title":"CPU limit","text":"<p>The CPU limit in kubernetes is controlled by the linux kernel and the Completely Fair Scheduler (CFS).</p> <p>It can be defined as the maximum CPU time a container can use every in a CPU cycle interval</p> <p>The CPU limit is translated to the CFS as the cpu_quota_us setting, and the cycle interval as the cpu_period_us</p> <p>The default cpu_period_us is 100 ms</p> <p>So a container with 200m as limit, cannot use more than 200 milicpu at every 100 miliseconds.</p> <p>Using CPU limits has some implications:</p> <ul> <li>Protecting other containers</li> </ul> <p>Ensures a container cannot use too much resources and affect others</p> <ul> <li>CPU throttling</li> </ul> <p>CPU under kubernetes is a compressible resource. When a container reaches that quota, it will have to wait to the next period (100 ms) to try to access to the CPU resources. This affects the performance and latency in the container and unwanted an uncontrolled situations can appear, like readiness or liveness probe failures.</p> <ul> <li>Underutilization</li> </ul> <p>Also, the node can have free CPU resources available, but they are not accesible by the limited container (underused) Not using CPU limits permits a better use of the CPU node resources</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#some-thougts-and-best-practices","title":"Some thougts and best practices","text":"<ul> <li> <p>Always define CPU requests. This also avoids the qos class besteffort</p> </li> <li> <p>In order to define a good value it is very important to have access to metrics about cpu consumption. Utilities like Goldilocks (vpa) o Robusta KRR can give recommendations.</p> </li> <li> <p>Using CPU limits gives more importance to protect the nodes and other containers during high CPU usage and to having a controlled environment.</p> </li> <li> <p>Some people recommends not using CPU limits and only use good CPU request values. It can also be disabled at kubelet level using cpuCFSQuota=0.</p> </li> <li> <p>For pods that can have more that 1 replicas it can be useful to use horizontal pod autoescaler or Keda</p> </li> <li> <p>For pods with only 1 replica,  it can be useful to use vertical pod autoescaling</p> </li> <li> <p>In some environments can be a good practice to change the cpu_period_us</p> </li> </ul>"},{"location":"kubernetes/tuning/cpu-requests-limits/#links","title":"Links","text":"<ul> <li>Resource Management for Pods and Containers  </li> </ul> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p> <ul> <li>Assign CPU Resources to Containers and Pods  </li> </ul> <p>https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/</p> <ul> <li>Pod Overhead  </li> </ul> <p>https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/</p> <ul> <li>For the Love of God, Stop Using CPU Limits on Kubernetes  </li> </ul> <p>https://home.robusta.dev/blog/stop-using-cpu-limits</p> <ul> <li>For the love of god, learn when to use CPU limits on Kubernetes  </li> </ul> <p>https://medium.com/@eliran89c/for-the-love-of-god-learn-when-to-use-cpu-limits-on-kubernetes-2225341e9dbd</p> <ul> <li>Why You Should Keep Using CPU Limits on Kubernetes  </li> </ul> <p>https://dnastacio.medium.com/why-you-should-keep-using-cpu-limits-on-kubernetes-60c4e50dfc61</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 1  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-1/</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 2  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-2/</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 3  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-3/</p> <ul> <li>CPU Limits in Kubernetes: Why Your Pod is Idle but Still Throttled: A Deep Dive into What Really Happens from K8s to Linux Kernel and Cgroups v2</li> </ul> <p>https://www.reddit.com/r/kubernetes/comments/1k28c00/cpu_limits_in_kubernetes_why_your_pod_is_idle_but/</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#tools","title":"Tools","text":"<ul> <li> <p>Kube capacity https://github.com/robscott/kube-capacity</p> </li> <li> <p>Robusta KRR https://github.com/robusta-dev/krr</p> </li> <li> <p>Goldilocks https://github.com/FairwindsOps/goldilocks</p> </li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/","title":"Graceful terminations in pods","text":"<p>Pods are ephemeral and there a lot of reasons why they can be deleted so it is critical to make our application to shutdown gracefully. In order to do it the proper way, we must understand the pod deletion processes.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#api-call","title":"Api call","text":"<p>The deletion starts with an request to the api server. Then the api server changes the status of the pod top to \"Terminating\". This triggers 2 parallel process.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#endpoint-deletion","title":"Endpoint deletion","text":"<p>The endpoint controller starts removing the pod from EndpointSlices and Endpoints. The endpoints are not deleted immediately from the EndpointSlices. The EndpointSlices change the status from this:</p> <pre><code>conditions:\n    ready: true\n    serving: true\n    terminating: false\n</code></pre> <p>to</p> <pre><code>conditions:\n    ready: false\n    serving: true\n    terminating: true\n</code></pre> <p>The pod stops receiving new connections. At the end, the pod related entries are removed in kube-proxy, coredns,...</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#pod-termination","title":"Pod termination","text":"<p>At the same time the pod termination process starts with 2 phases: the preStop hook and the SIGTERM</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#prestop-hook","title":"preStop hook","text":"<p>First of all, a preStop hook is executed if defined in the pod. The preStop hook is a command or http request you can use to execute in the pod. Here the application does not know it will be terminated.</p> <p>In addition to a command or http requests, there is a beta feature called \"sleep\" that pauses the container for a time.</p> <p>It is not a good idea to use a typical sleep command because the time to do a proper shutdown will be different in every situation. Another bad idea is to expose the http endpoint called in the http request. It should be internal.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#sigterm","title":"SIGTERM","text":"<p>After this, kubelet sends sends the SIGTERM (terminate) signal to the process with ID 1 to all the containers in the pod and it waits for a response that tells it is safe to delete it. If there is no response, the application is deleted inmediately with a SIGKILL signal.</p> <p>Because of this, the application needs to be prepared to manage that SIGTERM signal and do a graceful shutdown.</p> <p>The default signal received by the container is the SIGTERM. In a Dockerfile you can change the signal sent to the main process with the STOPSIGNAL instruction. For example, the nginx official image changes it to SIGQUIT, but the correct interpretation of this change depends of the container runtime.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#terminationgraceperiodseconds","title":"terminationGracePeriodSeconds","text":"<p>This setting is the time kubernetes waits before using that SIGKILL signal. Lets see some explanation about this setting:</p> <ul> <li>The terminationGracePeriodSeconds includes the time dedicated to the preStop hook and the SIGTERM.</li> <li>The default value is 30 seconds and it can be modified. If the preStop hook and SIGTERM need more time, increase it.</li> <li>If the process reaches the time defined in terminationGracePeriodSeconds while the preStop hook is being executed, kubelet extends this time 2 more seconds</li> <li>If you give to it the value zero, kubernetes uses inmediatly the SIGKILL signal.</li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/#note-about-race-condition","title":"Note about race condition","text":"<p>Because the deletion from the endpoints and the pod termination are executed at the same time, we can have a race condition where the pod has been deletes but the endpoint exists. The system tries to send traffic to a pod that does not exists.</p> <p>This can suggest the preStop hook is a safer way in that cases because it is executed before the SIGTERM and endpoint removal. There are another solutions like wait some seconds in the application's code where the SIGTERM is received in order to give time to the endpoint deletion, but that needed time can vary in every situation and maybe it is not the best option.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#best-practices","title":"Best practices","text":"<ul> <li>A good readiness probe is a must in every container, it tells kubelet when the container is ready to accept incoming connections</li> <li>Your application must manage the SIGTERM signal in order to get a gracefulshutdown and do proper tasks.</li> <li>A preStop hook prevents a race condition. Some people say you must always use it.</li> <li>It is a decision to take, what must be included in the preStop hook and what the SIGTERM must do in the application.</li> <li>Configure a proper terminationGracePeriodSeconds for every application</li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/#more-info","title":"More info","text":"<ul> <li>Pod Lifecycle  </li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/</p> <ul> <li>Pod Lifecycle spec  </li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle</p> <ul> <li>Dockerfile reference: STOPSIGNAL</li> </ul> <p>https://docs.docker.com/reference/dockerfile/#stopsignal</p> <ul> <li>Graceful shutdown in Kubernetes  </li> </ul> <p>https://learnk8s.io/graceful-shutdown</p> <ul> <li>How to Gracefully Shutdown Your Apps with a preStop Hook</li> </ul> <p>https://www.youtube.com/watch?v=ahCuWAsAPlc</p> <ul> <li>Decoding the pod termination lifecycle in Kubernetes: a comprehensive guide</li> </ul> <p>https://www.cncf.io/blog/2024/12/19/decoding-the-pod-termination-lifecycle-in-kubernetes-a-comprehensive-guide/</p>"},{"location":"kubernetes/tuning/memory-requests-limits/","title":"Memory requests and limits","text":""},{"location":"kubernetes/tuning/memory-requests-limits/#recommendations","title":"Recommendations","text":"<ul> <li>The values need regular observation to see the baseline memory usage and spikes</li> <li>Always use memory limits</li> <li>If you want overprovisioning, add memory requests to a level slightly above the average baseline memory usage and add memory limits to absorbe spikes.</li> <li>If not, always set your memory requests equal to your limits to a level to absorbe spikes.</li> <li>Use Horizontal pod autoescaler for workloads with replicas</li> <li>Investigate to use vertical pod autoescaling in workloads without replicas</li> </ul> <p>Example:</p> <ul> <li>Observed Peak: 2000Mi</li> <li>Observed Baseline: 1000Mi</li> <li>Request with Overprovision: 1100Mi</li> <li>Limit: 2300Mi or greater</li> </ul>"},{"location":"kubernetes/tuning/memory-requests-limits/#links","title":"Links","text":"<ul> <li>Kubernetes OOM and CPU Throttling</li> </ul> <p>https://sysdig.com/blog/troubleshoot-kubernetes-oom/</p> <ul> <li>What Everyone Should Know About Kubernetes Memory Limits, OOMKilled Pods, and Pizza Parties</li> </ul> <p>https://home.robusta.dev/blog/kubernetes-memory-limit</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/","title":"Pod disruption budget","text":"<p>Pod disruption budget, estable desde kubernetes 1.21, existe para ofrecer un mayor control sobre operaciones que suponen desalojos (disruptions) voluntarios en deployments o statefulsets principalmente.</p> <p>Podemos traducir disruption como \"interrupcion\" y eviction como \"desalojo\"</p> <p>El ejemplo mas tipico es al hacer un drain de un nodo, ya sea de forma manual o mediante herramientas como Cluster Autoescaler o Karpenter. PDB permite establecer un minimo de replicas que deben estar disponibles o bien un maximo de replicas que pueden no estar disponibles durante el proceso. De esta forma se detiene el drain hasta que no se hayan levantado las suficiente replicas en otros nodos.</p> <p>Esto es debido a que \"drain\" para funcionar utiliza la Eviction API, la cual respeta estos PDB. Borrar un pod no lo hace.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#creacion-de-un-pdb","title":"Creacion de un PDB","text":"<p>La creacion de un pdb tiene las siguientes configuraciones:  </p> <ul> <li> <p>Selector Primeramente debemos elegir a que pods aplica el pdb mediante un clasico label selector (matchLabels o matchExpressions)</p> </li> <li> <p>Definir el comportamiento Aqui podemos elegir si queremos un minimo de replicas levantadas (minAvailable) o un maximo de replicas no disponibles (maxUnavailable). Son configuraciones excluyentes.</p> </li> </ul> <p>Podemos especificar este valor mediante un numero</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: integer\nspec:\n  minAvailable: 5\n  selector:\n    matchLabels:\n      app: myapp\n</code></pre> <p>o bien un porcentaje</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: percentage\nspec:\n  maxUnavailable: 30%\n  selector:\n    matchLabels:\n      app: myapp\n</code></pre> <ul> <li>unhealthyPodEvictionPolicy Por defecto, pdb cuenta un pod como \"healthy\" cuando su status es type=\"Ready\" y status=\"True\".</li> </ul> <p>Con unhealthyPodEvictionPolicy, feature en beta desde kubernetes 1.27, se puede cambiar el criterio sobre como actuar sobre \"unhealthy\" pods.</p> <p>Con el valor IfHealthyBudget, que es el aplicado por defecto, permite que pods que esten levantados pero no healthy puedan ser desalojados solo cuando se este respetando los criterios del PDB. Este valor puede afectar negativamente a acciones voluntarias cuando tenemos aplicaciones con un mal funcionamiento (estado CrashLoopBackOff) o que reportan de forma incorrecta su estado Ready y cuentan con una proteccion via pdb.</p> <p>El valor AlwaysAllow  permite que pods que esten levantados pero no healthy puedan ser desalojados independientemente si se cumplen o no los criterios del PDB. Esta opcion es mas agresiva y se comporta mejor en los supuestos antes descritos.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#estado-de-un-pdb","title":"Estado de un pdb","text":"<p>El estado (.status) de un recurso pdb tiene varios campos</p> <ul> <li>currentHealthy es el numero de pods considerados actualmente como healthy</li> <li>desiredHealthy es el numero minimo de pods healthy que debe haber</li> <li>disruptionsAllowed es el numero de disruptions (interrupciones) permitidas</li> </ul> <p>Un pdb protege una aplicacion haciendo que su .status.currentHealthy no sea inferior a .status.desiredHealthy poniendo el disruptionsAllowed a 0</p> <ul> <li>expectedPods es el numero de pods que se esperan que esten healthy</li> <li>conditions nos muestra si nay mas pods que los requeridos por el pdb y se permiten interrupciones (SufficientPods) o no (InsufficientPods)</li> <li>disruptedPods muestra los pods marcados para ser desalojados pero que aun no han sido matados. Es basicamente un listado de los pods que van a ser desalojados. Este listado deberia estar vacio normalemente. Si se mantiene con varias entradas puede haber problemas de borrado de pods</li> </ul>"},{"location":"kubernetes/tuning/pod-disruption-budget/#algunas-recomendaciones","title":"Algunas recomendaciones","text":"<ul> <li>Tener un numero alto de replicas en una aplicacion stateless puede sugerir el uso de minAvailable (o maxUnavailable) con un porcentaje</li> <li>En aplicaciones stateless con pocas replicas se puede recurrir al uso de enteros</li> <li>En aplicaciones stateful hay que alinear la configuracion con la naturaleza de la misma. Por ejemplo en 5 replicas poner minAvailable a 3 o maxUnavailable a 2 para respetar un quorum.</li> <li>Aplicaciones con una sola replica puede ser recomendable o no usar pdb y asumir la perdida de servicio en las interrupciones voluntarias, o bien poner un pdb con maxUnavailable=0 para, de entrada, bloquear la interrupcion a la espera de una intervencion manual, como borrar el pdb.</li> <li>Utilizar unhealthyPodEvictionPolicy AlwaysAllow en aplicaciones con frecuentes CrashLoopBackOff y que queramos proteger mediante pdb</li> </ul> <p>Valores muy restrictivos perjudican las interrupciones voluntarias y valores muy agresivos pueden no proteger lo suficiente la aplicacion.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#links","title":"Links","text":"<ul> <li> <p>Disruptions https://kubernetes.io/docs/concepts/workloads/pods/disruptions/</p> </li> <li> <p>Specifying a Disruption Budget for your Application https://kubernetes.io/docs/tasks/run-application/configure-pdb/</p> </li> <li> <p>PodDisruptionBudget Spec https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/pod-disruption-budget-v1/</p> </li> <li> <p>Safely Drain a Node https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/</p> </li> <li> <p>API-initiated Eviction https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/</p> </li> </ul>"},{"location":"kubernetes/tuning/prioridades/","title":"Prioridades entre pods","text":""},{"location":"kubernetes/tuning/prioridades/#priority-classes-scheduling","title":"Priority classes (scheduling)","text":"<p>El uso de priority classes es una herramienta para dar prioridades a pods a la hora de ser desplegados en un cluster de kubernetes. Mediante ellas, si un pod con una prioridad mas alta no puede ser desplegado, el scheduler intentara por defecto desalojar pods con prioridades mas bajas para hacerle hueco.</p> <p>Por defecto, Kubernetes viene con 2 PriorityClass listas para ser asignadas a pods</p> <ul> <li>system-node-critical, con prioridad mayor, 2000001000</li> <li>system-cluster-critical, con prioridad 2000000000</li> </ul> <p>Asi, en una priorityclass se  puede definir</p> <ul> <li>Una descripcion</li> <li>El numero que especifica la prioridad</li> <li>globalDefault: Para espeficar si es la priority class por defecto</li> <li>preemptionPolicy: Admite 2 valores: Con Never los pods con mas prioridad estaran en la cola antes que los de menos prioridad, pero no se liberaran recursos. Es decir, si no hay recursos para ellos seguiran en la cola sin ser desplegados hasta que se hayan liberado por otros metodos. Con PreemptLowerPriority, que es el valor por defecto, se liberan recursos</li> </ul> <p>To preempt significa adelantarse</p>"},{"location":"kubernetes/tuning/prioridades/#lista-de-algunas-priority-classes-y-como-se-crean","title":"Lista de algunas priority classes y como se crean","text":"Priority Class Value Deployer system-node-critical 2000001000 kubeadm system-cluster-critical 2000000000 kubeadm high-priority 100000000 custom workflow-controller 1000000 argo workflows"},{"location":"kubernetes/tuning/prioridades/#lista-de-algunos-workloads-y-sus-default-priority-classes","title":"Lista de algunos workloads y sus default priority classes","text":"Workload Default cilium Daemonset system-node-critical ebs-csi-node Daemonset system-node-critical eks-pod-identity-agent Daemonset system-node-critical argo workflow controller workflow-controller karpenter system-cluster-critical aws-load-balancer-controller system-cluster-critical cilium-operator system-cluster-critical coredns system-cluster-critical karpenter system-cluster-critical ebs-csi-controller system-cluster-critical node-exporter system-cluster-critical karpenter system-cluster-critical metrics server system-cluster-critical vsphere-csi-controller system-cluster-critical"},{"location":"kubernetes/tuning/prioridades/#listar-pods-de-una-priorityclass","title":"Listar pods de una priorityclass","text":"<pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"system-cluster-critical\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre> <pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"system-node-critical\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre> <pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"high-priority\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre>"},{"location":"kubernetes/tuning/prioridades/#quality-of-service-classes","title":"Quality of Service Classes","text":""},{"location":"kubernetes/tuning/prioridades/#links","title":"Links","text":"<ul> <li> <p>Pod Priority and Preemption https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/</p> </li> <li> <p>Guaranteed Scheduling For Critical Add-On Pods https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</p> </li> <li> <p>PriorityClass spec https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/priority-class-v1/</p> </li> <li> <p>Pod Quality of Service Classes https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/ https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</p> </li> <li> <p>Node Pressure Eviction https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/</p> </li> </ul>"},{"location":"kubernetes/tuning/probes/","title":"Kubernetes probes and self healing","text":"<p>If the container has not probes defined, they will be considered as success</p>"},{"location":"kubernetes/tuning/probes/#startupprobe","title":"startupProbe","text":"<p>This was included after readinessProbe and livenessProbe. It is commonly used to setup a delay in pods with slow startup process. The readinessProbe and livenessProbe are not checked until the startupProbe is considered \"Success\".</p> <p>If your container usually starts in more than initialDelaySeconds + failureThreshold \u00d7 periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.</p> <p>If the probe is not ok, kubelet will kill the container and it will apply the pod restart policy:</p> <ul> <li>Always (default)</li> </ul> <p>The container is restarted</p> <ul> <li>OnFailure  </li> </ul> <p>The container is restart if the container had an exit status different than 0</p> <ul> <li>Never</li> </ul> <p>Never restart the container</p>"},{"location":"kubernetes/tuning/probes/#readinessprobe","title":"readinessProbe","text":"<p>This probe is related with considering the container is ready to accept petitions. It is useful when what to define when to start sending traffic to the container.</p> <ul> <li>If the probe fails, the container is pulled of from the services to stop receiving traffic</li> <li>The default result is \"Failure\". It must be accomplished to be considered as \"Success\"</li> </ul>"},{"location":"kubernetes/tuning/probes/#livenessprobe","title":"livenessProbe","text":"<p>This probe is related with considering the container is alive and running.</p> <p>It is useful like a way to tell kubelet the pod crashed, it encounters an issue or becomes unhealthy. The kubelet will automatically perform the correct action in accordance with the Pod's restartPolicy.</p>"},{"location":"kubernetes/tuning/probes/#recommendations","title":"Recommendations","text":"<ul> <li>Use startupProbe for slow starting apps</li> <li>The probes must be simple and lightweight</li> <li>Ensure the probe target is independent of the main application</li> <li>They can fail in heavy loaded environments</li> <li>In general, it is a best practice to define a livenessProbe and a readinessProbe. And they must be different.</li> <li>If using the same endpoint, set a higher failureThreshold value for the livenessProbe, that is, disconnect traffic and customers earlier, and if things are really bad, then restart.</li> </ul>"},{"location":"kubernetes/workloads/cronjob/","title":"Cronjob","text":""},{"location":"kubernetes/workloads/cronjob/#specstartingdeadlineseconds","title":"spec.startingDeadlineSeconds","text":"<p>This field defines the maximum time in seconds that a job can be delayed its initialization from the initial scheduled time.</p> <p>For example, a problem in the kubernetes cluster can cause the cronjob cannot start the job in the scheduled time. spec.startingDeadlineSeconds is the delay we accept, the tolerance window. If the delay is bigger than startingDeadlineSeconds, the job will not be executed and it will be considered failed.</p>"},{"location":"kubernetes/workloads/cronjob/#specconcurrencypolicy","title":"spec.concurrencyPolicy","text":"<p>This field controls if we permit or not the execution of multiple jobs from this cronjob at the same time.</p> <ul> <li> <p>The default value is \"Allow\"</p> </li> <li> <p>The \"Forbid\" policy does not allow concurrent instances. The new one will be skipped.</p> </li> </ul> <p>If the current job finishes and the second one is in the spec.startingDeadlineSeconds time, this will make the second instance start.</p> <ul> <li>The \"Replace\" policy replaces the currently running Job run with a new Job run</li> </ul>"},{"location":"kubernetes/workloads/cronjob/#links","title":"Links","text":"<ul> <li>CronJob</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/</p> <ul> <li>CronJob spec</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/cron-job-v1/</p>"},{"location":"kubernetes/workloads/job/","title":"Jobs","text":""},{"location":"kubernetes/workloads/job/#configuring-the-job","title":"Configuring the job","text":"<ul> <li>restartPolicy</li> </ul> <p>OnFailure</p> <ul> <li>Retries</li> </ul> <p>With spec.backoffLimit we can define the number of retries before considering a Job as failed. The default value is 6.</p> <p>By default, a Job will run uninterrupted unless a Pod fails (restartPolicy=Never) or a Container exits in error (restartPolicy=OnFailure), at which point the Job defers to the .spec.backoffLimit described above. Once .spec.backoffLimit has been reached the Job will be marked as failed and any running Pods will be terminated.</p> <ul> <li>activeDeadlineSeconds</li> </ul> <p>When a jobs reaches the number of seconds defined in spec.activeDeadlineSeconds, the pods will be terminated and the pod will have the status \"Failed with DeadlineExceeded as reason\". It is a good way to define a time we think the job is not going well.</p>"},{"location":"kubernetes/workloads/job/#clean-finished-jobs","title":"Clean finished jobs","text":""},{"location":"kubernetes/workloads/job/#via-cronjob","title":"Via Cronjob","text":"<p>pending</p>"},{"location":"kubernetes/workloads/job/#via-ttlsecondsafterfinished","title":"Via ttlSecondsAfterFinished","text":"<p>The spec.ttlSecondsAfterFinished keys permits to define an integer that represents the number of seconds to wait until the TTL Controller deletes the job. If we set this value to 0, the job will be deleted inmediately after it finishes, this is, when it has the \"Complete\" or \"Failed\" status.</p> <p>It is a best practice to define ttlSecondsAfterFinished in jobs that they are not controlled by a cronJob.</p>"},{"location":"kubernetes/workloads/job/#jobs-in-argocd","title":"Jobs in argocd","text":""},{"location":"kubernetes/workloads/job/#links","title":"Links","text":"<ul> <li>Jobs</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/job/</p> <ul> <li>Automatic Cleanup for Finished Jobs</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/</p>"},{"location":"languages/comment-comment-out/","title":"Comment, comment out and uncomment","text":"<p>In computing</p> <ul> <li> <p>Comment code is the action of adding a comment to a the code</p> </li> <li> <p>Comment out is the action where we exclude a block of code from being executed</p> </li> <li> <p>Uncomment is the actions that reverts this</p> </li> </ul>"},{"location":"languages/go-templating/control-structures/","title":"Control structures","text":""},{"location":"languages/go-templating/control-structures/#if","title":"If","text":"<ul> <li>Checks if a value is non-empty (not zero, not nil, not false, not empty string/slice/map).</li> <li>If the value is present and non-empty, executes the block.</li> <li>If the value is missing and missingkey=error is set, it will error.</li> </ul> <pre><code>{{- if .ignoreDifferences }}\nignoreDifferences:\n{{ toYaml .ignoreDifferences | nindent 4 }}\n{{- end }}\n</code></pre>"},{"location":"languages/go-templating/control-structures/#with","title":"With","text":"<ul> <li> <p>Checks if a value exists and is non-empty.</p> </li> <li> <p>If it exists, enters the block and sets . (dot) to that value inside the block.</p> </li> <li> <p>If the value is missing, the block is skipped without error, even with missingkey=error.</p> </li> </ul> <pre><code>{{- with .ignoreDifferences }}\nignoreDifferences:\n{{ toYaml . | nindent 4 }}\n{{- end }}\n</code></pre>"},{"location":"languages/go-templating/control-structures/#range","title":"Range","text":"<p>Pending</p>"},{"location":"languages/go-templating/missinkey-options/","title":"Missing key options","text":"<p>Control the behavior during execution if a map is indexed with a key that is not present in the map.</p> <p>For that we can assign some values to missingkey</p>"},{"location":"languages/go-templating/missinkey-options/#default-invalid","title":"default / invalid","text":"<p>The default behavior: Do nothing and continue execution. If printed, the result of the index operation is the string  \"NO VALUE\".</p>"},{"location":"languages/go-templating/missinkey-options/#zero","title":"zero","text":"<p>The operation returns the zero value for the map type's element.</p>"},{"location":"languages/go-templating/missinkey-options/#error","title":"error","text":"<p>Execution stops immediately with an error.</p>"},{"location":"languages/yaml/99-links/","title":"Links","text":"<ul> <li>Official website</li> </ul> <p>https://yaml.org/</p> <ul> <li>YAML Multiline</li> </ul> <p>https://yaml-multiline.info/</p> <ul> <li>YAML Learning Guide</li> </ul> <p>https://dev.to/wymdev/yaml-learning-guide-complete-tutorial-3nlc</p>"},{"location":"languages/yaml/structures/","title":"YAML Data Structures","text":""},{"location":"languages/yaml/structures/#lists-arrays","title":"Lists / Arrays","text":"<p>A list can be represented in two ways.</p> <p>Block style (multi-line format):</p> <pre><code>fruits:\n  - apple\n  - banana\n  - cherry\n</code></pre> <p>Abbreviated (inline) style:</p> <pre><code>fruits: [\"apple\", \"banana\", \"cherry\"]\n</code></pre> <p>Empty list:</p> <pre><code>containers: []\n</code></pre> <p>Block style also supports a standalone list without a key:</p> <pre><code>- Apple\n- Orange\n- Strawberry\n- Mango\n</code></pre> <p>By using the block style, you can make the array more readable, especially when dealing with longer lists or more complex structures.</p>"},{"location":"languages/yaml/structures/#maps-dictionaries","title":"Maps / Dictionaries","text":"<p>A dictionary can be represented in two ways.</p> <p>Block style (multi-line format):</p> <pre><code>martin:\n  name: Martin D'vloper\n  job: Developer\n  skill: Elite\n</code></pre> <p>Abbreviated (inline) style:</p> <pre><code>martin: {name: Martin D'vloper, job: Developer, skill: Elite}\n</code></pre> <p>Empty dictionary:</p> <pre><code>podSelector: {}\n</code></pre>"},{"location":"languages/yaml/structures/#empty-notations","title":"Empty Notations","text":"<p>In YAML, <code>{}</code> and <code>[]</code> are shorthand notations for empty objects and empty arrays, respectively:</p> <ul> <li><code>{}</code> - Empty Object (Dictionary): Represents an empty key-value structure.   For example, <code>podSelector: {}</code> selects all pods.</li> <li><code>[]</code> - Empty Array (List): Represents an empty list.   For example, <code>containers: []</code> means no containers are defined.</li> </ul> <p>Key Differences:</p> <ul> <li><code>{}</code> is used for objects (dictionaries) with key-value pairs.</li> <li><code>[]</code> is used for arrays (lists) of items.</li> </ul>"},{"location":"languages/yaml/structures/#block-scalars","title":"Block Scalars","text":"<p>Block scalars let you include multi-line strings. The chomping indicator controls how trailing newlines are handled:</p> <ul> <li><code>|</code> \u2014 Literal block: preserves newlines, keeps one trailing newline.</li> <li><code>|-</code> \u2014 Literal block with strip chomping: removes all trailing newlines.</li> <li><code>|+</code> \u2014 Literal block with keep chomping: preserves all trailing newlines.</li> <li><code>&gt;</code> \u2014 Folded block: newlines are replaced with spaces (except blank lines).</li> </ul> <pre><code>example1: |\n  This is a block scalar.\n  It preserves newlines.\n\nexample2: |-\n  This is a block scalar.\n  It strips trailing newlines.\n\nexample3: |+\n  This is a block scalar.\n  It keeps trailing newlines.\n</code></pre> <p>Folded scalar example:</p> <pre><code>description: &gt;\n  This is a block of text.\n  Newlines are replaced with spaces.\n</code></pre>"},{"location":"languages/yaml/structures/#links","title":"Links","text":"<ul> <li>https://yaml.org/spec/1.2.2/</li> <li>https://yaml-multiline.info/</li> <li>https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html</li> </ul>"},{"location":"misc/android/","title":"Android","text":""},{"location":"misc/android/#fix-location-in-mindgapps-and-lineageos","title":"Fix location in MindGapps and Lineageos","text":"<pre><code>adb shell pm grant com.google.android.gms android.permission.ACCESS_COARSE_LOCATION\nadb shell pm grant com.google.android.gms android.permission.ACCESS_FINE_LOCATION\n</code></pre>"},{"location":"misc/movements/","title":"Movements","text":""},{"location":"misc/movements/#acquisitions","title":"Acquisitions","text":""},{"location":"misc/movements/#ibm","title":"IBM","text":""},{"location":"misc/movements/#red-hat","title":"Red Hat","text":"<p>Acquired by IBM in 2019 for $34 billion.</p>"},{"location":"misc/movements/#centos","title":"CentOS","text":"<p>Community rebuild of RHEL, originally independent. Red Hat took over sponsorship and in 2020 shifted CentOS from a downstream RHEL rebuild to CentOS Stream (upstream rolling release), discontinuing CentOS 8 in December 2021.</p> <p>Forks: Rocky Linux, AlmaLinux</p>"},{"location":"misc/movements/#coreos","title":"CoreOS","text":"<p>Container-focused Linux distribution acquired by Red Hat in 2018. Container Linux was deprecated and replaced by Red Hat CoreOS (RHCOS), now the base OS for OpenShift nodes. The CoreOS team also contributed etcd (now a CNCF graduated project).</p>"},{"location":"misc/movements/#hashicorp","title":"HashiCorp","text":"<p>License changed from MPL-2.0 to BSL 1.1 in August 2023 (see License Changes), then acquired by IBM in April 2024 for $6.4 billion.</p>"},{"location":"misc/movements/#kubecost","title":"Kubecost","text":"<p>Kubernetes cost optimization startup acquired by IBM in September 2024.</p> <p>https://techcrunch.com/2024/09/17/ibm-acquires-kubernetes-cost-optimization-startup-kubecost/</p>"},{"location":"misc/movements/#broadcom","title":"Broadcom","text":""},{"location":"misc/movements/#vmware","title":"VMware","text":"<p>Acquired by Broadcom in November 2023 for ~$69 billion. Broadcom discontinued free products (vSphere Hypervisor/ESXi free tier), moved all licensing to subscriptions, and significantly reduced open-source contributions across VMware Tanzu projects.</p>"},{"location":"misc/movements/#carvel","title":"Carvel","text":"<p>Suite of Kubernetes tools (ytt, kapp, kbld, imgpkg, vendir, kapp-controller) originally created by the VMware Tanzu team. Donated to CNCF as a sandbox project in 2022. After the Broadcom acquisition, corporate sponsorship was significantly reduced, leaving the project mainly community-driven.</p>"},{"location":"misc/movements/#cisco","title":"Cisco","text":""},{"location":"misc/movements/#isovalent","title":"Isovalent","text":"<p>Company behind Cilium and Tetragon, acquired by Cisco in January 2024. Isovalent was founded by the original creators of Cilium and was the primary corporate maintainer of the project.</p>"},{"location":"misc/movements/#cilium","title":"Cilium","text":"<p>eBPF-based CNI plugin for Kubernetes networking, observability, and security, created by Isovalent. CNCF graduated project. After the Cisco acquisition, development continues under Cisco's cloud networking portfolio.</p>"},{"location":"misc/movements/#f5","title":"F5","text":""},{"location":"misc/movements/#nginx","title":"NGINX","text":"<p>High-performance web server, reverse proxy, and load balancer acquired by F5 in 2019. NGINX remains widely used as an ingress controller in Kubernetes (ingress-nginx). The open-source project continues, but commercial development is driven by F5.</p>"},{"location":"misc/movements/#microsoft","title":"Microsoft","text":""},{"location":"misc/movements/#github","title":"GitHub","text":"<p>Acquired by Microsoft in 2018 for $7.5 billion. Remains the largest code hosting platform and continues to operate largely independently under Microsoft ownership.</p>"},{"location":"misc/movements/#kinvolk","title":"Kinvolk","text":"<p>Berlin-based Linux and Kubernetes infrastructure company acquired by Microsoft in April 2021. Kinvolk created Flatcar Container Linux (a community fork of CoreOS Container Linux after Red Hat deprecated it) and Headlamp (open-source Kubernetes UI). The acquisition strengthened Azure's Linux and container expertise.</p>"},{"location":"misc/movements/#mirantis","title":"Mirantis","text":""},{"location":"misc/movements/#docker-enterprise","title":"Docker Enterprise","text":"<p>Mirantis acquired Docker's enterprise business (Docker Enterprise, UCP, DTR) in November 2019. Docker Inc retained Docker Desktop, Docker Hub, and the Docker CLI/Engine open-source projects.</p>"},{"location":"misc/movements/#progress-software","title":"Progress Software","text":""},{"location":"misc/movements/#chef","title":"Chef","text":"<p>Configuration management and infrastructure automation platform acquired by Progress Software in October 2020. Chef (including InSpec and Habitat) continues under Progress ownership.</p>"},{"location":"misc/movements/#pure-storage","title":"Pure Storage","text":""},{"location":"misc/movements/#portworx","title":"Portworx","text":"<p>Kubernetes-native storage platform acquired by Pure Storage in 2020. Portworx provides persistent storage, data protection, and disaster recovery for stateful workloads on Kubernetes.</p>"},{"location":"misc/movements/#suse","title":"SUSE","text":""},{"location":"misc/movements/#rancher-labs","title":"Rancher Labs","text":"<p>Kubernetes management platform acquired by SUSE in December 2020. Rancher provides multi-cluster Kubernetes management and remains a major open-source project under SUSE ownership.</p>"},{"location":"misc/movements/#license-changes","title":"License Changes","text":""},{"location":"misc/movements/#mongodb","title":"MongoDB","text":"<p>Changed from AGPL to SSPL (Server Side Public License) in October 2018. MongoDB pioneered the SSPL approach, specifically targeting cloud providers offering MongoDB as a managed service. This triggered broader industry debate and influenced later license changes by Elastic, Redis, and HashiCorp.</p>"},{"location":"misc/movements/#cockroachdb","title":"CockroachDB","text":"<p>CockroachDB changed from Apache 2.0 to BSL 1.1 (Business Source License) in 2019, the same approach later adopted by HashiCorp. BSL restricts production use by competing database-as-a-service providers.</p> <p>Fork: CockroachDB has a community version; YugabyteDB is a comparable open-source alternative.</p>"},{"location":"misc/movements/#hashicorp_1","title":"HashiCorp","text":"<p>License changed from MPL-2.0 to BSL 1.1 in August 2023, restricting competitive use of tools like Terraform, Vault, and Consul. Later acquired by IBM (see Acquisitions).</p> <p>Fork: OpenTofu (Terraform fork, CNCF sandbox project)</p>"},{"location":"misc/movements/#elasticsearch","title":"Elasticsearch","text":"<p>Elastic changed the license from Apache 2.0 to SSPL + Elastic License in January 2021, preventing cloud providers from offering managed Elasticsearch without contributing back.</p> <p>Fork: OpenSearch (by AWS, Apache 2.0 licensed)</p>"},{"location":"misc/movements/#grafana","title":"Grafana","text":"<p>Grafana Labs changed the license of Grafana, Loki, Tempo, and Mimir from Apache 2.0 to AGPL v3 in 2021. AGPL requires network service providers to publish their modifications, effectively targeting SaaS providers who embed Grafana without contributing back.</p>"},{"location":"misc/movements/#buoyant-linkerd","title":"Buoyant / Linkerd","text":"<p>Buoyant did not change the Linkerd project license (Apache 2.0) but in 2023 stopped publishing pre-built stable release artifacts. Users must now build from source or use Buoyant's commercial distribution. The CNCF project continues but the distribution model effectively pushes production users toward the paid offering.</p>"},{"location":"misc/movements/#redis","title":"Redis","text":"<p>Redis Ltd changed the license from BSD to RSALv2 + SSPLv1 in March 2024.</p> <p>Forks: Valkey (CNCF sandbox, Linux Foundation), Redict</p>"},{"location":"misc/movements/#company-shutdowns","title":"Company Shutdowns","text":""},{"location":"misc/movements/#weaveworks-february-2024","title":"Weaveworks (February 2024)","text":"<p>Pioneering cloud-native company behind Flux (GitOps, CNCF graduated) and WeaveNet (CNI plugin). Weaveworks shut down in February 2024 citing difficult market conditions. Both projects continue under CNCF governance independently of the company.</p>"},{"location":"networking/filtering-with-nodeport/","title":"Good/Bad news filtering in NSX-T/Nodeport","text":"<p>Lets assume this scenario</p> <ul> <li>Kubernetes cluster under vmware</li> <li>Loadbalancing is done manually, not using NSX Container Plug-in</li> <li>No Loadbalancer service type will work. We will use nodePort</li> </ul> <p>We want to create a gateway using envoy gateway that filters by source ip:</p> <ul> <li>Bad news:</li> </ul> <p>loadBalancerSourceRanges is not available in a nodePort service</p> <ul> <li>Good news:</li> </ul> <p>envoy gateway provides a CRD called SecurityPolicy that permits that filter</p> <ul> <li>Bad news:</li> </ul> <p>We need externalTrafficPolicy: Local to preserve source ip and make the filter work</p> <ul> <li>Good news:</li> </ul> <p>By default envoy exposes their gateways with externalTrafficPolicy: Local</p> <ul> <li>Bad news:</li> </ul> <p>In order to make externalTrafficPolicy: Local work, we need to make NSX-T to know in what node the gateway is deployed. Otherwise the traffic will be dropped. This is, we need to make a healthcheck</p> <ul> <li>Good news:</li> </ul> <p>We have spec.healthCheckNodePort permits to define a healthcheck where the externalTrafficPolicy is set to Local</p> <ul> <li>Bad news:</li> </ul> <p>It only works with LoadBalancer services</p> <ul> <li>Good news:</li> </ul> <p>We can achieve it with a monitor that makes a tcp check in the service node port, and then attach it to a server pool</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/","title":"Migrating kube-proxy from IPVS to nftables in kubeadm clusters","text":"<p>kube-proxy IPVS mode was deprecated in Kubernetes v1.35.0 (released December 2025) and is targeted for removal in v1.38. The recommended replacement on Linux is nftables mode, which provides equivalent O(1) lookup performance with incremental rule updates and active upstream development.</p> <p>nftables reached GA in v1.33 and is the recommended mode for Linux nodes. However, iptables remains the default for compatibility reasons. Clusters do not migrate automatically on upgrade \u2014 you must opt in explicitly by setting <code>mode: \"nftables\"</code> in the kube-proxy configuration.</p> <p>This guide covers the migration procedure for kubeadm-managed clusters.</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#requirements","title":"Requirements","text":"<ul> <li>Kubernetes 1.29+ (nftables alpha), 1.31+ (nftables beta), 1.33+ (nftables GA)</li> <li>Linux kernel 5.13 or newer on all nodes</li> <li><code>nft</code> command-line tool version 1.0.1 or newer on all nodes</li> </ul> <p>Verify on each node:</p> <pre><code>uname -r\nnft --version\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#check-current-kube-proxy-mode","title":"Check current kube-proxy mode","text":"<pre><code>kubectl get configmap kube-proxy -n kube-system -o jsonpath='{.data.config\\.conf}' | grep mode\n</code></pre> <p>Expected output if IPVS is configured:</p> <pre><code>mode: \"ipvs\"\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#migration-steps","title":"Migration steps","text":"<p>Node drain is not required. kube-proxy runs as a DaemonSet and restarts node-by-node. When it restarts in nftables mode, it automatically removes all existing IPVS rules. Established connections survive the restart because the kernel's connection tracking (conntrack) table is not affected. No node reboot is required either.</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#1-edit-the-kube-proxy-configmap","title":"1. Edit the kube-proxy ConfigMap","text":"<pre><code>kubectl edit configmap kube-proxy -n kube-system\n</code></pre> <p>Locate the <code>config.conf</code> key. Change <code>mode</code> from <code>ipvs</code> to <code>nftables</code> and remove any IPVS-specific blocks:</p> <pre><code># Before\nmode: \"ipvs\"\nipvs:\n  strictARP: true\n\n# After\nmode: \"nftables\"\n</code></pre> <p>If you had <code>ipvs.strictARP: true</code> set for MetalLB L2 compatibility, it is not needed in nftables mode and can be removed.</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#2-restart-the-kube-proxy-daemonset","title":"2. Restart the kube-proxy DaemonSet","text":"<pre><code>kubectl rollout restart daemonset kube-proxy -n kube-system\n</code></pre> <p>Wait for the rollout to complete:</p> <pre><code>kubectl rollout status daemonset kube-proxy -n kube-system\n</code></pre> <p>kube-proxy performs a node-by-node rolling restart. During the brief window when a node's kube-proxy pod is restarting, existing connections on that node are not disrupted \u2014 only new connection setup is affected momentarily. There is no need to cordon or drain nodes.</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#verify-the-migration","title":"Verify the migration","text":"<p>Check kube-proxy logs on any node to confirm nftables mode is active:</p> <pre><code>kubectl logs -n kube-system -l k8s-app=kube-proxy | grep -i nftables\n</code></pre> <p>Expected output:</p> <pre><code>Using nftables Proxier.\n</code></pre> <p>Confirm that IPVS rules no longer exist on a node:</p> <pre><code># Run on a node\nipvsadm -L --stats 2&gt;/dev/null || echo \"No IPVS rules\"\n</code></pre> <p>Confirm that nftables rules are present:</p> <pre><code># Run on a node\nnft list ruleset | grep -i kubernetes\n</code></pre> <p>Check that the <code>kube-ipvs0</code> dummy interface is gone from nodes:</p> <pre><code># Run on a node\nip link show kube-ipvs0 2&gt;/dev/null || echo \"kube-ipvs0 not present\"\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#persist-the-mode-across-kubeadm-upgrades","title":"Persist the mode across kubeadm upgrades","text":"<p>This is an important step. <code>kubeadm upgrade apply</code> regenerates the kube-proxy ConfigMap from its config. If the <code>KubeProxyConfiguration</code> is absent from the config passed to the upgrade, kubeadm fills in defaults \u2014 which means your <code>mode: \"nftables\"</code> will be silently overwritten back to iptables.</p> <p>There are two ways to prevent this.</p>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#option-a-upgrade-without-config-recommended","title":"Option A: upgrade without --config (recommended)","text":"<p>If you run <code>kubeadm upgrade apply</code> without <code>--config</code>, kubeadm reads the in-cluster <code>kubeadm-config</code> ConfigMap and does not touch the <code>kube-proxy</code> ConfigMap. The mode you set manually is preserved.</p> <pre><code># kubeadm reads in-cluster config, kube-proxy ConfigMap is left untouched\nkubeadm upgrade apply v1.3x.x\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#option-b-include-kubeproxyconfiguration-in-your-upgrade-config-file","title":"Option B: include KubeProxyConfiguration in your upgrade config file","text":"<p>If you use <code>--config</code>, you must explicitly include the <code>KubeProxyConfiguration</code> block in the file, otherwise kubeadm regenerates the kube-proxy config with defaults:</p> <pre><code>apiVersion: kubeadm.k8s.io/v1beta4\nkind: ClusterConfiguration\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: \"nftables\"\n</code></pre> <pre><code>kubeadm upgrade apply v1.3x.x --config kubeadm-config.yaml\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#new-node-joins","title":"New node joins","text":"<p>For nodes added after the migration, include the <code>KubeProxyConfiguration</code> in the kubeadm join config to ensure they start in nftables mode from the beginning:</p> <pre><code>apiVersion: kubeadm.k8s.io/v1beta4\nkind: JoinConfiguration\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: \"nftables\"\n</code></pre>"},{"location":"networking/kube-proxy-ipvs-to-nftables/#deprecation-timeline","title":"Deprecation timeline","text":"Kubernetes version IPVS status v1.34 and earlier Supported v1.35 Deprecated (warning emitted at startup) v1.38 (target) Removed"},{"location":"networking/kube-proxy-ipvs-to-nftables/#links","title":"Links","text":"<ul> <li>KEP-5495: Deprecate IPVS mode in kube-proxy</li> </ul> <p>https://github.com/kubernetes/enhancements/issues/5495</p> <ul> <li>NFTables mode for kube-proxy</li> </ul> <p>https://kubernetes.io/blog/2025/02/28/nftables-kube-proxy/</p> <ul> <li>Linux Kernel Version Requirements</li> </ul> <p>https://kubernetes.io/docs/reference/node/kernel-version-requirements/</p> <ul> <li>IPVS to NFTables migration guide</li> </ul> <p>https://dev.to/frozenprocess/ipvs-to-nftables-a-migration-guide-for-kubernetes-v135-24m5</p>"},{"location":"networking/network-policies/","title":"Network policies","text":"<p>The kubernetes network policies control the traffic that enters and leaves the pod.</p> <p>The network policy api v1 is stable since kubernetes 1.7 (2017)</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: my-network-policy\nspec:\n</code></pre>"},{"location":"networking/network-policies/#key-concepts","title":"Key concepts","text":"<ul> <li> <p>By default, all traffic is allowed unless there is a network policy selecting the pod</p> </li> <li> <p>By default, all traffic is denied if there is a network policy selecting the pod without rules allowing it</p> </li> <li> <p>The rules only can allow traffic. There is no way to deny traffic using rules.</p> </li> <li> <p>The way to deny traffic is with no rules (see below)</p> </li> <li> <p>If one rule at least allows the traffic, traffic is allowed.</p> </li> <li> <p>Connections between pods are bidirectional. If you want A and B to communicate, you need two rules, one matching each direction.</p> </li> </ul>"},{"location":"networking/network-policies/#choose-the-pods-the-policy-will-apply","title":"Choose the pods the policy will apply","text":"<p>This is done via <code>spec.podSelector</code>.</p> <p>An empty dictionary selects all pods. The selector only applies to pods in the same namespace as the policy, never pods from other namespaces.</p> <pre><code>spec:\n  podSelector: {}\n</code></pre> <p>Here we can use <code>matchLabels</code> and <code>matchExpressions</code>.</p>"},{"location":"networking/network-policies/#choose-what-traffic-is-permitted-to-enter-the-pod","title":"Choose what traffic is permitted to enter the pod","text":"<p>Ingress traffic is the network traffic that enters the pod and it is controlled by <code>spec.ingress</code>.</p> <p>An empty array denies all incoming traffic. Nothing is whitelisted.</p> <pre><code>spec:\n  ingress: []\n</code></pre>"},{"location":"networking/network-policies/#rule","title":"Rule","text":"<p>Inside an ingress rule we must indicate the origin of the traffic using <code>from</code>.</p> <p>Multiple selectors inside the <code>from</code> array act as OR, not AND.</p> <ul> <li><code>ipBlock.cidr</code></li> </ul> <p>With <code>ipBlock.cidr</code> we can define the source IPs via IPv4 or IPv6 CIDR ranges. <code>0.0.0.0/0</code> permits traffic from all IPs. Use <code>ipBlock.except</code> to deny a sub-range within the allowed CIDR.</p> <ul> <li><code>namespaceSelector</code></li> </ul> <p>With <code>namespaceSelector</code> we can use <code>matchLabels</code> and <code>matchExpressions</code> to select the namespaces the traffic comes from. An empty dictionary selects all namespaces.</p> <ul> <li><code>podSelector</code></li> </ul> <p>With <code>podSelector</code> we can use <code>matchLabels</code> and <code>matchExpressions</code> to select the pods the traffic comes from. An empty dictionary selects all pods.</p> <p>We can optionally add the port definition. Note that ports refer to pod ports, not service ports.</p> <pre><code>ports.port\nports.protocol can be TCP (default), UDP, or SCTP\nports.endPort\n</code></pre>"},{"location":"networking/network-policies/#choose-what-traffic-is-permitted-to-leave-the-pod","title":"Choose what traffic is permitted to leave the pod","text":"<p>Egress traffic is the network traffic that leaves the pod and it is controlled by <code>spec.egress</code>.</p> <p>An empty array denies all outgoing traffic. Nothing is whitelisted.</p> <pre><code>spec:\n  egress: []\n</code></pre> <p>The same selectors as ingress apply: <code>ipBlock</code>, <code>namespaceSelector</code>, and <code>podSelector</code>.</p> <p>Using only <code>to: namespaceSelector: {}</code> in egress restricts outgoing traffic to internal cluster traffic only.</p>"},{"location":"networking/network-policies/#policytypes","title":"policyTypes","text":""},{"location":"networking/network-policies/#links","title":"Links","text":"<ul> <li>Network Policies</li> </ul> <p>https://kubernetes.io/docs/concepts/services-networking/network-policies/</p> <ul> <li>Network policy editor by isovalent</li> </ul> <p>https://editor.networkpolicy.io/</p> <ul> <li>Securing Cluster Networking with Network Policies</li> </ul> <p>https://www.youtube.com/watch?v=3gGpMmYeEO8</p>"},{"location":"networking/onpremise-loadbalancers/","title":"On premise load balancers","text":"<p>On-premise Kubernetes load balancers provide LoadBalancer service type support for clusters running outside of cloud providers. While cloud providers automatically provision load balancers for LoadBalancer services, on-premise clusters need additional solutions to expose services externally. These load balancers typically work by assigning external IP addresses from a pool and using BGP, ARP, or Layer 2 protocols to advertise these IPs to the network infrastructure, making services accessible from outside the cluster.</p>"},{"location":"networking/onpremise-loadbalancers/#cilium-loadbalancer-ip-address-management-lb-ipam","title":"Cilium LoadBalancer IP Address Management (LB IPAM)","text":"<p>Cilium's LB IPAM feature provides LoadBalancer service type support for on-premise clusters by automatically managing IP address allocation and assignment for load balancer services.</p> <ul> <li>Website: https://cilium.io/</li> <li>GitHub: https://github.com/cilium/cilium</li> <li>Documentation: https://docs.cilium.io/en/stable/network/lb-ipam/</li> <li>Stats: \u2b50 22.5k stars, \ud83d\udc65 1000+ contributors</li> <li>Language: Go, eBPF</li> <li>Companies: Isovalent</li> <li>CNCF Relation: Graduated project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#metallb","title":"MetalLB","text":"<p>MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.</p> <ul> <li>Website: https://metallb.universe.tf/</li> <li>GitHub: https://github.com/metallb/metallb</li> <li>Stats: \u2b50 7.8k stars, \ud83d\udc65 210+ contributors</li> <li>Language: Go (68.6%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#kube-vip","title":"Kube-VIP","text":"<p>Kube-VIP provides Kubernetes Virtual IP and Load-Balancer for both control plane and Kubernetes services in bare-metal, edge, and virtualization environments.</p> <ul> <li>GitHub: https://github.com/kube-vip/kube-vip</li> <li>Stats: \u2b50 2.6k stars, \ud83d\udc65 98 contributors</li> <li>Language: Go (95.5%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#loxilbio","title":"loxilb.io","text":"<p>loxilb.io is a cloud-native load balancer for Kubernetes that provides high-performance layer 4 load balancing and can run in standalone mode or as a Kubernetes operator.</p> <ul> <li>Website: https://loxilb.io/</li> <li>GitHub: https://github.com/loxilb-io/loxilb</li> <li>Stats: \u2b50 1.8k stars, \ud83d\udc65 Active contributors</li> <li>Language: Go</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#purelb","title":"PureLB","text":"<p>PureLB is a load-balancer orchestrator for Kubernetes clusters that uses standard Linux networking and routing protocols, inspired by MetalLB.</p> <ul> <li>Website: https://purelb.gitlab.io/purelb/</li> <li>GitHub: https://github.com/purelb/purelb</li> <li>Stats: \u2b50 111 stars, \ud83d\udc65 76 contributors</li> <li>Language: Go (96.8%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Not a CNCF project</li> </ul>"},{"location":"networking/aws-load-balancer-controller/","title":"Intro","text":"<p>AWS Load Balancer Controller is kubernetes operator that permits to manage AWS Network Load Balancers (NLB) and Application Load Balancer (ALB) for a kubernetes cluster.</p> <p>We can create Application Load Balancers using:</p> <ul> <li>kubernetes ingress resources</li> <li>Gateway API resources</li> </ul> <p>We can create Network Load Balancers using:</p> <ul> <li>kubernetes services resources</li> <li>Gateway API resources</li> </ul>"},{"location":"networking/aws-load-balancer-controller/99-links/","title":"Links","text":"<ul> <li>Official Website</li> </ul> <p>https://kubernetes-sigs.github.io/aws-load-balancer-controller/l</p> <ul> <li>Github repo</li> </ul> <p>https://github.com/kubernetes-sigs/aws-load-balancer-controller</p> <ul> <li>Application Load Balancer</li> </ul> <p>https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</p> <ul> <li>Network Load Balancer</li> </ul> <p>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</p>"},{"location":"networking/aws-load-balancer-controller/pod-readiness-gate/","title":"Pod readiness gate","text":"<p>There is a very useful setting we can easily enable to increase the high availability in our aws load balancer controller exposed pods. This feature i called Pod readiness gate.</p>"},{"location":"networking/aws-load-balancer-controller/pod-readiness-gate/#explanation","title":"Explanation","text":"<p>Imagine we have a pod exposed to the internet using AWS Load Balancer Controller. For example:</p> <ul> <li>Using ALB and ingress annotations</li> <li>Using NLB and service annotations</li> </ul> <p>There is race condition when a deployment is restarted between:</p> <ul> <li>the pod is restarted and becomes Ready</li> <li>the pod is registered as target in the load balancer</li> </ul> <p>This settings injects an status in the pod that indicates if the pod is registered in the load balancer as target what avoid considering it as Ready until it can receive traffic. So the pod will not be Ready until it is registered</p> <p>This only works for loadbalancers where the target group has ip as target-type</p>"},{"location":"networking/aws-load-balancer-controller/pod-readiness-gate/#enabling","title":"Enabling","text":"<p>To enable it we must only label the namespace where the exposed ports are with this:</p> <pre><code>elbv2.k8s.aws/pod-readiness-gate-inject: enabled\n</code></pre> <p>Then, we need to restart the pods</p> <p>To check if it works, the pods must start with this new status.condition</p> <pre><code>target-health.elbv2.k8s.aws\n</code></pre> <p>False: when it is being registered True: it is registered as target</p>"},{"location":"networking/aws-load-balancer-controller/pod-readiness-gate/#links","title":"Links","text":"<ul> <li>Pod readiness gate</li> </ul> <p>https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/pod_readiness_gate/</p>"},{"location":"networking/aws-load-balancer-controller/service-annotations/","title":"Service annotations","text":"<p>This annotations permits to expose services via NLB adding them:</p> <ul> <li>to a kubernetes service</li> <li>to a gateway settings (gateway api)</li> </ul> <p>I will describe some of the more important annotations</p>"},{"location":"networking/aws-load-balancer-controller/service-annotations/#servicebetakubernetesioaws-load-balancer-nlb-target-type","title":"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type","text":"<p>This annotation sets the target type for the NLB. It accepts two values: <code>instance</code> or <code>ip</code>.</p>"},{"location":"networking/aws-load-balancer-controller/service-annotations/#instance-mode-default","title":"instance mode (default)","text":"<p>Routes traffic to EC2 instances through their NodePort. Works with any CNI plugin but adds an extra network hop through kube-proxy.</p>"},{"location":"networking/aws-load-balancer-controller/service-annotations/#ip-mode","title":"ip mode","text":"<p>Routes traffic directly to Pod IPs. Requires AWS VPC CNI (or equivalent) to assign secondary IP addresses to EC2 instance ENIs. Lower latency and better performance than instance mode, but more complex networking requirements and cannot be changed after NLB creation.</p>"},{"location":"networking/aws-load-balancer-controller/service-annotations/#cni-plugin-compatibility-with-ip-mode","title":"CNI Plugin Compatibility with IP Mode","text":"<ul> <li>AWS VPC CNI: \u2705 Full support</li> <li>Cilium: \u26a0\ufe0f ENI mode has known compatibility issues with AWS Load Balancer Controller (GitHub Issues #19250, #19981). CNI chaining mode works normally.</li> <li>Calico: \u26a0\ufe0f Policy-only mode works when used alongside AWS VPC CNI. Full CNI mode does not support IP targets.</li> </ul>"},{"location":"networking/cilium/99-tips/","title":"Tips","text":""},{"location":"networking/cilium/99-tips/#cilium-pod-does-not-start-in-flatcar","title":"Cilium pod does not start in flatcar","text":"<p>During some months/years, the defaul cilium installation did not start with flatcar. To make it work, it was neccesary to change the user domain from spc_to to unconfined_t in the cilium installation.</p> <p>The setting in helm chart</p> <pre><code>securityContext:\n  seLinuxOptions:\n    type: unconfined_t\n</code></pre> <p>Today this problem seems to be solved</p> <p>See more here</p> <ul> <li>Cilium CNi with k8s does not work with SELinux in permissive mode #891</li> </ul> <p>https://github.com/flatcar/Flatcar/issues/891</p>"},{"location":"networking/cilium/99-tips/#no-inter-node-communication-between-pods-in-vmware-vsphere","title":"No inter node communication between pods in Vmware Vsphere","text":"<p>There is a bug in the vmxnet driver that makes the pods don't have inter node connectivity using vxlan. Cilium forward the packets to the overlay but they don't reach the destiation pod/service</p> <p>The following command in the cilium agent</p> <pre><code>cilium-health status --verbose\n</code></pre> <p>shows</p> <pre><code>HTTP to agent:   Get \"&lt;http://10.42.4.211:4240/hello&gt;\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n</code></pre> <p>One solution is to change the tunnelPort to a different than the default one (8472)</p> <p>The setting in helm chart</p> <pre><code>tunnelPort: 8223\n</code></pre> <p>More info here</p> <ul> <li>Installation on Broadcom VMware ESXi / NSX\uf0c1</li> </ul> <p>https://docs.cilium.io/en/latest/installation/k8s-install-broadcom-vmware-esxi-nsx/</p>"},{"location":"networking/cilium/gateway-api/","title":"Gateway API","text":""},{"location":"networking/cilium/gateway-api/#supported-releases","title":"Supported releases","text":"Cilium release Supported Gateway Api release 1.16 v1.1.0 1.17 v1.2.0"},{"location":"networking/cilium/gateway-api/#enable-gateway-api-support","title":"Enable gateway api support","text":"<ul> <li>Deploy the gateway api crds</li> </ul> <p>https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml</p> <ul> <li> <p>Cilium must be deployed with kubeproxy replacement or nodePort.enabled=true</p> </li> <li> <p>Cilium must have l7Proxy=true (default)</p> </li> </ul> <p>Then enable gateway api with gatewayAPI.enabled=true in the helm chart</p>"},{"location":"networking/cilium/gateway-api/#links","title":"Links","text":"<ul> <li>Gateway API Support</li> </ul> <p>https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/</p> <ul> <li>Migrating from Ingress to Gateway</li> </ul> <p>https://docs.cilium.io/en/stable/network/servicemesh/ingress-to-gateway/ingress-to-gateway/</p>"},{"location":"networking/cilium/upgrade/","title":"Upgrade cilium","text":""},{"location":"networking/cilium/upgrade/#preparation","title":"Preparation","text":""},{"location":"networking/cilium/upgrade/#if-upgrading-to-a-new-minor-version","title":"If upgrading to a new minor version","text":"<ul> <li> <p>First update to the latest patch version of the current minor version.</p> </li> <li> <p>Read the release notes documentation for the new minor version, for example</p> </li> </ul> <pre><code>https://docs.cilium.io/en/v1.16/operations/upgrade/#upgrade-notes # when upgrading to 1.16\nhttps://docs.cilium.io/en/v1.17/operations/upgrade/#upgrade-notes # when upgrading to 1.17\nhttps://docs.cilium.io/en/v1.18/operations/upgrade/#upgrade-notes # when upgrading to 1.18\n</code></pre>"},{"location":"networking/cilium/upgrade/#deploy-a-pre-flight-check","title":"Deploy a pre-flight check","text":"<p>The preflight</p> <pre><code>preflight:\n  enabled: true\nagent: false\noperator:\n  enabled: false\n</code></pre> <p>Remember to delete the pre-flight check at the end</p>"},{"location":"networking/cilium/upgrade/#upgradecompatibility","title":"upgradeCompatibility","text":"<p>The upgradeCompatibility setting minimizes \"datapath disruption during the upgrade\". Configure it with the initial cilium version installed in the cluster</p>"},{"location":"networking/cilium/upgrade/#links","title":"Links","text":"<ul> <li>Upgrade Guide</li> </ul> <p>https://docs.cilium.io/en/stable/operations/upgrade</p>"},{"location":"networking/contour/deployment/","title":"Deployment","text":""},{"location":"networking/contour/deployment/#yaml","title":"YAML","text":"<p>https://projectcontour.io/quickstart/contour.yaml&gt;</p>"},{"location":"networking/contour/deployment/#helm","title":"Helm","text":"<p>https://charts.bitnami.com/bitnami</p>"},{"location":"networking/contour/deployment/#contour-gateway-provisioner","title":"Contour Gateway Provisioner","text":"<p>https://projectcontour.io/quickstart/contour-gateway-provisioner.yaml</p> <p>This method of installation still allows you to use any of the supported APIs for defining virtual hosts and routes:</p> <ul> <li>Ingress</li> <li>HTTPProxy</li> <li>Gateway API\u2019s HTTPRoute and TLSRoute</li> </ul>"},{"location":"networking/contour/deployment/#links","title":"Links","text":"<p>https://projectcontour.io/getting-started/ https://projectcontour.io/docs/main/deploy-options/ https://projectcontour.io/docs/main/config/gateway-api/</p>"},{"location":"networking/contour/expose/","title":"Expose services","text":""},{"location":"networking/contour/expose/#ingress","title":"Ingress","text":"<p>https://projectcontour.io/docs/main/config/ingress/</p>"},{"location":"networking/contour/expose/#httpproxy","title":"HTTPProxy","text":"<p>https://projectcontour.io/docs/main/config/fundamentals/</p>"},{"location":"networking/contour/expose/#gateway-api","title":"Gateway api","text":"<p>https://projectcontour.io/docs/main/config/gateway-api/</p>"},{"location":"networking/gateway-api/98-tips/","title":"Tips","text":""},{"location":"networking/gateway-api/98-tips/#parametersref-in-gatewayclass-and-gateway-parametersref","title":"parametersRef in GatewayClass and Gateway parametersRef","text":"<p>The <code>parametersRef</code> field exists at two levels in Gateway API, and they serve different purposes:</p> <ul> <li><code>GatewayClass.spec.parametersRef</code>: Cluster-wide default configuration   applied to all Gateways using that class</li> <li><code>Gateway.spec.infrastructure.parametersRef</code>: Per-Gateway   instance-specific overrides</li> </ul> <p>This design avoids the \"combinatorial explosion\" of GatewayClass resources. Without per-Gateway <code>parametersRef</code>, you'd need separate GatewayClasses for every configuration variation (e.g., <code>gateway-internet-ipv4</code>, <code>gateway-internet-ipv6</code>, <code>gateway-private-ipv4</code>, etc.).</p> <p>When both <code>GatewayClass.spec.parametersRef</code> and <code>Gateway.spec.infrastructure.parametersRef</code> are specified, the merging behavior is implementation-specific. The general recommendation:</p> <ul> <li>GatewayClass parametersRef: Provides sensible defaults for the class</li> <li>Gateway parametersRef: Allows team-specific or instance-specific overrides</li> </ul> <p>This follows the principle of least privilege\u2014use GatewayClass for defaults, Gateway for customization.</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/","title":"Contour to Envoy Gateway Migration Path","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#overview","title":"Overview","text":"<p>VMware Contour, a CNCF Incubating project, has announced a strategic migration path toward Envoy Gateway, a new CNCF project designed to consolidate the fragmented landscape of Envoy-based ingress controllers. This document outlines the migration strategy, timeline, and resources for developers transitioning from Contour to Envoy Gateway.</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#background","title":"Background","text":"<p>In May 2022, VMware announced that Contour would join forces with community leaders including Tetrate and Ambassador to build the new Envoy Gateway project under the Envoy banner. This initiative aims to create a single, cohesive, canonical implementation of a Kubernetes Gateway API reference implementation.</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#current-status","title":"Current Status","text":"<ul> <li>Contour Development: Continues with current release cadence and support policy</li> <li>Gateway API Development: VMware's Gateway API development efforts have moved to Envoy Gateway</li> <li>Contour's Gateway API: Remains in experimental state</li> <li>Feature Development: VMware continues feature development on Contour for existing users</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#long-term-direction","title":"Long-term Direction","text":"<ul> <li>Evolutionary Migration: The transition is designed to be gradual, not immediate</li> <li>Feature Parity Requirement: Migration evaluation is postponed until Envoy Gateway achieves feature parity with Contour</li> <li>User Protection: Existing Contour users remain fully supported throughout the transition</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#migration-tools-and-support","title":"Migration Tools and Support","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#planned-migration-tooling","title":"Planned Migration Tooling","text":"<ul> <li>HTTPProxy to Gateway API: Conversion tools to migrate from Contour's HTTPProxy resource to Gateway API</li> <li>Configuration Migration: Tooling to help migrate existing configurations</li> <li>Feature Gap Analysis: Tools to identify features not yet supported by Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#feature-considerations","title":"Feature Considerations","text":"<p>Current HTTPProxy functionality not yet available in Gateway API:</p> <ul> <li>Rate limiting</li> <li>Authentication mechanisms</li> <li>Advanced routing features</li> <li>Custom middleware integrations</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#timeline-and-roadmap","title":"Timeline and Roadmap","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#short-term-current","title":"Short Term (Current)","text":"<ul> <li>Contour maintains full development and support</li> <li>Users can continue using Contour without disruption</li> <li>Gateway API remains experimental in Contour</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#medium-term","title":"Medium Term","text":"<ul> <li>Envoy Gateway development continues toward feature parity</li> <li>Migration tooling development</li> <li>Community feedback and testing</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#long-term","title":"Long Term","text":"<ul> <li>Evaluation of Contour's direction as Envoy Gateway matures</li> <li>Potential transition of users to Envoy Gateway</li> <li>Contour may become a wrapper around Envoy Gateway core</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#benefits-of-migration","title":"Benefits of Migration","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-developers","title":"For Developers","text":"<ul> <li>Simplified API: Easier adoption of Envoy as API gateway \"out of the box\"</li> <li>Standardization: Single canonical implementation reduces fragmentation</li> <li>Community Support: Broader community backing under CNCF</li> <li>Gateway API Native: Built from ground up for Kubernetes Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-organizations","title":"For Organizations","text":"<ul> <li>Reduced Maintenance: Consolidated project reduces operational overhead</li> <li>Better Interoperability: Standard implementation improves compatibility</li> <li>Long-term Support: CNCF governance provides sustainability</li> <li>Vendor Neutrality: Community-driven development model</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#current-recommendations","title":"Current Recommendations","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-new-projects","title":"For New Projects","text":"<ul> <li>Evaluate Envoy Gateway: Consider starting with Envoy Gateway for new deployments</li> <li>Monitor Development: Track Envoy Gateway's feature development progress</li> <li>Test Migration Tools: Participate in migration tooling beta testing</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-existing-contour-users","title":"For Existing Contour Users","text":"<ul> <li>Continue Current Usage: No immediate action required</li> <li>Plan for Future: Begin planning migration strategy for long-term</li> <li>Stay Informed: Monitor announcements from both projects</li> <li>Test Compatibility: Evaluate current configurations against Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#resources-and-links","title":"Resources and Links","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#official-announcements","title":"Official Announcements","text":"<ul> <li>Contour Joins Forces With Community Leaders to Build New Envoy Gateway Project - VMware Open Source Blog</li> <li>Introducing Envoy Gateway - CNCF Blog</li> <li>VMware Hands Control of Kubernetes Ingress Controller Contour to CNCF - Data Center Knowledge</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#project-resources","title":"Project Resources","text":"<ul> <li>Project Contour - Official Contour Website</li> <li>Contour CNCF Project Page - CNCF</li> <li>Envoy CNCF Project Page - CNCF</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Mapping out the future of cluster ingress with Contour and Gateway API - CNCF Blog</li> <li>VMware Tanzu Platform Service Routing with Contour - VMware Tanzu</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#industry-analysis","title":"Industry Analysis","text":"<ul> <li>Envoy Gateway Makes Using Envoy Proxy Easier for Developers - Tetrate</li> <li>VMware's Contour becomes the CNCF's latest incubation-level project - SiliconANGLE</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#conclusion","title":"Conclusion","text":"<p>The migration from Contour to Envoy Gateway represents a strategic consolidation in the Envoy-based ingress controller ecosystem. While the transition is evolutionary and long-term, developers should begin familiarizing themselves with Envoy Gateway and planning for eventual migration. VMware's commitment to providing migration tools and maintaining Contour support ensures a smooth transition path for existing users.</p> <p>The success of this migration depends on Envoy Gateway achieving feature parity with Contour and the development of robust migration tooling. Organizations should monitor both projects' progress and participate in community testing to ensure their specific use cases are supported in the migration path.</p>"},{"location":"networking/gateway-api/external-dns-cert-manager/","title":"External dns and cert manager","text":"<p>This is how external dns and cert manager works with gateway api</p>"},{"location":"networking/gateway-api/external-dns-cert-manager/#external-dns","title":"External DNS","text":"<p>Based on external-dns v0.20.0</p> <p>We assume external-dns is well configured to work with the provider. Then we need to enable gateway api sources. If we deployed it using the helm chart, we must add the desired routes as sources in the values file</p> <pre><code>sources:\n  - service\n  - ingress\n  - gateway-httproute\n  - gateway-tcproute\n  - gateway-tlsroute\n  - gateway-grpcroute\n  - gateway-udproute\n  - crd # Enable creation of individual DNSRecords\n</code></pre> <p>It is possible to filter what routes are being watched for every external dns instance</p> <p>Future: ListenerSet support \u2014 GEP-1713 introduces <code>ListenerSet</code> (currently experimental as <code>XListenerSet</code>, <code>gateway.networking.x-k8s.io/v1alpha1</code>), which allows attaching multiple sets of listeners to a parent Gateway. External-dns does not yet support <code>XListenerSet</code> as a source. Once the resource graduates from experimental to stable and is renamed to <code>ListenerSet</code>, a dedicated source entry is expected to be added alongside the existing route sources.</p>"},{"location":"networking/gateway-api/external-dns-cert-manager/#annotations","title":"Annotations","text":"<p>The following table shows the relations between the gateway api resources and external-dns supported and recommneded annotations</p> <p>The format of the annotations is:</p> <pre><code>external-dns.alpha.kubernetes.io/annotation\n</code></pre> target hostname ttl controller provider specific Gateway YES NO NO NO NO HTTPRoute NO use listener YES YES YES TLSRoute NO use listener YES YES YES TCPRoute YES YES, recommended YES YES YES UDPRoute YES YES, recommended YES YES YES GRPCRoute ? ? ? ? ? <p>Provider specific annotations can be cloudflare-, aws-, scw-*</p>"},{"location":"networking/gateway-api/external-dns-cert-manager/#cert-manager","title":"Cert-Manager","text":"<p>Based on cert-manager 1.19</p> <p>We need a recent version of cert-manager and enable gateway api in the values file</p> <pre><code>config:\n  apiVersion: controller.config.cert-manager.io/v1alpha1\n  kind: ControllerConfiguration\n  enableGatewayAPI: true\n</code></pre> <p>In order to get a certificate we need to:</p> <ul> <li>Annotate a Gateway resource with an issuer or cluster issuer</li> </ul> <pre><code>cert-manager.io/issuer: foo\ncert-manager.io/cluster-issuer: foo\n</code></pre> <p>The certificate generation is considered to be managed by a sre team, so annotations in route resources are ignored</p> <ul> <li>Enable a HTTPS listener</li> </ul> <p>The spec.dnsNames field in the generated certificate is taken from the hostname in the listener</p> <p>It must have a tls section in Terminate mode and certificateRefs must be a secret in the same namespace as the gateway</p> <p>Future: ListenerSet support \u2014 GEP-1713 defines <code>ListenerSet</code> (currently <code>XListenerSet</code>, experimental) to allow independent teams to attach their own listeners with separate certificates to a shared parent Gateway. Cert-manager does not yet support annotating <code>XListenerSet</code> resources directly. When the resource graduates to stable, cert-manager support will likely extend to reading issuer annotations from <code>ListenerSet</code> objects, enabling per-team certificate provisioning without granting write access to the parent Gateway.</p>"},{"location":"networking/gateway-api/external-dns-cert-manager/#migration","title":"Migration","text":"<ul> <li> <p>Creating a gateway with DNS challenge will create a temporary TXT record with challenge token until the secret is created. It must not offer conflicts with existing ingress certificates / DNS entries.</p> </li> <li> <p>For creating a gateway with HTTP-01 challenge see the links section below</p> </li> </ul>"},{"location":"networking/gateway-api/external-dns-cert-manager/#links","title":"Links","text":"<ul> <li>External DNS: Gateway API Route Sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway-api/</p> <ul> <li>External DNS: Gateway sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway/</p> <ul> <li>External DNS: Annotations</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/annotations/annotations/</p> <ul> <li>Cert manager: Annotated Gateway resource</li> </ul> <p>https://cert-manager.io/docs/usage/gateway/</p> <ul> <li>Cert manager: HTTP-01 solver</li> </ul> <p>https://cert-manager.io/docs/configuration/acme/http01/</p>"},{"location":"networking/gateway-api/gateway-api-implementations/","title":"Gateway API Implementations","text":"<p>!!! info \"Last updated\"     This document and the table below were last updated on 2026-02-23. Implementation statuses, GitHub stars, and contributor counts change frequently \u2014 verify against the official implementations page for the latest status.</p>"},{"location":"networking/gateway-api/gateway-api-implementations/#requirements","title":"Requirements","text":"<p>For inclusion in this documentation, Gateway API implementations must meet the following criteria:</p> <ol> <li>Open Source: The implementation must be open source software</li> <li>Official Listing: Must be listed on the official Gateway API implementations page</li> </ol>"},{"location":"networking/gateway-api/gateway-api-implementations/#available-open-source-implementations","title":"Available Open Source Implementations","text":"Implementation Status Type GitHub Stars Contributors Description Envoy Gateway \u2705 GA Envoy 2.5k 308+ CNCF Graduated Envoy subproject, latest: v1.7.0 Istio \u2705 GA Envoy 37.3k 1,147 CNCF Graduated service mesh kgateway \u2705 GA Envoy 5.3k 243 CNCF Sandbox AI-powered API Gateway (formerly Gloo) Cilium \u2705 Conformant eBPF 23.8k 955+ CNCF Graduated eBPF-based networking, sidecarless mesh support NGINX Gateway Fabric \u2705 GA NGINX 674 56 Open source NGINX implementation Traefik Proxy \u2705 GA Other 56.8k 891 Open source cloud-native application proxy Linkerd \u2705 SM Other 11.1k 375+ CNCF Graduated service mesh (service mesh conformance) Contour \ud83d\udfe1 Partial Envoy 3.8k 226 CNCF Incubating Envoy-based ingress controller AWS Load Balancer Controller \ud83d\udfe1 Partial Other 4.3k 251 Provisions ALB/NLB on AWS; Gateway API support added in v3.0 (2026) Emissary-Ingress \ud83d\udd34 Stale Envoy 4.5k 215 CNCF Incubating project, no recent conformance reports Apache APISIX \ud83d\udd34 Stale Other 15.7k 480 Apache Foundation API Gateway, no recent conformance reports Easegress \ud83d\udd34 Stale Other 5.9k 65+ CNCF Sandbox Cloud Native traffic orchestration Flomesh Service Mesh \ud83d\udd34 Stale Other 66 9+ Community driven lightweight service mesh HAProxy Ingress \ud83d\udd34 Stale Other 791 50+ Community driven ingress controller, no recent conformance reports Kuma \ud83d\udd34 Stale Other 3.9k 114 CNCF Sandbox service mesh, no recent conformance reports LoxiLB \ud83d\udd34 Stale Other 1.8k 20+ Open source load balancer WSO2 APK \ud83d\udd34 Stale Other 166 30+ Open source API management solution ingate \u26a0\ufe0f Retiring Other 685 Multiple Kubernetes SIG Network reference impl \u2014 retiring early 2026 <p>Status Legend: \u2705 GA = Generally Available, fully conformant | \u2705 Conformant = Passes full conformance tests (no GA label) | \u2705 SM = Service mesh conformance only | \ud83d\udfe1 Partial = Partially conformant (passes some but not all conformance tests) | \ud83d\udd34 Stale = No recent conformance reports submitted to the official registry | \u26a0\ufe0f Retiring = Deprecated / being retired</p> <p>Notes:</p> <ul> <li>ingate was a migration path from NGINX Ingress Controller to Gateway API \u2014 SIG Network recommends migrating away immediately</li> <li>GitHub Stars are approximate and were last checked on 2026-02-23</li> <li>Status is based on the official conformance registry</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#deprecatedlegacy-implementations","title":"Deprecated/Legacy Implementations","text":"<ul> <li>Acnodal EPIC Open Source External Gateway platform</li> <li>GitHub: epic-gateway/resource-model | \u2b50 0 | \ud83d\udc65 2 contributors</li> <li>Airlock Microgateway Open source WAAP solution</li> <li>GitHub: airlock/microgateway | \u2b50 18 | \ud83d\udc65 3-5 contributors</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#commercialproprietary-implementations","title":"Commercial/Proprietary Implementations","text":"<p>The following commercial or proprietary Gateway API implementations are officially listed:</p> <ul> <li>LiteSpeed Ingress Controller Commercial web ADC controller with Gateway API support</li> <li>Tyk Gateway Cloud-native API Gateway working towards Gateway API implementation</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#cloud-provider-implementations","title":"Cloud Provider Implementations","text":"<p>Cloud providers offer managed Gateway API implementations:</p>"},{"location":"networking/gateway-api/gateway-api-implementations/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>AWS Load Balancer Controller Partially conformant \u2014 provisions ALB/NLB with Gateway API support (added in v3.0, January 2026). See main table above.</li> <li>AWS Gateway API Controller Integrates with Amazon VPC Lattice for EKS clusters</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#microsoft-azure","title":"Microsoft Azure","text":"<ul> <li>Azure Application Gateway for Containers Managed application load balancing solution</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>GKE Gateway API Controller Integrates with Google Cloud Load Balancers</li> <li>Google Cloud Service Mesh Supports Envoy-based and Proxyless-GRPC mesh implementations</li> </ul>"},{"location":"networking/gateway-api/gateway-backend/","title":"Gateway to Backend (upstream)","text":"<p>The connection between the gateway and the final pods (via services) is known as Upstream connections.</p>"},{"location":"networking/gateway-api/gateway-backend/#tls","title":"TLS","text":"<p>A typical situation is when we want to terminate the TLS connection in the gateway but the final pod also uses TLS.</p> <p>In order to configure it we must use a gateway api resource called BackendTLSPolicy</p>"},{"location":"networking/gateway-api/gateway-backend/#backendtlspolicy","title":"BackendTLSPolicy","text":"<p>This resource has 2 main fields:</p>"},{"location":"networking/gateway-api/gateway-backend/#spectargetrefs","title":"spec.targetRefs","text":"<p>It permits to specify a list of kubernetes resources that will receive this policy. This usually is a kubernetes service, but it can be a Gateway or an HTTPRoute Here we can specify the group, kind and name of the resource. An additional sectionName can be specify with different meaning depending of the targeted resource:</p> <ul> <li>Service: Port name</li> <li>Gateway: Listener name</li> <li>HTTPRoute: HTTPRouteRule name</li> </ul>"},{"location":"networking/gateway-api/gateway-backend/#specvalidation","title":"spec.validation","text":"<p>Here we configure how the gateway validates the final certificate, providing:</p> <ul> <li>hostname and subjectAltNames</li> <li>if we want to use the system CAs or a custom one</li> </ul>"},{"location":"networking/gateway-api/gateway-backend/#backendtlspolicy-with-self-signed-certificates","title":"BackendTLSPolicy with self signed certificates","text":"<p>But this resource don't includes the possibility skip the verification of the pods certificate, required when working with self signed certificates. Then the upstream connection fails.</p> <p>https://github.com/kubernetes-sigs/gateway-api/issues/3761</p> <p>In order to handle it, we must use solutions provided by the gateway api implementation.</p>"},{"location":"networking/gateway-api/gateway-backend/#envoy-gateway","title":"Envoy Gateway","text":"<p>In Envoy Gateway implementations we can achieve this creating an Envoy Gateway Backend resource that specifies the tls verification must be skipped, and modify the route rule to use this backend resource as backendRefs.</p> <p>Backend resources must be enabled first in the envoy gateway deployment</p>"},{"location":"networking/gateway-api/gateway-backend/#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>In AWS Load Balancer Controller and a TCPRoute we can achieve this for example creating a TargetGroupConfiguration with a default configuration with TLS as protocol.</p> <p>AWS Load Balancer Controller seems to not verify by default the backend certificate</p> <pre><code>apiVersion: gateway.k8s.aws/v1beta1\nkind: TargetGroupConfiguration\nmetadata:\n  name: broker-expose-85309-11170\nspec:\n  defaultConfiguration:\n    protocol: TLS\n</code></pre>"},{"location":"networking/gateway-api/gateway-backend/#links","title":"Links","text":"<ul> <li>BackendTLSPolicy</li> </ul> <p>https://gateway-api.sigs.k8s.io/api-types/backendtlspolicy/</p> <ul> <li>Upstream TLS</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/tls/#upstream-tls</p> <ul> <li>Backend TLS: Gateway to Backend (envoy gateway)</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/security/backend-tls/</p> <ul> <li>Backend TLS: Skip TLS Verification (envoy gateway)</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/security/backend-skip-tls-verification/</p> <ul> <li>Target groups for your Network Load Balancers (AWS Load Balancer Controller)</li> </ul> <p>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html</p>"},{"location":"networking/gateway-api/linking-apps/","title":"Linking Apps","text":""},{"location":"networking/gateway-api/linking-apps/#external-dns","title":"External DNS","text":""},{"location":"networking/gateway-api/linking-apps/#gateways","title":"Gateways","text":"<ul> <li>Gateway sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway/</p>"},{"location":"networking/gateway-api/linking-apps/#routes","title":"Routes","text":"<ul> <li>Gateway API Route Sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway-api/</p>"},{"location":"networking/gateway-api/linking-apps/#cert-manager","title":"Cert-Manager","text":"<p>First we need to enable Gateway Api Support in our controller</p> <pre><code>config:\n  apiVersion: controller.config.cert-manager.io/v1alpha1\n  kind: ControllerConfiguration\n  enableGatewayAPI: true\n</code></pre> <p>Then we can annotate a gateway to generate the certificates</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: example\n  annotations:\n    cert-manager.io/issuer: foo\n</code></pre> <ul> <li>Annotated Gateway resource</li> </ul> <p>https://cert-manager.io/docs/usage/gateway/</p> <ul> <li>Envoy Proxy</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/security/tls-cert-manager/</p>"},{"location":"networking/gateway-api/listeners/","title":"Listeners","text":"<p>Listeners are logical endpoints associated in a gateway resource (spec.listeners). In a listener we define how the Gateway accepts incoming network traffic. A simple analogy can be as different doors to a building.</p> <p>We can specify different things here</p> <ul> <li>Protocol</li> <li>Port</li> <li>Hostname</li> <li>TLS configuration</li> <li>AllowedRoutes</li> </ul> <p>The listeners must be distinct in a gateway, this is a unique combination of Port, Protocol, and hostname (if supported by the protocol)</p>"},{"location":"networking/gateway-api/listeners/#about-protocols","title":"About protocols","text":"<p>Hostname specifies the virtual hostname to match for protocol types that define this concept. When unspecified, all hostnames are matched. This field is ignored for protocols that don't require hostname based matching.</p> <p>Implementations MUST apply Hostname matching appropriately for each of the following protocols:</p> <ul> <li>TLS: The Listener Hostname MUST match the SNI.</li> <li>HTTP: The Listener Hostname MUST match the Host header of the request.</li> <li>HTTPS: The Listener Hostname SHOULD match both the SNI and Host header</li> </ul>"},{"location":"networking/gateway-api/listeners/#http","title":"HTTP","text":"<p>If we expect plain HTTP traffic.</p> <ul> <li>Usually port 80</li> <li>HTTPRoute resources as AllowedRoutes</li> <li>We can define a hostname. It must match the host header of the requests.   If no hostname is defined, all hostnames are matched.</li> </ul>"},{"location":"networking/gateway-api/listeners/#https","title":"HTTPS","text":"<p>If we expect HTTP traffic with TLS termination. We usually define:</p> <ul> <li>Usually port 443</li> <li>HTTPRoute resources as AllowedRoutes</li> <li>TLS section must be added in Terminate mode (the gateway terminates the TLS   downstream connection)</li> <li>We can define a hostname. It should match both the SNI and Host header of the   requests. This does not require the SNI and Host header to be the same.   If no hostname is defined, all hostnames are matched.</li> </ul>"},{"location":"networking/gateway-api/listeners/#tls","title":"TLS","text":"<p>If we expect generic TLS traffic</p> <ul> <li>Any port can be configured</li> <li>A hostname can be defined. The Listener Hostname MUST match the SNI.</li> <li>A TLS section must be defined</li> <li>If we use Terminate mode, the gateway terminates the TLS downstream connection</li> </ul> <p>The gapi documentation tells the route type supported is TLSRoute resources as an extended feature so it will be supported depending on the implementation. But I have TCPRoute resources working here as AllowedRoutes</p> <ul> <li>If we use Passthrough mode, the service (pod) terminates the TLS downstream   connection. TLSRoute resources as AllowedRoutes</li> </ul>"},{"location":"networking/gateway-api/listeners/#tcp-and-udp","title":"TCP and UDP","text":"<p>If we expect raw TCP connections or UDP traffic</p> <ul> <li>Any port can be configured</li> <li>TCPRoute and UDPRoute resources as AllowedRoutes</li> <li>Hostname is ignored</li> </ul>"},{"location":"networking/gateway-api/listeners/#grpc","title":"GRPC","text":"<ul> <li>GRPC - gRPC over HTTP/2 with TLS \u2192 connects to GRPCRoute</li> </ul>"},{"location":"networking/gateway-api/listeners/#table","title":"Table","text":"<p>This table shows all the protocols available in listeners and:</p> <ul> <li>the supported routes per protocol</li> <li>if the hostname field can be used and how</li> <li>if the TLS section can be used and how</li> </ul> Protocol Route Supported hostname match TLS section Note HTTP HTTPRoute Must: hostname header Not supported HTTP GRPCRoute ?? Not supported HTTPS HTTPRoute Should: SNI and host header Terminate HTTPS GRPCRoute ?? ?? TLS TLSRoute Must: SNI Passthrough TLS TLSRoute Must: SNI Terminate Implementation dependant TLS TCPRoute Must: SNI Terminate TCP TCPRoute Ignored Not supported UDP UDPRoute Ignored Not supported"},{"location":"networking/gateway-api/listeners/#listener-status","title":"Listener status","text":"<p>The listener status can be obtained from the gateway resource status field</p> <ul> <li>Type Accepted</li> </ul> <p>The listener is accepted or not</p> <ul> <li>Type Conflicted</li> </ul> <p>There are conflicts with this listener</p> <ul> <li> <p>Type ResolvedRefs</p> </li> <li> <p>Type OverlappingTLSConfig</p> </li> </ul> <p>Listeners have overlapping TLS certificates. See Overlapping TLS and ALPN</p> <ul> <li>Type Programmed</li> </ul>"},{"location":"networking/gateway-api/merging/","title":"Merging from class","text":"<p>When creating a GatewayClass there is an optional field spec.parametersref that permits to configure it custom implementation settings</p> <p>Also, when creating a gateway we can configure it with custom implementation settings using spec.infrastructure.parametersref</p> <p>The gateway api specification recommends the GatewayClass to provide defaults that can be overriden by a Gateway, but when both setting exists, the merging behaviour depends of the implementation</p>"},{"location":"networking/gateway-api/merging/#aws-load-balancer-controller","title":"Aws load balancer controller","text":"<p>The resource we can use to configure both is called LoadBalancerConfiguration</p> <p>In this implementation Gateways ALWAYS inherit the LoadBalancerConfiguration from their GatewayClass.</p> <ul> <li>When there are no conflicting fields, both configurations are simply combined</li> <li>When there are conflicting fields between the two configurations we can choose the desired behaviour using spec.mergingMode in the LoadBalancerConfiguration resource applied in the GatewayClass. The 2 possible values are prefer-gateway-class and prefer-gateway</li> </ul>"},{"location":"networking/gateway-api/merging/#envoy-gateway","title":"Envoy gateway","text":"<p>The resource we can use to configure both is called EnvoyProxy</p> <p>In 1.6 version it is not well documenented. We can read</p> <p>\"You can also attach the EnvoyProxy resource to the GatewayClass using the parametersRef field. This configuration is discouraged if you plan on creating multiple Gateways linking to the same GatewayClass and would like different infrastructure configurations for each of them.\"</p> <p>This suggest a replacement model. The Gateway inherits from GatewayClass only if Gateway doesn't specify its own EnvoyProxy</p>"},{"location":"networking/gateway-api/overlapping-tls-alpn/","title":"Overlapping TLS and ALPN","text":"<p>When two listeners on the same Gateway share a TLS certificate with overlapping SANs, Gateway API requires implementations to raise an <code>OverlappingTLSConfig</code> condition with reason <code>OverlappingCertificates</code>. This behavior is standardized in GEP-3567, which graduated to Standard conformance in Gateway API v1.5.0.</p> <p>A common scenario: a wildcard listener (<code>*.example.com</code>) and a bare domain listener (<code>example.com</code>) both reference the same certificate. Since cert-manager includes both <code>*.example.com</code> and <code>example.com</code> as SANs in a single cert (because the wildcard does not cover the bare domain per RFC 6125), the overlap is triggered.</p>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#why-it-matters-http2-connection-coalescing","title":"Why it matters: HTTP/2 connection coalescing","text":"<p>HTTP/2 allows clients to reuse a single TCP connection for multiple hostnames if the TLS certificate covers both (connection coalescing). This becomes a problem with multiple listeners:</p> <ol> <li>A client connects to <code>app.example.com</code> (matched by the wildcard listener)</li> <li>The cert also covers <code>example.com</code> (matched by the bare domain listener)</li> <li>The client reuses that connection for <code>example.com</code> requests</li> <li>The proxy cannot distinguish which listener should handle the request    since both arrive on the same connection</li> </ol>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#how-gateway-api-addresses-this","title":"How Gateway API addresses this","text":""},{"location":"networking/gateway-api/overlapping-tls-alpn/#overlappingtlsconfig-condition-part-a","title":"OverlappingTLSConfig condition (Part A)","text":"<p>All conformant implementations MUST report the <code>OverlappingTLSConfig</code> condition on affected listeners. This is a Standard conformance requirement since Gateway API v1.5.0 \u2014 not an implementation-specific extension.</p>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#http-421-misdirected-request-part-b","title":"HTTP 421 Misdirected Request (Part B)","text":"<p>The spec RECOMMENDS that implementations return HTTP 421 (Misdirected Request, RFC 9110 \u00a715.5.20) instead of disabling HTTP/2 entirely. This is tracked as the <code>GatewayReturn421</code> conformance feature.</p> <p>With 421 support, when a client sends a coalesced request to the wrong listener, the server responds with 421 and the client self-corrects by opening a new connection to the correct endpoint. HTTP/2 stays active for correctly routed traffic.</p>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#gateway-level-tls-config-part-c","title":"Gateway-level TLS config (Part C)","text":"<p>GEP-91 moves mutual TLS (mTLS) client certificate validation to the Gateway level. This mitigates coalescing security risks by ensuring cert validation happens before listener routing rather than after.</p>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#impact","title":"Impact","text":"Scenario HTTP/2 Behaviour Implementation supports 421 Stays active Misdirected requests bounce with 421; clients reconnect Implementation disables h2 (legacy) Downgraded to HTTP/1.1 All traffic works, no multiplexing or header compression"},{"location":"networking/gateway-api/overlapping-tls-alpn/#solutions","title":"Solutions","text":""},{"location":"networking/gateway-api/overlapping-tls-alpn/#spec-level-portable-across-implementations","title":"Spec-level (portable across implementations)","text":"<ul> <li>Separate certificates: Use a non-wildcard cert for the bare domain   listener so SANs do not overlap. This eliminates the condition entirely.</li> <li>Rely on 421 responses: If the implementation supports <code>GatewayReturn421</code>,   no action is needed \u2014 misdirected requests self-correct automatically.</li> </ul>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#implementation-specific","title":"Implementation-specific","text":"<ul> <li>Envoy Gateway: Configure a   <code>ClientTrafficPolicy</code> to force   <code>h2</code> ALPN advertisement, accepting the coalescing risk explicitly.</li> </ul>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#detecting-the-condition","title":"Detecting the condition","text":"<pre><code>kubectl get gateway &lt;name&gt; -n &lt;namespace&gt; -o json | \\\n  jq '.status.listeners[].conditions[] | select(.type == \"OverlappingTLSConfig\")'\n</code></pre>"},{"location":"networking/gateway-api/overlapping-tls-alpn/#links","title":"Links","text":"<ul> <li>GEP-3567: Overlapping TLS Config</li> <li>GEP-91: Client Certificate Validation</li> <li>RFC 9110 \u00a715.5.20 \u2014 421 Misdirected Request</li> <li>Gateway API v1.5.0 release notes</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/98-tips/","title":"Tips","text":""},{"location":"networking/gateway-api/aws%20lbc/98-tips/#gateway-listener-vs-aws-load-balancer-listener","title":"Gateway Listener vs AWS Load Balancer Listener","text":"<p>See gateway-vs-aws-listeners.md for detailed information about the difference between Gateway API listeners and AWS Load Balancer listeners, including lifecycle, configuration, and settings management.</p>"},{"location":"networking/gateway-api/aws%20lbc/98-tips/#tls-section-ignore","title":"TLS section ignore","text":"<p>AWS load balancer controller seems to ignore the tls section in the listeners because the certificates are discovered in ACM via hostname matching</p>"},{"location":"networking/gateway-api/aws%20lbc/98-tips/#tlsroute-and-sni-routing","title":"TLSRoute and SNI Routing","text":"<p>See nlb-sni-limitations.md for detailed information about why AWS NLB does not support SNI routing, TLSRoute non-conformance to Gateway API spec, and recommended architectural patterns using TCPRoute.</p>"},{"location":"networking/gateway-api/aws%20lbc/98-tips/#protocols-in-network-load-balancer","title":"Protocols in Network Load Balancer","text":"<p>See protocols.md for detailed information about Gateway API and AWS NLB protocol concepts, mappings, and common scenarios.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/","title":"Gateway api","text":"<p>It is possible to use the Aws Load Balancer Controller as gateway api implementation. To us it, you need:</p> <ul> <li>A recent version of Aws Load Balancer Controller  (&gt;= v2.13.0)</li> <li>Clusters with Amazon VPC CNI plugin or with native AWS VPC networking, like cilium with eni ipam mode</li> <li>The official gateway api crds</li> <li>The Aws Load Balancer Controller gateway api crds</li> <li>Enabling it via feature gates (NLBGatewayAPI and ALBGatewayAPI)</li> </ul> <p>via helm chart</p> <pre><code>controllerConfig:\n  featureGates:\n    NLBGatewayAPI: true\n    ALBGatewayAPI: true\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/#choose-nlb-or-alb","title":"Choose NLB or ALB","text":"<p>The L4 loadbalacing is done via AWS NLB and the L7 via AWS ALB. Choose what balancer to use is done when creating the gateway class:</p> <pre><code>spec.controllerName: gateway.k8s.aws/nlb\nspec.controllerName: gateway.k8s.aws/alb\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/#aws-load-balancer-controller-gateway-api-crds","title":"Aws Load Balancer Controller gateway api crds","text":"<p>The Aws Load Balancer Controller gateway api crds are:</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/#loadbalancerconfiguration","title":"LoadBalancerConfiguration","text":"<p>This CRD permits to configure the loadbalancer at 2 levels:</p> <ul> <li>the GatewayClass (global settings) via spec.parametersRef</li> <li>the Gateway (specific settings) via spec.infrastructure.parametersRef</li> </ul> <p>How to resolve conflicts in settings is controlled via \"mergingMode\" in the GatewayClass's configuration</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/#targetgroupconfiguration","title":"TargetGroupConfiguration","text":"<p>This CRD, binded to a kubernetss service, permits to configure the AWS Target Groups, for example, defining targetType and health checks with default settings or specific settings per route</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-api/#listenerruleconfiguration","title":"ListenerRuleConfiguration","text":"<p>This CRD permits to configure AWS ALB settings not implemented in the standard Gateway API CRDs.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/","title":"Gateway Listener vs AWS Load Balancer Listener","text":""},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#the-name-conflict","title":"The Name Conflict","text":"<p>There is a name conflict between two different concepts that both use the term \"listener\":</p> <ol> <li>Gateway API Listener - A logical endpoint defined in a Gateway resource (Kubernetes concept)</li> <li>AWS Load Balancer Listener - An actual listener on the AWS Load Balancer that handles incoming connections (AWS infrastructure concept)</li> </ol> <p>This naming overlap can cause confusion when working with the AWS Load Balancer Controller.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#gateway-api-listener-kubernetes-concept","title":"Gateway API Listener (Kubernetes Concept)","text":"<p>A Gateway API listener is defined in the <code>Gateway.spec.listeners[]</code> section of a Gateway resource.</p> <p>Purpose: Defines a logical endpoint that:</p> <ul> <li>Specifies what protocol and port the Gateway accepts traffic on</li> <li>Defines which Route resources can attach to it</li> <li>Configures hostname matching rules</li> <li>Sets up TLS configuration</li> </ul> <p>Important: Creating a Gateway with listeners does NOT create AWS listeners in the load balancer.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#aws-load-balancer-listener-aws-infrastructure","title":"AWS Load Balancer Listener (AWS Infrastructure)","text":"<p>An AWS listener is an actual component on the AWS Load Balancer (NLB or ALB) that:</p> <ul> <li>Listens for connection requests on a specific protocol and port</li> <li>Forwards requests to target groups</li> <li>Handles TLS termination (for TLS listeners)</li> </ul> <p>Important: AWS listeners are created when Route resources are deployed, not when the Gateway is created.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#lifecycle-flow","title":"Lifecycle Flow","text":""},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#step-1-create-gateway","title":"Step 1: Create Gateway","text":"<p>When you create a Gateway resource:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\nspec:\n  gatewayClassName: aws-nlb-gateway-class\n  listeners:\n  - name: tcp-app\n    protocol: TCP\n    port: 8080\n</code></pre> <p>What happens:</p> <ul> <li>\u2705 AWS Load Balancer (NLB or ALB) is created</li> <li>\u274c AWS listeners are NOT created yet</li> <li>The Gateway resource defines the load balancer type:</li> <li><code>gateway.k8s.aws/nlb</code> \u2192 Creates Network Load Balancer (NLB)</li> <li><code>gateway.k8s.aws/alb</code> \u2192 Creates Application Load Balancer (ALB)</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#step-2-create-route","title":"Step 2: Create Route","text":"<p>When you create a Route resource that references the Gateway:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: TCPRoute\nmetadata:\n  name: my-tcp-route\nspec:\n  parentRefs:\n  - name: my-gateway\n    sectionName: tcp-app\n  rules:\n  - backendRefs:\n    - name: my-service\n      port: 9000\n</code></pre> <p>What happens:</p> <ul> <li>\u2705 AWS listener is NOW created on the load balancer</li> <li>The AWS listener is configured based on the Route and LoadBalancerConfiguration settings</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#configuring-aws-listener-settings","title":"Configuring AWS Listener Settings","text":"<p>Although AWS listeners are created when Routes are deployed, you can pre-configure their settings using a <code>LoadBalancerConfiguration</code> resource.</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#loadbalancerconfiguration","title":"LoadBalancerConfiguration","text":"<p>Settings are located under <code>spec.listenerConfigurations</code> in the LoadBalancerConfiguration resource:</p> <pre><code>apiVersion: gateway.k8s.aws/v1beta1\nkind: LoadBalancerConfiguration\nmetadata:\n  name: example-config\nspec:\n  listenerConfigurations:\n    - protocolPort: TCP:8080\n      defaultCertificate: arn:aws:acm:...\n      certificates: [arn-1, arn-2]\n      sslPolicy: ELBSecurityPolicy-TLS13-1-2-Res-2021-06\n      alpnPolicy: HTTP2Preferred\n      targetGroupStickiness:\n        enabled: true\n        durationSeconds: 3600\n      mutualAuthentication:\n        mode: verify\n        trustStoreArn: arn:aws:...\n      listenerAttributes:\n        - key: tcp.idle_timeout.seconds\n          value: \"350\"\n</code></pre> <p>Available configuration options:</p> <ul> <li>protocolPort - Protocol + port combination (e.g., <code>TCP:8080</code>, <code>TLS:443</code>)</li> <li>defaultCertificate - Default SSL certificate ARN</li> <li>certificates - List of additional certificate ARNs</li> <li>sslPolicy - TLS security policy</li> <li>alpnPolicy - ALPN policy for protocol negotiation</li> <li>targetGroupStickiness - Session stickiness configuration</li> <li>mutualAuthentication - mTLS authentication settings</li> <li>quicEnabled - Enable QUIC protocol for UDP</li> <li>listenerAttributes - Other AWS listener attributes</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#where-loadbalancerconfiguration-is-used","title":"Where LoadBalancerConfiguration is Used","text":"<p>The LoadBalancerConfiguration can be referenced at two levels:</p>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#1-gatewayclass-level-global-settings","title":"1. GatewayClass Level (Global Settings)","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: aws-nlb-gateway-class\nspec:\n  controllerName: gateway.k8s.aws/nlb\n  parametersRef:\n    group: gateway.k8s.aws\n    kind: LoadBalancerConfiguration\n    name: global-config\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#2-gateway-level-specific-settings","title":"2. Gateway Level (Specific Settings)","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\nspec:\n  gatewayClassName: aws-nlb-gateway-class\n  infrastructure:\n    parametersRef:\n      group: gateway.k8s.aws\n      kind: LoadBalancerConfiguration\n      name: gateway-specific-config\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#configuration-merging","title":"Configuration Merging","text":"<p>When both GatewayClass and Gateway have LoadBalancerConfiguration:</p> <ul> <li>Both configurations are merged</li> <li>The merge behavior is controlled by <code>spec.mergingMode</code> in the GatewayClass's LoadBalancerConfiguration</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#summary","title":"Summary","text":"Concept What It Is When Created Purpose Gateway Listener Logical endpoint in Gateway resource When Gateway is created Defines what traffic Gateway accepts and which Routes can attach AWS Listener Actual AWS Load Balancer listener When Route is deployed Handles incoming connections on AWS infrastructure LoadBalancerConfiguration Settings resource Before Gateway/Route Pre-configures how AWS listeners will be set up"},{"location":"networking/gateway-api/aws%20lbc/gateway-vs-aws-listeners/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Gateway listener \u2260 AWS listener - they are different concepts despite the naming overlap</li> <li>Creating a Gateway creates the AWS Load Balancer only, not AWS listeners</li> <li>AWS listeners are created when Route resources are deployed</li> <li>Use LoadBalancerConfiguration to pre-configure AWS listener settings before Routes are created</li> <li>LoadBalancerConfiguration can be applied at GatewayClass level (global) or Gateway level (specific)</li> <li>Each L4 Gateway listener supports exactly one L4 Route resource</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/","title":"AWS NLB SNI Routing Limitations","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#overview","title":"Overview","text":"<p>AWS Network Load Balancer (NLB) does not support SNI (Server Name Indication) routing, which has important implications when using Gateway API with the AWS Load Balancer Controller. This limitation affects TLSRoute implementation and requires specific architectural patterns.</p>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#what-is-sni-routing","title":"What is SNI Routing?","text":"<p>SNI (Server Name Indication) is a TLS extension that allows clients to specify the hostname they're connecting to during the TLS handshake, before the encrypted connection is established.</p> <p>SNI Routing refers to a load balancer's ability to:</p> <ol> <li>Inspect the SNI field in the TLS ClientHello message (without decrypting the payload)</li> <li>Route traffic to different backend targets based on the hostname in the SNI field</li> <li>Pass through the encrypted TLS connection to the backend</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#example-use-case","title":"Example Use Case","text":"<pre><code>Client \u2192 Load Balancer (inspects SNI) \u2192 Backend\n  |            |\n  |            \u251c\u2500 app1.example.com \u2192 Service A\n  |            \u2514\u2500 app2.example.com \u2192 Service B\n  |\n  TLS handshake includes hostname\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#why-nlb-cannot-support-sni-routing","title":"Why NLB Cannot Support SNI Routing","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#layer-4-operation","title":"Layer 4 Operation","text":"<p>Network Load Balancers operate at Layer 4 (TCP/UDP):</p> <ul> <li>They forward TCP/UDP streams without inspecting application-layer data</li> <li>They work with raw packets, IP addresses, and port numbers</li> <li>No visibility into TLS handshake details</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#routing-capabilities","title":"Routing Capabilities","text":"<p>NLB can only route based on:</p> <ul> <li>Source/destination IP addresses</li> <li>Source/destination ports</li> <li>Protocol (TCP/UDP)</li> </ul> <p>NLB cannot route based on:</p> <ul> <li>Hostnames in SNI field</li> <li>HTTP headers</li> <li>TLS certificate names</li> <li>Application-layer content</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#tls-support-in-nlb","title":"TLS Support in NLB","text":"<p>While NLB supports TLS, it operates in two modes:</p> <ol> <li>TLS Passthrough: Forwards encrypted traffic without inspection (cannot see SNI)</li> <li>TLS Termination: Decrypts traffic but still cannot route based on SNI (single certificate per listener)</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#gateway-api-implications","title":"Gateway API Implications","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#tlsroute-non-conformance","title":"TLSRoute Non-Conformance","text":"<p>According to AWS Load Balancer Controller maintainer zac-nixon (source):</p> <p>\"Our TLSRoute is not conformant to the Gateway API spec, AWS NLB does not support SNI routing.\"</p> <p>The Gateway API TLSRoute specification expects:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TLSRoute\nmetadata:\n  name: example\nspec:\n  parentRefs:\n  - name: tls-gateway\n  hostnames:\n  - \"app1.example.com\"  # SNI-based routing expected\n  - \"app2.example.com\"  # SNI-based routing expected\n  rules:\n  - backendRefs:\n    - name: backend-service\n</code></pre> <p>This pattern does not work with AWS NLB because it cannot route based on hostnames.</p>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#recommendation-use-tcproute","title":"Recommendation: Use TCPRoute","text":"<p>The AWS Load Balancer Controller team recommends using TCPRoute instead:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TCPRoute\nmetadata:\n  name: example\nspec:\n  parentRefs:\n  - name: tcp-gateway\n    sectionName: tcp-443\n  rules:\n  - backendRefs:\n    - name: gateway-proxy-service\n      port: 443\n</code></pre> <p>Why TCPRoute is better:</p> <ul> <li>Clear expectations: Simple TCP port forwarding</li> <li>Conformant to Gateway API spec</li> <li>Matches NLB actual capabilities</li> <li>No confusion about hostname-based routing</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#architectural-patterns","title":"Architectural Patterns","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#pattern-1-two-tier-gateway-architecture","title":"Pattern 1: Two-Tier Gateway Architecture","text":"<p>The recommended architecture for SNI-based routing on AWS:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Client  \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n     \u2502 TLS (SNI: app1.example.com)\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AWS NLB (via AWS LBC + TCPRoute)    \u2502\n\u2502 - Simple TCP:443 forwarding         \u2502\n\u2502 - No SNI inspection                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502 TLS passthrough\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Gateway Proxy (Envoy/Nginx/etc)     \u2502\n\u2502 - Inspects SNI field                \u2502\n\u2502 - Routes based on hostname          \u2502\n\u2502 - Uses its own Gateway API resources\u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502\n     \u251c\u2500 app1.example.com \u2192 Service A\n     \u2514\u2500 app2.example.com \u2192 Service B\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#pattern-2-tls-termination-at-nlb","title":"Pattern 2: TLS Termination at NLB","text":"<p>For single hostname scenarios:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Client  \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n     \u2502 TLS\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 AWS NLB (TLS Listener + TCPRoute)   \u2502\n\u2502 - Terminates TLS                    \u2502\n\u2502 - Uses ACM certificate              \u2502\n\u2502 - Forwards to single backend        \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502 HTTP or TLS re-encryption\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Backend Service                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See tcproute-acm.md for implementation details.</p>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#implementation-example","title":"Implementation Example","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#aws-load-balancer-controller-layer","title":"AWS Load Balancer Controller Layer","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: nlb-gateway\nspec:\n  gatewayClassName: aws-load-balancer-controller\n  listeners:\n  - name: tls-443\n    protocol: TCP\n    port: 443\n---\napiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TCPRoute\nmetadata:\n  name: to-gateway-proxy\nspec:\n  parentRefs:\n  - name: nlb-gateway\n    sectionName: tls-443\n  rules:\n  - backendRefs:\n    - name: envoy-gateway-service  # or nginx-gateway-fabric-service\n      port: 443\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#gateway-implementation-layer-envoy-gateway-example","title":"Gateway Implementation Layer (Envoy Gateway Example)","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-gateway\nspec:\n  gatewayClassName: envoy-gateway-class\n  listeners:\n  - name: https\n    protocol: HTTPS\n    port: 443\n    hostname: \"*.example.com\"\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - name: wildcard-cert\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: app1-route\nspec:\n  parentRefs:\n  - name: envoy-gateway\n  hostnames:\n  - \"app1.example.com\"\n  rules:\n  - backendRefs:\n    - name: app1-service\n      port: 8080\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: app2-route\nspec:\n  parentRefs:\n  - name: envoy-gateway\n  hostnames:\n  - \"app2.example.com\"\n  rules:\n  - backendRefs:\n    - name: app2-service\n      port: 8080\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#comparison-alb-vs-nlb","title":"Comparison: ALB vs NLB","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#application-load-balancer-alb","title":"Application Load Balancer (ALB)","text":"<p>Supports:</p> <ul> <li>HTTP/HTTPS protocols</li> <li>Host-based routing</li> <li>Path-based routing</li> <li>TLS termination with SNI support</li> <li>Multiple certificates per listener</li> </ul> <p>Gateway API Support:</p> <ul> <li>HTTPRoute: Full support</li> <li>TLSRoute: Not applicable (terminates TLS)</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#network-load-balancer-nlb","title":"Network Load Balancer (NLB)","text":"<p>Supports:</p> <ul> <li>TCP/UDP protocols</li> <li>IP-based routing</li> <li>Port-based routing</li> <li>TLS passthrough</li> <li>TLS termination (single cert per listener)</li> </ul> <p>Gateway API Support:</p> <ul> <li>TCPRoute: Full support</li> <li>UDPRoute: Full support</li> <li>TLSRoute: Non-conformant (no SNI routing)</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#decision-matrix","title":"Decision Matrix","text":"Requirement Use ALB Use NLB + Gateway Proxy HTTP/HTTPS routing \u2713 - SNI-based TLS routing \u2713 \u2713 (at proxy layer) Non-HTTP protocols - \u2713 Static IP addresses - \u2713 Lowest latency - \u2713 WebSocket/gRPC \u2713 \u2713 Custom TLS policies - \u2713 (at proxy layer)"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#issue-tlsroute-listener-rejection","title":"Issue: TLSRoute Listener Rejection","text":"<p>Error:</p> <pre><code>Listener does not allow route attachment, kind does not match between listener and route\n</code></pre> <p>Cause: Attempting to attach TLSRoute to an NLB Gateway expecting SNI routing functionality.</p> <p>Solution: Use TCPRoute instead of TLSRoute.</p>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#issue-multiple-hostnames-with-nlb","title":"Issue: Multiple Hostnames with NLB","text":"<p>Problem: Need to route multiple hostnames (app1.example.com, app2.example.com) but using NLB.</p> <p>Solution:</p> <ol> <li>Use TCPRoute to forward to a gateway proxy (Envoy/Nginx)</li> <li>Configure SNI routing at the proxy layer using Gateway API resources</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#issue-tls-termination-with-multiple-certificates","title":"Issue: TLS Termination with Multiple Certificates","text":"<p>Problem: NLB listener can only have one certificate, need multiple domains.</p> <p>Solution:</p> <ul> <li>Use a wildcard certificate (*.example.com)</li> <li>Use a certificate with multiple SANs</li> <li>Use ALB instead (supports SNI with multiple certificates)</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#references","title":"References","text":"<ul> <li>GitHub Issue #4561 - Discussion about TLSRoute limitations</li> <li>Gateway API TLSRoute Spec</li> <li>AWS NLB Documentation</li> <li>AWS Load Balancer Controller Gateway API Guide</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/nlb-sni-limitations/#see-also","title":"See Also","text":"<ul> <li>Gateway vs AWS Listeners - Mapping between Gateway API and AWS ELB concepts</li> <li>TCPRoute with ACM - TLS termination at NLB using ACM certificates</li> <li>Protocols - Protocol support in AWS Load Balancer Controller</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/protocols/","title":"Protocols in Network Load Balancer","text":""},{"location":"networking/gateway-api/aws%20lbc/protocols/#two-different-protocol-concepts","title":"Two Different Protocol Concepts","text":"<p>When working with AWS Load Balancer Controller and Gateway API, there are two different protocol concepts that serve different purposes:</p> <ol> <li>Gateway API Listener Protocol (Kubernetes-level) - Defined in <code>Gateway.spec.listeners[].protocol</code></li> <li>AWS NLB Listener Protocol (AWS infrastructure-level) - Defined in the actual AWS Network Load Balancer listener</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#gateway-api-listener-protocol","title":"Gateway API Listener Protocol","text":"<p>The Gateway API listener protocol tells the Gateway API:</p> <ul> <li>What type of traffic pattern to expect</li> <li>Which Route types can attach to this listener</li> </ul> <p>Supported values: <code>HTTP</code>, <code>HTTPS</code>, <code>TLS</code>, <code>TCP</code>, <code>UDP</code></p>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#aws-nlb-listener-protocol","title":"AWS NLB Listener Protocol","text":"<p>The AWS NLB listener protocol tells AWS how to handle the connection at the load balancer level.</p> <p>Supported values: <code>TCP</code>, <code>TLS</code>, <code>UDP</code>, <code>TCP_UDP</code>, <code>QUIC</code>, <code>TCP_QUIC</code></p> <p>Critical AWS NLB behavior:</p> <ul> <li><code>TLS</code> protocol: Load balancer terminates TLS, decrypts traffic, and forwards plain TCP to backends</li> <li><code>TCP</code> protocol on port 443: Used for TLS passthrough - load balancer passes encrypted traffic through without decrypting</li> </ul>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#how-aws-listener-protocol-is-determined","title":"How AWS Listener Protocol is Determined","text":"<p>The AWS listener protocol is explicitly configured via the <code>LoadBalancerConfiguration</code> resource using the <code>protocolPort</code> field.</p> <p>The format is <code>&lt;AWS_PROTOCOL&gt;:&lt;PORT&gt;</code> where AWS_PROTOCOL is one of: TCP, TLS, UDP, TCP_UDP, QUIC, TCP_QUIC</p>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#common-scenarios","title":"Common Scenarios","text":""},{"location":"networking/gateway-api/aws%20lbc/protocols/#scenario-1-tls-termination-at-nlb","title":"Scenario 1: TLS Termination at NLB","text":"<p>Gateway listener with <code>TLS</code> protocol + LoadBalancerConfiguration with <code>TLS:443</code> protocolPort</p> <p>Result: AWS NLB listener with <code>TLS</code> protocol \u2192 Terminates TLS, sends plain TCP to pods</p>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#scenario-2-tls-passthrough-end-to-end-encryption","title":"Scenario 2: TLS Passthrough (End-to-End Encryption)","text":"<p>Gateway listener with <code>TLS</code> protocol + LoadBalancerConfiguration with <code>TCP:443</code> protocolPort</p> <p>Result: AWS NLB listener with <code>TCP</code> protocol \u2192 Passes encrypted traffic to pods, pods terminate TLS</p>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#scenario-3-plain-tcp-database","title":"Scenario 3: Plain TCP (Database)","text":"<p>Gateway listener with <code>TCP</code> protocol + LoadBalancerConfiguration with <code>TCP:5432</code> protocolPort</p> <p>Result: AWS NLB listener with <code>TCP</code> protocol \u2192 Plain TCP forwarding</p>"},{"location":"networking/gateway-api/aws%20lbc/protocols/#protocol-mapping-table","title":"Protocol Mapping Table","text":"Gateway Listener Protocol LoadBalancerConfiguration protocolPort AWS NLB Listener Protocol Behavior <code>TLS</code> (Terminate) <code>TLS:443</code> <code>TLS</code> NLB terminates TLS, sends plain TCP to backends <code>TLS</code> (Passthrough) <code>TCP:443</code> <code>TCP</code> NLB passes encrypted traffic through to backends <code>TCP</code> <code>TCP:port</code> <code>TCP</code> Plain TCP forwarding <code>UDP</code> <code>UDP:port</code> <code>UDP</code> Plain UDP forwarding"},{"location":"networking/gateway-api/aws%20lbc/protocols/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Gateway listener protocol determines which Route types can attach (TCPRoute, TLSRoute, UDPRoute, etc.)</li> <li>AWS listener protocol (configured via LoadBalancerConfiguration) determines the actual AWS NLB behavior</li> <li>For TLS termination: Use AWS protocol <code>TLS</code> in LoadBalancerConfiguration</li> <li>For TLS passthrough: Use AWS protocol <code>TCP</code> on port 443 in LoadBalancerConfiguration</li> <li>Each L4 Gateway listener can handle traffic for exactly one L4 Route resource</li> <li>Certificates for TLS termination are discovered from ACM via hostname matching</li> </ol>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/","title":"TCPRoute and AWS Certificate Manager","text":"<p>This document provides a way to expose a TCPRoute to the internet with this features:</p> <ul> <li>We are using AWS Load Balancer controller as gateway api implementation</li> <li>The Gateway is up and exposed as network load balancer</li> <li>We have pre deployed certificate in AWS Certificate Manager (ACM) for the myroute.my-domain.com domain</li> <li>We want to terminate the TLS connection in the gateway</li> <li>The final pod has a self signed certificate</li> </ul> <p>It seems this cannot be achieved using a TLSRoute so we must use a TCPRoute (no hostname verification)</p> <ul> <li>[Gateway API] gateway.k8s.aws/nlb listeners with protocol: TLS and tls.mode: Passthrough appear to build a TLS listener</li> </ul> <p>https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/4556</p>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#gateway-class","title":"Gateway Class","text":"<p>We deploy the GatewayClass with optional LoadBalancerConfiguration via spec.parametersRef</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: nlb\nspec:\n  controllerName: gateway.k8s.aws/nlb\n  parametersRef:\n    group: gateway.k8s.aws\n    kind: LoadBalancerConfiguration\n    name: mylbc\n    namespace: whatever\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#gateway","title":"Gateway","text":"<p>Then we must deploy the gateway using our gateway class. The gateway api listener:</p> <ul> <li>must have TLS as protocol</li> <li>the hostname must MUST match the SNI</li> <li>we only permit TCPRoute resources attached to this listener</li> <li>the tls mode must be Terminate. The secret will be ignored</li> </ul> <p>Additional settings can be specified via spec.infrastructure</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: mygateway\nspec:\n  gatewayClassName: nlb\n  listeners:\n    - name: mylistener\n      protocol: TLS\n      port: 5432\n      hostname: \"myroute.my-domain.com\"\n      allowedRoutes:\n        namespaces:\n          from: All\n        kinds:\n          - kind: TCPRoute\n            group: gateway.networking.k8s.io\n      tls:\n        mode: Terminate\n        certificateRefs:\n          - name: fake\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#loadbalancerconfiguration-gateway","title":"LoadBalancerConfiguration (gateway)","text":"<p>The creation of the Gateway only deploys the load balancer We can configure the gateway with the loadbalancer name and the configuration that the aws listeners will inherit when creating routes.</p> <pre><code>apiVersion: gateway.k8s.aws/v1beta1\nkind: LoadBalancerConfiguration\nmetadata:\n  name: mygateway\nspec:\n  loadBalancerName: mygateway\n  listenerConfigurations:\n    - protocolPort: \"TLS:5432\"\n      defaultCertificate: &lt;ARN OF our ACM certificate&gt;\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#tcproute","title":"TCPRoute","text":"<p>The TCPRoute will match this listener.</p> <p>If using external-secrets operator with gateway api features enabled, we can create the dns entry</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TCPRoute\nmetadata:\n  name: myroute\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: myroute.my-domain.com\nspec:\n  parentRefs:\n    - name: mygateway\n      sectionName: mylistener\n      namespace: wherethegatewayis\n  rules:\n    - backendRefs:\n        - name: myservice\n          port: 5432\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#targetgroupconfiguration","title":"TargetGroupConfiguration","text":"<p>Finally the TargetGroupConfiguration uses ip as targetType and specifies the connection as TLS</p> <pre><code>apiVersion: gateway.k8s.aws/v1beta1\nkind: TargetGroupConfiguration\nmetadata:\n  name: myroute\nspec:\n  targetReference:\n    name: myservice\n  defaultConfiguration:\n    targetType: ip\n    protocol: TLS\n</code></pre>"},{"location":"networking/gateway-api/aws%20lbc/tcproute-acm/#links","title":"Links","text":""},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/","title":"Backend routing to self signed","text":""},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#situation","title":"Situation","text":"<ul> <li>The argocd server is served with self signed certificate</li> <li>We have a pod that serves a process with a self signed certificate</li> <li>The CA of that self signed certificate is not under any kubernetes secret</li> <li>HTTPRoute</li> </ul> <p>We have an HTTPRoute using the 443 or 80 port as backend</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: argocd\nspec:\n  parentRefs:\n    - name: mygateway\n  hostnames:\n    - whatever.fqdn.com\n  rules:\n    - name: argocd-server\n      backendRefs:\n        - name: argocd-server\n          port: 443\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n</code></pre> <p>One example is the argocd server. By default uses secure mode, this is the argocd-server service has 80 and 443 ports configured with 8080 as target port in the container. And this 8080 port serves https with a self signed certificate. The ca is not written in a secret or configmap.</p>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#problem","title":"Problem","text":"<p>Accessing  https://whatever.fqdn.com gives ERR_TOO_MANY_REDIRECTS error</p> <pre><code>kubectl port-forward svc/argocd-server 9443:443 -n argocd\ncurl http://127.0.0.1:9443 # gives Temporary Redirect\ncurl -L http://127.0.0.1:9443  # works\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#reason","title":"Reason","text":"<p>The ERR_TOO_MANY_REDIRECTS error occurs because of a redirect loop between the Gateway and ArgoCD. Here's what's happening:</p> <ul> <li>Gateway receives HTTPS request from client</li> <li>Gateway forwards HTTP request to ArgoCD backend (port 443, but without TLS)</li> <li>ArgoCD receives plain HTTP but expects/serves HTTPS</li> <li>ArgoCD redirects to HTTPS (back to the same URL)</li> <li>Loop repeats infinitely</li> </ul> <p>The core issue is that the Gateway is sending an HTTP request to ArgoCD's HTTPS port 443, but ArgoCD is configured to serve HTTPS on that port. ArgoCD sees the plain HTTP request and redirects it to HTTPS, creating the loop.</p>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#other-solutions","title":"Other solutions","text":"<ul> <li>Enable the insecure mode in argocd (less secure)</li> <li>Extract the ca, write it in a configmap/secret and use it in the BackendTLSPolicy (too much work)</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#solution-backend-and-skip-tls-verification","title":"Solution: Backend and Skip TLS Verification","text":"<ul> <li>We will tell the gateway it must communicate with the destination using tls via a BackendTLSPolicy</li> <li>The destination won't be a kubernetes service. We will use a Backend resource, an envoy gateway related CRD</li> </ul> <p>This is not recommended for production because of man in the middle attacks. Also it requires enabling the backend api.</p>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#enable-the-backend-api","title":"Enable the backend api","text":"<p>Enable at controller level. For example, via values,yaml</p> <pre><code>config:\n  envoyGateway:\n    extensionApis:\n      enableBackend: true\n</code></pre> <p>Create a backend</p> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: argocd-backend\nspec:\n  endpoints:\n    - fqdn:\n        hostname: argocd-server.argocd.svc.cluster.local\n        port: 443\n  tls:\n    insecureSkipVerify: true\n</code></pre> <p>And change the backendref</p> <pre><code>      backendRefs:\n        - name: argocd-backend\n          group: gateway.envoyproxy.io\n          kind: Backend\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/backend-selfsigned-tls/#links","title":"Links","text":"<ul> <li>TLS Configuration</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/tls/</p> <ul> <li>BackendTLSPolicy</li> </ul> <p>https://gateway-api.sigs.k8s.io/api-types/backendtlspolicy/</p> <ul> <li>Backend Routing</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/traffic/backend/</p> <ul> <li>Backend TLS: Skip TLS Verification</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/security/backend-skip-tls-verification/</p>"},{"location":"networking/gateway-api/envoy-gateway/escaped-slashes-path-normalization/","title":"Escaped slashes and path normalization","text":"<p>Envoy's default behavior for URLs containing <code>%2F</code> (URL-encoded <code>/</code>) is <code>UnescapeAndRedirect</code>. This means Envoy decodes <code>%2F</code> back to <code>/</code>, normalizes the path, and issues a 307 redirect with the decoded URL.</p> <p>This breaks applications that rely on <code>%2F</code> as a literal value in the path rather than a path separator.</p>"},{"location":"networking/gateway-api/envoy-gateway/escaped-slashes-path-normalization/#rabbitmq-management-api-example","title":"RabbitMQ Management API example","text":"<p>RabbitMQ Management API uses <code>%2F</code> to represent the default vhost <code>/</code> in URL paths:</p> <pre><code>GET /api/permissions/%2F/agent_2048\n</code></pre> <p>With the default Envoy behavior:</p> <ol> <li>Envoy receives <code>/api/permissions/%2F/agent_2048</code></li> <li>Decodes <code>%2F</code> to <code>/</code>, resulting in <code>/api/permissions//agent_2048</code></li> <li>Merges the double slash <code>//</code> into <code>/</code>, resulting in <code>/api/permissions/agent_2048</code></li> <li>Issues a 307 redirect with the normalized path</li> <li>RabbitMQ API receives the wrong path and the call fails</li> </ol>"},{"location":"networking/gateway-api/envoy-gateway/escaped-slashes-path-normalization/#fix-clienttrafficpolicy","title":"Fix: ClientTrafficPolicy","text":"<p>Apply a <code>ClientTrafficPolicy</code> targeting the Gateway listener with two settings:</p> <ul> <li><code>escapedSlashesAction: KeepUnchanged</code>: Preserves <code>%2F</code> as-is without decoding</li> <li><code>disableMergeSlashes: true</code>: Prevents <code>//</code> from being collapsed to <code>/</code></li> </ul> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: ClientTrafficPolicy\nmetadata:\n  name: https\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: eg-web\n    sectionName: https\n  path:\n    escapedSlashesAction: KeepUnchanged\n    disableMergeSlashes: true\n</code></pre> <p>The <code>targetRef.sectionName</code> field scopes the policy to a specific listener. If omitted, it applies to all listeners on the Gateway.</p>"},{"location":"networking/gateway-api/envoy-gateway/escaped-slashes-path-normalization/#other-escapedslashesaction-values","title":"Other escapedSlashesAction values","text":"Value Behavior <code>UnescapeAndRedirect</code> Decode <code>%2F</code> and 307 redirect (default) <code>UnescapeAndForward</code> Decode <code>%2F</code> and forward without redirect <code>KeepUnchanged</code> Preserve <code>%2F</code> as-is <code>RejectRequest</code> Return 400 Bad Request"},{"location":"networking/gateway-api/envoy-gateway/overlapping-tls-alpn/","title":"OverlappingTLSConfig and ALPN fallback to HTTP/1.1","text":"<p>When two listeners on the same Gateway share a TLS certificate with overlapping SANs, Envoy Gateway raises an <code>OverlappingTLSConfig</code> condition with reason <code>OverlappingCertificates</code>.</p> <p>A common scenario: a wildcard listener (<code>*.example.com</code>) and a bare domain listener (<code>example.com</code>) both reference the same certificate. Since cert-manager includes both <code>*.example.com</code> and <code>example.com</code> as SANs in a single cert (because the wildcard does not cover the bare domain per RFC 6125), the overlap is triggered.</p>"},{"location":"networking/gateway-api/envoy-gateway/overlapping-tls-alpn/#why-it-matters-http2-connection-coalescing","title":"Why it matters: HTTP/2 connection coalescing","text":"<p>HTTP/2 allows clients to reuse a single TCP connection for multiple hostnames if the TLS certificate covers both (connection coalescing). This becomes a problem with multiple listeners:</p> <ol> <li>A client connects to <code>app.example.com</code> (matched by the wildcard listener)</li> <li>The cert also covers <code>example.com</code> (matched by the bare domain listener)</li> <li>The client reuses that connection for <code>example.com</code> requests</li> <li>The proxy cannot distinguish which listener should handle the request    since both arrive on the same connection</li> </ol> <p>To prevent this, Envoy Gateway disables HTTP/2 advertisement via ALPN. Without <code>h2</code> in the ALPN negotiation, clients fall back to HTTP/1.1 which does not support connection coalescing. Each hostname gets its own connection and listener matching works correctly.</p>"},{"location":"networking/gateway-api/envoy-gateway/overlapping-tls-alpn/#impact","title":"Impact","text":"<ul> <li>All traffic works, but over HTTP/1.1 (no multiplexing, no header compression)</li> <li>For internal tooling and admin UIs this is usually negligible</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/overlapping-tls-alpn/#solutions-if-http2-is-needed","title":"Solutions if HTTP/2 is needed","text":"<ul> <li>Separate certificates: Use a non-wildcard cert for the bare domain   listener so SANs do not overlap</li> <li>Explicit ALPN via policy: Configure a   <code>ClientTrafficPolicy</code> to force <code>h2</code> advertisement, accepting the   coalescing risk</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/overlapping-tls-alpn/#detecting-the-condition","title":"Detecting the condition","text":"<pre><code>kubectl get gateway &lt;name&gt; -n &lt;namespace&gt; -o json | \\\n  jq '.status.listeners[].conditions[] | select(.type == \"OverlappingTLSConfig\")'\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/","title":"Envoy Gateway Custom Resource Definitions (CRDs)","text":"<p>Envoy Gateway extends Kubernetes with custom resources to configure and manage the data plane. This section documents the key CRDs available.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/#policy-crds","title":"Policy CRDs","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/#backendtrafficpolicy","title":"BackendTrafficPolicy","text":"<p>Configures traffic policies for backend connections, including:</p> <ul> <li>Load balancing strategies</li> <li>Connection limits and timeouts</li> <li>Circuit breaking</li> <li>Health checks</li> <li>Retry and timeout policies</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#clienttrafficpolicy","title":"ClientTrafficPolicy","text":"<p>Defines policies for client-facing traffic, such as:</p> <ul> <li>TCP/HTTP keep-alive settings</li> <li>Connection timeouts</li> <li>Client IP detection (X-Forwarded-For)</li> <li>HTTP/2 and HTTP/3 configuration</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#securitypolicy","title":"SecurityPolicy","text":"<p>Implements security controls for routes:</p> <ul> <li>CORS (Cross-Origin Resource Sharing)</li> <li>JWT authentication</li> <li>OIDC authentication</li> <li>Basic authentication</li> <li>Rate limiting</li> <li>Authorization policies</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#envoyextensionpolicy","title":"EnvoyExtensionPolicy","text":"<p>Extends Envoy functionality with external processing:</p> <ul> <li>External authentication services</li> <li>External authorization</li> <li>WASM extensions</li> <li>Custom Envoy filters</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#routing-and-filtering","title":"Routing and Filtering","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/#httproutefilter","title":"HTTPRouteFilter","text":"<p>Provides reusable HTTP filters for Gateway API HTTPRoutes:</p> <ul> <li>Request/response header modification</li> <li>URL rewriting</li> <li>Request mirroring</li> <li>Request redirects</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#backend","title":"Backend","text":"<p>Defines backend service references with additional capabilities:</p> <ul> <li>Backend references for multiple protocols</li> <li>Service discovery integration</li> <li>Fallback configurations</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#backend-security-risks","title":"Backend Security Risks","text":"<p>Security analysis of the Backend CRD:</p> <ul> <li>Risk assessment per feature (ReferenceGrant bypass, DynamicResolver, localhost exposure)</li> <li>Mitigation strategies (RBAC, policy engines, network policies)</li> <li>When to use and when to avoid Backend resources</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#infrastructure-configuration","title":"Infrastructure Configuration","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/#envoyproxy","title":"EnvoyProxy","text":"<p>Configures the Envoy Proxy infrastructure:</p> <ul> <li>Deployment strategy and replicas</li> <li>Pod template specifications</li> <li>Service configuration</li> <li>Bootstrap configuration</li> <li>Telemetry and logging</li> <li>Shutdown configuration</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#envoypatchpolicy","title":"EnvoyPatchPolicy","text":"<p>Low-level Envoy configuration patching:</p> <ul> <li>Direct xDS resource modification</li> <li>JSON patches for fine-grained control</li> <li>Advanced customization beyond standard CRDs</li> <li>Emergency configuration overrides</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#policy-attachment-model","title":"Policy Attachment Model","text":"<p>Most Envoy Gateway policies use the Gateway API Policy Attachment model:</p> <ul> <li>Gateway-level: Policies attached to Gateway resources affect all routes</li> <li>HTTPRoute-level: Policies attached to HTTPRoute resources affect specific routes</li> <li>Inheritance: Route-level policies override Gateway-level policies</li> <li>Conflict resolution: Most specific policy wins</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/#common-fields","title":"Common Fields","text":"<p>All policy CRDs share common patterns:</p> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: &lt;PolicyKind&gt;\nmetadata:\n  name: example-policy\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway  # or HTTPRoute\n    name: example-gateway\n  # Policy-specific configuration\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/#version-compatibility","title":"Version Compatibility","text":"<p>CRDs are versioned independently from Envoy Gateway releases. Check the API reference for compatibility:</p> <ul> <li><code>v1alpha1</code> - Experimental features, subject to breaking changes</li> <li><code>v1beta1</code> - Stable features, minimal breaking changes expected</li> <li><code>v1</code> - Production-ready, backward compatibility guaranteed</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/","title":"Backend Security Risks","text":"<p>The Backend CRD extends Gateway API routing beyond standard Kubernetes Services. While this flexibility is useful, it introduces a significant attack surface that must be understood and mitigated. Envoy Gateway ships with Backend support disabled by default for this reason.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#overview","title":"Overview","text":"<ul> <li>CRD: <code>Backend</code> (<code>gateway.envoyproxy.io/v1alpha1</code>)</li> <li>Risk category: Routing boundary bypass</li> <li>Default state: Disabled -- must be explicitly enabled in the <code>EnvoyGateway</code> configuration</li> <li>Core concern: A Backend resource can route traffic to destinations that would otherwise be unreachable or restricted</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#why-backend-is-disabled-by-default","title":"Why Backend Is Disabled by Default","text":"<p>The Backend API can be misused to allow traffic to be sent to otherwise restricted destinations. Unlike standard Kubernetes Service references, which are bound by namespace boundaries and <code>ReferenceGrant</code> controls, a Backend resource can target arbitrary FQDNs, IPs, and Unix domain sockets.</p> <p>Enabling it requires an explicit opt-in via the <code>extensionApis.enableBackend</code> field in the <code>EnvoyGateway</code> configuration.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#risk-assessment","title":"Risk Assessment","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#referencegrant-bypass","title":"ReferenceGrant Bypass","text":"<p>Gateway API uses <code>ReferenceGrant</code> to control cross-namespace references. This is a fundamental security boundary -- a route in namespace <code>A</code> cannot reference a Service in namespace <code>B</code> without an explicit grant.</p> <p>Backend resources circumvent this model. A Backend with an FQDN or IP endpoint does not require a <code>ReferenceGrant</code>, because the target is not a namespaced Kubernetes resource. This effectively breaks the trust model that Gateway API provides.</p> <p>Severity: Medium</p> <p>Impact: Routes can reach services in namespaces they were never authorized to access, without the target namespace owner's consent.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#envoy-proxy-localhost-exposure","title":"Envoy Proxy Localhost Exposure","text":"<p>A Backend resource pointing to <code>127.0.0.1</code> or the Envoy proxy's own address can expose internal management interfaces, most critically the Envoy admin endpoint (typically on port <code>19000</code>).</p> <p>The admin endpoint exposes full configuration dumps, cluster and listener statistics, runtime modification capabilities, server shutdown, heap profiling, and logging level changes.</p> <p>Severity: High</p> <p>Impact: Configuration leak, runtime manipulation, potential denial of service via the admin interface.</p> <p>Built-in mitigation: Loopback addresses (<code>127.0.0.1</code>, <code>::1</code>) and <code>localhost</code> are explicitly forbidden in Backend endpoints. However, this guardrail may not cover all edge cases (e.g., a pod IP that resolves to the proxy itself).</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#open-proxy-via-dynamicresolver","title":"Open Proxy via DynamicResolver","text":"<p>The <code>DynamicResolver</code> Backend type allows Envoy to act as a forward proxy, dynamically resolving hostnames at request time without prior configuration. This effectively makes Envoy an open proxy that can route traffic to any destination reachable from the cluster network.</p> <p>Severity: High</p> <p>Impact: All network-reachable endpoints become accessible through the gateway. This includes internal services, cloud metadata endpoints (e.g., <code>169.254.169.254</code>), and external systems. It disables all routing controls that the Gateway API was designed to enforce.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#internal-service-exposure","title":"Internal Service Exposure","text":"<p>A Backend with a static IP or FQDN can expose cluster-internal services that were never meant to receive external traffic. Any user with permission to create a Backend resource can target sensitive internal infrastructure such as <code>kube-dns</code>, the Kubernetes API server, databases, or monitoring systems.</p> <p>Severity: Medium to High (depends on network policies in place)</p> <p>Impact: Unauthorized access to internal services, cloud provider metadata endpoints, or other sensitive infrastructure.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#unix-domain-socket-risks","title":"Unix Domain Socket Risks","text":"<p>Backend resources can target Unix domain sockets on the Envoy proxy pod filesystem. Envoy Gateway does not manage the lifecycle of Unix sockets -- administrators must manually create and mount them into the Envoy proxy pods. A misconfigured socket path could expose unintended local services.</p> <p>Severity: Low to Medium</p> <p>Impact: Depends on what is listening on the socket. Could expose sidecar processes, local agents, or other sensitive endpoints.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#risk-summary","title":"Risk Summary","text":"Risk Severity Mitigated By ReferenceGrant bypass Medium RBAC on Backend CRD Envoy admin exposure High Built-in loopback prohibition Open proxy (DynamicResolver) High RBAC, avoid DynamicResolver Internal service exposure Medium-High RBAC, network policies Unix socket exposure Low-Medium Pod security, RBAC"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#mitigation-strategies","title":"Mitigation Strategies","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#restrict-access-with-kubernetes-rbac","title":"Restrict Access with Kubernetes RBAC","text":"<p>This is the primary and most important control. Only cluster administrators should be able to create, modify, or delete Backend resources. Create a dedicated <code>ClusterRole</code> that grants Backend management and bind it exclusively to trusted platform admin groups.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#enforce-with-policy-engines","title":"Enforce with Policy Engines","text":"<p>Use Kyverno or OPA/Gatekeeper to enforce additional constraints on Backend resources, such as denying endpoints that target internal CIDRs (<code>10.0.0.0/8</code>, <code>172.16.0.0/12</code>, <code>192.168.0.0/16</code>, <code>169.254.0.0/16</code>) or blocking <code>DynamicResolver</code> backends entirely.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#apply-network-policies","title":"Apply Network Policies","text":"<p>Network policies on the Envoy proxy pods limit the blast radius even if a Backend resource is misconfigured. Restrict egress from the gateway namespace to only the CIDRs and ports that are legitimately needed.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#audit-backend-resources-regularly","title":"Audit Backend Resources Regularly","text":"<p>Monitor for Backend resource creation and modification. Periodically list all Backend resources across namespaces and inspect them for <code>DynamicResolver</code> usage or IP-based endpoints targeting internal ranges.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#when-to-use-backend-despite-the-risks","title":"When to Use Backend Despite the Risks","text":"<p>Backend resources are appropriate when:</p> <ol> <li>External FQDN routing is needed and the target is a well-known, controlled external service</li> <li>Legacy IP-based services exist that cannot be represented as Kubernetes Services</li> <li>Unix domain sockets are required for sidecar communication patterns</li> </ol> <p>In all cases, pair with RBAC restrictions and network policies.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#when-to-avoid-backend","title":"When to Avoid Backend","text":"<ul> <li>Do not use Backend when a standard Kubernetes Service reference would work</li> <li>Do not enable <code>DynamicResolver</code> unless you have a specific forward-proxy use case with strong compensating controls</li> <li>Do not use Backend to work around <code>ReferenceGrant</code> requirements -- fix the grants instead</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend-risks/#related-resources","title":"Related Resources","text":"<ul> <li>Backend - Backend CRD functional documentation</li> <li>SecurityPolicy - Gateway security controls</li> <li>Envoy Gateway Backend Documentation - Official documentation</li> <li>Gateway API Security Model - ReferenceGrant and trust boundaries</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/","title":"Backend","text":"<p>Backend extends Gateway API backend references with additional endpoint types beyond standard Kubernetes Services.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>Backend</code></li> <li>Purpose: Extended backend reference supporting FQDN, IP, and Unix socket endpoints</li> <li>Usage: Referenced from HTTPRoute, GRPCRoute, or other route types via <code>extensionRef</code></li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#key-features","title":"Key Features","text":"<ul> <li>FQDN-based backends (external services)</li> <li>IP-based backends (legacy systems)</li> <li>Unix domain socket endpoints</li> <li>Fallback backend configuration</li> <li>AppProtocol selection (<code>http</code>, <code>https</code>, <code>h2c</code>, <code>h2</code>, <code>grpc</code>, <code>grpcs</code>, <code>tcp</code>, <code>udp</code>)</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: external-api\nspec:\n  endpoints:\n    - fqdn: api.example.com\n      port: 443\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: my-route\nspec:\n  parentRefs:\n    - name: my-gateway\n  rules:\n    - backendRefs:\n        - group: gateway.envoyproxy.io\n          kind: Backend\n          name: external-api\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#endpoint-types","title":"Endpoint Types","text":"Type Field Use case FQDN <code>fqdn</code> + <code>port</code> External services, cloud APIs IP <code>ip</code> + <code>port</code> Legacy systems without DNS Unix socket <code>unix</code> Sidecar communication"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#fallback-configuration","title":"Fallback Configuration","text":"<pre><code>spec:\n  endpoints:\n    - fqdn: primary.example.com\n      port: 443\n  fallback:\n    backendRef:\n      name: fallback-service\n      port: 8080\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#use-backend-when","title":"Use Backend When","text":"<ol> <li>Routing to services outside the cluster (FQDN or IP)</li> <li>Fallback behavior is needed between endpoints</li> <li>Unix domain socket communication with a sidecar</li> <li>Mixed protocols not expressible via standard Service references</li> </ol> <p>Use a standard Kubernetes Service reference for all in-cluster workloads.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#security-note","title":"Security Note","text":"<p>Backend support is disabled by default in Envoy Gateway. Enable it explicitly via <code>extensionApis.enableBackend</code>. See Backend Security Risks for a full risk assessment before enabling.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#official-documentation","title":"Official Documentation","text":"<ul> <li>Backend Routing</li> <li>Failover</li> <li>Routing outside Kubernetes</li> <li>Multicluster Service Routing</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backend/#related-resources","title":"Related Resources","text":"<ul> <li>BackendTrafficPolicy - Backend traffic configuration</li> <li>HTTPRoute - Gateway API routing</li> <li>SecurityPolicy - TLS and security settings</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/","title":"BackendTrafficPolicy","text":"<p>BackendTrafficPolicy configures how Envoy Gateway handles traffic to backend services, including load balancing, timeouts, circuit breaking, health checks, and retry policies.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>BackendTrafficPolicy</code></li> <li>Attachment: Gateway or HTTPRoute via <code>targetRef</code></li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#key-features","title":"Key Features","text":"<ul> <li>Load balancing algorithms (RoundRobin, LeastRequest, Random, Maglev, ConsistentHash)</li> <li>Circuit breaking (connection/request/retry limits)</li> <li>Active and passive health checks</li> <li>Retry policies with backoff</li> <li>Connection and request timeouts</li> <li>TCP keep-alive settings</li> <li>HTTP/2 configuration</li> <li>Proxy protocol</li> <li>DNS refresh settings</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: backend-policy\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: my-route\n  loadBalancer:\n    type: LeastRequest\n  timeout:\n    http:\n      requestTimeout: 30s\n  circuitBreaker:\n    maxConnections: 1024\n    maxRequests: 1024\n  retry:\n    numRetries: 3\n    retryOn:\n      triggers: [5xx, reset, connect-failure]\n    perRetryTimeout: 5s\n  healthCheck:\n    active:\n      interval: 5s\n      timeout: 2s\n      unhealthyThreshold: 2\n      http:\n        path: /health\n        expectedStatuses: [200]\n    passive:\n      consecutive5xxErrors: 5\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#key-configuration-sections","title":"Key Configuration Sections","text":"Section Purpose <code>loadBalancer</code> Algorithm selection; ConsistentHash supports SourceIP, Header, Cookie <code>circuitBreaker</code> Max connections, pending requests, retries, and connection pools <code>healthCheck.active</code> Periodic HTTP/TCP probe with thresholds <code>healthCheck.passive</code> Outlier detection based on consecutive errors <code>retry</code> Retry triggers, count, per-retry timeout, and backoff interval <code>timeout.tcp.connectTimeout</code> Max time to establish upstream connection <code>timeout.http.requestTimeout</code> Max time for a complete request/response cycle <code>tcpKeepalive</code> Keep-alive probes, idle time, and interval <code>proxyProtocol</code> PROXY protocol version V1 or V2 for upstream connections <code>dns</code> DNS refresh rate and TTL respect"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#policy-precedence","title":"Policy Precedence","text":"<p>HTTPRoute-level policies override Gateway-level policies. The most specific <code>targetRef</code> wins.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#official-documentation","title":"Official Documentation","text":"<ul> <li>Circuit Breakers</li> <li>Load Balancing</li> <li>HTTP Timeouts</li> <li>Retry</li> <li>Connection Limit</li> <li>Local Rate Limit</li> <li>Global Rate Limit</li> <li>Session Persistence</li> <li>Response Compression</li> <li>Request Buffering</li> <li>Zone Aware Routing</li> <li>Response Override</li> <li>Direct Response</li> <li>Backend TLS: Gateway to Backend</li> <li>Backend Mutual TLS: Gateway to Backend</li> <li>Backend TLS: Skip TLS Verification</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/backendtrafficpolicy/#related-resources","title":"Related Resources","text":"<ul> <li>ClientTrafficPolicy - Client-facing traffic policies</li> <li>SecurityPolicy - Security controls</li> <li>EnvoyProxy - Proxy infrastructure configuration</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/","title":"ClientTrafficPolicy","text":"<p>ClientTrafficPolicy configures how Envoy Gateway handles incoming client connections, including TLS, protocol settings, client IP detection, and connection management.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>ClientTrafficPolicy</code></li> <li>Attachment: Gateway via <code>targetRef</code></li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#key-features","title":"Key Features","text":"<ul> <li>TCP keep-alive configuration</li> <li>Connection limits and buffer sizes</li> <li>Request and idle timeouts</li> <li>HTTP/1.1, HTTP/2, and HTTP/3 (QUIC) settings</li> <li>Client IP detection via X-Forwarded-For or custom header</li> <li>TLS minimum/maximum version and cipher suites</li> <li>Client certificate validation (mTLS)</li> <li>Path normalization</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: ClientTrafficPolicy\nmetadata:\n  name: client-policy\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: my-gateway\n  tcpKeepalive:\n    probes: 3\n    idleTime: 300s\n    interval: 60s\n  timeout:\n    http:\n      requestReceivedTimeout: 30s\n      idleTimeout: 300s\n  tls:\n    minVersion: \"1.2\"\n  clientIPDetection:\n    xForwardedFor:\n      numTrustedHops: 1\n  connection:\n    connectionLimit:\n      value: 10000\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#key-configuration-sections","title":"Key Configuration Sections","text":"Section Purpose <code>tcpKeepalive</code> Keep-alive probes, idle time, and probe interval <code>timeout.http</code> <code>requestReceivedTimeout</code>, <code>requestTimeout</code>, <code>idleTimeout</code>, <code>maxConnectionDuration</code> <code>connection.connectionLimit</code> Max concurrent connections with optional close delay <code>connection.bufferLimit</code> Per-connection buffer size <code>http1</code> Trailers, header case preservation, HTTP/1.0 handling <code>http2</code> Stream/connection window sizes, max concurrent streams <code>http3</code> Enable QUIC/HTTP3 on the listener <code>tls.minVersion</code> / <code>tls.maxVersion</code> Accepted TLS versions (<code>1.2</code>, <code>1.3</code>) <code>tls.ciphers</code> Allowed cipher suites list <code>tls.clientValidation</code> CA refs and optional flag for mTLS <code>clientIPDetection.xForwardedFor</code> Number of trusted proxy hops <code>clientIPDetection.customHeader</code> Use a custom header for client IP <code>path</code> Slash merging and escaped slash handling <code>headers.enableEnvoyHeaders</code> Add <code>X-Envoy-*</code> headers to requests"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#official-documentation","title":"Official Documentation","text":"<ul> <li>Client Traffic Policy</li> <li>HTTP3</li> <li>Connection Limit</li> <li>Secure Gateways</li> <li>Mutual TLS: External Clients to the Gateway</li> <li>TLS Passthrough</li> <li>TLS Termination for TCP</li> <li>Using cert-manager For TLS Termination</li> <li>Accelerating TLS Handshakes using Private Key Provider</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/clienttrafficpolicy/#related-resources","title":"Related Resources","text":"<ul> <li>BackendTrafficPolicy - Backend traffic policies</li> <li>SecurityPolicy - Security controls</li> <li>EnvoyProxy - Proxy infrastructure configuration</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/","title":"EnvoyExtensionPolicy","text":"<p>EnvoyExtensionPolicy extends Envoy Gateway functionality through external processing, WASM plugins, Lua scripts, and external authorization services.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>EnvoyExtensionPolicy</code></li> <li>Attachment: Gateway or HTTPRoute via <code>targetRef</code></li> <li>Purpose: Add custom processing logic to the request/response path</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#key-features","title":"Key Features","text":"<ul> <li>External authorization (ExtAuth) over HTTP or gRPC</li> <li>External Processing (ExtProc) for full request/response manipulation</li> <li>WASM extensions (HTTP URL, OCI image, or ConfigMap source)</li> <li>Lua script extensions</li> <li>Policy-based extension configuration per route or gateway</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#extauth-example","title":"ExtAuth Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyExtensionPolicy\nmetadata:\n  name: ext-auth-policy\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: my-route\n  extAuth:\n    http:\n      backendRef:\n        name: auth-service\n        port: 9000\n      path: /verify\n      headersToBackend: [Authorization, Cookie]\n      headersToDownstream: [X-Auth-User, X-Auth-Groups]\n      failOpen: false\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#wasm-example","title":"WASM Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyExtensionPolicy\nmetadata:\n  name: wasm-plugin\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: my-gateway\n  wasm:\n    - name: custom-filter\n      code:\n        image:\n          url: oci://ghcr.io/myorg/wasm-filter:v1.0.0\n          sha256: abcdef1234567890...\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#key-configuration-sections","title":"Key Configuration Sections","text":"Section Purpose <code>extAuth.http</code> HTTP-based external auth service <code>extAuth.grpc</code> gRPC-based external auth service <code>extAuth.*.failOpen</code> Allow traffic if auth service is unavailable <code>extProc</code> Full request/response external processing via gRPC <code>extProc.processingMode</code> Controls which parts (headers/body) are sent to the processor <code>wasm[].code.http</code> Load WASM from an HTTP URL with SHA256 verification <code>wasm[].code.image</code> Load WASM from an OCI registry <code>wasm[].code.configMapRef</code> Load WASM from a ConfigMap <code>wasm[].config</code> Arbitrary plugin configuration passed to the WASM filter"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#policy-precedence","title":"Policy Precedence","text":"<p>HTTPRoute-level policies override Gateway-level policies.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#official-documentation","title":"Official Documentation","text":"<ul> <li>External Processing</li> <li>Wasm Extensions</li> <li>Build a Wasm Image</li> <li>Lua Extensions</li> <li>OPA Sidecar with Unix Domain Socket</li> <li>External Authorization</li> <li>Envoy Gateway Extension Server</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyextensionpolicy/#related-resources","title":"Related Resources","text":"<ul> <li>SecurityPolicy - Built-in security features</li> <li>BackendTrafficPolicy - Backend configuration</li> <li>EnvoyProxy - Proxy infrastructure</li> <li>Envoy ExtAuth Filter - Envoy documentation</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/","title":"EnvoyPatchPolicy","text":"<p>EnvoyPatchPolicy provides low-level control over Envoy's xDS configuration via JSON patches (RFC 6902). It is an escape hatch for cases where standard CRDs cannot express the required configuration.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>EnvoyPatchPolicy</code></li> <li>Attachment: Gateway via <code>targetRef</code></li> <li>Use with caution: Direct xDS patches may break across Envoy Gateway versions</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#key-features","title":"Key Features","text":"<ul> <li>JSON Patch operations: <code>add</code>, <code>remove</code>, <code>replace</code>, <code>copy</code>, <code>move</code>, <code>test</code></li> <li>Targets any xDS resource: <code>Listener</code>, <code>RouteConfiguration</code>, <code>Cluster</code>, <code>ClusterLoadAssignment</code>, <code>Secret</code></li> <li>Multiple patches per policy</li> <li>Resource selection by name</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyPatchPolicy\nmetadata:\n  name: custom-patch\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: my-gateway\n  type: JSONPatch\n  jsonPatches:\n    - type: Cluster\n      name: default/backend-service/http\n      operation:\n        op: replace\n        path: /connect_timeout\n        value: \"30s\"\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#resource-naming-conventions","title":"Resource Naming Conventions","text":"Resource Name format Listener <code>&lt;namespace&gt;/&lt;gateway-name&gt;/&lt;listener-name&gt;</code> RouteConfiguration <code>&lt;namespace&gt;/&lt;httproute-name&gt;</code> Cluster <code>&lt;namespace&gt;/&lt;service-name&gt;/&lt;port-name-or-number&gt;</code> <p>Use the Envoy admin API to discover current resource names:</p> <pre><code>kubectl port-forward -n envoy-gateway-system deploy/envoy-gateway 19000:19000\ncurl -s localhost:19000/config_dump | jq '.configs[] | select(.[\"@type\"] | contains(\"Listener\"))'\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#when-to-use","title":"When to Use","text":"<p>Prefer standard CRDs for all configuration. Use <code>EnvoyPatchPolicy</code> only when:</p> <ul> <li>A required Envoy feature has no equivalent CRD yet</li> <li>An emergency workaround is needed before an upstream fix</li> <li>Testing experimental Envoy capabilities</li> </ul> <p>Always document why a patch is needed and plan to remove it once native CRD support is available.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#official-documentation","title":"Official Documentation","text":"<ul> <li>Envoy Patch Policy</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoypatchpolicy/#related-resources","title":"Related Resources","text":"<ul> <li>EnvoyProxy - Proxy infrastructure configuration</li> <li>BackendTrafficPolicy - Backend traffic policies</li> <li>ClientTrafficPolicy - Client traffic policies</li> <li>Envoy Documentation - xDS API reference</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/","title":"EnvoyProxy","text":"<p>EnvoyProxy configures the Envoy Proxy data plane infrastructure: deployment strategy, resource allocation, networking, observability, and bootstrap configuration.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>EnvoyProxy</code></li> <li>Purpose: Configure Envoy proxy deployment and runtime behaviour</li> <li>Attachment: Referenced from a <code>Gateway</code> via <code>infrastructure.parametersRef</code></li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#key-features","title":"Key Features","text":"<ul> <li>Deployment strategy (Deployment or DaemonSet) with replicas and rolling update settings</li> <li>Resource requests/limits and node scheduling (affinity, tolerations, topology spread)</li> <li>Service type: <code>LoadBalancer</code>, <code>NodePort</code>, or <code>ClusterIP</code></li> <li>Metrics export (Prometheus, OpenTelemetry)</li> <li>Structured access logging (File or OpenTelemetry sink)</li> <li>Distributed tracing (OpenTelemetry)</li> <li>Custom or merged bootstrap configuration</li> <li>Graceful shutdown drain timeout</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyProxy\nmetadata:\n  name: custom-proxy\n  namespace: envoy-gateway-system\nspec:\n  provider:\n    type: Kubernetes\n    kubernetes:\n      envoyDeployment:\n        replicas: 3\n        pod:\n          resources:\n            requests:\n              cpu: 500m\n              memory: 512Mi\n            limits:\n              cpu: 1000m\n              memory: 1Gi\n      envoyService:\n        type: LoadBalancer\n  telemetry:\n    metrics:\n      prometheus:\n        disable: false\n  shutdown:\n    drainTimeout: 30s\n    minDrainDuration: 5s\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#attaching-to-a-gateway","title":"Attaching to a Gateway","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\nspec:\n  gatewayClassName: envoy\n  infrastructure:\n    parametersRef:\n      group: gateway.envoyproxy.io\n      kind: EnvoyProxy\n      name: custom-proxy\n      namespace: envoy-gateway-system\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 80\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#key-configuration-sections","title":"Key Configuration Sections","text":"Section Purpose <code>provider.kubernetes.envoyDeployment</code> Replicas, rolling update strategy, pod template <code>provider.kubernetes.envoyDaemonSet</code> DaemonSet mode pod template <code>provider.kubernetes.envoyService</code> Service type and annotations (e.g. cloud LB annotations) <code>telemetry.metrics</code> Prometheus scrape endpoint and OTel metric sinks <code>telemetry.accessLog</code> Log format (Text/JSON) and sinks (File, OTel) <code>telemetry.tracing</code> OTel tracing provider and sampling rate <code>bootstrap.type</code> <code>Replace</code> or <code>Merge</code> for custom Envoy bootstrap YAML <code>shutdown</code> <code>drainTimeout</code> and <code>minDrainDuration</code> for hitless restarts <code>logging.level</code> Per-component log level (<code>default</code>, <code>admin</code>, <code>connection</code>, ...)"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#official-documentation","title":"Official Documentation","text":"<ul> <li>Customize EnvoyProxy</li> <li>Deployment Mode</li> <li>Graceful Shutdown and Hitless Upgrades</li> <li>Gateway Namespace Mode</li> <li>Standalone Deployment Mode</li> <li>Gateway Address</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/envoyproxy/#related-resources","title":"Related Resources","text":"<ul> <li>ClientTrafficPolicy - Client-facing policies</li> <li>BackendTrafficPolicy - Backend policies</li> <li>EnvoyPatchPolicy - Low-level configuration</li> <li>Envoy Documentation - Envoy Proxy docs</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/","title":"HTTPRouteFilter","text":"<p>HTTPRouteFilter provides reusable HTTP request and response manipulation filters that can be referenced from Gateway API HTTPRoute resources via <code>extensionRef</code>.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>HTTPRouteFilter</code></li> <li>Usage: Referenced from HTTPRoute via <code>filters[].type: ExtensionRef</code></li> <li>Benefit: Define header/URL transformations once, reuse across multiple routes</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#key-features","title":"Key Features","text":"<ul> <li>Add, set, or remove request headers</li> <li>Add, set, or remove response headers</li> <li>URL path rewriting (prefix replace or full replace)</li> <li>Hostname rewriting</li> <li>HTTP redirects (scheme, hostname, path, port, status code)</li> <li>Request mirroring to a shadow backend</li> <li>Dynamic header values via Envoy command operators (e.g. <code>%REQ(:authority)%</code>)</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#basic-example","title":"Basic Example","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: HTTPRouteFilter\nmetadata:\n  name: security-headers\nspec:\n  responseHeaderModifier:\n    add:\n      - name: Strict-Transport-Security\n        value: max-age=31536000; includeSubDomains\n      - name: X-Content-Type-Options\n        value: nosniff\n    remove:\n      - Server\n      - X-Powered-By\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: my-route\nspec:\n  parentRefs:\n    - name: my-gateway\n  rules:\n    - matches:\n        - path:\n            type: PathPrefix\n            value: /api\n      filters:\n        - type: ExtensionRef\n          extensionRef:\n            group: gateway.envoyproxy.io\n            kind: HTTPRouteFilter\n            name: security-headers\n      backendRefs:\n        - name: backend-service\n          port: 8080\n</code></pre>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#key-configuration-sections","title":"Key Configuration Sections","text":"Section Purpose <code>requestHeaderModifier.add</code> Append headers to the upstream request <code>requestHeaderModifier.set</code> Overwrite headers on the upstream request <code>requestHeaderModifier.remove</code> Remove headers from the upstream request <code>responseHeaderModifier.add</code> Append headers to the downstream response <code>responseHeaderModifier.set</code> Overwrite headers on the downstream response <code>responseHeaderModifier.remove</code> Remove headers from the downstream response <code>urlRewrite.path</code> Replace path prefix or full path before forwarding <code>urlRewrite.hostname</code> Rewrite the Host header before forwarding <code>requestRedirect</code> Return a redirect response (scheme, hostname, path, port, statusCode) <code>requestMirror.backendRef</code> Shadow-copy requests to a secondary backend"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#official-documentation","title":"Official Documentation","text":"<ul> <li>HTTP Request Headers</li> <li>HTTP Response Headers</li> <li>HTTP Redirects</li> <li>HTTP URL Rewrite</li> <li>HTTPRoute Request Mirroring</li> <li>HTTP Routing</li> <li>HTTPRoute Traffic Splitting</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/httproutefilter/#related-resources","title":"Related Resources","text":"<ul> <li>SecurityPolicy - CORS, authentication, rate limiting</li> <li>BackendTrafficPolicy - Backend connection policies</li> <li>Gateway API HTTPRoute - Gateway API specification</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/","title":"SecurityPolicy","text":"<p>SecurityPolicy is an EnvoyGateway CRD that implements security controls at the gateway or route level. It enforces access policies, authentication, and traffic management.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#overview","title":"Overview","text":"<ul> <li>API Group: <code>gateway.envoyproxy.io/v1alpha1</code></li> <li>Kind: <code>SecurityPolicy</code></li> <li>Scope: Gateway or HTTPRoute via <code>targetRef</code></li> <li>Purpose: Enforce security policies and access controls</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#what-securitypolicy-permits","title":"What SecurityPolicy Permits","text":""},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#authentication-methods","title":"Authentication Methods","text":"<ul> <li>JWT: Validate JSON Web Tokens from configured issuers (Auth0, Okta, custom providers)</li> <li>OIDC: OpenID Connect-based authentication with providers like Google, Keycloak</li> <li>Basic Auth: Username/password authentication with htpasswd-stored credentials</li> <li>mTLS: Client certificate validation</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#authorization-controls","title":"Authorization Controls","text":"<ul> <li>Role-based access control (RBAC) using JWT claims</li> <li>Method-based rules (allow specific HTTP verbs per role)</li> <li>Deny-by-default with explicit allow rules</li> <li>Claim-to-header mapping for downstream services</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#cross-origin-resource-sharing-cors","title":"Cross-Origin Resource Sharing (CORS)","text":"<ul> <li>Restrict origins (specific hosts, wildcards, patterns)</li> <li>Control allowed HTTP methods</li> <li>Manage request/response headers</li> <li>Configure preflight caching and credentials</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Local: Per-Envoy-instance rate limiting</li> <li>Global: Distributed rate limiting via external service</li> <li>Client selection by source IP, headers, or query parameters</li> <li>Flexible time units (minute, hour, day)</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#ip-based-access-control","title":"IP-Based Access Control","text":"<ul> <li>Allowlists: Restrict to specific CIDR ranges</li> <li>Denylists: Block specific IP ranges</li> <li>Support for IPv4 and IPv6</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#attachment","title":"Attachment","text":"<p>Policies attach to either:</p> <ul> <li>Gateway (applies to all routes)</li> <li>HTTPRoute (specific route)</li> </ul> <p>HTTPRoute policies override Gateway policies.</p>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#official-documentation","title":"Official Documentation","text":"<ul> <li>JWT Authentication</li> <li>JWT Claim-Based Authorization</li> <li>OIDC Authentication</li> <li>Basic Authentication</li> <li>API Key Authentication</li> <li>CORS</li> <li>IP Allowlist/Denylist</li> <li>External Authorization</li> <li>HTTP Header and Method Based Authorization</li> <li>Credential Injection</li> <li>Local Rate Limit</li> <li>Global Rate Limit</li> </ul>"},{"location":"networking/gateway-api/envoy-gateway/crds/securitypolicy/#related-resources","title":"Related Resources","text":"<ul> <li>ClientTrafficPolicy - Connection management</li> <li>EnvoyExtensionPolicy - External services</li> <li>HTTPRouteFilter - Request/response modification</li> </ul>"},{"location":"networking/gateway-api/kgateway/gateway-classes/","title":"Gateway Classes","text":""},{"location":"networking/gateway-api/kgateway/gateway-classes/#kgateway-vs-kgateway-waypoint-gatewayclasses","title":"kgateway vs kgateway-waypoint GatewayClasses","text":""},{"location":"networking/gateway-api/kgateway/gateway-classes/#kgateway","title":"kgateway","text":"<ul> <li>Purpose: Standard class for managing Gateway API ingress traffic</li> <li>Use case: Regular ingress/gateway functionality for external traffic routing</li> <li>Description: \"Standard class for managing Gateway API ingress traffic\"</li> </ul>"},{"location":"networking/gateway-api/kgateway/gateway-classes/#kgateway-waypoint","title":"kgateway-waypoint","text":"<ul> <li>Purpose: Specialized class for Istio ambient mesh waypoint proxies</li> <li>Use case: Service mesh integration with Istio ambient mode</li> <li>Description: \"Specialized class for Istio ambient mesh waypoint proxies\"</li> <li>Special annotation: ambient.istio.io/waypoint-inbound-binding: PROXY/15088 - indicates integration with Istio ambient mesh waypoint functionality</li> </ul>"},{"location":"networking/gateway-api/kgateway/gateway-classes/#comparison","title":"Comparison","text":"<p>Common characteristics</p> <ul> <li>Both use the same controller: kgateway.dev/kgateway</li> <li>Both are managed by the same kgateway installation</li> <li>Both have Accepted and SupportedVersion conditions</li> </ul> <p>The main difference is that kgateway-waypoint is specifically designed for Istio service mesh ambient mode waypoint proxy functionality, while kgateway is for standard ingress traffic routing.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/","title":"Migrate from NGINX Ingress Controller","text":"<p>This guide provides reference documentation for migrating common NGINX Ingress Controller annotations to Gateway API.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/#available-migration-guides","title":"Available Migration Guides","text":"<ul> <li>force-ssl-redirect - Force HTTP to HTTPS redirects</li> <li>backend-protocol - Specify backend protocol (HTTP, HTTPS, gRPC)</li> <li>ssl-passthrough - TLS passthrough configuration</li> <li>rewrite-target - URL path rewriting</li> <li>proxy-body-size - Request body size limits</li> <li>external-authentication - External authentication (auth-url, auth-signin, auth-response-headers)</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/#additional-resources","title":"Additional Resources","text":"<p>See 99-links.md for additional migration resources and documentation.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/99-links/","title":"Links","text":"<ul> <li>Migrating from Ingress</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/getting-started/migrating-from-ingress/</p> <ul> <li>Nginx ingress controller annotations</li> </ul> <p>https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/</p> <ul> <li>A Welcome Guide for Ingress-NGINX Users</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/getting-started/migrating-from-ingress-nginx/</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/backend-protocol/","title":"nginx.ingress.kubernetes.io/backend-protocol","text":"<p>Purpose: Specifies the protocol used to communicate with the backend service. For example, it is a common behaviour to use HTTPS for backends using that protocol.</p> <p>Valid values: <code>HTTP</code> (default), <code>HTTPS</code>, <code>GRPC</code>, <code>GRPCS</code>, <code>FCGI</code>, <code>AUTO_HTTP</code></p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/backend-protocol/#at-service-level","title":"At service level","text":"<p>We can specify the protocol at service level with <code>spec.ports.[].appProtocol</code> in the backend kubernetes service resource. This works as a hint for be treated by different implementations, like gateway api or service mesh solutions.</p> <p>Valid values are:</p> <ul> <li>IANA standard service names (http, https, tcp, grpc, http2,...)</li> <li>kubernetes.io/h2c (HTTP/2 over cleartext)</li> <li>kubernetes.io/ws (WebSocket over cleartext)</li> <li>kubernetes.io/wss (WebSocket over TLS)</li> <li>Custom implementations</li> </ul> <p>Important: Gateway API implementations SHOULD honor the <code>appProtocol</code> field (per GEP-1911), but it's not a MUST requirement. So take care how every gateway api implementation uses or not appProtocol</p> <p>If a Route cannot use the specified protocol, implementations MUST set <code>ResolvedRefs</code> condition to <code>False</code> with reason <code>UnsupportedProtocol</code></p> <p>See this:</p> <ul> <li>GEP-1911: Backend Protocol Selection</li> </ul> <p>https://gateway-api.sigs.k8s.io/geps/gep-1911/</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/backend-protocol/#at-gateway-api-implementation-level","title":"At gateway api implementation level","text":"<p>The protocol can be specified at implementation crd level</p> <ul> <li>Envoy gateway:</li> </ul> <p>It provides spec.appProtocols under its Backend resource (gateway.envoyproxy.io/v1alpha1)</p> <ul> <li>Kgateway</li> </ul> <p>It provides appProtocol under spec.static in its Backend* resource (gateway.kgateway.dev/v1alpha1)</p> <ul> <li>Istio</li> </ul> <p>It uses appProtocol or the port name</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/backend-protocol/#note-about-web-certificates-in-https-backends","title":"Note about web certificates in HTTPS backends","text":"<p>When the backend service uses HTTPS and we want to validate the certificates we need to use a <code>BackendTLSPolicy</code> resource</p> <p>But if we want to ignore that backends certificate we must use the implementation crd. Currently there is not a native gateway api implementation</p> <ul> <li>Envoy gateway:</li> </ul> <p>It provides spec.tls.insecureSkipVerify under its Backend resource (gateway.envoyproxy.io/v1alpha1)</p> <ul> <li>Kgateway</li> </ul> <p>It provides spec.tls.insecureSkipVerify under its BackendConfigPolicy resource (gateway.kgateway.dev/v1alpha1)</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/","title":"External Authentication","text":"<p>This guide covers how to migrate NGINX Ingress Controller external authentication annotations to Gateway API.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#nginx-ingress-annotations-overview","title":"NGINX Ingress Annotations Overview","text":"<p>NGINX Ingress Controller provides three key annotations for external authentication:</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#nginxingresskubernetesioauth-url","title":"nginx.ingress.kubernetes.io/auth-url","text":"<p>Purpose: Specifies the URL of the external authentication service that validates requests.</p> <p>How it works: Before forwarding a request to the backend, NGINX sends a subrequest to this URL. If the auth service returns 2xx, the request proceeds. If it returns 401/403, access is denied.</p> <p>Example:</p> <pre><code>nginx.ingress.kubernetes.io/auth-url: \"https://oauth2-proxy.auth-system.svc.cluster.local/oauth2/auth\"\n</code></pre> <p>Common values:</p> <ul> <li>OAuth2 Proxy: <code>http://oauth2-proxy.namespace.svc.cluster.local/oauth2/auth</code></li> <li>Authelia: <code>http://authelia.namespace.svc.cluster.local/api/verify</code></li> <li>Custom auth service: <code>http://my-auth.namespace.svc.cluster.local/verify</code></li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#nginxingresskubernetesioauth-signin","title":"nginx.ingress.kubernetes.io/auth-signin","text":"<p>Purpose: Specifies where to redirect users when authentication fails (401/403).</p> <p>How it works: When the auth service denies access, NGINX redirects the user to this URL to initiate authentication (e.g., OAuth2 login flow).</p> <p>Example:</p> <pre><code>nginx.ingress.kubernetes.io/auth-signin: \"https://oauth2-proxy.example.com/oauth2/start?rd=$escaped_request_uri\"\n</code></pre> <p>Common patterns:</p> <ul> <li>OAuth2 Proxy: <code>https://auth.example.com/oauth2/start?rd=$escaped_request_uri</code></li> <li>Authelia: <code>https://auth.example.com/?rd=$escaped_request_uri</code></li> <li>Custom redirect: <code>https://login.example.com/login?redirect_uri=$scheme://$host$request_uri</code></li> </ul> <p>Variables available:</p> <ul> <li><code>$escaped_request_uri</code> - The original request URI (URL-encoded)</li> <li><code>$scheme</code> - http or https</li> <li><code>$host</code> - Original hostname</li> <li><code>$request_uri</code> - Full request URI</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#nginxingresskubernetesioauth-response-headers","title":"nginx.ingress.kubernetes.io/auth-response-headers","text":"<p>Purpose: Forwards specific headers from the authentication response to the backend application.</p> <p>How it works: After successful authentication, the auth service returns headers (like user email, groups, tokens). This annotation specifies which headers to forward to your backend.</p> <p>Example:</p> <pre><code>nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-User,X-Auth-Request-Email,X-Auth-Request-Access-Token\n</code></pre> <p>Common headers forwarded:</p> <ul> <li><code>X-Auth-Request-User</code> - Username or user ID</li> <li><code>X-Auth-Request-Email</code> - User email address</li> <li><code>X-Auth-Request-Groups</code> - User groups/roles</li> <li><code>X-Auth-Request-Access-Token</code> - OAuth2 access token</li> <li><code>X-Auth-Request-Preferred-Username</code> - Preferred username from OIDC</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#gateway-api-migration","title":"Gateway API Migration","text":"<p>In Gateway API, external authentication is achieved using implementation-specific policies. The exact approach varies by Gateway implementation:</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#envoy-gateway-recommended","title":"Envoy Gateway (Recommended)","text":"<p>Envoy Gateway provides native OIDC authentication through SecurityPolicy, eliminating the need for external authentication proxies.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#key-concepts","title":"Key Concepts","text":"<ul> <li>Uses SecurityPolicy resource for OIDC configuration</li> <li>Supports OIDC providers: Google, Azure AD, Keycloak, Auth0, etc.</li> <li>Can target HTTPRoute or Gateway resources</li> <li>Cookie-based session management</li> <li>Automatic token refresh</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#basic-configuration","title":"Basic Configuration","text":"<pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: SecurityPolicy\nmetadata:\n  name: oidc-auth\nspec:\n  targetRefs:\n    - kind: HTTPRoute\n      name: myapp\n  oidc:\n    provider:\n      issuer: \"https://accounts.google.com\"\n    clientID: \"your-client-id\"\n    clientSecret:\n      name: \"client-secret\"\n    redirectURL: \"https://myapp.example.com/oauth2/callback\"\n    logoutPath: \"/oauth2/logout\"\n    scopes: [openid, email, profile]\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#advanced-features","title":"Advanced Features","text":"<ul> <li>Gateway-wide auth: Target Gateway resource to protect all routes</li> <li>Cookie domains: Use <code>cookieDomain</code> for subdomain sharing</li> <li>Header forwarding: Use BackendTrafficPolicy to forward claims</li> </ul> <p>See Envoy Gateway OIDC docs for complete examples.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#istio","title":"Istio","text":"<p>Istio does not have native OIDC authentication. Instead, it focuses on JWT validation and requires external services for OIDC flows.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#approach-1-jwt-validation-recommended","title":"Approach 1: JWT Validation (Recommended)","text":"<p>Use RequestAuthentication and AuthorizationPolicy for JWT validation:</p> <pre><code>apiVersion: security.istio.io/v1\nkind: RequestAuthentication\nmetadata:\n  name: jwt-auth\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  jwtRules:\n    - issuer: \"https://accounts.google.com\"\n      jwksUri: \"https://www.googleapis.com/.well-known/jwks.json\"\n      audiences: [\"myapp.example.com\"]\n      outputClaimToHeaders:\n        - header: x-auth-request-email\n          claim: email\n---\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: require-jwt\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  action: ALLOW\n  rules:\n    - from:\n        - source:\n            requestPrincipals: [\"*\"]  # Require valid JWT\n</code></pre> <p>Features:</p> <ul> <li>Validates JWT tokens from Authorization header</li> <li>Can forward claims to backend via headers</li> <li>Supports claim-based authorization rules</li> <li>High-performance native validation</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#approach-2-full-oidc-with-oauth2-proxy","title":"Approach 2: Full OIDC with OAuth2-Proxy","text":"<p>For complete OIDC flow (login/logout), deploy OAuth2-Proxy and configure via EnvoyFilter:</p> <ul> <li>Deploy OAuth2-Proxy service</li> <li>Create EnvoyFilter with ExtAuthz configuration</li> <li>Point ExtAuthz to OAuth2-Proxy endpoint</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#approach-3-authservice-istio-ecosystem","title":"Approach 3: Authservice (Istio Ecosystem)","text":"<p>Use istio-ecosystem/authservice for OIDC within the mesh:</p> <ul> <li>Provides transparent OIDC login/logout</li> <li>Token acquisition and refresh</li> <li>Session management</li> </ul> <p>Key Points:</p> <ul> <li>No native OIDC: Requires external services</li> <li>Best for: JWT validation scenarios</li> <li>Complex setup: Multiple components needed for full OIDC</li> </ul> <p>See Istio RequestAuthentication docs for details.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#kgateway","title":"Kgateway","text":"<p>Kgateway (from Solo.io) provides OIDC authentication through ExtAuthPolicy with advanced features.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#key-concepts_1","title":"Key Concepts","text":"<ul> <li>Uses ExtAuthPolicy resource (API: <code>security.policy.gloo.solo.io/v2</code>)</li> <li>Requires ext-auth-server component (included with Kgateway)</li> <li>Supports OIDC providers: Google, Azure AD, Keycloak, Okta, Auth0</li> <li>Apply to routes via label selectors</li> <li>Session storage: Cookie-based or Redis</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#basic-oidc-configuration","title":"Basic OIDC Configuration","text":"<pre><code>apiVersion: security.policy.gloo.solo.io/v2\nkind: ExtAuthPolicy\nmetadata:\n  name: oidc-auth\nspec:\n  applyToRoutes:\n    - route:\n        labels:\n          oauth: \"true\"\n  config:\n    server:\n      name: ext-auth-server\n      namespace: gloo-system\n    glooAuth:\n      configs:\n        - oauth2:\n            oidcAuthorizationCode:\n              appUrl: \"https://myapp.example.com\"\n              callbackPath: /oauth2/callback\n              clientId: \"your-client-id\"\n              clientSecretRef:\n                name: oauth-secret\n              issuerUrl: \"https://accounts.google.com\"\n              scopes: [openid, email, profile]\n              headers:\n                idTokenHeader: jwt\n              logoutPath: /logout\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#advanced-features_1","title":"Advanced Features","text":"<p>Redis Session Storage (production):</p> <pre><code>session:\n  failOnFetchFailure: true\n  redis:\n    cookieName: oidc-session\n    options:\n      host: redis.gloo-system.svc.cluster.local:6379\n</code></pre> <p>JWT Validation:</p> <pre><code>glooAuth:\n  configs:\n    - oauth2:\n        accessTokenValidation:\n          jwt:\n            remoteJwks:\n              url: https://www.googleapis.com/.well-known/jwks.json\n              refreshInterval: 60s\n</code></pre> <p>Combined Auth (OIDC OR JWT):</p> <pre><code>glooAuth:\n  configs:\n    - oauth2:\n        oidcAuthorizationCode: {...}\n    - oauth2:\n        accessTokenValidation: {...}\n  booleanExpr: \"configs[0] || configs[1]\"  # Either succeeds\n</code></pre> <p>Key Points:</p> <ul> <li>Most feature-rich OIDC implementation</li> <li>Commercial support available (Solo.io Enterprise)</li> <li>Requires ext-auth-server deployment</li> <li>Best for: Complex auth scenarios, multi-method auth</li> </ul> <p>See Solo.io ExtAuthPolicy docs for complete examples.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#important-considerations","title":"Important Considerations","text":""},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#security","title":"Security","text":"<ul> <li>Header Injection: Be careful which headers you forward to avoid header injection attacks</li> <li>Token Exposure: Avoid forwarding sensitive tokens unless necessary</li> <li>Validation: Always validate auth service responses</li> <li>TLS: Use HTTPS for auth service communication in production</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#performance","title":"Performance","text":"<ul> <li>Latency: External auth adds latency to every request</li> <li>Caching: Implement caching in auth service when possible</li> <li>Connection Pooling: Configure connection pooling to auth service</li> <li>Timeouts: Set appropriate timeouts to avoid cascading failures</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#gateway-implementation-differences","title":"Gateway Implementation Differences","text":"Feature Envoy Gateway Istio Kgateway Native OIDC \u2705 Built-in (SecurityPolicy) \u274c Requires OAuth2-Proxy/Authservice \u2705 Built-in (ExtAuthPolicy) JWT Validation \u2705 Via SecurityPolicy \u2705 Native (RequestAuthentication) \u2705 Via ExtAuthPolicy Session Management \u2705 Cookie-based \u26a0\ufe0f Via external proxy \u2705 Cookie or Redis External Proxy Needed \u274c No \u2705 Yes (for OIDC flow) \u274c No Complexity \ud83d\udfe2 Low (single resource) \ud83d\udd34 High (multiple components) \ud83d\udfe1 Medium (requires ext-auth server) Production Maturity \ud83d\udfe1 Growing \ud83d\udfe2 Very mature \ud83d\udfe2 Mature (Solo.io) Combined Auth Methods \u26a0\ufe0f Limited \u2705 Flexible (EnvoyFilter) \u2705 Boolean expressions Commercial Support \u274c Community only \u2705 Available \u2705 Solo.io Enterprise <p>Recommendations:</p> <ul> <li>Choose Envoy Gateway if: You want native OIDC with minimal complexity and are okay with a newer project</li> <li>Choose Istio if: You only need JWT validation, already use Istio mesh, or need maximum production maturity</li> <li>Choose Kgateway if: You need advanced auth features, commercial support, or complex auth scenarios (multi-method, API gateway use cases)</li> </ul> <p>Future: GEP-2994 is working on standardizing external auth in Gateway API</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#testing","title":"Testing","text":"<p>Test your migration thoroughly:</p> <ol> <li>Unauthenticated access: Verify users are redirected to login</li> <li>Authentication flow: Test complete OAuth2/OIDC flow</li> <li>Header forwarding: Verify correct headers reach backend</li> <li>Session management: Test session expiration and renewal</li> <li>Error handling: Test auth service failures</li> </ol>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#references","title":"References","text":""},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#gateway-api","title":"Gateway API","text":"<ul> <li>Gateway API GEP-2994: External Authorization</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#envoy-gateway","title":"Envoy Gateway","text":"<ul> <li>Envoy Gateway SecurityPolicy API</li> <li>Envoy Gateway OIDC Authentication</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#istio_1","title":"Istio","text":"<ul> <li>Istio RequestAuthentication</li> <li>Istio AuthorizationPolicy</li> <li>Istio Authservice (OIDC for Istio)</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#kgateway_1","title":"Kgateway","text":"<ul> <li>Kgateway API Reference</li> <li>Solo.io ExtAuthPolicy Documentation</li> <li>Kgateway OAuth with Keycloak</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/external-authentication/#external-auth-tools","title":"External Auth Tools","text":"<ul> <li>NGINX Ingress External Authentication</li> <li>OAuth2 Proxy Documentation</li> <li>Authelia Documentation</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/force-ssl-redirect/","title":"nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"","text":"<p>Purpose: Forces HTTP traffic to redirect to HTTPS automatically.</p> <p>In Gateway API, you need to create:</p> <ol> <li>A Gateway with both HTTP (port 80) and HTTPS (port 443) listeners</li> <li>An HTTPRoute attached to the HTTP listener that redirects to HTTPS</li> <li>An HTTPRoute attached to the HTTPS listener that routes to your backend</li> </ol> <p>2. HTTPRoute for HTTP to HTTPS Redirect:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-redirect\nspec:\n  parentRefs:\n  - name: eg\n    sectionName: http  # Attach to HTTP listener\n  hostnames:\n  - \"www.example.com\"\n  rules:\n  - filters:\n    - type: RequestRedirect\n      requestRedirect:\n        scheme: https\n        statusCode: 301  # Permanent redirect (default)\n</code></pre> <p>References:</p> <ul> <li>Envoy Gateway HTTP Redirects</li> <li>Gateway API HTTP Redirect/Rewrite Guide</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/proxy-body-size/","title":"nginx.ingress.kubernetes.io/proxy-body-size","text":"<p>This annotation sets the maximum allowed size of the client request body. If the request body exceeds this limit, the gateway returns a 413 (Request Entity Too Large) error. Usually it is used for</p> <ul> <li>File upload services</li> <li>API endpoints accepting large payloads</li> <li>Services handling image/video uploads</li> <li>Webhook receivers with large payloads</li> </ul> <p>Security consideration: Setting this value too high can make your service vulnerable to DoS attacks via large request bodies. Set reasonable limits based on your application's actual needs.</p> <p>The default value in nginx is 1m (1 megabyte) and some common values are: <code>8m</code>, <code>50m</code>, <code>100m</code>, or <code>0</code> (unlimited)</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/proxy-body-size/#gateway-api-implementation","title":"Gateway API Implementation","text":""},{"location":"networking/gateway-api/migration-from-nginx-controller/proxy-body-size/#envoy-gateway","title":"Envoy Gateway","text":"<p>It provides spec.requestBuffer.limit under the BackendTrafficPolicy resource. It supports SI units: Ki, Mi, Gi</p> <p>Important limitation: Due to an Envoy limitation, requests with body size \u2264 16KB (16384 bytes) are not rejected. You must specify a value &gt; 16KB for the policy to work correctly.</p> <pre><code>apiVersion: gateway.envoyproxy.io/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: request-size-limit\nspec:\n  targetRefs:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: my-route\n  requestBuffer:\n    limit: 50Mi  # 50 megabytes\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/proxy-body-size/#istio","title":"Istio","text":"<p>It can be configured using an EnvoyFilter resource to modify the Envoy configuration:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: EnvoyFilter\nmetadata:\n  name: request-size-limit\nspec:\n  workloadSelector:\n    labels:\n      app: my-app\n  configPatches:\n  - applyTo: ROUTE_CONFIGURATION\n    match:\n      context: GATEWAY\n    patch:\n      operation: MERGE\n      value:\n        per_filter_config:\n          envoy.filters.http.buffer:\n            max_request_bytes: 52428800  # 50MB in bytes\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/proxy-body-size/#kgateway","title":"Kgateway","text":"<p>It supports request body size limits through its policy mechanisms (similar to Envoy Gateway's BackendTrafficPolicy):</p> <pre><code>apiVersion: gateway.kgateway.dev/v1alpha1\nkind: BackendTrafficPolicy\nmetadata:\n  name: request-size-limit\nspec:\n  targetRefs:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: my-route\n  connection:\n    bufferLimit: 50Mi\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/rewrite-target/","title":"nginx.ingress.kubernetes.io/rewrite-target","text":"<p>This annotation permite to change the url path it is sent to the backend service.</p> <p>Examples:</p> <ul> <li>You want to expose your app at /v1/api/* but your backend service expects paths without the /v1 prefix.</li> <li>Strip application context paths</li> <li>Route multiple frontend paths to a single backend path</li> <li>Map public URLs to different internal paths</li> </ul> <p>It can be archieved using the standard <code>URLRewrite</code> filter in HTTPRoute rules.</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/rewrite-target/#option-1-replaceprefixmatch-most-common","title":"Option 1: ReplacePrefixMatch (most common)","text":"<p>Replaces the matched path prefix with a new prefix:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: api-rewrite\nspec:\n  parentRefs:\n  - name: my-gateway\n  hostnames:\n  - \"api.example.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /v1\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: /  # Strips /v1 prefix\n    backendRefs:\n    - name: api-service\n      port: 80\n</code></pre> <p>Example: <code>/v1/users/123</code> \u2192 <code>/users/123</code></p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/rewrite-target/#option-2-replacefullpath","title":"Option 2: ReplaceFullPath","text":"<p>Replaces the entire path with a fixed path:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: api-rewrite\nspec:\n  parentRefs:\n  - name: my-gateway\n  hostnames:\n  - \"api.example.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /old-api\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplaceFullPath\n          replaceFullPath: /new-api\n    backendRefs:\n    - name: api-service\n      port: 80\n</code></pre> <p>Example: <code>/old-api/users/123</code> \u2192 <code>/new-api</code> (entire path replaced)</p>"},{"location":"networking/gateway-api/migration-from-nginx-controller/rewrite-target/#option-3-hostname-rewrite","title":"Option 3: Hostname Rewrite","text":"<p>Can also rewrite the hostname:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: api-rewrite\nspec:\n  rules:\n  - filters:\n    - type: URLRewrite\n      urlRewrite:\n        hostname: \"internal-api.example.com\"\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: /api\n    backendRefs:\n    - name: api-service\n      port: 80\n</code></pre>"},{"location":"networking/gateway-api/migration-from-nginx-controller/rewrite-target/#advanced-implementation-specific-extensions","title":"Advanced Implementation-Specific Extensions","text":"<ul> <li>In Envoy Gateway we can use HTTPRouteFilter resource (spec.urlRewrite)</li> <li>In Istio we can use VirtualService resource (spec.http)</li> </ul>"},{"location":"networking/gateway-api/migration-from-nginx-controller/ssl-passthrough/","title":"nginx.ingress.kubernetes.io/ssl-passthrough","text":"<p>This is applied to the connection between the client and the gateway (downstream connection). It enables SSL/TLS passthrough mode so the the Gateway forwards encrypted traffic directly to backend pods without terminating TLS at the gateway level. Routing is performed based on the SNI (Server Name Indication) field in the TLS handshake.</p> <p>Common use cases:</p> <ul> <li>Applications that need to handle TLS termination themselves</li> <li>Mutual TLS (mTLS) where the application validates client certificates</li> <li>End-to-end encryption requirements</li> <li>Legacy applications with embedded TLS configuration</li> </ul> <p>Performance note: In NGINX Ingress, this feature introduces a performance penalty as it bypasses NGINX's normal processing and uses a TCP proxy.</p> <p>The only way to enable passthrough in gateway api is using a TLS listener protocol and a tlsroute.</p> <p>See more here https://gateway-api.sigs.k8s.io/guides/tls/</p>"},{"location":"networking/gateway-api/routes/00-routes/","title":"Gateway API Routes","text":"<p>Gateway API routes are fundamental resources that define how traffic flows from Gateways to backend services. They provide declarative configuration for traffic routing, load balancing, and request processing within Kubernetes clusters.</p>"},{"location":"networking/gateway-api/routes/00-routes/#route-types-overview","title":"Route Types Overview","text":"<p>Gateway API defines several route types to handle different traffic patterns and protocols:</p>"},{"location":"networking/gateway-api/routes/00-routes/#standard-routes-ga","title":"Standard Routes (GA)","text":"<ul> <li>HTTPRoute - HTTP/HTTPS traffic routing with advanced features</li> <li>GRPCRoute - gRPC-specific routing with protocol awareness</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#experimental-routes","title":"Experimental Routes","text":"<ul> <li>TCPRoute - Layer 4 TCP traffic routing</li> <li>TLSRoute - TLS passthrough and SNI-based routing  </li> <li>UDPRoute - UDP traffic routing for stateless protocols</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#core-concepts","title":"Core Concepts","text":""},{"location":"networking/gateway-api/routes/00-routes/#route-attachment","title":"Route Attachment","text":"<p>Routes attach to Gateways through <code>parentRefs</code>, creating a binding between the Gateway listener and the route configuration:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-route\nspec:\n  parentRefs:\n  - name: example-gateway\n    sectionName: http-listener\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#traffic-matching","title":"Traffic Matching","text":"<p>Routes define matching criteria to select which requests they handle:</p> <ul> <li>Hostname matching - Route based on Host header</li> <li>Path matching - Route based on URL paths (exact, prefix, regex)</li> <li>Header matching - Route based on HTTP headers</li> <li>Method matching - Route based on HTTP methods</li> <li>Query parameter matching - Route based on URL parameters</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#backend-selection","title":"Backend Selection","text":"<p>Routes forward matched traffic to backend services with configurable weights and filters:</p> <pre><code>spec:\n  rules:\n  - backendRefs:\n    - name: service-a\n      port: 80\n      weight: 90\n    - name: service-b  \n      port: 80\n      weight: 10\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#request-processing","title":"Request Processing","text":"<p>Routes can modify requests and responses through filters:</p> <ul> <li>Request header modification - Add, set, or remove headers</li> <li>Response header modification - Transform response headers</li> <li>URL rewriting - Modify paths and hostnames</li> <li>Request mirroring - Duplicate traffic to additional backends</li> <li>Request redirection - Return HTTP redirects</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#route-hierarchy","title":"Route Hierarchy","text":"<p>Routes follow a hierarchical attachment model:</p> <ol> <li>Gateway - Defines listeners and infrastructure</li> <li>Route - Defines traffic routing rules</li> <li>Backend - Target services for traffic forwarding</li> </ol>"},{"location":"networking/gateway-api/routes/00-routes/#cross-namespace-routing","title":"Cross-Namespace Routing","text":"<p>Routes can reference backends in different namespaces with proper RBAC configuration:</p> <pre><code>spec:\n  rules:\n  - backendRefs:\n    - name: backend-service\n      namespace: backend-ns\n      port: 8080\n</code></pre> <p>Requires ReferenceGrant in the target namespace:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-routes\n  namespace: backend-ns\nspec:\n  from:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    namespace: frontend-ns\n  to:\n  - group: \"\"\n    kind: Service\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#route-status-and-conditions","title":"Route Status and Conditions","text":"<p>Routes report their status through conditions:</p> <ul> <li>Accepted - Route configuration is valid</li> <li>ResolvedRefs - All references can be resolved</li> <li>PartiallyInvalid - Some rules are invalid but others work</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#best-practices","title":"Best Practices","text":""},{"location":"networking/gateway-api/routes/00-routes/#security","title":"Security","text":"<ul> <li>Use ReferenceGrants for cross-namespace access</li> <li>Implement proper RBAC for route management</li> <li>Validate input through admission controllers</li> <li>Use TLS termination at the Gateway level</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#performance","title":"Performance","text":"<ul> <li>Minimize regex matching in favor of exact/prefix matching</li> <li>Use appropriate backend weights for load distribution</li> <li>Configure timeouts and retry policies appropriately</li> <li>Monitor route performance and adjust configurations</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#maintainability","title":"Maintainability","text":"<ul> <li>Use descriptive names and annotations</li> <li>Group related routes logically</li> <li>Document complex routing logic</li> <li>Version control route configurations</li> </ul>"},{"location":"networking/gateway-api/routes/grpcroute/","title":"GRPCRoute (standard)","text":""},{"location":"networking/gateway-api/routes/grpcroute/#purpose","title":"Purpose","text":"<p>GRPCRoute provides gRPC-specific routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route gRPC traffic based on service names, method names, and header matching</li> <li>Load balance gRPC calls across multiple backend services with protocol awareness</li> <li>Handle streaming RPCs for both unary and streaming gRPC communication patterns</li> <li>Apply gRPC-specific filters for request/response transformation and middleware integration</li> <li>Support service mesh integration with advanced gRPC features like retries and circuit breaking</li> <li>Enable microservices communication with type-safe, high-performance RPC calls</li> <li>Manage API versioning through service and method-level routing rules</li> </ul> <p>GRPCRoute operates at the application layer with deep understanding of gRPC protocols, providing sophisticated routing for modern microservices architectures. It's ideal for internal service-to-service communication, API gateways serving gRPC clients, and hybrid environments mixing HTTP and gRPC traffic in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/grpcroute/#reference","title":"Reference","text":"<ul> <li>Info</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/grpc-routing/ https://gateway-api.sigs.k8s.io/api-types/grpcroute/</p> <ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#grpcroute</p>"},{"location":"networking/gateway-api/routes/httproute/","title":"HTTPRoute (standard)","text":""},{"location":"networking/gateway-api/routes/httproute/#purpose","title":"Purpose","text":"<p>HTTPRoute provides HTTP-specific routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route HTTP traffic based on hostnames, paths, headers, and query parameters</li> <li>Load balance traffic across multiple backend services</li> <li>Transform requests through header manipulation, URL rewriting, and redirects</li> <li>Split traffic for A/B testing, canary deployments, and gradual rollouts</li> <li>Mirror traffic to additional backends for testing and monitoring</li> <li>Apply filters for request/response modification and middleware integration</li> </ul> <p>HTTPRoute acts as the configuration layer that defines how HTTP requests should be processed and forwarded to backend services, providing fine-grained control over HTTP traffic routing within your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/httproute/#reference","title":"Reference","text":"<ul> <li> <p>Spec https://gateway-api.sigs.k8s.io/api-types/httproute/ https://gateway-api.sigs.k8s.io/reference/spec/#httproute</p> </li> <li> <p>Info https://gateway-api.sigs.k8s.io/guides/http-routing/ https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/ https://gateway-api.sigs.k8s.io/guides/http-header-modifier/ https://gateway-api.sigs.k8s.io/guides/traffic-splitting/ https://gateway-api.sigs.k8s.io/guides/http-request-mirroring/</p> </li> </ul>"},{"location":"networking/gateway-api/routes/tcproute/","title":"TCPRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/tcproute/#purpose","title":"Purpose","text":"<p>TCPRoute provides Layer 4 TCP traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route TCP traffic based on port and destination matching</li> <li>Load balance TCP connections across multiple backend services</li> <li>Proxy TCP streams for non-HTTP protocols like databases, message queues, and custom applications</li> <li>Handle persistent connections for stateful services requiring session affinity</li> <li>Bridge network protocols for legacy applications and services that don't use HTTP</li> </ul> <p>TCPRoute operates at the transport layer, providing simple but powerful routing for any TCP-based service without requiring protocol-specific knowledge, making it ideal for databases (PostgreSQL, MySQL), message brokers (Redis, RabbitMQ), and other TCP services in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/tcproute/#reference","title":"Reference","text":"<ul> <li>Info</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/tcp/</p> <ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#tcproute</p>"},{"location":"networking/gateway-api/routes/tlsroute/","title":"TLSRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/tlsroute/#purpose","title":"Purpose","text":"<p>The TLSRoute resource is similar to TCPRoute, but can be configured to match against TLS-specific metadata. This allows more flexibility in matching streams for a given TLS listener.</p> <p>TLSRoute provides TLS-aware traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route TLS traffic based on SNI (Server Name Indication) without terminating encryption</li> <li>Preserve end-to-end encryption by passing through encrypted traffic to backend services</li> <li>Handle TLS passthrough for services that need to manage their own TLS termination</li> <li>Route encrypted protocols like HTTPS, secure databases, and other TLS-wrapped services</li> <li>Support SNI-based routing for multiple domains sharing the same IP address</li> <li>Maintain certificate control at the backend service level</li> </ul> <p>TLSRoute operates at the TLS layer, inspecting only the SNI information in the TLS handshake to make routing decisions while keeping the payload encrypted. This makes it ideal for scenarios where backend services need direct control over TLS termination and certificate management, or when compliance requirements mandate end-to-end encryption.</p>"},{"location":"networking/gateway-api/routes/tlsroute/#reference","title":"Reference","text":"<ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#tlsroute</p>"},{"location":"networking/gateway-api/routes/udproute/","title":"UDPRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/udproute/#purpose","title":"Purpose","text":"<p>UDPRoute provides Layer 4 UDP traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route UDP traffic based on port and destination matching</li> <li>Load balance UDP packets across multiple backend services</li> <li>Handle stateless protocols like DNS, DHCP, and logging services</li> <li>Support connectionless communication for services that don't maintain persistent connections</li> <li>Route multimedia traffic for streaming protocols and real-time applications</li> <li>Manage fire-and-forget messaging patterns common in distributed systems</li> </ul> <p>UDPRoute operates at the transport layer for connectionless UDP traffic, providing simple packet forwarding without session tracking. This makes it ideal for services like DNS servers, syslog collectors, game servers, streaming media applications, and other UDP-based services that require fast, low-overhead packet delivery in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/udproute/#reference","title":"Reference","text":"<ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#udproute</p>"},{"location":"networking/metallb/98-tips/","title":"Tips","text":""},{"location":"networking/metallb/98-tips/#no-installed-keys-could-decrypt-the-message","title":"No installed keys could decrypt the message","text":"<p>After a metallb version update, the daemonset logs show</p> <pre><code>memberlist: failed to receive: No installed keys could decrypt the message from=XXX.XXX.XXX.XXX:36946\n</code></pre> <p>Solution</p> <pre><code>kubectl delete pods -n metallb-system --all\n</code></pre> <p>Then we can see</p> <pre><code>memberlist join successfully\n</code></pre>"},{"location":"networking/services/traffic-policies/","title":"Kubernetes Service Traffic Policies","text":""},{"location":"networking/services/traffic-policies/#overview","title":"Overview","text":"<p>Traffic policies in Kubernetes Services control how traffic is routed to pods through kube-proxy. Understanding <code>externalTrafficPolicy</code> and <code>internalTrafficPolicy</code> is crucial when using LoadBalancer or NodePort services, especially in on-premise environments.</p>"},{"location":"networking/services/traffic-policies/#externaltrafficpolicy","title":"externalTrafficPolicy","text":"<p>Describes how nodes distribute service traffic they receive on externally-facing addresses (NodePorts, ExternalIPs, and LoadBalancer IPs). This field does NOT apply to traffic from within the cluster destined to LoadBalancer or ExternalIP\u2014such traffic always uses <code>Cluster</code> semantics.</p>"},{"location":"networking/services/traffic-policies/#externaltrafficpolicy-cluster-default","title":"externalTrafficPolicy Cluster (Default)","text":"<p>With externalTrafficPolicy Cluster, kube-proxy sends the traffic to any pod in the cluster matched by the service.</p> <p>Here the client source IP is masqueraded (hidden)</p> <p>Advantages:</p> <ul> <li>Better load distribution across all pods</li> <li>No traffic concentration on specific nodes</li> <li>Works with any pod placement strategy</li> </ul> <p>Disadvantages:</p> <ul> <li>Additional network hop (latency)</li> <li>Client source IP is not preserved</li> <li>Breaks internal pod-to-external-IP communication (pods cannot reach the service's external LoadBalancer IP)</li> </ul> <p>Use case: External clients connecting to your service</p>"},{"location":"networking/services/traffic-policies/#externaltrafficpolicy-local","title":"externalTrafficPolicy Local","text":"<p>Here the externalTrafficPolicy Local, each node delivers traffic only to node-local endpoints without masquerading the client source IP. Traffic mistakenly sent to a node with no endpoints is dropped (not forwarded).</p> <p>How it works:</p> <ul> <li>Node receives traffic and only proxies to pods on the same node</li> <li>Client source IP is preserved</li> <li>Assumes external load balancers will balance traffic between nodes</li> <li>If a node has no local endpoints, traffic is dropped (not redirected)</li> </ul> <p>Advantages:</p> <ul> <li>Preserves client source IP (visible to pods)</li> <li>Lower latency (no cross-node network hop)</li> <li>Better performance (minimal proxying)</li> <li>Internal pods CAN reach the service's external IP</li> </ul> <p>Disadvantages:</p> <ul> <li>Uneven load distribution</li> <li>Traffic concentrates on nodes where pods are running</li> <li>Dropped traffic if receiving node has no local endpoints</li> <li>Requires external load balancer intelligence</li> <li>Doesn't work well with single-replica deployments on unscheduled nodes</li> </ul> <p>Use case: On-premise environments, performance-critical services, source IP preservation, or with external load balancers that understand node topology</p>"},{"location":"networking/services/traffic-policies/#internaltrafficpolicy","title":"internalTrafficPolicy","text":"<p>Describes how nodes distribute service traffic they receive on the ClusterIP. Controls pod-to-service communication within the cluster. Only applies to traffic destined to the ClusterIP; similar logic applies to NodePort access from within the cluster.</p>"},{"location":"networking/services/traffic-policies/#internaltrafficpolicy-cluster-default","title":"internalTrafficPolicy Cluster (Default)","text":"<p>Routes internal traffic to all ready endpoints in the cluster evenly. Pods can reach any service endpoint regardless of node location.</p>"},{"location":"networking/services/traffic-policies/#internaltrafficpolicylocal","title":"internalTrafficPolicyLocal","text":"<p>Routes traffic only to node-local endpoints. Reduces cross-node traffic and improves latency for latency-sensitive applications. Requires at least one endpoint per node for reliability. Works with ClusterIP, NodePort, and LoadBalancer services.</p>"},{"location":"networking/services/traffic-policies/#internal-cluster-access-to-external-ip","title":"Internal Cluster Access to External IP","text":"<p>When a pod calls an ExternalIP, the request doesn't actually travel out to the Physical Load Balancer and back in.</p> <ul> <li>The Linux networking stack (via kube-proxy) intercepts the packet as soon as it leaves the pod.</li> <li>kube-proxy sees the destination is an ExternalIP that it \"owns.\"</li> <li>It immediately applies the routing rules defined by your externalTrafficPolicy right there on the source node.</li> </ul> <p>With externalTrafficPolicy local if there are not pods in the same node where the call is done, the traffic is dropped. Solutions here can be using the internal service: <code>my-service.namespace.svc.cluster.local</code>, a daemonset or other mechanisms to ensure the pods doing the calls have endpoints it the same node</p> <p>The behaviour tries to avoid</p> <ul> <li>routing loops: If the node sent the packet out to the Load Balancer, and the Load Balancer sent it right back to the same node (which can happen), you would create an infinite loop</li> <li>hairpinning: Most Cloud Provider Load Balancers are not designed to handle \"hairpin\" traffic (traffic originating from one of its own targets and coming back to itself).</li> </ul>"},{"location":"networking/services/traffic-policies/#load-balancer-responsibilities","title":"Load Balancer Responsibilities","text":""},{"location":"networking/services/traffic-policies/#cluster-mode","title":"Cluster Mode","text":"<p>What the load balancer must do:</p> <ul> <li>Distribute traffic to ANY node in the cluster</li> <li>Doesn't need to know which nodes have service endpoints</li> <li>All traffic reaching any node is handled by kube-proxy and forwarded to correct endpoints</li> </ul> <p>Best for: Cloud environments where load balancer has no visibility into cluster topology</p>"},{"location":"networking/services/traffic-policies/#local-mode","title":"Local Mode","text":"<p>What the load balancer must do:</p> <ul> <li>Discover which nodes have endpoints for the service (via health checks or endpoint watcher)</li> <li>Only send traffic to nodes that actually have service endpoints</li> <li>Remove nodes from the load balancer pool if they have no endpoints</li> <li>Monitor endpoint changes and update pool membership</li> </ul> <p>Requirements:</p> <ul> <li>Load balancer must support endpoint-aware routing</li> <li>Requires integration with cluster (health check endpoint, endpoint discovery, or controller)</li> </ul> <p>Load Balancer Support:</p> <ul> <li>MetalLB: Speaker daemon announces service endpoints via BGP/ARP</li> <li>AWS NLB:</li> <li>Instance target type: Native support for <code>Local</code> via NodePort health checks</li> <li>IP target type: Direct pod routing; <code>Local</code> may not be effective (recommended: use <code>Cluster</code>)</li> <li>Azure LB: Integrated with AKS, automatic endpoint tracking per node</li> <li>GCP LB: Native support via Network Endpoint Groups (NEGs)</li> <li>F5/Citrix/HAProxy: Requires external controller integration</li> </ul> <p>Best for: On-premise environments with sophisticated load balancers, or cloud providers with native integration</p>"},{"location":"networking/services/traffic-policies/#recommendations","title":"Recommendations","text":""},{"location":"networking/services/traffic-policies/#externaltrafficpolicy-selection","title":"externalTrafficPolicy Selection","text":"Scenario Recommended Public cloud (auto-scaling, many nodes) <code>Cluster</code> On-premise (fixed node topology) <code>Local</code> Internal pod-to-external-IP needed <code>Cluster</code> Source IP preservation critical <code>Local</code> Performance critical (lower latency) <code>Local</code>"},{"location":"networking/services/traffic-policies/#internaltrafficpolicy-selection","title":"internalTrafficPolicy Selection","text":"Scenario Recommended Standard pod communication <code>Cluster</code> (default) Reduce cross-node traffic <code>Local</code> Topology-aware routing desired Consider <code>trafficDistribution: PreferClose</code> Zone-local deployments <code>Local</code> + multi-zone pods"},{"location":"networking/services/traffic-policies/#links","title":"Links","text":"<ul> <li>Kubernetes Service - externalTrafficPolicy</li> <li>Kubernetes Service - internalTrafficPolicy</li> <li>Kubernetes Service API Reference</li> </ul>"},{"location":"observability/architecture/","title":"Architecture","text":"<p>This page outlines a pragmatic, operations-focused view of an observability stack.</p>"},{"location":"observability/architecture/#highlevel-layers","title":"High\u2011level layers","text":"<ol> <li>Instrumentation (generation)</li> <li>Application SDKs, auto-instrumentation, exporters emit logs/metrics/traces (L/M/T)</li> <li> <p>Standard protocols: OTLP over gRPC/HTTP; legacy inputs supported via receivers</p> </li> <li> <p>Ingestion/collection</p> </li> <li>Agents/DaemonSets/sidecars and/or a gateway OpenTelemetry Collector receive telemetry</li> <li> <p>Fan\u2011in from nodes, apps, infrastructure, cloud services</p> </li> <li> <p>Processing/enrichment</p> </li> <li>Pipelines apply sampling, filtering, redaction, resource detection, attribute transforms</li> <li>Batching, retry, timeouts, queueing/backpressure control to stabilize flow</li> <li> <p>Routing by signal/tenant/team to specific backends</p> </li> <li> <p>Export/transport</p> </li> <li>OTLP/gRPC or HTTP to remote backends; TLS/mTLS and auth (API keys, OIDC)</li> <li> <p>Circuit breakers, exponential backoff, persistent queues for resilience</p> </li> <li> <p>Storage backends (by signal)</p> </li> <li>Metrics: Prometheus/Thanos/Mimir, VictoriaMetrics</li> <li>Traces: Tempo/Jaeger/Elastic APM</li> <li>Logs: Loki/Elasticsearch/OpenSearch</li> <li> <p>Sometimes a columnar data lake/warehouse for long\u2011term retention and cost control</p> </li> <li> <p>Query/visualization &amp; alerting</p> </li> <li>Grafana/Kibana/Tempo/Jaeger UIs; SLOs and alert rules</li> <li>Routing to Alertmanager, PagerDuty, Opsgenie, Slack, email</li> </ol>"},{"location":"observability/architecture/#where-the-opentelemetry-collector-fits","title":"Where the OpenTelemetry Collector fits","text":"<p>The OTel Collector spans multiple layers:</p> <ul> <li>Layer 2 (Ingestion/collection): receivers (otlp, prometheus, filelog, k8s events)</li> <li>Layer 3 (Processing/enrichment): processors (batch, memory_limiter, attributes/resource, transform, tail_sampling, routing)</li> <li>Layer 4 (Export/transport): exporters (otlp[gRPC/HTTP], prometheusremotewrite, loki, tempo/jaeger), TLS/mTLS, retries/queues</li> </ul> <p>It is not a storage backend or UI (does not cover layers 5 or 6).</p>"},{"location":"observability/architecture/#reference-view-kubernetes-opentelemetry","title":"Reference view (Kubernetes + OpenTelemetry)","text":"<ul> <li>Workloads emit OTLP \u2192 node/sidecar/agent collector (DaemonSet or sidecar)</li> <li>Optional gateway collector (centralized, stateless) \u2192 processes and routes signals</li> <li>Backends per signal; Grafana on top for dashboards, logs, traces, exemplars</li> </ul> <pre><code>Apps/SDKs \u2500\u2500OTLP\u2500\u2500&gt; Node/Sidecar Collector \u2500\u2500OTLP\u2500\u2500&gt; Gateway Collector \u2500\u2500&gt; Backends\n                                              \u2502                          \u251c\u2500&gt; Metrics TSDB\n                                              \u2502                          \u251c\u2500&gt; Traces Store\n                                              \u2502                          \u2514\u2500&gt; Logs Store\n                                              \u2514\u2500\u2500 kube/system exporters (kube-state, cAdvisor)\n</code></pre>"},{"location":"observability/architecture/#collector-deployment-patterns","title":"Collector deployment patterns","text":"<ul> <li>Sidecar: per\u2011pod isolation; simplest context propagation; higher overhead</li> <li>DaemonSet (agent): per\u2011node collector for all workloads; good default</li> <li>Gateway: centralized fan\u2011in; enforces org\u2011wide policy (sampling, PII scrubbing)</li> <li>Common to use Agent (DaemonSet) + Gateway for scale and control</li> </ul>"},{"location":"observability/architecture/#signalspecific-notes","title":"Signal\u2011specific notes","text":"<ul> <li>Metrics: prefer low\u2011cardinality labels; use histograms; remote write to long\u2011term TSDB</li> <li>Traces: sampling strategies (tail\u2011based at gateway for best value; head\u2011based at source for low overhead)</li> <li>Logs: structure at source (JSON); drop/trim noisy lines early; labels/indices budgeted</li> </ul>"},{"location":"observability/architecture/#reliability-and-cost-levers","title":"Reliability and cost levers","text":"<ul> <li>Backpressure: memory_limiter + queued_retry processors in OTel Collector</li> <li>Batching: reduce connection churn and backend CPU</li> <li>Redaction: attributes and body processors for PII/compliance</li> <li>Multi\u2011route: split traffic by environment/tenant to different clusters/backends</li> <li>Retention tiers: hot (short), warm (mid), cold (cheap/archival)</li> </ul>"},{"location":"observability/architecture/#minimal-otel-pipeline-conceptual","title":"Minimal OTel pipeline (conceptual)","text":"<p>Receivers \u2192 processors \u2192 exporters, per signal:</p> <pre><code>receivers: otlp, prometheus, filelog\nprocessors: memory_limiter, batch, attributes(resource), transform, tail_sampling\nexporters: otlphttp(tempo), prometheusremotewrite(mimir), loki\n</code></pre>"},{"location":"observability/architecture/#ops-best-practices","title":"Ops best practices","text":"<ul> <li>Start with a single protocol (OTLP) end\u2011to\u2011end</li> <li>Keep metrics cardinality in check; gate label additions</li> <li>Make sampling an explicit decision (prove value, then tune)</li> <li>Treat collectors as stateless and horizontally scalable</li> <li>Version and test pipelines as code; lint configs; add golden queries/alerts</li> </ul>"},{"location":"observability/architecture/#see-also","title":"See also","text":"<ul> <li>OpenTelemetry Collector: pipelines and processors (receivers \u2192 processors \u2192 exporters)</li> <li>OTLP protocol (gRPC/HTTP)</li> <li>Backend options: Mimir/Thanos, Tempo/Jaeger, Loki/Elasticsearch</li> </ul>"},{"location":"observability/tools/","title":"Observatility Tool","text":""},{"location":"observability/tools/#ecosystems","title":"Ecosystems","text":"Ecosystem Metrics Collector Traces Collector Logs Collector Dashboard Enterprise Opentelemetry OT Collector OT Collector OT Collector Grafana G.Mimir Alloy G.Tempo Alloy / Beyla Loki Alloy Grafana Grafana Cloud / Grafana Enterprise Prometheus Prometheus Elasticsearch Elasticsearch Kibana Elastic Cloud Jaeger Jaeger Collector Jaeger UI"},{"location":"observability/tools/#log-collectors","title":"Log collectors","text":"<ul> <li>Logstash</li> </ul> <p>It is the logs collector from the Elasticsearch ecosystem</p> <ul> <li>Opentelemetry collector</li> </ul> <p>Thel opentelemetry collector can act as a log collector</p> <ul> <li>Grafana Alloy</li> </ul> <p>It is the new name of the Grafana Agent, an Opentelemetry collector distribution from the grafana ecosystem. It also replaces Grafana Promtail.</p> <ul> <li>Fluentbit and fluentd</li> </ul> <p>Both are well known logs collector. Fluentbit is lightweight and simpler.</p> <ul> <li>Vector</li> </ul> <p>Vector is the collector from datadog</p>"},{"location":"observability/alertmanager/amc-beta/","title":"AlertmanagerConfig v1beta1","text":"<p>Does AlertmanagerConfig v1beta1 exists?</p> <ul> <li>v1beta1 was announced but not fully deployed:</li> </ul> <p>The prometheus-operator team announced v1beta1 for AlertmanagerConfig in version 0.57.0 (June 2022), but it was implemented as an opt-in feature requiring conversion webhooks.</p> <ul> <li>Default CRDs still use v1alpha1</li> </ul> <p>The standard CRD files in example/prometheus-operator-crd/ directory only contain v1alpha1. The v1beta1 version would be in example/prometheus-operator-crd-full/ but requires additional webhook setup.</p> <ul> <li> <p>Documentation vs. Reality Gap:</p> </li> <li> <p>OpenShift/OKD documentation shows v1beta1 because they have their own implementation</p> </li> <li>The official prometheus-operator CRDs in their GitHub repo still primarily use v1alpha1</li> <li>Third-party CRD catalogs (like Datree's) follow the standard CRDs, hence only v1alpha1</li> </ul> <p>Current Status: The v1beta1 API exists in the codebase but is not the default deployment method. Most users continue using v1alpha1.</p>"},{"location":"observability/alertmanager/inhibit-rules/","title":"Inhibit rules","text":"<p>A Prometheus inhibit rule is a configuration in Alertmanager that prevents certain alerts from firing when other higher-priority or related alerts are already active. It's a way to reduce alert noise by suppressing redundant or less important notifications.</p>"},{"location":"observability/alertmanager/inhibit-rules/#structure","title":"Structure","text":"<p>In an inhibit rule we must define:</p> <ul> <li>Source alerts</li> </ul> <p>They are alerts that will supress other alerts when active.</p> <ul> <li>Target alerts:</li> </ul> <p>They are the alerts that will be suppressed when source alerts match</p>"},{"location":"observability/alertmanager/inhibit-rules/#example-cases","title":"Example cases","text":"<ul> <li>Supress alerts related with diskspace, cpu usage or memory usage when the node is down</li> <li>Supress warning alerts when critical alerts are firing about the same topic</li> <li>Supress a less important alert when a more important alert. For example, don't alert an application is down when its loadbalancer is down, that causes the application is down</li> </ul> <p>In prometheus operator this can be achieved using an AlertmanagerConfig kubernetes resource spec.inhibitRules</p>"},{"location":"observability/alertmanager/send-alerts/","title":"Send alerts","text":""},{"location":"observability/alertmanager/send-alerts/#to-slack","title":"To slack","text":"<p>Create a Slack Incoming Webhook</p> <ul> <li>Go to https://api.slack.com/apps and create a new app (or use existing)</li> </ul> <p>Then enable \"Incoming Webhooks\"\"</p> <p></p> <p>And add a webhook to your workspace, select the channel where you want to post alerts and click allow.</p> <p>Take note of the webhook URL</p> <p>It looks like https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXX</p>"},{"location":"observability/alertmanager/send-alerts/#to-microsoft-teams-2","title":"To Microsoft Teams 2","text":"<p>Choose a channel and go to \"Workflows\"</p> <p>Here create a workflow</p> <ul> <li>using the template \"send webhook alerts to a channel\"</li> <li>choosing the team and channel</li> <li>finally take note of the url</li> </ul> <p>In order to make this work we need Alertmanager &gt;= 0.28.0. If using prometheus operator we also need the crds that supports it</p>"},{"location":"observability/grafana/azure-ad-auth/","title":"Autenticacion via Azure","text":""},{"location":"observability/grafana/azure-ad-auth/#creacion-de-la-aplicacion","title":"Creacion de la aplicacion","text":"<p>Creamos una nueva aplicacion desde App Registrations &gt; New Registration</p> <ul> <li>Supported Account types: Por ejemplo \"Accounts in this organizational directory only\"</li> <li>Redirect URI En el asistente inicial, ponemos https://migrafana.dominio.com</li> </ul>"},{"location":"observability/grafana/azure-ad-auth/#postconfiguracion-de-la-aplicacion","title":"Postconfiguracion de la aplicacion","text":"<ul> <li> <p>Segunda Redirect URl En Manage - Authentication - Web - Redirect URls creamos una nueva con https://migrafana.dominio.com/login/azuread</p> </li> <li> <p>Client Secret En \"Manage - Certificates &amp; secrets - Client secrets\" creamos un nuevo client secret y colocamos su valor en la variable GF_AUTH_AZUREAD_CLIENT_SECRET</p> </li> <li> <p>Groups claim En \"Manage - Token Configuration\" agregamos un groups claim seleccionando \"security groups \"y \"Groups assigned to the application\"</p> </li> <li> <p>Api permissions Deberiamos tener Microsoft Graph - User.Read</p> </li> <li> <p>App Roles Creamos 3 roles con el mismo valor Display Name, Value y Description y eligiendo \"Users/Groups\" como \"Allowed member types\". Ese valor sera Viewer, Editor y GrafanaAdmin</p> </li> <li> <p>Agregar grupos Desde Enterprise Applications entramos en nuestra aplicacion y en Manage - Users and groups elegimos los grupos y usuarios que queremos y los mapemos a los roles deseados</p> </li> </ul>"},{"location":"observability/grafana/azure-ad-auth/#lista-de-variables-de-entorno","title":"Lista de variables de entorno","text":""},{"location":"observability/grafana/azure-ad-auth/#autenticacion","title":"Autenticacion","text":"<pre><code>GF_AUTH_AZUREAD_AUTH_URL: valor que figura como \"OAuth 2.0 authorization endpoint (v2)\" en \"Endpoints\"\nGF_AUTH_AZUREAD_TOKEN_URL: valor que figura como \"OAuth 2.0 token endpoint (v2)\" en \"Endpoints\"\nGF_AUTH_AZUREAD_CLIENT_ID: valor que figura como \"Application (client) ID\" en Overview\nGF_AUTH_AZUREAD_CLIENT_SECRET: valor del secret creado anteriormente\n\n</code></pre>"},{"location":"observability/grafana/azure-ad-auth/#usuarios-y-asignacion-de-roles","title":"Usuarios y asignacion de roles","text":"<p>Si no hay definido ningun rol en la aplicacion, el valor asignado sera el indicado en GF_USERS_AUTO_ASSIGN_ORG_ROLE. Este valor por defecto es Viewer y tambien puede ser Admin, Editor y None.</p> <p>Este funcionamiento de asignar un rol por defecto puede quitarse poniendo a true GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT, lo que no permite el login si no hay un rol definido para el usuario.</p> Variable Valor raz. Funcion GF_USERS_AUTO_ASSIGN_ORG_ROLE Viewer permite definir el rol predeterminado para los usuarios de la organizacion principal GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT deshabilita la asignacion de un rol por defecto GF_AUTH_AZUREAD_SKIP_ORG_ROLE_SYNC evita traerse los roles del Azure. <p>Puede ser una buena practica habilitar GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT, lo que fuerza a crear roles de aplicacion</p>"},{"location":"observability/grafana/azure-ad-auth/#misc","title":"Misc","text":"Variable Valor raz. Funcion GF_AUTH_AZUREAD_ALLOW_ASSIGN_GRAFANA_ADMIN false Deshabilita al rol GrafanaAdmin el tener privilegios de administrador GF_AUTH_AZUREAD_ALLOW_SIGN_UP true GF_AUTH_AZUREAD_AUTO_LOGIN false Habilitarlo se salta la pantalla de login GF_AUTH_AZUREAD_ENABLED true Habilita Azure ad Auth GF_AUTH_AZUREAD_NAME \"Azure AD\" Nombre de la configuracion GF_AUTH_AZUREAD_SCOPES \"openid email profile\" GF_AUTH_AZUREAD_USE_PKCE true GF_AUTH_AZUREAD_ALLOWED_ORGANIZATIONS identificador de la organizacion que queremos permitir el acceso GF_AUTH_AZUREAD_ALLOWED_GROUPS grupos que permitimos el acceso separado por comas o espacios de i GF_AUTH_AZUREAD_ALLOWED_DOMAIN dominios que permitmos el acceso separado por comas o espacios"},{"location":"observability/grafana/azure-ad-auth/#links","title":"Links","text":"<ul> <li> <p>Configure Azure AD OAuth2 authentication https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/azuread/</p> </li> <li> <p>Configure Grafana https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/</p> </li> </ul>"},{"location":"observability/kubernetes/0-metrics-api/","title":"Kubernetes Metrics Api","text":"<p>The Kubernetes Metrics API is a standardized way to access some metrics using the kubernetes api.</p> <p>It has some different components:</p>"},{"location":"observability/kubernetes/0-metrics-api/#resource-metrics-api","title":"Resource Metrics API","text":"<p>Provides basic resource usage metrics (cpu and memory) for pods and nodes. It is used for:</p> <ul> <li>Autoescaling purposes via horizontal pod autoescaler</li> <li>The kubectl top command</li> </ul> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta1.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.metrics.k8s.io\nspec:\n  group: metrics.k8s.io\n  version: v1beta1\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n  ...\n</code></pre> <p>In order to get and check the metrics provided by the resource metrics api, we can</p> <pre><code>kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\"\n</code></pre> <p>There are 2 known implementations of the resource metrics api</p> <ul> <li>Metrics server</li> </ul> <p>Metrics server is the lightest and simplest implementation. Metrics server queries some kubelet  endpoints to get that information.</p> <p>More info here: metrics-server</p> <ul> <li>Prometheus adapter</li> </ul> <p>Prometheus adapter can replace metrics server and it can also serve other Kubernetes Metrics Api components, like custom and external metrics api. It queries a prometheus instance to get that queries.</p> <p>More info here: prometheus-adapter</p>"},{"location":"observability/kubernetes/0-metrics-api/#custom-metrics-api","title":"Custom Metrics API","text":"<p>The Custom Metrics API is a Kubernetes Metrics APi component that permits to access some custom metrics via the kubernetes api.</p> <p>It permits to define and use that custom metrics for monitoring and autoescaling purposes.</p> <pre><code>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1\n</code></pre> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta2.custom.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta2.custom.metrics.k8s.io\nspec:\n  group: custom.metrics.k8s.io\n  version: v1beta2\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n  ...\n</code></pre> <p>The most typical implementation of the custom metrics api is the prometheus adapter. For this, the prometheus adapter permits advanced autoescaling based on different metrics other than the cpu and memory.</p> <p>Again, more info here: prometheus-adapter</p> <p>Zalando has another implementation called kube-metrics-adapter</p>"},{"location":"observability/kubernetes/0-metrics-api/#external-metrics-api","title":"External Metrics API","text":"<p>The External Metrics API is a Kubernetes Metrics APi component that permits to access some external metrics via the kubernetes api. Those metrics are not necessarily tied to kubernetes objects.</p> <p>Those metrics also can be used to define and use that external metrics for monitoring and autoescaling purposes, and provides advanced autoescaling features.</p> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta1.external.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.external.metrics.k8s.io\nspec:\n  group: external.metrics.k8s.io\n    version: v1beta1\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n</code></pre>"},{"location":"observability/kubernetes/0-metrics-api/#links","title":"Links","text":"<p>More info at https://github.com/kubernetes/metrics</p>"},{"location":"observability/kubernetes/events/","title":"Kubernetes Events Visibility","text":"<p>Kubernetes events are ephemeral (stored for only 1 hour in etcd). To maintain visibility and enable historical analysis, events need to be exported to an observability backend.</p>"},{"location":"observability/kubernetes/events/#solutions-for-events-export","title":"Solutions for Events Export","text":""},{"location":"observability/kubernetes/events/#grafana-alloy-recommended-for-loki","title":"Grafana Alloy (Recommended for Loki)","text":"<p>Status: Active - Replacement for deprecated Grafana Agent (EOL Nov 2025)</p> <p>Grafana Alloy with the <code>loki.source.kubernetes_events</code> component tails events from the Kubernetes API and forwards them to Loki.</p>"},{"location":"observability/kubernetes/events/#kubernetes-event-exporter","title":"kubernetes-event-exporter","text":"<p>Status: Active - Maintained by resmoio (fork of deprecated opsgenie version)</p> <p>Exports events to multiple destinations: Loki, Elasticsearch, Kafka, Webhooks, etc.</p> <p>Bitnami Helm Chart: <code>bitnami/kubernetes-event-exporter</code></p>"},{"location":"observability/kubernetes/events/#fluent-bit","title":"Fluent Bit","text":"<p>Status: Active - Native kubernetes_events input plugin (v3.1+)</p> <p>Uses Kubernetes watch stream (not polling) to collect events.</p> <p>Important: Must be deployed as a Deployment with single replica, NOT as DaemonSet (no leader election).</p>"},{"location":"observability/kubernetes/events/#fluentd","title":"Fluentd","text":"<p>Status: Active - No native events input plugin</p> <p>Can output to Loki via <code>fluent-plugin-grafana-loki</code>, but requires another tool (like Fluent Bit or Event Exporter) to collect events first.</p> <p>Use case: Events processing pipeline (not collection)</p>"},{"location":"observability/kubernetes/events/#logstash-metricbeat","title":"Logstash + Metricbeat","text":"<p>Status: Active - Requires Metricbeat for events collection</p> <p>Metricbeat has a kubernetes event metricset that collects from the API, then forwards to Logstash for processing.</p> <p>Use case: Elastic stack processing pipeline, not for direct collection</p>"},{"location":"observability/kubernetes/events/#data-prepper","title":"Data Prepper","text":"<p>Status: Active - OpenSearch alternative to Logstash, no native events input</p> <p>Data Prepper is a server-side data collector for filtering, enriching, and transforming data for OpenSearch. Requires another tool (Fluent Bit, kubernetes-event-exporter, or OTel Collector) to collect events first.</p> <p>Use case: OpenSearch stack processing pipeline, not for direct collection</p>"},{"location":"observability/kubernetes/events/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>Status: Active - Vendor-neutral observability data collector with native Kubernetes events receivers</p> <p>Has two receivers for events collection:</p> <ul> <li>k8seventsreceiver - Dedicated Kubernetes events receiver</li> <li>k8sobjectsreceiver - General Kubernetes objects receiver (commonly used for events)</li> </ul> <p>Can export to multiple backends: Loki (otlphttp), Elasticsearch, OpenSearch, Prometheus, etc.</p> <p>Important: Deploy as single-replica Deployment (not DaemonSet)</p> <p>Helm chart preset: Use <code>kubernetesEvents</code> preset for quick setup</p>"},{"location":"observability/kubernetes/events/#kube-state-metrics-prometheus-only","title":"kube-state-metrics (Prometheus Only)","text":"<p>Status: Active</p> <p>Exposes limited event metrics for Prometheus (not full event details):</p> <ul> <li><code>kube_event_count</code> - Count of events</li> <li><code>kube_event_unique_events_total</code> - Unique events by reason</li> </ul> <p>Use case: High-level alerting, not event investigation</p>"},{"location":"observability/kubernetes/events/#deprecated-solutions","title":"Deprecated Solutions","text":"<ul> <li>EventRouter (vmware-archive) - Archived, no longer maintained</li> <li>Grafana Agent - EOL November 1, 2025, replaced by Grafana Alloy</li> <li>opsgenie/kubernetes-event-exporter - Deprecated, use resmoio fork</li> </ul>"},{"location":"observability/kubernetes/events/#best-practices","title":"Best Practices","text":"<ol> <li>Use JSON format - Faster LogQL queries than logfmt</li> <li>Filter namespaces - Reduce noise by watching specific namespaces</li> <li>Store in Loki - Events are text/structured data, not time-series metrics</li> <li>Set retention - Configure Loki retention based on compliance needs</li> <li>Use labels wisely - Namespace, type, reason are good labels; avoid pod names (high cardinality)</li> <li>Single replica deployment - Deploy Fluent Bit and OTel Collector as Deployment (not DaemonSet) to avoid duplicate event collection</li> </ol>"},{"location":"observability/kubernetes/events/#recommendation","title":"Recommendation","text":"<ul> <li>For Loki users: Use Grafana Alloy with <code>loki.source.kubernetes_events</code> (recommended) or Fluent Bit</li> <li>For vendor-neutral/multi-backend: Use OpenTelemetry Collector (supports Loki, Elasticsearch, OpenSearch, etc.)</li> <li>For multi-destination needs: Use resmoio/kubernetes-event-exporter</li> <li>For Elastic stack: Use Metricbeat + Logstash or kubernetes-event-exporter</li> <li>For OpenSearch stack: Use OpenTelemetry Collector, Fluent Bit + Data Prepper, or kubernetes-event-exporter</li> <li>For Prometheus metrics only: Use kube-state-metrics</li> </ul>"},{"location":"observability/kubernetes/events/#links","title":"Links","text":"<ul> <li>Grafana Alloy</li> <li>loki.source.kubernetes_events</li> <li>resmoio/kubernetes-event-exporter</li> <li>Fluent Bit kubernetes_events</li> <li>Fluent Bit Loki output</li> <li>Fluent Bit OpenSearch output</li> <li>OpenTelemetry Collector for Kubernetes</li> <li>OTel k8seventsreceiver</li> <li>OTel k8sobjectsreceiver</li> <li>kube-state-metrics</li> <li>Metricbeat Kubernetes event metricset</li> <li>OpenSearch Data Prepper</li> <li>Kubernetes Events API</li> </ul>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/","title":"Metrics server and prometheus adapter","text":"<p>kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"</p>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#metrics-server","title":"Metrics Server","text":"<p>A lightweight, short-term, in-memory store for resource usage metrics. It is specifically designed to provide metrics for the kubectl top commands and Kubernetes autoscaling.</p> <p>Purpose: Metrics Server is a cluster-wide aggregator of resource usage data. It collects metrics like CPU and memory usage from the kubelets and exposes them via the Kubernetes Metrics API. Use Cases: It is primarily used for Horizontal Pod Autoscaling (HPA), Vertical Pod Autoscaling (VPA), and the kubectl top command. Limitations: Metrics Server provides only the most recent metrics and does not store historical data.</p> <p>https://github.com/kubernetes-sigs/metrics-server</p> <p>\"Metrics Server is meant only for autoscaling purposes. For example, don't use it to forward metrics to monitoring solutions, or as a source of monitoring solution metrics. In such cases please collect metrics from Kubelet /metrics/resource endpoint directly.\"</p> <p>Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.</p> <p>Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler. Metrics API can also be accessed by kubectl top, making it easier to debug autoscaling pipelines.</p>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#prometheus-adapter","title":"Prometheus Adapter","text":"<p>Used to expose custom metrics from Prometheus to Kubernetes. It allows you to use Prometheus metrics for Kubernetes autoscaling (HPA) and other purposes.</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter</p> <p>Purpose: Prometheus Adapter is used to expose custom metrics and resource metrics from Prometheus to the Kubernetes Metrics API. It allows you to use Prometheus as a source for metrics used by Kubernetes components like HPA. Use Cases: It is used to enable HPA based on custom metrics collected by Prometheus, as well as to expose resource metrics collected by Prometheus. Advantages: Prometheus Adapter can provide historical data and more complex metrics than Metrics Server.</p>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#metrics-server-vs-prometheus-adapter","title":"Metrics server vs prometheus adapter","text":"<p>Metrics server and prometheus adapter are mutually exclusive because they both provide</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/issues/561</p>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#kube-state-metrics","title":"Kube State Metrics","text":"<p>https://github.com/kubernetes/kube-state-metrics</p>"},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#cadvisor","title":"CAdvisor","text":""},{"location":"observability/kubernetes/metrics-server-prometheus-adapter/#node-exporter","title":"Node exporter","text":"<p>https://github.com/prometheus/node_exporter</p>"},{"location":"observability/kubernetes/metrics/","title":"Metrics","text":"<p>There are 3 ways to get kubernetes metrics:</p> <ul> <li>The resource metrics api</li> <li>The custom metrics api</li> <li>The external metrics api</li> </ul>"},{"location":"observability/kubernetes/metrics/#resource-metrics-api-metrics-api","title":"Resource Metrics API (metrics api)","text":"<p>The resource metrics api provides basic resource usage metrics (cpu/memory) for pods and nodes.</p> <p>The API is implemented by metrics-server and prometheus-adapter and it has been created for the following purposes:</p> <ul> <li>Make kubectl top command work</li> <li>Horizontal pod autoescaler using cpu/memory</li> <li>Vertical pod autoescaler</li> </ul> <p>The metrics are collected from kubelet and published under /apis/metrics.k8s.io/v1beta1. For example we can get those metrics this way</p> <pre><code>kubectl get NodeMetrics # /apis/metrics.k8s.io/v1beta1/nodes?limit=500\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes | jq -r\n</code></pre> <pre><code>kubectl get PodMetrics -A # /apis/metrics.k8s.io/v1beta1/pods?limit=500\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/pods | jq -r\n</code></pre> <p>We can get who is providing that metrics with</p> <pre><code>kubectl get apiservice v1beta1.metrics.k8s.io\n</code></pre> <ul> <li>Kubernetes Metrics (v1beta1)</li> </ul> <p>https://kubernetes.io/docs/reference/external-api/metrics.v1beta1/</p> <ul> <li>Resource metrics pipeline</li> </ul> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/</p>"},{"location":"observability/kubernetes/metrics/#metrics-server","title":"Metrics server","text":"<p>It is a lightweight addon that collects \"Resource metrics\" from kubelet and exposes them in the metrics api.</p> <p>It is designed for auotoescaling, not for monitoring purposes, the usage metrics are not accurated. Also it must not be used for horizontal autoscaling based on other resources than CPU/Memory.</p> <ul> <li>Metrics server in github</li> </ul> <p>https://github.com/kubernetes-sigs/metrics-server</p>"},{"location":"observability/kubernetes/metrics/#custom-metrics-api","title":"Custom metrics API","text":"<p>The Custom Metrics API allows you to define and use custom metrics for autoscaling and monitoring purposes. These metrics can be collected from various sources, such as Prometheus, and are used for more advanced autoscaling scenarios.</p> <p>https://kubernetes.io/docs/reference/external-api/custom-metrics.v1beta2/</p>"},{"location":"observability/kubernetes/metrics/#external-metrics-api","title":"External metrics API","text":"<p>The External Metrics API allows you to use metrics from sources external to the Kubernetes cluster for autoscaling purposes. These metrics can be anything that is relevant to your application, such as metrics from a cloud provider or an external monitoring system.</p> <p>https://kubernetes.io/docs/reference/external-api/external-metrics.v1beta1/</p>"},{"location":"observability/kubernetes/metrics/#other-tools","title":"Other tools","text":""},{"location":"observability/kubernetes/metrics/#prometheus-adapter","title":"Prometheus adapter","text":"<p>The Prometheus Adapter is a component that registers itself using the Kubernetes API aggregation layer so it can serve metrics apis querying a prometheus instance. This allows you to query custom metrics using the Kubernetes API, similar to how you query built-in metrics.</p> <p>For example it can serve resource metrics, replacing metrics server, using prometheus metrics as source of truth. Because we can configure the metrics we want to create querying prometheus, we can use horizontal pod autoescaling features with prometheus queries.</p> <ul> <li>Prometheus adapter in github</li> </ul> <p>https://github.com/kubernetes-sigs/prometheus-adapter</p> <ul> <li>Prometheus adapter helm chart readme</li> </ul> <p>https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-adapter/README.md</p>"},{"location":"observability/kubernetes/metrics/#kube-state-metrics","title":"Kube state metrics","text":"<p>Kube state metrics is a kubernetes addon that talks with the kubernetes api and exposes information about the state of the some objects in the cluster. Kube state metrics holds an entire snapshot of Kubernetes state in memory and continuously generates new metrics based off of it. It shows information like:</p> <ul> <li>labels</li> <li>annotations</li> <li>startup times</li> <li>termination times</li> <li>status</li> <li>phase</li> <li>namespace</li> <li>name of the pod</li> </ul> <p>There is default list of the objects that kube-state-metrics manages but it can be modified.</p> <p>That exposed metrics can be sent to a prometheus instance. Some distributions like kube-prometheus and kube-prometheus-stack install kube-state-metrics so, if you are using them , you dont need to deploy kube-state-metrics again. The official helm chart is maintained in the promethus-community repo.</p> <ul> <li>Metrics for Kubernetes Object States</li> </ul> <p>https://kubernetes.io/docs/concepts/cluster-administration/kube-state-metrics/</p> <ul> <li>Kube state metrics in github</li> </ul> <p>https://github.com/kubernetes/kube-state-metrics</p>"},{"location":"observability/kubernetes/metrics/#node-exporter","title":"Node exporter","text":"<p>pending https://github.com/prometheus/node_exporter https://prometheus.io/docs/guides/node-exporter/</p>"},{"location":"observability/kubernetes/metrics/#other-links","title":"Other links","text":"<ul> <li>Kubernetes metrics API type definitions and clients</li> </ul> <p>https://github.com/kubernetes/metrics</p> <ul> <li>Kubernetes api aggregation layer</li> </ul> <p>https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/aggregation.md</p> <ul> <li>Instrumentation</li> </ul> <p>https://kubernetes.io/docs/reference/instrumentation/</p> <ul> <li>Tools for Monitoring Resources</li> </ul> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-usage-monitoring/</p> <ul> <li>Metrics For Kubernetes System Components</li> </ul> <p>https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/</p> <ul> <li>Prometheus Community Kubernetes Helm Charts</li> </ul> <p>https://github.com/prometheus-community/helm-charts</p>"},{"location":"observability/kubernetes/sources/","title":"Sources","text":""},{"location":"observability/kubernetes/sources/#kubernetes-native-metrics","title":"Kubernetes native metrics","text":"<ul> <li>Metrics For Kubernetes System Components</li> </ul> <p>https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/</p> <ul> <li>Kubernetes Metrics Reference</li> </ul> <p>https://kubernetes.io/docs/reference/instrumentation/metrics/</p> <ul> <li>Kubernetes Component SLI Metrics</li> </ul> <p>https://kubernetes.io/docs/reference/instrumentation/slis/</p>"},{"location":"observability/kubernetes/sources/#kubelet","title":"Kubelet","text":"<p>Kubelet listens https requests by default in the 10250 port, including the metrics. The http insecure port is disabled by default and it can be enabled with the readOnlyPort setting</p> <p>The cadvisor port is 4194</p> <p>Prometheus operator creates a service called kubelet with the following argument</p> <pre><code>--kubelet-service=kube-system/kubelet\n</code></pre> <p>These are the metrics exposed</p> <ul> <li>/metrics</li> <li>/metrics/cadvisor</li> <li>/metrics/resource</li> <li>/metrics/probes</li> <li>/metrics/slis</li> </ul> <p>Cadvisor (kubelet) https://github.com/google/cadvisor</p> <p>cAdvisor \u2013 a Docker daemon metrics \u2013 containers monitoring</p> <p>Provides resource usage and performance characteristics of running containers. It is integrated into the Kubelet in Kubernetes and is responsible for collecting, aggregating, processing, and exporting information about running containers.</p> <p>Note that kubelet also exposes metrics in /metrics/cadvisor, /metrics/resource and /metrics/probes endpoints. Those metrics do not have the same lifecycle.</p> <p>https://github.com/google/cadvisor</p>"},{"location":"observability/kubernetes/sources/#kube-state-metrics","title":"Kube state metrics","text":"<p>https://github.com/kubernetes/kube-state-metrics kube-state-metrics \u2013 deployments, pods, nodes</p> <p>Provides metrics about the state of Kubernetes objects (e.g., deployments, nodes, pods), but not resource usage metrics.</p> <p>Some considerations:</p> <ul> <li> <p>They by default they are deployed in an insecured way. It is a common workaround to add endpoint protection with kube-rbac-proxy</p> </li> <li> <p>The helm chart is located here https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics</p> </li> <li> <p>All thee metrics start with \"kube_\" prefix. We can explore the metrics here https://github.com/kubernetes/kube-state-metrics/blob/main/docs/README.md#exposed-metrics</p> </li> <li> <p>By default kube-state-metrics creates the \"namespace\" label with the value of the namespace where it is deployed and the \"exported_namespace\" label with the namespace of the observed cotnainer. With \"honorLabels: true\" the value of \"namespace\" will be the namespace of the observed container.</p> </li> </ul> <p>Kubestate metrics</p>"},{"location":"observability/kubernetes/sources/#metrics-server","title":"Metrics server","text":"<p>https://github.com/kubernetes-sigs/metrics-server</p> <p>Provides resource usage metrics (e.g., CPU, memory) for Kubernetes objects</p>"},{"location":"observability/kubernetes/sources/#prometheus-adapter","title":"Prometheus adapter","text":"<p>https://github.com/kubernetes-sigs/prometheus-adapter</p> <p>Allows custom metrics to be exposed to the Kubernetes API, but it relies on Prometheus to collect the metrics.</p>"},{"location":"observability/kubernetes/sources/#node-exporter","title":"Node exporter","text":"<p>node-exporter: EC2 instances metrics \u2013 CPU, memory, network</p>"},{"location":"observability/loki/perlas/","title":"Perlas","text":""},{"location":"observability/loki/perlas/#grafana-loki-no-inicia-y-se-queda-en-containercreating","title":"Grafana loki no inicia y se queda en ContainerCreating","text":"<p>En modo singlebinary</p> <pre><code>kubectl delete pod/loki-0 persistentvolumeclaim/storage-loki-0 --grace-period=0 --force\n</code></pre>"},{"location":"observability/opentelemetry/0-index/","title":"Open Telemetry","text":""},{"location":"observability/opentelemetry/0-index/#components","title":"Components","text":""},{"location":"observability/opentelemetry/0-index/#opentelemetry-collector","title":"Opentelemetry Collector","text":"<p>Vendor-agnostic way to receive, process and export telemetry data.</p> <p>https://opentelemetry.io/docs/collector/ https://opentelemetry.io/docs/platforms/kubernetes/collector/components/</p>"},{"location":"observability/opentelemetry/0-index/#time-series-database","title":"Time Series database","text":"<p>Examples:</p>"},{"location":"observability/opentelemetry/0-index/#trace-database","title":"Trace Database","text":""},{"location":"observability/opentelemetry/0-index/#column-store","title":"Column Store","text":""},{"location":"observability/opentelemetry/0-index/#deployment","title":"Deployment","text":"<ul> <li>Using Opentelemetry operator via raw yaml</li> </ul> <p>https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml</p> <ul> <li>Using Opentelemetry operator via helm chart</li> </ul> <p>https://opentelemetry.io/docs/platforms/kubernetes/helm/operator/</p> <ul> <li>Using Opentelemetry Collector</li> </ul> <p>https://opentelemetry.io/docs/collector/installation/</p> <ul> <li>Using OpenTelemetry Collector Kubernetes Distro</li> </ul> <p>https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions/otelcol-k8s https://opentelemetry.io/docs/platforms/kubernetes/collector/components/</p>"},{"location":"observability/opentelemetry/env/","title":"Environment variables","text":"<p>OTEL_EXPORTER_OTLP_PROTOCOL</p> <p>OTEL_EXPORTER_OTLP_ENDPOINT</p> <p>OTEL_EXPORTER_OTLP_HEADERS</p>"},{"location":"observability/prometheus/ingress-certificates/","title":"Metrics for ingress and certificates","text":""},{"location":"observability/prometheus/ingress-certificates/#tools","title":"Tools","text":"<ul> <li>Kube state metrics</li> </ul> <p>It gets info about kubernetes secrets but it needs a customresourceconfig to check the expiration field</p> <p>https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md</p> <ul> <li>Blackbox exporter</li> </ul> <p>The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP, ICMP and gRPC.</p> <p>https://github.com/prometheus/blackbox_exporter</p> <ul> <li>X.509 Certificate Exporter</li> </ul> <p>https://github.com/enix/x509-certificate-exporter</p> <p>Generate metrics for certificates, with focus on expiration</p>"},{"location":"observability/prometheus/links/","title":"Links","text":""},{"location":"observability/prometheus/links/#prometheus-operator","title":"Prometheus operator","text":"<p>https://prometheus-operator.dev/</p>"},{"location":"observability/prometheus/links/#prometheus-operator-api-reference","title":"Prometheus Operator Api reference","text":"<p>https://prometheus-operator.dev/docs/api-reference/api/</p>"},{"location":"observability/prometheus/links/#oficial-prometheus-registry","title":"Oficial prometheus registry","text":"<p>https://quay.io/prometheus/prometheus</p> <p>https://samber.github.io/awesome-prometheus-alerts/rules</p> <p>https://github.com/zalando-incubator/kube-metrics-adapter</p>"},{"location":"observability/prometheus/metric-types/","title":"Metrics types","text":"<p>Prometheus offer 4 core metric types</p>"},{"location":"observability/prometheus/metric-types/#counter","title":"Counter","text":"<p>A counter is a prometheus metric that represents a numeric value that can increase (cumulative) o be reset to zero on restart.</p>"},{"location":"observability/prometheus/metric-types/#gauge","title":"Gauge","text":"<p>A gauge is a a prometheus metric that represents a numeric value that can increase o decrease.</p>"},{"location":"observability/prometheus/metric-types/#histogram","title":"Histogram","text":"<p>An histogram is a prometheus metric that permits to see the evolution of a metric.</p>"},{"location":"observability/prometheus/metric-types/#buckets","title":"Buckets","text":"<p>Histograms count observations in predefined buckets. Each bucket represents a range of values. Histogram buckets are useful for understanding the performance and latency of web services by providing a detailed breakdown of request durations.</p> <pre><code>&lt;basename&gt;_bucket{le=\"&lt;upper inclusive bound&gt;\"}\n</code></pre> <p>This example tells asks for HTTP requests that had a duration of 0.1 seconds or less.</p> <pre><code>http_request_duration_seconds_bucket{le=\"0.1\"}\n</code></pre>"},{"location":"observability/prometheus/metric-types/#sum-of-observations","title":"Sum of Observations","text":"<p>Histograms also keep a sum of all observed values, which can be used to calculate the average</p> <pre><code>&lt;basename&gt;_sum\n</code></pre> <p>This example is tracking the total sum of the durations of all HTTP requests in seconds</p> <pre><code>http_request_duration_seconds_sum\n</code></pre>"},{"location":"observability/prometheus/metric-types/#cumulative-counts","title":"Cumulative Counts","text":"<p>Histograms maintain a cumulative count of observations for each bucket. The count of events that have been observed</p> <pre><code>&lt;basename&gt;_count\n</code></pre> <p>or</p> <pre><code>&lt;basename&gt;_bucket{le=\"+Inf\"}\n</code></pre> <p>The le stands for \"less than or equal to,\" and \"+Inf\" (positive infinity) means that this bucket includes all HTTP requests, regardless of their duration.</p> <p>Lets see this example</p> <pre><code>http_request_duration_seconds_bucket{le=\"+Inf\"}\nhttp_request_duration_seconds_count\n</code></pre> <p>The metric name http_request_duration_seconds_count indicates that it is tracking the total number of HTTP requests that have been observed.</p> <p>It is useful for calculating the average request duration when combined with the corresponding sum metric (http_request_duration_seconds_sum). For example, dividing the sum of durations by the count gives the average duration of an HTTP request.</p> <p>This gives the average duration of etcd commits called by backend</p> <pre><code>etcd_disk_backend_commit_duration_seconds_sum/etcd_disk_backend_commit_duration_seconds_count\n</code></pre> <p>Use the histogram_quantile() function to calculate quantiles from histograms or even aggregations of histograms. A histogram is also suitable to calculate an Apdex score. When operating on buckets, remember that the histogram is cumulative. See histograms and summaries for details of histogram usage and differences to summaries.</p>"},{"location":"observability/prometheus/metric-types/#know-the-metric-type","title":"Know the metric type","text":""},{"location":"observability/prometheus/metric-types/#via-prometheus-ui","title":"Via prometheus UI","text":"<p>To know the metric type in the prometheus UI, we must go to Query &gt; Explain</p>"},{"location":"observability/prometheus/metric-types/#summary","title":"Summary","text":"<p>pending</p>"},{"location":"observability/prometheus/metric-types/#links","title":"Links","text":"<ul> <li>Metrics types</li> </ul> <p>https://prometheus.io/docs/concepts/metric_types/</p> <ul> <li>Histograms and summaries</li> </ul> <p>https://prometheus.io/docs/practices/histograms/</p>"},{"location":"observability/prometheus/operator-alerting/","title":"Prometheus operator and alerting","text":""},{"location":"observability/prometheus/operator-alerting/#configure-the-prometheus-resource","title":"Configure the prometheus resource","text":"Section Explanation spec.alerting Configure the alertmanager endpoints to send the alerts <p>There are a lot of other prometheus and kubernetes settings we can configure (replicas, retention, persistent storage, resources,...)</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-alertmanager-resource","title":"Setup the alertmanager resource","text":"<p>This will deploy an alertmanager statefulset and its settings</p> <ul> <li>metadata.namespace</li> </ul> <p>pending</p> <ul> <li>metadata.labels</li> </ul> <p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#alertmanager-configuration","title":"Alertmanager configuration","text":"<p>There are 3 ways to configure alertmanager</p> <ul> <li>spec.configSecret  </li> </ul> <p>With this we can choose a Kubernetes secret that contains a native alertmanager configuration (global) in the alertmanager.yaml key.</p> <pre><code>kubectl create secret generic myalertmanagerconfig --from-file=alertmanager.yaml=myfile.yaml\n</code></pre> <p>And in the alertmanager resource</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  configSecret: myalertmanagerconfig\n</code></pre> <p>The default value is alertmanager-name_of-the_alertmanager_instance. If the alertmanager.yaml key does not exist or a secret is not defined, a default configuration will deployed dropping alert notifications.</p> <ul> <li>spec.alertmanagerConfiguration  </li> </ul> <p>Experimental feature that takes precedence over the configSecret field as a global configuration. We can choose an alertmanagerconfig kubernetes resource.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  alertmanagerConfiguration:\n    name: myalertmanagerconfig\n</code></pre> <ul> <li>spec.alertmanagerConfigSelector</li> </ul> <p>Choose what labels should have an alertmanagerconfig resource to be selected for to merge and configure Alertmanager with.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  alertmanagerConfigSelector:\n    matchLabels:\n      mylabel: myvalue\n</code></pre> <ul> <li>spec.alertmanagerConfigNamespaceSelector</li> </ul> <p>Choose in what namespaces search for alertmanagerconfig resources to be selected for to merge and configure Alertmanager with.</p> <ul> <li>spec.alertmanagerConfigMatcherStrategy</li> </ul> <p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-alertmanagerconfig-resource","title":"Setup the alertmanagerconfig resource","text":"<p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-prometheusrule-resources","title":"Setup the prometheusrule resources","text":"<p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#links","title":"Links","text":"<ul> <li>Prometheus: Alerting overwiew</li> </ul> <p>https://prometheus.io/docs/alerting/latest/overview/</p> <ul> <li>Prometheus: Alerting rules</li> </ul> <p>https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/</p> <ul> <li>Prometheus Operator: Alerting Routes</li> </ul> <p>https://prometheus-operator.dev/docs/developer/alerting/</p> <ul> <li>Why does alertmanagerconfigs automatically add a namespace matcher</li> </ul> <p>https://github.com/prometheus-operator/prometheus-operator/discussions/3733</p>"},{"location":"observability/prometheus/operator-prometheus/","title":"Prometheus operator instance","text":"<p>This prometheus kubernetes resource will deploy a prometheus statefulset and its settings under \"spec\"</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\nspec:\n</code></pre> <p>There are some the settings we can configure</p>"},{"location":"observability/prometheus/operator-prometheus/#choose-what-to-discover","title":"Choose what to discover","text":"<p>The prometheus operator also permits to define some namespaced kubernetes resources like:</p> <ul> <li>PodMonitor (scrape metrics from a group of pods)</li> <li>Probe (how to scrape metrics from prober exporters such as the blackbox exporter)</li> <li>PrometheusRule (defines alerting and recording rules)</li> <li>ScrapeConfig (currently at Alpha level)</li> <li>ServiceMonitor (scrape metrics from a group of services)</li> </ul> <p>We must choose what ServiceMonitors, PodMonitors, Probes and ScrapeConfigs will be related with our prometheus instance. With the following settings we can select by labels the namespaces to discover that resources.</p> <ul> <li>spec.podMonitorNamespaceSelector</li> <li>spec.probeNamespaceSelector</li> <li>spec.ruleNamespaceSelector</li> <li>spec.scrapeConfigNamespaceSelector</li> <li>spec.serviceMonitorNamespaceSelector</li> </ul> <p>An empty label \"{}\" selector matches all namespaces</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\nspec:\n  podMonitorNamespaceSelector: {}\n  probeNamespaceSelector: {}\n  ruleNamespaceSelector: {}\n  scrapeConfigNamespaceSelector: {}\n  serviceMonitorNamespaceSelector: {}\n</code></pre> <p>A \"null\" label selector matches the namespace where the prometheus instance has been deployed</p> <p>Also we can filter by labels in those resources. Only the resources created with that labels defined here will be related with our prometheus instance.</p> <ul> <li>spec.podMonitorSelector</li> <li>spec.probeSelector</li> <li>spec.scrapeConfigSelector</li> <li>spec.serviceMonitorSelector</li> <li>spect.ruleSelector</li> </ul> <p>Again, an empty label \"{}\" selector matches all objects. A \"null\" label selector matches no objects.</p>"},{"location":"observability/prometheus/operator-prometheus/#alertmanager","title":"Alertmanager","text":"<p>With spec.alerting.alertmanagers we can define alertmanager endpoints where to send alerts</p>"},{"location":"observability/prometheus/operator-prometheus/#other-settings","title":"Other settings","text":"<ul> <li>spec.version and spec.image</li> </ul> <p>With spec.version we can choose the prometheus release we want to deploy. It is neccesary to ensure the Prometheus Operator knows which version of Prometheus is being configured. With spec.image we can configure the container image.</p> <p>The operator itself has a default release for both settings</p> <p>example:</p> <pre><code>spec:\n    image: quay.io/prometheus/prometheus:v3.1.0\n    version: v3.1.0\n</code></pre> <ul> <li>spec.replicas</li> </ul> <p>With spec.replicas we can configure the name of instances we want in our prometheus statefulset. The default number is 1</p> <ul> <li>spec.storage, spec.volumes and spec.volumeMounts</li> </ul> <p>We can configure the persistence here</p> <ul> <li>spec.retention and spec.retentionSize</li> </ul> <p>Limit the data will be stored. Retention by date (24h default) and retentionSize by size.</p> <ul> <li>spec.logLevel and spec.logFormat</li> </ul> <p>We can configure the logformat and change the verbosity of the prometheus and config-reloader containers</p> <ul> <li> <p>spec.scrapeInterval, spec.scrapeTimeout and spec.scrapeProtocols</p> </li> <li> <p>spec.externalUrl</p> </li> </ul> <p>If we want to expose prometheus, for example, via ingress, we must configure this to generate correct URLs</p> <ul> <li>spec.resources</li> </ul> <p>Tune the kubernetes requests and limits of the prometheus instance</p> <ul> <li>spec.priorityClassName</li> </ul> <p>Give a priorityclass to the prometheus pods</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/","title":"Prometheus Long-Term Storage","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#overview","title":"Overview","text":"<p>Prometheus, while excellent for real-time monitoring and short-term metric storage, has inherent limitations when it comes to long-term data retention and horizontal scalability. By default, Prometheus stores data locally and is designed for deployments lasting weeks to months rather than years.</p> <p>This document explores five major open-source solutions that extend Prometheus capabilities for long-term storage: Mimir, Cortex, Thanos, VictoriaMetrics, and GreptimeDB.</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#governance-and-company-dependency-risk","title":"Governance and Company Dependency Risk","text":"Solution Risk Level Primary Controller Governance Model Community Independence Mimir \ud83d\udfe1 Medium-High Grafana Labs (single company) Corporate-controlled Limited - Grafana Labs drives roadmap Cortex \ud83d\udfe2 Low CNCF (vendor-neutral) Community governance High - Multi-vendor collaboration Thanos \ud83d\udfe2 Low-Medium CNCF (vendor-neutral) Community governance High - No single company dominance VictoriaMetrics \ud83d\udd34 High VictoriaMetrics company Corporate-controlled Low - Single company development GreptimeDB \ud83d\udd34 High Greptime Inc. Corporate-controlled Low - Single company development"},{"location":"observability/prometheus/prometheus-long-term-storage/#solution-architectures","title":"Solution Architectures","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#grafana-mimir","title":"Grafana Mimir","text":"<p>Origin: Fork of Cortex by Grafana Labs (2022) Architecture: Microservices-based with horizontal scaling Repository: grafana/mimir \u2b50 4.1k+ stars, 500+ contributors CNCF Status: Not a CNCF project (Grafana Labs proprietary) Key Contributors: Grafana Labs, Microsoft, Red Hat, Bloomberg, GitLab</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#key-components","title":"Key Components","text":"<ul> <li>Distributor: Receives metrics and forwards to ingesters</li> <li>Ingester: Writes metrics to storage and serves recent queries</li> <li>Store Gateway: Queries historical data from object storage</li> <li>Query Frontend: Optimizes and splits queries</li> <li>Compactor: Compacts and processes blocks in object storage</li> <li>Ruler: Evaluates recording and alerting rules</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-benefits","title":"Architecture Benefits","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502 Distributor \u2502\u2500\u2500\u2500\u25b6\u2502  Ingester   \u2502\n\u2502   (Remote   \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502   Write)    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502Query Frontend\u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502             \u2502    \u2502             \u2502    \u2502 (S3/GCS/etc)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#features","title":"Features","text":"<ul> <li>Multi-Tenancy: Native support with tenant isolation</li> <li>Streaming: Real-time query results streaming</li> <li>Advanced Compaction: Intelligent block compaction strategies</li> <li>Autoscaling: Kubernetes-native autoscaling support</li> <li>Monitoring: Extensive built-in observability</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex","title":"Cortex","text":"<p>Origin: CNCF project originally developed by Weaveworks Architecture: Microservices-based, highly configurable Repository: cortexproject/cortex \u2b50 5.5k+ stars, 800+ contributors CNCF Status: Graduated project (2020) Key Contributors: Weaveworks, Grafana Labs, Red Hat, Google, AWS, Microsoft</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex-components","title":"Cortex Components","text":"<ul> <li>Distributor: Load balances incoming metrics</li> <li>Ingester: Stores recent metrics in memory and periodically flushes to storage</li> <li>Querier: Handles PromQL queries</li> <li>Query Frontend: Splits and caches queries</li> <li>Store Gateway: Reads historical data from object storage</li> <li>Compactor: Compacts blocks and handles retention</li> <li>Ruler: Evaluates rules and generates alerts</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-pattern","title":"Architecture Pattern","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502 Distributor \u2502\u2500\u2500\u2500\u25b6\u2502  Ingester   \u2502\n\u2502 (Remote     \u2502    \u2502             \u2502    \u2502 (Ring-based)\u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Query     \u2502\u25c0\u2500\u2500\u2500\u2502   Querier   \u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502  Frontend   \u2502    \u2502             \u2502    \u2502   Backend   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex-features","title":"Cortex Features","text":"<ul> <li>Flexible Deployment: Multiple deployment modes (single binary, microservices)</li> <li>Multi-Tenancy: Comprehensive tenant isolation</li> <li>Ring-based Architecture: Consistent hashing for load distribution</li> <li>Configurable Storage: Multiple storage backend options</li> <li>CNCF Graduated: Mature project with strong community support</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos","title":"Thanos","text":"<p>Origin: Developed by Improbable, now CNCF project Architecture: Sidecar-based approach with global querying Repository: thanos-io/thanos \u2b50 13k+ stars, 1,000+ contributors CNCF Status: Incubating project (2019) Key Contributors: Improbable, Red Hat, Google, Polar Signals, GitLab</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos-components","title":"Thanos Components","text":"<ul> <li>Sidecar: Connects to Prometheus instances and uploads blocks</li> <li>Store Gateway: Provides unified interface to historical data</li> <li>Query: Global query layer across multiple Prometheus instances</li> <li>Query Frontend: Query optimization and caching</li> <li>Compactor: Compacts and downsamples historical data</li> <li>Receiver: Ingests metrics via remote write (alternative to sidecar)</li> <li>Ruler: Evaluates rules on historical data</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-model","title":"Architecture Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus A\u2502\u25c0\u2500\u2500\u25b6\u2502Thanos Sidecar\u2502   \u2502Object Storage\u2502\n\u2502             \u2502    \u2502             \u2502\u2500\u2500\u25b6 \u2502   Backend   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502             \u2502\n                                      \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502             \u2502\n\u2502 Prometheus B\u2502\u25c0\u2500\u2500\u25b6\u2502Thanos Sidecar\u2502\u2500\u2500\u25b6\u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                            \u25b2\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502Thanos Query \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502             \u2502    \u2502 (Global)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos-features","title":"Thanos Features","text":"<ul> <li>Global View: Query across multiple Prometheus instances</li> <li>Minimal Changes: Requires minimal changes to existing Prometheus setups</li> <li>Downsampling: Automatic data downsampling for long-term storage</li> <li>Deduplication: Automatic deduplication of metrics</li> <li>Multi-Cloud: Works across different cloud providers and on-premises</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics","title":"VictoriaMetrics","text":"<p>Origin: Developed by VictoriaMetrics team, focused on performance and cost efficiency Architecture: Single binary or cluster mode with emphasis on resource efficiency Repository: VictoriaMetrics/VictoriaMetrics \u2b50 12k+ stars, 400+ contributors CNCF Status: Not a CNCF project (independent open-source) Key Contributors: VictoriaMetrics, Individual contributors, Community-driven development</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-components","title":"VictoriaMetrics Components","text":"<p>Single Binary Mode:</p> <ul> <li>All-in-One: Complete solution in a single binary for smaller deployments</li> <li>Embedded Storage: Built-in time series database optimized for compression</li> <li>HTTP API: Prometheus-compatible API for seamless integration</li> </ul> <p>Cluster Mode:</p> <ul> <li>VMStorage: Storage nodes handling data persistence and queries</li> <li>VMInsert: Ingestion nodes for distributing incoming metrics</li> <li>VMSelect: Query nodes for handling PromQL queries</li> <li>VMAgent: Lightweight agent for metrics collection and remote write</li> <li>VMAlert: Alerting component compatible with Prometheus rules</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-architecture","title":"VictoriaMetrics Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502  VMAgent    \u2502\u2500\u2500\u2500\u25b6\u2502  VMInsert   \u2502\n\u2502 (Remote     \u2502    \u2502(Collection) \u2502    \u2502 (Ingestion) \u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502  VMSelect   \u2502\u25c0\u2500\u2500\u2500\u2502 VMStorage   \u2502\n\u2502             \u2502    \u2502  (Query)    \u2502    \u2502(Persistence)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-features","title":"VictoriaMetrics Features","text":"<ul> <li>High Performance: Up to 20x better performance than Prometheus</li> <li>Resource Efficiency: Minimal CPU and memory usage</li> <li>Superior Compression: 10x better compression ratios than Prometheus</li> <li>Prometheus Compatibility: Drop-in replacement with full PromQL support</li> <li>Multi-Tenancy: Built-in tenant isolation in cluster mode</li> <li>Downsampling: Automatic data downsampling for long-term retention</li> <li>Backfilling: Support for importing historical data</li> <li>Stream Aggregation: Real-time metric aggregation capabilities</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb","title":"GreptimeDB","text":"<p>Origin: Developed by Greptime team, modern cloud-native time series database Architecture: SQL-based time series database with storage/compute separation Repository: GreptimeTeam/greptimedb \u2b50 4.2k+ stars, 300+ contributors CNCF Status: Not a CNCF project (independent open-source) Key Contributors: Greptime Inc., PingCAP, TiKV contributors, Community developers</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-components","title":"GreptimeDB Components","text":"<p>Core Architecture:</p> <ul> <li>Frontend: SQL query layer and protocol handlers (MySQL, PostgreSQL, PromQL)</li> <li>Datanode: Storage engine for time series data with advanced compression</li> <li>Metanode: Cluster metadata management and coordination</li> <li>Object Storage: S3-compatible storage backend for data persistence</li> </ul> <p>Deployment Modes:</p> <ul> <li>Standalone: Single binary for development and small deployments</li> <li>Cluster: Distributed mode with separate frontend, datanode, and metanode</li> <li>Cloud: Managed service with automatic scaling and optimization</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-architecture","title":"GreptimeDB Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502  Frontend   \u2502\u2500\u2500\u2500\u25b6\u2502  Datanode   \u2502\n\u2502 (Remote     \u2502    \u2502 (PromQL)    \u2502    \u2502 (Storage)   \u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502  Frontend   \u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502             \u2502    \u2502 (Query)     \u2502    \u2502 (S3/GCS)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-features","title":"GreptimeDB Features","text":"<ul> <li>SQL + PromQL: Native SQL support with &gt;90% PromQL compatibility</li> <li>Exceptional Performance: 5x more resource efficient than Mimir</li> <li>Superior Compression: Advanced compression algorithms for cost optimization</li> <li>Elastic Scaling: Independent scaling of storage and compute resources</li> <li>Multi-Protocol: MySQL, PostgreSQL, PromQL, and InfluxDB protocols</li> <li>Cloud-Native: Kubernetes-native with operator support</li> <li>Time Travel: Historical data queries with SQL temporal functions</li> <li>Real-time Analytics: Built-in stream processing capabilities</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#deployment-complexity","title":"Deployment Complexity","text":"Aspect Mimir Cortex Thanos VictoriaMetrics GreptimeDB Setup Complexity Medium High Low-Medium Low Low Operational Overhead Medium High Low Very Low Low Prometheus Changes Remote write only Remote write only Minimal (sidecar) Remote write only Remote write only Learning Curve Medium High Low-Medium Low Low-Medium"},{"location":"observability/prometheus/prometheus-long-term-storage/#scalability-performance","title":"Scalability &amp; Performance","text":"Feature Mimir Cortex Thanos VictoriaMetrics GreptimeDB Horizontal Scaling Excellent Excellent Good Excellent Excellent Query Performance High High Medium-High Very High Very High Ingestion Rate Very High High Medium Exceptional Exceptional Memory Efficiency Optimized Good Good Exceptional Exceptional Storage Efficiency High High Very High (downsampling) Exceptional Exceptional"},{"location":"observability/prometheus/prometheus-long-term-storage/#features-capabilities","title":"Features &amp; Capabilities","text":"Feature Mimir Cortex Thanos VictoriaMetrics GreptimeDB Multi-Tenancy \u2705 Advanced \u2705 Advanced \u2705 Basic \u2705 Advanced \u2705 Advanced Global Querying \u2705 \u2705 \u2705 Excellent \u2705 \u2705 Deduplication \u2705 \u2705 \u2705 Advanced \u2705 \u2705 Downsampling \u2705 \u2705 \u2705 Automatic \u2705 Automatic \u2705 Automatic Rule Evaluation \u2705 \u2705 \u2705 \u2705 \u2705 Alerting \u2705 \u2705 \u2705 \u2705 \u2705 Stream Processing \u2705 \u274c \u274c \u2705 Advanced \u2705 Native"},{"location":"observability/prometheus/prometheus-long-term-storage/#storage-retention","title":"Storage &amp; Retention","text":"Aspect Mimir Cortex Thanos VictoriaMetrics GreptimeDB Object Storage S3, GCS, Azure S3, GCS, Azure, Swift S3, GCS, Azure, Swift S3, GCS, Azure, Local S3, GCS, Azure Compression Excellent Good Excellent Exceptional Exceptional Retention Policies Flexible Flexible Flexible Very Flexible Very Flexible Block Management Advanced Standard Advanced Optimized Advanced"},{"location":"observability/prometheus/prometheus-long-term-storage/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-mimir-when","title":"Choose Mimir When","text":"<ul> <li>Modern Deployments: Starting fresh or modernizing existing setups</li> <li>High Performance Required: Need maximum query and ingestion performance</li> <li>Grafana Ecosystem: Already using Grafana and want tight integration</li> <li>Multi-Tenancy: Require advanced tenant isolation and management</li> <li>Real-time Analytics: Need streaming query capabilities</li> </ul> <p>Ideal For: SaaS platforms, multi-tenant environments, high-traffic applications</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-cortex-when","title":"Choose Cortex When","text":"<ul> <li>CNCF Compliance: Need a graduated CNCF project for governance</li> <li>Flexibility Required: Need highly configurable deployment options</li> <li>Mature Solution: Want proven technology with extensive production use</li> <li>Community Support: Prefer established community and ecosystem</li> <li>Hybrid Deployments: Need to support various deployment patterns</li> </ul> <p>Ideal For: Enterprise environments, regulated industries, complex multi-cloud setups</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-thanos-when","title":"Choose Thanos When","text":"<ul> <li>Existing Prometheus: Want to extend current Prometheus deployments with minimal changes</li> <li>Global Visibility: Need to query across multiple clusters/regions</li> <li>Cost Optimization: Storage costs are a primary concern (excellent compression/downsampling)</li> <li>Gradual Migration: Want to incrementally adopt long-term storage</li> <li>Multi-Cloud: Operating across different cloud providers</li> </ul> <p>Ideal For: Large-scale Kubernetes deployments, multi-cluster environments, cost-sensitive operations</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-victoriametrics-when","title":"Choose VictoriaMetrics When","text":"<ul> <li>Resource Efficiency: Need maximum performance with minimal resource usage</li> <li>Cost Optimization: Primary concern is reducing infrastructure costs</li> <li>Simple Operations: Want minimal operational complexity</li> <li>High Performance: Need exceptional ingestion and query performance</li> <li>Prometheus Compatibility: Require seamless migration from existing Prometheus setups</li> <li>Single Binary Deployment: Prefer simple deployment models</li> </ul> <p>Ideal For: High-scale cost-conscious environments, resource-constrained deployments, performance-critical applications</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-greptimedb-when","title":"Choose GreptimeDB When","text":"<ul> <li>Modern Architecture: Want a cloud-native, SQL-based time series database</li> <li>Maximum Efficiency: Need the best resource efficiency and cost optimization</li> <li>Multi-Protocol Support: Require SQL, PromQL, and other protocol compatibility</li> <li>Advanced Analytics: Want built-in SQL capabilities for complex queries</li> <li>Elastic Scaling: Need independent storage and compute scaling</li> <li>Simplified Operations: Prefer managed service options with minimal operational overhead</li> </ul> <p>Ideal For: Modern cloud-native deployments, analytical workloads, cost-sensitive high-scale environments</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#operational-considerations","title":"Operational Considerations","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>All solutions provide comprehensive metrics for monitoring:</p> <ul> <li>Ingestion Metrics: Rate, errors, latency</li> <li>Query Metrics: Performance, cache hit rates</li> <li>Storage Metrics: Object storage operations, compaction status</li> <li>Resource Metrics: CPU, memory, disk usage per component</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#backup-disaster-recovery","title":"Backup &amp; Disaster Recovery","text":"<ul> <li>Object Storage: Primary data durability mechanism</li> <li>Multi-Region: Configure object storage for cross-region replication</li> <li>Metadata Backup: Backup configuration and metadata regularly</li> <li>Testing: Regular disaster recovery testing procedures</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#security-considerations","title":"Security Considerations","text":"<ul> <li>Authentication: Integration with existing identity providers</li> <li>Authorization: Role-based access control (RBAC)</li> <li>Encryption: Data encryption in transit and at rest</li> <li>Network Security: Proper network segmentation and policies</li> <li>Secrets Management: Secure handling of storage credentials</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cost-analysis","title":"Cost Analysis","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#infrastructure-costs","title":"Infrastructure Costs","text":"Component Mimir Cortex Thanos VictoriaMetrics GreptimeDB Compute High (microservices) High (microservices) Medium (fewer components) Low (efficient) Very Low (optimized) Storage Medium (efficient compression) Medium Low (excellent compression) Very Low (superior compression) Very Low (advanced compression) Network Medium Medium Low (query optimization) Low (optimized protocols) Low (efficient protocols)"},{"location":"observability/prometheus/prometheus-long-term-storage/#operational-costs","title":"Operational Costs","text":"<ul> <li>Mimir: Lower operational overhead due to better defaults</li> <li>Cortex: Higher operational overhead due to configuration complexity</li> <li>Thanos: Lowest operational overhead for existing Prometheus users</li> <li>VictoriaMetrics: Minimal operational overhead with single binary option</li> <li>GreptimeDB: Very low operational overhead with cloud-native automation</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#conclusion","title":"Conclusion","text":"<p>The choice between Mimir, Cortex, Thanos, VictoriaMetrics, and GreptimeDB depends on your specific requirements:</p> <ul> <li>Mimir offers the best performance and modern features for new deployments</li> <li>Cortex provides maximum flexibility and is ideal for complex enterprise requirements</li> <li>Thanos offers the easiest migration path and excellent global querying for existing Prometheus users</li> <li>VictoriaMetrics delivers exceptional resource efficiency and cost optimization with superior performance</li> <li>GreptimeDB provides modern SQL-based architecture with maximum efficiency and cloud-native automation</li> </ul> <p>All five solutions successfully address Prometheus's long-term storage limitations, but the optimal choice depends on your organization's technical requirements, operational capabilities, performance needs, and cost constraints.</p> <p>Consider starting with a proof-of-concept deployment to evaluate which solution best fits your specific use case and operational constraints.</p>"},{"location":"observability/prometheus/promql/","title":"Prometheus Query Language","text":"<p>PromQL is a query language to select and aggregate time series data in real time.</p>"},{"location":"observability/prometheus/promql/#simple-query","title":"Simple query","text":"<p>We can ask for a literal metric</p> <pre><code>http_requests_total\n</code></pre> <p>or filtering with labels</p> <pre><code>http_requests_total{job=\"prometheus\"}\n</code></pre> <p>We can match the strings in different ways</p> <pre><code>=: Select labels that are exactly equal to the provided string.\n!=: Select labels that are not equal to the provided string.\n=~: Select labels that regex-match the provided string.\n!~: Select labels that do not regex-match the provided string.\n</code></pre> <ul> <li>An empty label will match the time series that don't have that label</li> </ul> <pre><code>container_cpu_usage_seconds_total{pod=\"\"}\n</code></pre> <ul> <li>If we add multiple matches, all of them must pass</li> </ul> <pre><code>http_requests_total{job=\"prometheus\",group=\"canary\"}\n</code></pre> <ul> <li>The \"name\" labels can be used by match different metrics by name</li> </ul>"},{"location":"observability/prometheus/promql/#range-vector-selectors-with","title":"Range Vector Selectors with []","text":"<p>We can specify how many seconds back in time values should be fetched adding [number] at the end.</p> <p>This selects all http_requests_total samples from the last 5 minutes where the job label is prometheus.</p> <pre><code>http_requests_total{job=\"prometheus\"}[5m]\n</code></pre>"},{"location":"observability/prometheus/promql/#offset-modifier","title":"Offset modifier","text":""},{"location":"observability/prometheus/promql/#modifier","title":"@ modifier","text":""},{"location":"observability/prometheus/promql/#links","title":"Links","text":"<p>https://prometheus.io/docs/prometheus/latest/querying/basics/ https://prometheus.io/docs/prometheus/latest/querying/operators/ https://prometheus.io/docs/prometheus/latest/querying/functions/</p>"},{"location":"observability/prometheus/servicemonitor/","title":"Service monitor","text":"<p>These are the more basic settings to create a servicemonitor</p>"},{"location":"observability/prometheus/servicemonitor/#joblabel","title":"jobLabel","text":"<p>jobLabel selects the label from the associated Kubernetes Service object which will be used as the job label for all metrics.</p> <p>For example if jobLabel is set to foo and the Kubernetes Service object is labeled with foo: bar, then Prometheus adds the job=\"bar\" label to all ingested metrics.</p> <p>If the value of this field is empty or if the label doesn\u2019t exist for the given Service, the job label of the metrics defaults to the name of the associated Kubernetes Service.</p>"},{"location":"observability/prometheus/servicemonitor/#selector","title":"selector","text":""},{"location":"observability/prometheus/servicemonitor/#endpoints","title":"endpoints","text":""},{"location":"observability/prometheus/servicemonitor/#honorlabels","title":"honorLabels","text":"<p>When true, honorLabels preserves the metric\u2019s labels when they collide with the target\u2019s labels.</p> <p>for example, the following metric \"kube_pod_container_resource_requests\"</p> <p>without honorlabels:</p> <pre><code>namespace: where my kubestate metrics instance is deployed\npod: name of the kube-state-metrics pod\ncontainer: kube-state-metrics\nexported_namespace: where the pod that generates metrics is located\nexported_pod: name of the pod that generates metrics\nexported_container: name of the container that generates metrics\n</code></pre> <p>with honorlabels:</p> <pre><code>namespace: where the pod that generates metrics is located\npod: name of the pod that generates metrics\ncontainer: name of the container that generates metrics\n</code></pre>"},{"location":"observability/prometheus/time-series-selectors/","title":"Time series selectors","text":"<p>These are the basic building-blocks that instruct PromQL what data to fetch.</p>"},{"location":"observability/prometheus/time-series-selectors/#range-vector-selectors","title":"Range Vector Selectors","text":"<p>Range vector literals work like instant vector literals, except that they select a range of samples back from the current instant. Syntactically, a float literal is appended in square brackets ([]) at the end of a vector selector to specify for how many seconds back in time values should be fetched for each resulting range vector element. Commonly, the float literal uses the syntax with one or more time units, e.g. [5m]. The range is a left-open and right-closed interval, i.e. samples with timestamps coinciding with the left boundary of the range are excluded from the selection, while samples coinciding with the right boundary of the range are included in the selection.</p> <p>In this example, we select all the values recorded less than 5m ago for all time series that have the metric name http_requests_total and a job label set to prometheus:</p> <p>http_requests_total{job=\"prometheus\"}[5m]</p>"},{"location":"observability/prometheus/functions/increase/","title":"Increase","text":"<p>The \"increase\" function must be used with</p> <ul> <li>counters (floats and histograms)</li> <li>range vector</li> </ul> <p>This function calculates if a counter has increased in the time series in the range vector.</p> <p>This example calculates if http_requests_total counter has increased during the last 5 minutes</p> <pre><code>increase(http_requests_total{job=\"api-server\"}[5m])\n</code></pre> <p>if we want to trigger an alert we can use</p> <pre><code>increase(http_requests_total{job=\"api-server\"}[5m]) &gt; 0\n</code></pre> <p>increase acts on histogram samples by calculating a new histogram where each component (sum and count of observations, buckets) is the increase between the respective component in the first and last native histogram in v. However, each element in v that contains a mix of float samples and histogram samples within the range, will be omitted from the result vector, flagged by a warn-level annotation.</p> <p>increase should only be used with counters (for both floats and histograms). It is syntactic sugar for rate(v) multiplied by the number of seconds under the specified time range window, and should be used primarily for human readability. Use rate in recording rules so that increases are tracked consistently on a per-second basis.</p>"},{"location":"observability/prometheus/labels/98-tips/","title":"Tips","text":""},{"location":"observability/prometheus/labels/98-tips/#add-label-using-scrape-classes","title":"Add label using Scrape Classes","text":"<p>We can use spec.scrapeClasses to add a global label to all metrics in a prometheus instance</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\nspec: \n  scrapeClasses:\n    - name: default-cluster\n      default: true # this is the default scrape class\n      relabelings:\n        - action: replace\n          targetLabel: mylabel\n          replacement: \"mydeiredvalue\"\n</code></pre> <p>More info here https://prometheus-operator.dev/docs/developer/scrapeclass/</p>"},{"location":"observability/prometheus/labels/98-tips/#job-label-and-prometheus-operator","title":"Job label and Prometheus Operator","text":"<p>A job label is an special prometheus label used by create a logical organization of labels</p> <p>When using dashboards, specially 3rd party dashboards, it is very relevant to check if the queries expect a concrete job name. Different dashboards for the same application can require different job names</p> <p>Using Prometheus operator in kubernetes, the typical way to scrape metrics is using the following resources:</p> <ul> <li>Servicemonitor</li> <li>PodMonitor</li> </ul> <p>In both resources we can use the following key to configure the value of the job label:</p> <pre><code>spec.jobLabel\n</code></pre> <p>This key offers the possibility to configure the job label value:</p> <ul> <li>Choosing the value of a label in the matched service</li> <li>If we dont use spec.jobLabel or leave it empty, the job label will have the value of the matching service</li> </ul> <p>Example:</p> <p>We have a service called \"myservice\" with the label component: frontend and we use a Servicemonitor matching this service</p> <ul> <li>with spec.jobLabel: component, job label will have \"frontend\" for all scraped metrics</li> <li>if we we dont use spec.jobLabel or is it empty, the value of the job label will \"myservice\"</li> </ul>"},{"location":"observability/prometheus/labels/98-tips/#relabeling","title":"Relabeling","text":"<p>Sometimes using spec.jobLabel or leaving empty does not meet our requirements, because we want a value that is not provided by any label or the service name. In that case we can use relabelings at endpoint level to give the desired value</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myservicemonitor\nspec:\n  endpoints:\n    - relabelings:\n        - action: replace\n          targetLabel: job\n          replacement: mydesiredjoblabel\n</code></pre>"},{"location":"observability/prometheus/labels/labels/","title":"Labels","text":"<p>The prometheus labels are key-value pairs attached to metrics. They offer additional context, information and metadata to the metrics and permit things like identify, filter and categorize metrics.</p> <p>Example</p> <pre><code> metric_name{label1=\"value1\", label2=\"value2\"} value\n</code></pre>"},{"location":"observability/prometheus/labels/labels/#metric-labels","title":"Metric labels","text":"<p>This labels are generated from the application that expose the metrics:</p> <ul> <li>they are embedded in the metric data itself</li> <li>they describe the metric</li> </ul> <p>They have high cardinality risk</p>"},{"location":"observability/prometheus/labels/labels/#target-labels","title":"Target labels","text":"<p>This labels are added during the scraping process. They describe where the metrics came from, not what they represent.</p> <p>They have low cardinality risk</p>"},{"location":"observability/prometheus/labels/labels/#core-target-label-job","title":"Core Target Label: job","text":"<p>The job label identifies the scrape configuration.</p> <p>In prometheus operator we can give the job label the value of a kubernetes label from the associated pod. This is done using spec.jobLabel in a podmonitor or servicemonitor resource.</p>"},{"location":"observability/prometheus/labels/labels/#core-target-label-instance","title":"Core Target Label: instance","text":"<p>Instance label is the host and port of the target</p> <p>More information about Jobs and instances here https://prometheus.io/docs/concepts/jobs_instances/</p>"},{"location":"observability/prometheus/labels/labels/#internal-labels-__","title":"Internal labels (__)","text":"<p>There are some internal labels (prefixed with __) are temporary labels used by Prometheus during the scraping process. They're not stored with metrics but control how scraping happens.</p> <p>Examples:</p> <pre><code>__address_ # as the target's original address\n__metrics_path__ # as the endpoint path\n__scheme__=\"http\" # as the protocol to use\n__param_* # as query parameters\n__meta_* # Contains discovery metadata\n__meta_kubernetes_ # Contains kubernetes discovery metadata\n__meta_consul_* # Consul discovery metadata\n__meta_ec2_* # EC2 discovery metadata\n</code></pre> <p>With Relabeling it is possible to use internal labels to create permanent labels using relabel_configs</p> <p>There can be other informational labels, not true internal labels so they cannot be used in relabeling</p> <pre><code>__scrape_interval__ # Shows configured interval\n__scrape_timeout__ # Shows configured timeout\n</code></pre>"},{"location":"observability/prometheus/labels/labels/#sources-of-target-labels","title":"Sources of Target Labels","text":"<p>Sources of Target Labels can be defined:</p> <ul> <li>using an static configuration</li> </ul> <pre><code>  - job_name: 'api-servers'\n    static_configs:\n    - targets: ['api1:8080', 'api2:8080']\n      labels:\n        env: 'production'\n        team: 'backend'\n</code></pre> <ul> <li>using Service Discovery (Kubernetes, Consul, etc.)</li> </ul> <pre><code> - job_name: 'kubernetes-pods'\n    kubernetes_sd_configs:\n    - role: pod\n</code></pre> <p>Auto-discovers and adds labels like:</p> <pre><code>  {\n    job=\"kubernetes-pods\",\n    instance=\"10.244.0.15:8080\",\n    __meta_kubernetes_pod_name=\"webapp-123\",\n    __meta_kubernetes_namespace=\"production\"\n  }\n</code></pre> <ul> <li>using ServiceMonitor/PodMonitor from prometheus operator</li> </ul>"},{"location":"observability/prometheus/labels/labels/#honorlabels","title":"honorLabels","text":"<p>honorLabels controls how Prometheus handles label conflicts when scraping metrics</p> <p>Default behavior (honorLabels: false):</p> <ul> <li>Prometheus overwrites target labels with its own labels</li> <li>Target's job label gets renamed to exported_job</li> <li>Target's instance label gets renamed to exported_instance</li> <li>Prometheus uses its own job and instance values</li> </ul> <p>With honorLabels: true:</p> <ul> <li>Prometheus keeps the target's original labels</li> <li>Target's labels take precedence over Prometheus labels</li> <li>No renaming happens</li> </ul> <pre><code>  Example: Target exposes: my_metric{job=\"custom-job\", instance=\"app-1\"}\n\nhonorLabels: false (default)\nmy_metric{job=\"serviceMonitor/namespace/name\", instance=\"10.0.0.1:8080\", exported_job=\"custom-job\", exported_instance=\"app-1\"}\n\nhonorLabels: true  \nmy_metric{job=\"custom-job\", instance=\"app-1\"}\n</code></pre> <p>Use honorLabels: true:</p> <ul> <li>When targets provide meaningful job/instance labels</li> <li>For federation setups</li> <li>When you want to preserve application-defined labels</li> </ul> <p>Common use case: Kubernetes service discovery often uses honorLabels: true to preserve pod labels as metric labels.</p>"},{"location":"observability/prometheus/labels/labels/#cardinality-risk","title":"Cardinality risk","text":"<p>Cardinality is the number of unique label combinations for a metric.</p> <p>The problem is each unique combination creates a separate time series. Prometheus stores each series individually.</p> <p>Cardinality risk is when you create too many unique time series, causing Prometheus performance and storage problems. It can crash or severely degrade your monitoring system.</p> <p>High Cardinality Risk is when too many label combinations create too many time series. This can be caused by:</p> <ul> <li>Unique Identifiers:</li> </ul> <pre><code>requests{user_id=\"abc123\"}        # millions of users\nrequests{request_id=\"xyz789\"}     # every request unique\nrequests{timestamp=\"1634567890\"}  # every second unique\n</code></pre> <ul> <li>Unbounded Values:</li> </ul> <pre><code>response_time{url=\"/user/12345/profile\"}  # infinite URLs\nerrors{error_msg=\"Connection timeout\"}    # many error messages\n</code></pre> <p>The solutions can be:</p> <ul> <li>Use Bounded Labels (limited values)</li> </ul> <pre><code>requests{user_type=\"premium\"}     # premium, basic, free\nrequests{status_class=\"4xx\"}      # 2xx, 3xx, 4xx, 5xx\n</code></pre> <ul> <li>Group/Aggregate:</li> </ul> <pre><code>requests{region=\"us-east\", user_tier=\"paid\"}\n</code></pre> <ul> <li>Use Histograms for Ranges (instead of exact response times)</li> </ul> <pre><code>http_request_duration_bucket{le=\"0.1\"}  # predefined buckets\n</code></pre>"},{"location":"observability/prometheus/labels/labels/#links","title":"Links","text":"<p>https://github.com/prometheus-operator/prometheus-operator/issues/3246</p> <p>relabelings vs metricRelabelings</p>"},{"location":"observability/prometheus-adapter/0-intro-configuration/","title":"Intro and configuration","text":"<p>Prometheus adapter is a kubernetes addon that permits to expose custom metrics to the kubernetes metrics API. For that, prometheus adapter asks to a prometheus intance in order to build the desired metrics.</p> <p>It can be used to serve the resource metrics api, this is cpu and memory from pods and nodes. It can be used, for example, for autoescaling purposes using Horizontal Pod Autoescaler, replacing metrics-server.</p> <ul> <li>Resource metrics api</li> </ul> <p>For example, to access the resource metrics api we can</p> <pre><code>kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\"\n</code></pre> <ul> <li>Custom metrics api</li> <li>External metrics</li> </ul>"},{"location":"observability/prometheus-adapter/0-intro-configuration/#the-parameters","title":"The parameters","text":"<p>The prometheus instance is passed with the parameter --prometheus-url. For example:</p> <pre><code>--prometheus-url=\n</code></pre> <pre><code>--prometheus-url=https://prometheus.monitoring.svc:9090/\n</code></pre> <p>The configuration file of the prometeus adapter is passed with the parameter --config. For example:</p> <pre><code>--config=/etc/adapter/config.yaml\n</code></pre>"},{"location":"observability/prometheus-adapter/0-intro-configuration/#the-configuration-file","title":"The configuration file","text":"<p>The config.yaml file has some section:</p> <pre><code>resourceRules:  for resource metrics\nrules:    for custom metrics\nexternalRules:   for external metrics\n</code></pre> <p>Every rule has 4 parts.</p> <ul> <li>Discovery</li> <li>Association</li> <li>Naming</li> <li>Querying</li> </ul>"},{"location":"observability/prometheus-adapter/0-intro-configuration/#links","title":"Links","text":"<ul> <li>Prometheus adapter github</li> </ul> <p>https://github.com/kubernetes-sigs/prometheus-adapter/</p> <ul> <li>Prometheus adapter helm chart</li> </ul> <p>https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config.md https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config-walkthrough.md https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/walkthrough.md https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/sample-config.yaml https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/externalmetrics.md</p>"},{"location":"observability/prometheus-adapter/1-discovery/","title":"Discovery","text":"<p>This represents how the adapter should find all Prometheus metrics for this rule. The discovery is done via the seriesQuery and seriesFilters fields</p> <ul> <li>seriesQuery</li> </ul> <p>seriesQuery is a prometheus series query passed to the /api/v1/series prometheus endpoint  to find a sef of prometheus series.</p> <ul> <li>seriesFilters</li> </ul> <p>seriesFilters can help us to add additional filters if it is necessary. We can use here \"is: regex\" or \"isNot: regex\"</p>"},{"location":"observability/prometheus-adapter/2-association/","title":"Association","text":"<p>specifies how the adapter should determine which Kubernetes resources a particular metric is associated with.</p>"},{"location":"observability/prometheus-adapter/3-naming/","title":"Naming","text":"<p>Naming is the process where we define how the metric will be exposed. The naming is done using the \"name\" field and permits to convert a prometheus metric intro a custom metric and viceversa. We have 2 fields here:</p> <ul> <li>matches</li> </ul> <p>With the \"matches\" field we define a pattern to select a prometheus metric using a regular expression.</p> <ul> <li>as:</li> </ul> <p>With the \"as\" field we transform the metric in another.</p> <p>Default: With no \"as\" field, if the matches field does not contain capture groups, the default will be $0. If it containes a single capture group the default will be $1.</p> <pre><code># this turns any name &lt;name&gt;_total to &lt;name&gt;_per_second\nname:\n  matches: \"^(.*)_total$\"\n  as: \"${1}_per_second\"\n</code></pre>"},{"location":"observability/prometheus-adapter/4-querying/","title":"Querying","text":""},{"location":"observability/prometheus-adapter/4-querying/#metricsquery-field","title":"metricsQuery field","text":"<p>This field is a Go template that will be a Prometheus query and we can use this variables</p> <ul> <li>Series</li> </ul> <p>It is the metric name</p> <ul> <li>LabelMatchers:</li> </ul> <p>A comma-separated list of label matchers matching the given objects. Currently, this is the label for the particular group-resource, plus the label for namespace, if the group-resource is namespaced.</p> <ul> <li>GroupBy</li> </ul> <p>A comma-separated list of labels to group by. Currently, this contains the group-resource label used in LabelMatchers.</p> <p>There are other advanced and less used variables: LabelValuesByName and GroupBySlice</p>"},{"location":"observability/prometheus-adapter/4-querying/#seriesquery-field","title":"seriesQuery field","text":""},{"location":"observability/prometheus-adapter/4-querying/#resources-field","title":"resources field","text":""},{"location":"observability/prometheus-adapter/4-querying/#seriesfilters","title":"seriesFilters","text":""},{"location":"observability/prometheus-adapter/5-resource-metrics/","title":"Kubernetes resource metrics","text":"<p>The most simple way to make horizontal pod autoescaler and kubectl top work is to deploy metrics adapter. But if we want, for example, to use custom metrics with HPA, we need prometheus adapter.</p> <p>In order to make prometheus adapter provide that functions, we need:</p> <ul> <li> <p>Deploy metrics adapter deployment</p> </li> <li> <p>Create an v1beta1.metrics.k8s.io apiService</p> </li> <li> <p>Configure the prometheus adapter (ConfigMap)</p> </li> </ul>"},{"location":"observability/prometheus-adapter/5-resource-metrics/#deployment","title":"Deployment","text":"<p>We have 2 ways to deploy the prometheus adapter:</p> <ul> <li>Official manifests</li> </ul> <p>They include the deployment, the v1beta1.metrics.k8s.io apiService and the ConfigMap</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/tree/master/deploy/manifests</p> <ul> <li>Helm chart</li> </ul> <p>They include the deployment. To deploy the apiService and the ConfigMap we need to enable the rules.resource section in our values.yaml file and enable the APIVersions Capability with \"apiregistration.k8s.io/v1\" as value.</p> <p>https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter</p>"},{"location":"observability/prometheus-adapter/5-resource-metrics/#configuration","title":"Configuration","text":"<p>I have found 3 different configurations of the adapter. They are similar but not exactly the same</p> <ul> <li>From prometheus-adapter github repo</li> </ul> <p>https://raw.githubusercontent.com/kubernetes-sigs/prometheus-adapter/refs/heads/master/deploy/manifests/config-map.yaml</p> <ul> <li>From kube-prometheus</li> </ul> <p>https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/refs/heads/main/manifests/prometheusAdapter-configMap.yaml</p> <ul> <li>From prometheus-community helm chart</li> </ul> <p>In the default values.yaml file from the prometheus-adapter helm chart, we have some commented lines about the resource rules.</p> <p>https://raw.githubusercontent.com/prometheus-community/helm-charts/refs/heads/main/charts/prometheus-adapter/values.yaml</p>"},{"location":"observability/prometheus-adapter/9-tips/","title":"Tips","text":""},{"location":"observability/prometheus-adapter/9-tips/#incorrect-values-in-kubectl-top","title":"Incorrect values in kubectl top","text":"<p>If we have incorrect and very high values using \"kubectl top\" we are probably getting the same values twice:</p> <ul> <li>From kubelet /metrics/resource</li> <li>From kubelet /metrics/cadvisor</li> </ul> <p>\"This is because container_cpu_usage_seconds_total and container_memory_working_set_bytes used there are exposed by both endpoints, resulting in double counting. Therefore, you will need to take measures such as assigning a metrics_path label using relabel_configs and narrowing it down to one.</p> <p>As an alternative solution to avoid using /metrics/resource, you could follow the approach mentioned here(I used it). However, in that case, you'll need Prometheus Node Exporter, and you'll also need to assign node names to node label using relabel_configs.</p> <p>More info at:</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/issues/639</p> <p>Another option is that we can have more that one service being matched by the kubelet service monitor</p>"},{"location":"operating-systems/too-many-open-files/","title":"Too Many Open Files","text":"<p>This error is related with the max open file descriptors limit.</p> <p>To see list the opened file descriptors per process (open file descriptors, pid an process name):</p> <pre><code>for pid in $(ls /proc | grep -E '^[0-9]+$'); do\n  echo \"$(ls /proc/$pid/fd 2&gt;/dev/null | wc -l) $pid $(cat /proc/$pid/comm 2&gt;/dev/null)\"\ndone | sort -nr | head\n</code></pre>"},{"location":"operating-systems/too-many-open-files/#systemd-service","title":"Systemd service","text":"<p>In order to see if the problem is related with a systemd service, we can see the limits this way</p> <p>With containerd:</p> <pre><code>systemctl show containerd | grep LimitNOFILE\ncat /proc/$(pgrep containerd | head -1)/limits | grep files\n</code></pre> <p>In order to solve it we can:</p> <ul> <li>Change the DefaultLimitNOFILE settings in the /etc/systemd/system.conf file, affecting to all systemd services.</li> </ul> <p>The default value is 1024:524288, where 1024 is the soft limit and 524288 the hard limit</p> <ul> <li>Or adding a systemd dropping in /etc/systemd/system/containerd.service.d/limits.conf with our desired LimitNOFILE</li> </ul>"},{"location":"operating-systems/too-many-open-files/#system-process","title":"System process","text":"<p>If the problem is related with a non systemd service, we must probably increase fs.inotify.max_user_instances. This is the upper limit on the number of INotify instances (file descriptors) that can be created per real user ID</p> <p>To see the current settings</p> <pre><code>sysctl fs.inotify.max_user_instances\ncat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>To increase it temporarily (sometimes the default value is 128)</p> <pre><code>sysctl fs.inotify.max_user_instances=512\n</code></pre> <p>To make it persistent</p> <pre><code>cat &lt;&lt; EOF &gt; /etc/sysctl.d/10-open-files.conf\nfs.inotify.max_user_instances = 512\nEOF\nsystemctl restart systemd-sysctl.service\n</code></pre> <p>And check again the current settings</p>"},{"location":"operating-systems/bottlerocket/98-tips/","title":"Tips","text":""},{"location":"operating-systems/bottlerocket/98-tips/#get-admin-console-access-in-eks","title":"Get admin console access in EKS","text":"<p>Connect the ec2 instance, for example, using session manager. Then</p> <pre><code>enter-admin-container\nsudo sheltie\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#get-logs","title":"Get logs","text":"<p>We can generate the logs or get kubelet logs, for example</p> <pre><code>logdog\njournalctl -u kubelet\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#get-the-kubernetes-images","title":"Get the kubernetes images","text":"<pre><code>ctr --namespace k8s.io images list\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#re-push-the-kubernetes-images","title":"Re-Push the  kubernetes images","text":"<p>This will ask you for the password</p> <pre><code>ctr --namespace k8s.io image push -u MYUSER URL-OF-THE-IMAGE\n</code></pre>"},{"location":"operating-systems/bottlerocket/99-links/","title":"Links","text":"<ul> <li>AWS Bottlerocket</li> </ul> <p>https://aws.amazon.com/bottlerocket/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D</p> <ul> <li>Github</li> </ul> <p>https://github.com/bottlerocket-os/bottlerocket</p> <ul> <li>Website</li> </ul> <p>https://bottlerocket.dev/</p>"},{"location":"operating-systems/hyperv/tips/","title":"Tips","text":""},{"location":"operating-systems/hyperv/tips/#a-virtual-machine-is-stuck-powering-off","title":"A virtual machine is stuck powering off","text":"<p>Identify the Id of the vm via powershell</p> <pre><code>get-vm | ft VMName, VMId\n</code></pre> <p>Then got to task manager to see the processes. Under the \"command line\" column, end the virtual machine process (vmwp.exe) that has the VMId as parameter</p>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/","title":"Citrix workspace in Ubuntu 24.04","text":""},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#install-dependencies","title":"Install dependencies","text":"<p>As root</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/apt/sources.list.d/jammy.list\ndeb &lt;http://gb.archive.ubuntu.com/ubuntu&gt; jammy main\napt update\napt install libwebkit2gtk-4.0-dev\nrm -f /etc/apt/sources.list.d/jammy.list\napt update\n</code></pre>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#install","title":"Install","text":"<p>As user, download the tarball from here</p> <ul> <li>Citrix Workspace app https://www.citrix.com/downloads/workspace-app/linux/</li> </ul> <p>Uncompress and exec the installer</p> <pre><code>./setupwfc\n</code></pre>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#links","title":"Links","text":"<ul> <li>Install, Uninstall, and Update https://docs.citrix.com/en-us/citrix-workspace-app-for-linux/install.html</li> </ul>"},{"location":"operating-systems/linux/gdm/","title":"GDM","text":"<p>GNOME CLASSIC VS UBUNTU vs WAYLAND VS XORG ls /usr/share/xsessions/</p> <p>https://help.ubuntu.com/stable/ubuntu-help/gnome-classic.html.en https://www.reddit.com/r/Ubuntu/comments/hby264/can_anyone_help_me_with_what_does_gnome_classic/</p> <ul> <li> <p>Gnome classic (wayland?) Exec=gnome-session-classic TryExec=gnome-session</p> </li> <li> <p>Gnome classic on Xorg Exec=env GNOME_SHELL_SESSION_MODE=classic gnome-session TryExec=gnome-session GNOME 46 has been released in coordination with the latest GTK version, 4.14</p> </li> <li> <p>Ubuntu (wayland?) Exec=env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gnome-session --session=ubuntu TryExec=/usr/bin/gnome-shell</p> </li> <li> <p>Ubuntu on Xorg Exec=env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gnome-session --session=ubuntu TryExec=/usr/bin/gnome-shell</p> </li> </ul> <p>Classic emulates gnome 2 The Xorg sessions are legacy and shouldn\u2019t be used unless you have a specific need.</p>"},{"location":"operating-systems/linux/gnome-oled/","title":"Gnome tips for ultrawide oled monitor","text":"<p>This is based in Ubuntu 24.04</p>"},{"location":"operating-systems/linux/gnome-oled/#prevent-burn-in-issues","title":"Prevent burn in issues","text":""},{"location":"operating-systems/linux/gnome-oled/#pure-black-theme","title":"Pure black theme","text":"<p>Here we must search for gnome-shell and gtk/3/4 themes.</p> <p>We can find some themes here. Some of them provide both gnome-shell and gtk themes</p> <p>https://www.pling.com/browse?cat=366&amp;ord=latest</p> <p>Ir order to install them we need to install the gnome-tweaks package and the User Themes extension</p> <p>gnome-extensions-app</p> <p>https://www.reddit.com/r/gnome/comments/15c16hx/anyone_have_a_matte_black_theme/</p>"},{"location":"operating-systems/linux/gnome-oled/#gnome-shell-themes","title":"Gnome shell themes","text":"<p>The gnome shell themes change the appearance of the GNOME Shell interface, including the top bar, system menus, notifications, and the overview screen.</p> <p>The system themes are installed in /usr/share/themes/NAME-OF-THEME/gnome-shell and they can be installed by the user in the ./themes/NAME-OF-THEME/gnome-shell directory</p>"},{"location":"operating-systems/linux/gnome-oled/#gtk34-themes","title":"Gtk/3/4 themes","text":"<p>They change the appearance of GTK applications, including window decorations, buttons, sliders, and other UI elements.</p> <p>The system themes are installed in</p> <ul> <li>/usr/share/themes/NAME-OF-THEME/gtk-3.0</li> <li>/usr/share/themes/NAME-OF-THEME/gtk-4.0</li> </ul> <p>And they can be installed by de user</p> <ul> <li>GTK3: ~/.themes/NAME-OF-THEME/gtk-3.0</li> <li>GTK4: ~/.themes/NAME-OF-THEME/gtk-4.0</li> </ul>"},{"location":"operating-systems/linux/gnome-oled/#full-black-desktop-background","title":"Full black desktop background","text":"<pre><code>gsettings set org.gnome.desktop.background picture-options 'none'\ngsettings set org.gnome.desktop.background primary-color '#000000'\n</code></pre> <p>Other option is to use some pure black wallpapers combined with the \"Wallpaper slideshow\" gnome-shell extension</p>"},{"location":"operating-systems/linux/gnome-oled/#auto-hide-the-dock","title":"Auto hide the dock","text":"<p>We can enable autohide in the dock via settings - Ubuntu Desktop - Dock and enable \"Auto-hide the Dock\"</p>"},{"location":"operating-systems/linux/gnome-oled/#dont-show-desktop-icons","title":"Dont show desktop icons","text":"<p>Go to settings &gt; Ubuntu Desktop and disable Desktop icons</p>"},{"location":"operating-systems/linux/gnome-oled/#auto-hide-the-top-bar","title":"Auto hide the top bar","text":"<p>We can hide the top back installing the \"Hide Top Bar\" gnome-shell extension</p>"},{"location":"operating-systems/linux/gnome-oled/#other-applications","title":"Other applications","text":"<ul> <li>Chrome and firefox</li> </ul> <p>https://darkreader.org/</p> <ul> <li>VSCODE</li> </ul> <p>Choose the dark high contrast theme</p>"},{"location":"operating-systems/linux/gnome-oled/#organize-windows","title":"Organize windows","text":"<p>We can use Ubuntu tiling asistant installing the gnome-shell-extension-ubuntu-tiling-assistant package</p> <p>https://extensions.gnome.org/extension/3733/tiling-assistant/ https://github.com/Leleat/Tiling-Assistant</p> <p>There are other similar tools like Linux Powertoys https://github.com/domferr/Linux-PowerToys</p>"},{"location":"platforms/aws/cli/98-tips/","title":"Tips","text":""},{"location":"platforms/aws/cli/98-tips/#install-as-user","title":"Install as user","text":"<pre><code>mkdir ~/apps ~/bin\nbash install --install-dir ~/apps/aws-cli --bin-dir ~/bin\n~/bin/aws --version\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#list-s3-buckets","title":"List s3 buckets","text":"<pre><code>aws s3 ls --profile myprofile\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#get-my-kubeconfig","title":"Get my kubeconfig","text":"<pre><code>export AWS_PROFILE=myprofile\naws eks update-kubeconfig --region region-code --name my-cluster --kubeconfig pathtomykubeconfig\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#login-to-ecr","title":"Login to ecr","text":"<pre><code>aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#other","title":"Other","text":"<pre><code>aws eks get-token  --cluster-name my-cluster\n\naws sts get-caller-identity\n\naws sts get-session-token\n</code></pre> <p>To delete a profile with the AWS CLI, you need to manually remove the profile's configuration from the AWS configuration files. The AWS CLI stores profiles in two files: ~/.aws/config and ~/.aws/credentials.</p> <p>Steps to Delete a Profile: Open the Configuration Files:</p> <p>Open ~/.aws/config and ~/.aws/credentials in a text editor. Remove the Profile from ~/.aws/config:</p> <p>Locate the profile section you want to delete. Profile sections start with [profile profile-name] for named profiles. Delete the entire section for the profile. Remove the Profile from ~/.aws/credentials:</p> <p>Locate the profile section you want to delete. Profile sections start with [profile-name]. Delete the entire section for the profile.</p>"},{"location":"platforms/aws/cli/authentication/","title":"Authentication","text":""},{"location":"platforms/aws/cli/authentication/#common","title":"Common","text":"<p>List profiles</p> <pre><code>aws configure list-profiles\n</code></pre> <p>Choose profile</p> <pre><code>export AWS_PROFILE=profile\n</code></pre> <p>Files</p> <p>~/.aws/credentials</p> <p>~/.aws/config</p>"},{"location":"platforms/aws/cli/authentication/#standard-profile","title":"Standard profile","text":"<p>Configure standard profile</p> <pre><code>aws configure\n</code></pre>"},{"location":"platforms/aws/cli/authentication/#single-sign-on-ssl","title":"Single Sign on (ssl)","text":"<p>Configure sso profile</p> <pre><code>aws configure sso\n</code></pre> <p>Login to a sso profile</p> <pre><code>aws sso login --profile=profile\n</code></pre>"},{"location":"platforms/aws/ecr/aws-cli/","title":"Aws cli and ecr","text":"<p>Get a token valid for 12 hours</p> <pre><code>aws ecr get-login-password --region MYREGION \n</code></pre> <p>Get a token valid for 12 hours and login in docker</p> <pre><code>aws ecr get-login-password --region MYREGION | docker login --username AWS --password-stdin MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com\n</code></pre> <p>Create a repository</p> <pre><code>aws ecr create-repository --repository-name REPOSITORYNAME\n</code></pre>"},{"location":"platforms/aws/ecr/policies/","title":"Iam Policies","text":"<p>Example policies:</p> <ul> <li> <p>AmazonEC2ContainerRegistryPowerUser is a predefined policy that permits an user to pull and push images</p> </li> <li> <p>Project power user as an user that can pull and push images only under MYPROJECT/* repositories</p> </li> <li> <p>AmazonEC2ContainerRegistryPullOnly is a predefined policy that can be assigned to service accounts to pull images</p> </li> <li> <p>Project puller to be assigned to service accounts to pull images only located under MYPROJECT/* repositories</p> </li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/","title":"All images from private ECR and no inet","text":""},{"location":"platforms/aws/ecr/private-ecr-no-inet/#situation","title":"Situation","text":"<ul> <li> <p>The cluster has been predeployed with kubeadm and temporary internet access. The control plane is working but that container images will not be accesible if they need to be downloaded again.</p> </li> <li> <p>We dont have internet access in our kubernetes cluster The cluster must get all the container images from an Amazon Elastic Container Registry. AWS gives a 12 hours valid token for that. We need to renew it properly. We cannot access to helm repositories</p> </li> <li> <p>All the secrets will be stored in AWS Secrets Manager and we will use external secrets operator v0.12.1</p> </li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#preparation","title":"Preparation","text":""},{"location":"platforms/aws/ecr/private-ecr-no-inet/#power-user","title":"Power user","text":"<p>We will do some tasks like create repositories, iam policies, iam users, push image or create secrets. We need permissions to do that operations. For example, to push images we ca use the predefined AmazonEC2ContainerRegistryPowerUser policy or use a more precise setup.</p> <p>Assume that user will be called \"power-user\"</p>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#iam-user-to-pull-images","title":"Iam user to pull images","text":"<p>We must create an iam user to permite kubelet and the service accounts to pull the images. We can use the generic AmazonEC2ContainerRegistryPullOnly predefined policy or do a more precise setup.</p> <p>Assume that user will be called \"image-puller\"</p> <p>Also create an access key and store the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in secrets manager:</p> <pre><code>image-puller-AWS_ACCESS_KEY_ID\nimage-puller-AWS_SECRET_ACCESS_KEY\nimage-puller-REGION\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#bootstrap-external-secrets-operator","title":"Bootstrap external-secrets operator","text":"<p>Because all the container images will be located in a private repository we need to bootstrap external secrets operator in order to provide kubelet the credentials to that private repository in a secure way.</p>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#external-secrets-repository","title":"External secrets repository","text":"<p>Go to Elastic Container Registry and create a repository called, for example, MYPROJECT/external-secrets. Take care about the resulting Resource ARN and URL of the repository.</p> <pre><code>MYREPOARN\nMYREPOURL  (MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com)\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#push-the-external-secrets-image","title":"Push the external secrets image","text":"<p>Pull the external-secrets operator official image for our release</p> <pre><code>docker pull oci.external-secrets.io/external-secrets/external-secrets:v0.12.1\ndocker tag oci.external-secrets.io/external-secrets/external-secrets:v0.12.1 MYREPOURL/external-secrets:v0.12.1\n</code></pre> <p>As power user push it to the new repository</p> <pre><code>aws ecr get-login-password --region MYREGION | docker login --username AWS --password-stdin MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com\ndocker push oci.external-secrets.io/external-secrets/external-secrets:v0.12.1 MYREPOURL/external-secrets:v0.12.1\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#create-the-kubelet-credentials","title":"Create the kubelet credentials","text":"<p>For that we need to create a kubernetes.io/dockerconfigjson kubernetes secret in the external-secrets with the credentials. We will call it \"puller\".</p> <ul> <li>The server will be MYREPOURL</li> <li>The username will be AWS</li> <li>The password is a 12h valid token</li> </ul> <pre><code>aws configure --profile image-puller # use the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and REGION\nexport AWS_PROFILE=image-puller\nkubectl create ns external-secrets\nkubectl create secret docker-registry puller -n external-secrets --docker-server=MYREPOURL --docker-username=AWS --docker-password=$(aws ecr get-login-password) --dry-run -o yaml\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#deploy-the-operator","title":"Deploy the operator","text":"<p>In order to deploy the operator we need to use the official yaml (not the helm chart), change the url of the images to our private repository repository and give the service accounts the kubelet credentials.</p> <p>See this kustomization file and the imagepullsecrets patch file</p> <p>Finally deploy external secrets operator with</p> <pre><code>kubectl apply -k .\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#notes-about-pending-tasks","title":"Notes about pending tasks","text":"<p>Now we have a puller secret in the external-secrets namespace that can pull images from our container but we have several things to do:</p> <ul> <li>Create new repositories in ECR and pull all the images. It will not be treated here</li> <li>Change all the service accounts use the ecr-auth credentials as pullsecret.</li> <li>Convert the puller secret to be an managed (not manual) secret.</li> <li>Distribute the puller secret to all namespaces.</li> <li>The token lives 12 hours. We also needs to renew it.</li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#links","title":"Links","text":"<ul> <li>Pushing a Docker image to an Amazon ECR private repository</li> </ul> <p>https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</p> <ul> <li>External secrets operator helm chart</li> </ul> <p>https://github.com/external-secrets/external-secrets/tree/main/deploy/charts/external-secrets</p> <ul> <li>Generators</li> </ul> <p>https://external-secrets.io/latest/guides/generator/</p> <ul> <li>External secrets operator and ecr generator</li> </ul> <p>https://external-secrets.io/latest/api/generator/ecr/</p> <ul> <li>External secrets operator cluster external secret</li> </ul> <p>https://external-secrets.io/latest/api/clusterexternalsecret/</p>"},{"location":"queues/nats/99-links/","title":"Links","text":""},{"location":"queues/nats/99-links/#websites","title":"Websites","text":"<ul> <li> <p>Official Nats website https://nats.io/</p> </li> <li> <p>Official Nats documentation https://docs.nats.io/</p> </li> <li> <p>Github nats org https://github.com/nats-io</p> </li> <li> <p>Awesome Nats https://github.com/synadia-io/awesome-nats</p> </li> <li> <p>Learn NATS by Example</p> </li> </ul> <p>https://natsbyexample.com/</p>"},{"location":"queues/nats/99-links/#monitoring-and-dashboards","title":"Monitoring and dashboards","text":"<ul> <li>Nats Surveyor</li> </ul> <p>https://github.com/nats-io/nats-surveyor</p> <ul> <li>Prometheus nats exporter</li> </ul> <p>https://github.com/nats-io/prometheus-nats-exporter/</p> <ul> <li> <p>Nats Dashboard https://natsdashboard.com/ https://github.com/mdawar/nats-dashboard</p> </li> <li> <p>Nats-top</p> </li> </ul> <p>https://github.com/nats-io/nats-top</p>"},{"location":"queues/nats/99-links/#videos","title":"Videos","text":"<ul> <li> <p>Nats Youtube Channel https://www.youtube.com/c/nats_messaging</p> </li> <li> <p>Synadia Communications Youtube Channel https://www.youtube.com/@SynadiaCommunications</p> </li> <li> <p>Cloud Native Live: Advanced NATS https://www.youtube.com/watch?v=3vHKKmVkb80</p> </li> </ul>"},{"location":"queues/nats/jetstream/01-stream/","title":"Stream","text":"<p>A nats stream is a messsage store with some settings like the retention. It is related with nats subjects, and any message published there will be stored in the configured storage</p> <p>Streams are responsible for storing the published messages,</p>"},{"location":"queues/nats/jetstream/01-stream/#configuration","title":"Configuration","text":"<p>We can see the settings of an stream with</p> <pre><code>nats stream info STREAM\n</code></pre>"},{"location":"queues/nats/jetstream/01-stream/#basic-settings","title":"Basic Settings","text":"<ul> <li> <p>The name of the stream</p> </li> <li> <p>Description</p> </li> <li> <p>The storage type. It can be file (default) or memory</p> </li> <li> <p>The list of subjects to bind</p> </li> <li> <p>The replicas to keep for each message</p> </li> <li> <p>Mirror stream. A stream can be configured as a mirror of another stream.</p> </li> </ul>"},{"location":"queues/nats/jetstream/01-stream/#limits-and-discard-policy","title":"Limits and discard policy","text":"<ul> <li>Maximum Messages (MaxMsgs)</li> </ul> <p>Maximum number of messages stored in the stream. Adheres to Discard Policy, removing oldest or refusing new messages if the Stream exceeds this number of messages.</p> <ul> <li>Maximum Age (MaxAge)</li> </ul> <p>Maximum age to keep any message in the Stream, expressed in nanoseconds</p> <ul> <li>Maximum Bytes (MaxBytes)</li> </ul> <p>Maximum number of bytes stored in the stream, maximum bytes to keep. Adheres to Discard Policy, removing oldest or refusing new messages if the Stream exceeds this size.</p> <ul> <li> <p>Maximum Message Size (MaxMsgSize) The largest message that will be accepted by the Stream. The size of a message is a sum of payload and headers.</p> </li> <li> <p>Maximum Consumers (MaxConsumers) Maximum number of Consumers allowed, that can be defined for a given Stream, -1 for unlimited. Cannot be edited</p> </li> <li> <p>Discard policy</p> </li> </ul> <p>DiscardOld (default) will delete the oldest messages in order to maintain the limit.</p> <p>DiscardNew will reject new messages from being appended to the stream if it would exceed one of the limits. An extension to this policy is DiscardNewPerSubject which will apply this policy on a per-subject basis within the stream. An extension to this policy is DiscardNewPerSubject which will apply this policy on a per-subject basis within the stream.</p> <ul> <li>DiscardNewPerSubject</li> </ul> <p>If true, applies discard new semantics on a per subject basis. Requires DiscardPolicy to be DiscardNew and the MaxMsgsPerSubject to be set.</p>"},{"location":"queues/rabbitmq/links/","title":"Links","text":"<ul> <li>RabbitMQ Cluster Kubernetes Operator</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/operator-overview</p> <ul> <li>RabbitMQ Messaging Topology Operator</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/install-topology-operator</p>"},{"location":"queues/rabbitmq/updates/","title":"Updates","text":"<p>We can do 2 updates if we are using the rabbitmq cluster kubernetes operator:</p> <ul> <li>Update the operator</li> <li>Update the cluster</li> </ul>"},{"location":"queues/rabbitmq/updates/#operator","title":"Operator","text":"<p>In order to update the operator we only need to update the manifest to the desired releases. All the releases are located here:</p> <p>https://github.com/rabbitmq/cluster-operator/releases</p> <p>The latest version of the operator is located here:</p> <p>https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml</p> <p>Sometimes updating the rabbitmq kubernetes operator also updates the pods of the cluster:</p> <ul> <li> <p>If we are not using spec.image in the definition of the cluster, we are using the default rabbitmq container image. The new release of the operator will probably have a newer default rabbitmq container image, but the operator update will maintain the previous one in exiting clusters. Newer clusters without spec.image will use the new default one.</p> </li> <li> <p>It is possible that updating the operator changes another fields of the podSpec so the pods will be restarted.</p> </li> </ul> <p>A good workaround can be pause the reconciliation, upgrade the operator, resume the reconciliation when you decide to do it</p> <p>To know the default rabbitmq release the operator deploys and if it will cause a restart of the pods, see the changelog</p>"},{"location":"queues/rabbitmq/updates/#pause-the-reconciliation","title":"Pause the reconciliation","text":"<p>In order to pause the reconciliation we need to add this label to the cluster</p> <pre><code>rabbitmq.com/pauseReconciliation: true\n</code></pre>"},{"location":"queues/rabbitmq/updates/#links","title":"Links","text":"<ul> <li>Upgrading the RabbitMQ Kubernetes Operators</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/upgrade-operatorhttps://www.rabbitmq.com/kubernetes/operator/upgrade-operator</p> <ul> <li>Pause Reconciliation for a RabbitMQCluster</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/using-operator#pause</p>"},{"location":"security/cert-manager/98-tips/","title":"Tips","text":""},{"location":"security/cert-manager/98-tips/#pod-identity-agent","title":"Pod identity agent","text":"<p>Para poder usar AWS Pod identity agent en un issuer es necesario habilitar en el controller</p> <pre><code>--issuer-ambient-credentials\n</code></pre> <p>Para cluster issuer viene habilitado por defecto</p> <p>Para poder usarlo es posible que necesites una version mas reciente de cert-manager. En 1.12 no parece funcionar</p>"},{"location":"security/cert-manager/98-tips/#http-tls-handshake-error-from-xxxx-eof","title":"http: TLS handshake error from XXXX EOF","text":"<p>Let cert manager to manage the certificates. Other ways can cause argocd sync and webhook certificates fail</p>"},{"location":"security/cert-manager/98-tips/#auto-clean-secrets","title":"Auto clean secrets","text":"<p>By default cert-manager does not remove a secret when the certificate is removed. We can enable it with the following controller option:</p> <pre><code>--enable-certificate-owner-ref\n</code></pre> <p>For example, deleting an ingress resource removes the certificate. With this setting, also the secret</p> <p>This setting makes the certificate resource as an owner of secret where the tls certificate is stored.</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/","title":"Aws Secrets Manager with PIA","text":""},{"location":"security/external-secrets/aws-secrets-manager-pia/#role-and-policies","title":"Role and policies","text":"<p>Create a role called, for example external-secrets with this trust policy (trust relationship)</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>And with this permission policy called, for example AllowPullSecrets</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"secretsmanager:GetResourcePolicy\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecretVersionIds\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre> <p>The policy can be more precise for your secret</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#aws-secrets-manager","title":"Aws Secrets Manager","text":"<p>Go to Aws Secrets Manager and Store a new secret with \"secret type\" \"Other type of secret\" and put some key/value</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#deploy-external-secrets","title":"Deploy external secrets","text":"<p>Deploy the external dns helm chart</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#configure-the-eks-cluster","title":"Configure the eks cluster","text":"<ul> <li> <p>Under addons, deploy the pod identity agent plugin if not using Eks Auto Mode</p> </li> <li> <p>Under access, create a pod identity association between the external-secrets role and the external-secrets service account</p> </li> </ul>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#create-a-secret-store","title":"Create a secret store","text":"<p>In order to check if it is working, create a secret store</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: mystore\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: YOURREGION\n</code></pre> <p>... and see the logs in the secret manager pod</p> <p>Probably a restart of the external-secrets deployment is needed</p> <pre><code>kubectl rollout restart deployment external-secrets -n external-secrets\n</code></pre>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#links","title":"Links","text":"<ul> <li>AWS Secrets Manager</li> </ul> <p>https://external-secrets.io/latest/provider/aws-secrets-manager/</p>"},{"location":"security/external-secrets/azure-key-vault/","title":"Azure Key Vault","text":""},{"location":"security/external-secrets/azure-key-vault/#authentication-authtype","title":"Authentication (authType)","text":"<p>Defining the (Cluster)SecretStore, external Secrets Operator supports 3 authentication types defined in .spec.provider.azurekv.authType:</p> <p>In all cases you must configure \"environmentType\" and \"vaultUrl\", but there are some differences in setup between the authTypes</p>"},{"location":"security/external-secrets/azure-key-vault/#serviceprincipal","title":"ServicePrincipal","text":"<p>This Azure Service Principal is the default authType and it can be used with:</p> <ul> <li>ClientID and ClientSecret</li> <li>ClientCertificate in PEM format</li> </ul> <p>If we want to use this authentication type, we also need to configure:</p> <ul> <li> <p>authSecretRef: the secret that stores that credential</p> </li> <li> <p>tenantId: the Azure Tenant ID</p> </li> </ul> <pre><code>apiVersion: external-secrets.io/v1\nkind: SecretStore\nmetadata:\n  name: azure-backend\nspec:\n  provider:\n    azurekv:\n      authType: ServicePrincipal\n</code></pre>"},{"location":"security/external-secrets/azure-key-vault/#managedidentity-not-recommended","title":"ManagedIdentity (not recommended)","text":"<p>Uses aad-pod-identity, which was deprecated in 2022 and replaced by Azure Workload Identity</p>"},{"location":"security/external-secrets/azure-key-vault/#workloadidentity","title":"WorkloadIdentity","text":"<p>Replaces aad-pod-identity and requires configuring \"serviceAccountRef\".</p> <p>Optional settings:</p> <ul> <li>tenantId</li> <li>authSecretRef</li> </ul>"},{"location":"security/external-secrets/azure-key-vault/#settings-table","title":"Settings Table","text":"Setting ServicePrincipal ManagedIdentity WorkloadIdentity authType \u2705 Required \u2705 Required \u2705 Required vaultUrl \u2705 Required \u2705 Required \u2705 Required environmentType \u2705 Required \u2705 Required \u2705 Required tenantId \u2705 Required \u274c Not used \u26aa Optional authSecretRef \u2705 Required \u274c Not used \u26aa Optional serviceAccountRef \u274c Not used \u274c Not used \u2705 Required"},{"location":"security/external-secrets/azure-key-vault/#supported-object-types","title":"Supported Object Types","text":"<p>External Secrets Operator can manage all 3 types of objects: secrets, certificates, and keys (jwk)</p> <pre><code>  data:\n    - secretKey: database-username\n        remoteRef:\n            key: database-username # secret without prefix (default value)\n    - secretKey: database-password\n        remoteRef:\n            key: secret/database-password # secret with prefix\n    - secretKey: db-client-cert\n        remoteRef:\n            key: cert/db-client-cert # certificate with prefix\n    - secretKey: encryption-pubkey\n        remoteRef:\n            key: key/encryption-pubkey # key with prefix\n</code></pre>"},{"location":"security/external-secrets/azure-key-vault/#links","title":"Links","text":"<ul> <li>External Secrets Operator and AzureAD</li> </ul> <p>https://external-secrets.io/latest/provider/azure-key-vault/</p> <ul> <li>Api Spec</li> </ul> <p>https://external-secrets.io/latest/api/spec/#external-secrets.io/v1beta1.AzureKVProvider</p> <ul> <li>AAD Pod identity (deprecated)</li> </ul> <p>https://azure.github.io/aad-pod-identity/docs/</p> <ul> <li>Azure AD Workload Identity</li> </ul> <p>https://azure.github.io/azure-workload-identity/docs/</p> <ul> <li>Azure AD Workload Identity Federation</li> </ul> <p>https://docs.microsoft.com/en-us/azure/active-directory/develop/workload-identity-federation</p>"},{"location":"security/external-secrets/backup/","title":"Cloud secret storages and backup","text":"<p>Both have secrets versioning</p>"},{"location":"security/external-secrets/backup/#azure-key-vault","title":"Azure key vault","text":"<ul> <li>Permits manual backups</li> </ul> <p>https://learn.microsoft.com/en-us/azure/key-vault/general/backup</p> <ul> <li>You can enable purge protection at key vault level</li> </ul> <p>https://learn.microsoft.com/en-gb/azure/key-vault/general/soft-delete-overview</p>"},{"location":"security/external-secrets/backup/#aws-secrets-manager","title":"AWS secrets manager","text":"<ul> <li> <p>Permits manual backups https://docs.aws.amazon.com/cli/latest/reference/secretsmanager/</p> </li> <li> <p>If you delete a secret, it is maintained 7 days at least https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_delete-secret.html</p> </li> <li> <p>You can add a replicacion between regions https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/</p> </li> </ul>"},{"location":"security/external-secrets/datafrom/","title":"dataFrom in external secret","text":"<p>dataFrom is a way to get secrets from the secrets provider and it is used when we want to fetch all the keys from the provider with the ability to do some filters.</p> <p>It is an array with entries with \"sourceRef\" as mandatory field and, optionally, \"extract\", \"find\" and \"rewrite\".</p> <p>If multiple entries are specified, the Secret keys are merged in the specified order</p>"},{"location":"security/external-secrets/datafrom/#sourceref","title":"sourceRef","text":"<p>First of all we need to setup the secret store, cluster secret store o generator to obtain the data</p> <p>SourceRef points to a store or generator which contains secret values ready to use. Use this in combination with Extract or Find pull values out of a specific SecretStore. When sourceRef points to a generator Extract or Find is not supported. The generator returns a static map of values</p> <p>SourceRef allows you to override the source from which the secret will be pulled from. You can define at maximum one property.</p> <p>Example: Using a secret store</p> <pre><code>  dataFrom:\n  - sourceRef:\n      storeRef:\n        name: mysecretstore\n        kind: SecretStore # default\n</code></pre> <p>Example: Using a cluster secret store</p> <pre><code>  dataFrom:\n  - sourceRef:\n      storeRef:\n        name: myclustersecretstore\n        kind: ClusterSecretStore\n</code></pre> <p>Example: Using a generator</p> <p>When sourceRef points to a generator, extract or find is not supported.</p> <pre><code>  dataFrom:\n  - sourceRef:\n      generatorRef:\n        apiVersion: generators.external-secrets.io/v1alpha1\n        kind: ECRAuthorizationToken\n        name: \"my-ecr\"\n</code></pre>"},{"location":"security/external-secrets/datafrom/#extract-optional","title":"extract (optional)","text":"<p>Used to extract multiple key/value pairs from one secret</p> <p>mandatory field: key</p> <p>Key is the key used in the Provider, mandatory</p> <p>optional fields:</p> <ul> <li>metadataPolicy</li> <li>property</li> <li>version</li> <li>conversionStrategy</li> <li>decodingStrategy</li> </ul> <p>Documentation about extract</p> <p>https://external-secrets.io/latest/guides/all-keys-one-secret/</p>"},{"location":"security/external-secrets/datafrom/#find-optional","title":"find (optional)","text":"<p>Used to create a secret in kubernetes from several secrets in the provider. With find we fetch them based on certain criteria like \"path\", \"name\" and \"tags\". We can use them when needed combined in the same \"find\" field.</p> <ul> <li>path Specifies a root path to start the find operations. It is only supported in some providers.</li> </ul> <pre><code>  dataFrom:\n  - find:\n      path: path-to-filter\n</code></pre> <ul> <li>name Finds secrets based on the name and an optional regular expression</li> </ul> <pre><code>  dataFrom:\n  - find:\n      name:\n        regexp: \".*myfilter.*\"\n</code></pre> <ul> <li>tags Find secrets based on tags</li> </ul> <pre><code>  dataFrom:\n  - find:\n      tags:\n        environment: \"prod\"\n        application: \"app-name\"\n</code></pre> <p>Another optional fields:</p> <ul> <li>conversionStrategy</li> <li>decodingStrategy</li> </ul> <p>Documentation about find:</p> <p>https://external-secrets.io/latest/guides/getallsecrets/</p>"},{"location":"security/external-secrets/datafrom/#rewrite-optional","title":"rewrite (optional)","text":"<p>Used to rewrite secret Keys after getting them from the secret Provider Multiple Rewrite operations can be provided. They are applied in a layered order (first to last)</p>"},{"location":"security/external-secrets/datafrom/#regexp","title":"regexp","text":"<p>Used to rewrite with regular expressions. The resulting key will be the output of a regexp.ReplaceAll operation.</p> <ul> <li> <p>source Used to define the regular expression of a re.Compiler.</p> </li> <li> <p>target Used to define the target pattern of a ReplaceAll operation.</p> </li> </ul>"},{"location":"security/external-secrets/datafrom/#transform","title":"transform","text":"<p>Used to apply string transformation on the secrets. The resulting key will be the output of the template applied by the operation.</p> <ul> <li>template: Used to define the template to apply on the secret name. .value will specify the secret name in the template.</li> </ul> <p>Documentation about rewrite:</p> <p>https://external-secrets.io/latest/guides/datafrom-rewrite/</p>"},{"location":"security/external-secrets/datafrom/#datafrom-array","title":"dataFrom array","text":"entry sourceRef storeRef sourceRef generatorRef extract key (m) extract other metadataPolicy, property, version, conversionStrategy, decodingStrategy find path find name regexp find tags find other conversionStrategy, decodingStrategy rewrite regexp source, target rewrite transform template"},{"location":"security/external-secrets/datafrom/#links","title":"Links","text":"<ul> <li>ExternalSecretDataFromRemoteRef</li> </ul> <p>https://external-secrets.io/latest/api/spec/#external-secrets.io/v1beta1.ExternalSecretDataFromRemoteRef</p> <ul> <li>External secret</li> </ul> <p>https://external-secrets.io/latest/api/externalsecret/</p>"},{"location":"security/external-secrets/external-secret/","title":"External Secret spec","text":""},{"location":"security/external-secrets/external-secret/#elegir-el-secret-store","title":"Elegir el secret store","text":"<p>Para elegir el secret store del cual traernos los secretos se usa spec.secretStoreRef, donde spec.secretStoreRef.name sera el nombre y spec.secretStoreRef.kind el tipo de secret store (SecretStore o ClusterSecretStore)</p>"},{"location":"security/external-secrets/external-secret/#datos-a-traernos-del-proveedor","title":"Datos a traernos del proveedor","text":"<p>Para traernos los secrets podemos usar:</p> <ul> <li>spec.dataFrom para traernos todas las properties de la key</li> <li>spec.data para elegir cuales traernos</li> </ul>"},{"location":"security/external-secrets/external-secret/#data-specdata","title":"Data (spec.data)","text":"<p>Data permite especificar la relacion entre las keys del secret a crear y el dato almacenado en el proveedor. Asi, dentro de cada una de las relaciones declaradas podemos especificar</p> <p>secretKey es el nombre que le damos a esta relacion</p> <p>remoteRef especifica que dato traernos del proveedor</p> <ul> <li>key: Es valor mas importante, porque es realmente la clave a traerse</li> <li>conversionStrategy: Default | Unicode</li> <li> <p>decodingStrategy: Para elegir si la clave es codificada en el proveedor y debe ser descodificada o no. Opciones: None | Auto | Base64 | Base64URL</p> </li> <li> <p>metadataPolicy: Si queremos traernos tags o labels. Puede ser None (por defecto) o Fetch</p> </li> <li>property: Si queremos traernos una property concreta (depende del proveedor)</li> <li>version: Si queremos traernos una version concreta (depende del proveedor)</li> </ul> <p>sourceRef permite especificar un secret store diferente al de spec.secretStoreRef y es obligatorio si este ultimo no existe. Se declara mediante  storeRef.name y storeRef.kind</p>"},{"location":"security/external-secrets/external-secret/#data-specdatafrom","title":"Data (spec.dataFrom)","text":"<p>Pendiente</p>"},{"location":"security/external-secrets/external-secret/#secret-a-crear-spectarget","title":"Secret a crear (spec.target)","text":"<p>Se hace mediante spec.target elegimos que secret y como crearlo.</p>"},{"location":"security/external-secrets/external-secret/#name","title":"Name","text":"<p>spec.target.name permite elegir el nombre del secret a crear. Si no se especifica, sera el nombre del externalsecret</p>"},{"location":"security/external-secrets/external-secret/#creationpolicy","title":"creationPolicy","text":"<p>spec.target.creationPolicy sirve para elegir de forma se crea el secret</p> <ul> <li>Owner es el valor por defecto. External secrets operator le pone al secret un ownerReference y lo hace susceptible del garbage colector de Kubernetes.</li> </ul> <p>Si al intentar crear el secret se encuentra uno ya existente con otro ownerReference, se genera un conflicto y falla. Si al intentar crear el secret se encuentra uno ya existente sin ownerReference, le pone un ownerReference y lo actualiza</p> <ul> <li> <p>Orphan lo crea sin ownerReference y queda fuera del garbage colector de Kubernetes</p> </li> <li> <p>Merge no crea ningun secret, sino que espera que ya exista y hacer un merge</p> </li> <li> <p>None no hace nada</p> </li> </ul>"},{"location":"security/external-secrets/external-secret/#deletionpolicy","title":"deletionPolicy","text":"<p>spec.target.deletionPolicy permite elegir que hacer con el secret cuando se borra el secret en el proveedor de secretos.</p> <ul> <li> <p>Retain es el valor por defecto y mas cauteloso. Mantiene el secret creado y el externalsecret entra en estado SecretSyncedError</p> </li> <li> <p>Delete borra el secret y el externalsecret no se considera como fallido ni tendra el estado SecretSyncedError. Tambien ocurre al crear un nuevo external secret con esta opcion si falla al mapear con el secret del proveedor.</p> </li> <li> <p>Merge borra las entradas del secret, pero no el secret en si. Al igual que con delete, o se considera como fallido ni tendra el estado SecretSyncedError.</p> </li> </ul>"},{"location":"security/external-secrets/external-secret/#immutable","title":"immutable","text":"<p>spec.target.immutable permite hacer inmutable el secret</p>"},{"location":"security/external-secrets/external-secret/#template","title":"Template","text":"<p>Pendiente</p>"},{"location":"security/external-secrets/external-secret/#otras-configuraciones","title":"Otras configuraciones","text":"<ul> <li>spec.refreshInterval Permite especificar cada cuanto leer los valores del provider. Se puede expresar en varias unidades como \"s\", \"m\" o \"h\" y esta basado en time.ParseDuration de go</li> </ul> <p>El valor por defecto es una hora Si se configura a 0, el valor se trae solo una vez al crearse</p>"},{"location":"security/external-secrets/external-secret/#ejemplo","title":"Ejemplo","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: myexternalsecret\nspec:\n  data:\n  - remoteRef:\n      key: mipassword # key de secret store donde buscar el valor\n    secretKey: mikey # key a escribir en el secret\n  refreshInterval: 1h # cada cuanto leer el secret del proveedor\n  secretStoreRef:  # especificar el secret store del cual obtener los datos\n    kind: ClusterSecretStore\n    name: nombredelsecretstore\n  target:  # definicion del secret a crear\n    name: secret # nombre del secret a crear. si o se especifica, sera el nombre del external secret\n    creationPolicy:  # Owner (por defecto) Merge o None\n    deletionPolicy:     # Delete, Merge, Retain\n</code></pre>"},{"location":"security/external-secrets/external-secret/#links","title":"Links","text":"<ul> <li> <p>ExternalSecret https://external-secrets.io/latest/api/externalsecret/</p> </li> <li> <p>Lifecycle https://external-secrets.io/latest/guides/ownership-deletion-policy/</p> </li> <li> <p>Decoding strategy https://external-secrets.io/latest/guides/decoding-strategy/</p> </li> </ul>"},{"location":"security/external-secrets/tips/","title":"Tips","text":""},{"location":"security/external-secrets/tips/#reduce-the-secretstore-and-clustersecretstore-calls","title":"Reduce the secretstore and clustersecretstore calls","text":"<p>By default the controller validates the secretstore and clustersecretstore every 5 minutes We can change this default behaviour with the store-requeue-interval parameter</p> <p>In the helm chart</p> <pre><code>extraArgs:\n  store-requeue-interval: 1h # increase to 1 h\n</code></pre> <p>Also we can override the controller's default value in the secretstore resource</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: myss\nspec:\n  refreshInterval: 30m\n</code></pre> <p>and clustersecretstore resource</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: mycss\nspec:\n  refreshInterval: 2h\n</code></pre>"},{"location":"security/external-secrets/tips/#reduce-the-externalsecret-and-clusterexternalsecret-calls","title":"Reduce the externalsecret and clusterexternalsecret calls","text":"<p>pending</p>"},{"location":"security/external-secrets/tips/#template-bad-character-u002d-","title":"template bad character U+002D '-'","text":"<p>This is a limitation of the go template language. You can not use \"-\" in variable names (secretKey)</p> <pre><code>secretKey: my-key # this fails\n</code></pre> <pre><code>secretKey: myKey # this works\n</code></pre>"},{"location":"security/external-secrets/tls-secret/","title":"Create a tls secret","text":"<p>If we have uploaded a certificate and private key, to our secret manager, we can create a tls kubernetes secret</p>"},{"location":"security/external-secrets/tls-secret/#in-aws-secrets-manager","title":"In aws secrets manager","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: tls-aws-secrets-mamager\nspec:\n  template:\n    type: kubernetes.io/tls\n    data:\n        - remoteRef:\n            decodingStrategy: Base64 # if the value is in base64\n            key: my-aws-secret # name of the aws secret\n            property: tls.crt # key inside the aws secret\n        secretKey: tls.crt # key we want in the secret\n        - remoteRef:\n            decodingStrategy: Base64 # if the value is in base64\n            key: my-aws-secret # name of the aws secret\n            property: tls.key  # key inside the aws secret\n        secretKey: tls.key # key we want in the secret\n  secretStoreRef:\n    kind: SecretStore\n    name: mystore # it can be a ClusterSecretStore \n</code></pre>"},{"location":"security/external-secrets/updates/","title":"Updates","text":"<p>First update to the latest patch release of the current installed major version Next update to latest patch releaes of the next major version</p>"},{"location":"security/external-secrets/updates/#from-015x-to-016x","title":"From 0.15.X to 0.16.X","text":"<p>Some breaking changes</p> <ul> <li>Removal of Conversion Webhooks and SecretStore/v1alpha1, ExternalSecret/v1alpha1 and their cluster counterparts</li> <li>Promotion of ExternalSecret/v1 and SecretStore/v1 and their cluster counterparts</li> <li>Removal of v1 templating engine</li> </ul> <p>Check if you can upgrade</p> <pre><code>kubectl get crd \\\n    externalsecrets.external-secrets.io\\\n    secretstores.external-secrets.io\\\n    clustersecretstores.external-secrets.io\\\n    clusterexternalsecrets.external-secrets.io\\\n    -o jsonpath='{.items[*].status.storedVersions[?(@==\"v1alpha1\")]}' | \\\n    grep -q v1alpha1 &amp;&amp; echo \"NOT SAFE! REMOVE v1alpha1 FROM YOUR STORED VERSIONS\" || echo \"Safe to Continue\"\n</code></pre> <p>If you get this error</p> <pre><code>conversion webhook for external-secrets.io/v1, Kind=ExternalSecret failed: the server could not find the requested resource\n</code></pre> <p>Simply force a refresh in an external secret from that cluster external secret</p> <pre><code>kubectl annotate externalsecrets.external-secrets.io MYCES force-sync=$(date +%s) --overwrite\n</code></pre> <ul> <li>Promoting to 0.16 #4662</li> </ul> <p>https://github.com/external-secrets/external-secrets/issues/4662</p>"},{"location":"security/external-secrets/updates/#from-016x-to-017x","title":"From 0.16.X to 0.17.X","text":"<p>Breaking change</p> <ul> <li>v0.17.0 Stops serving v1beta1 apis</li> </ul> <p>So it is needed to update the manifests v1beta1 to v1 prior to updating from v0.16 to v0.17. We need at least v0.16.2 because it supports v1.</p> <pre><code>#!/bin/bash\n\necho \"Searching apiVersion: external-secrets.io/v1 in yaml files\"\necho \"############### Showing alpha references ###############\"\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1alpha\"\necho\n\necho \"############### Showing beta references ###############\"\necho\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1beta\"\n\necho \"############### Showing non alpha or beta references ###############\"\necho\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1\" | grep -v \"v1alpha\" | grep -v \"v1beta\"\n</code></pre> <pre><code>#!/bin/bash\necho \"Changing all v1beta1 references to v1\"\nfind . \\( -name \"*.yaml\" -o -name \"*.yml\" \\) -print0 | while IFS= read -r -d '' yaml_file; do\n    if grep \"apiVersion: external-secrets.io/v1beta1\" \"$yaml_file\" &gt;/dev/null; then\n        echo \"Changing v1beta1 to v1 in $yaml_file\"\n        sed -i 's/apiVersion: external-secrets.io\\/v1beta1/apiVersion: external-secrets.io\\/v1/g' \"$yaml_file\"\n    fi\ndone\n</code></pre>"},{"location":"security/external-secrets/recipes/dockerconfigjson/","title":"Create a dockerconfigjson","text":""},{"location":"security/external-secrets/recipes/dockerconfigjson/#using-user-password-and-url","title":"Using user, password and url","text":"<p>With this template we can create an external secret to pull images from a private repository if we have the username, password and url stored in our secret store. The important thing here is the template. The data section can be different for every secret store.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: privatepull\nspec:\n  data:\n  - remoteRef:\n      key: PULL-APPS-U\n    secretKey: username\n  - remoteRef:\n      key: PULL-APPS-P\n    secretKey: password\n  - remoteRef:\n      key: PULL-APPS-URL\n    secretKey: url\n  secretStoreRef:\n    kind: SecretStore\n    name: mystore\n  target:\n    template:\n      data:\n        .dockerconfigjson: |\n          {\n            \"auths\": {\n              \"{{ .url  }}\": {\n                \"username\": \"{{ .username }}\",\n                \"password\": \"{{ .password }}\",\n                \"email\": \"\"\n              }\n            }\n          }\n      type: kubernetes.io/dockerconfigjson\n</code></pre>"},{"location":"security/external-secrets/recipes/dockerconfigjson/#using-the-base64","title":"Using the base64","text":"<p>pending</p>"},{"location":"security/external-secrets/recipes/harbor-auth/","title":"Vmware harbor auth","text":"<p>This recipe creates the Authorization header using the username and password</p> <p>The spec.data section changes depending of the secret store provider</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: harbor-credentials\nspec:\n  data:\n    - remoteRef:\n        key: secret/harbor-credentials-u\n      secretKey: username\n    - remoteRef:\n        key: secret/harbor-credentialsr-p\n      secretKey: password\n  target:\n    template:\n      data:\n        auth: '{{ printf \"Basic %s\" (printf \"%s:%s\" .username .password | b64enc) }}'\n  secretStoreRef:\n    ...\n</code></pre>"},{"location":"security/external-secrets/recipes/kured-teams/","title":"Kured notifications to","text":"<p>Microsoft changed the url of the webhook created in Teams. In order to support kured notifications, this recipe adapts that webhook url to the new format.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kured-settings\nspec:\n ...\n  target:\n    template:\n      data:\n        KURED_NOTIFY_URL: '{{ list .msteamsWebhookUrl \"template=json&amp;messagekey=text\" | join \"?\" | replace \"https://\" \"generic://\" }}'\n</code></pre> <p>And in the kured values.yaml</p> <pre><code>extraEnvVars:\n  - name: KURED_NOTIFY_URL\n    valueFrom:\n      secretKeyRef:\n        name: kured-settings\n        key: KURED_NOTIFY_URL\n</code></pre>"},{"location":"security/external-secrets/recipes/kured-teams/#links","title":"Links","text":"<ul> <li>MS Teams notifications are currently broken with new message format</li> </ul> <p>https://github.com/kubereboot/kured/issues/1024</p> <ul> <li>Add ability to set cli flags with environment variables</li> </ul> <p>https://github.com/kubereboot/kured/issues/383</p>"},{"location":"security/infisical/98-tips/","title":"Tips","text":""},{"location":"security/infisical/98-tips/#error-unsupported-state-or-unable-to-authenticate-data","title":"Error \"Unsupported state or unable to authenticate data\"","text":"<ul> <li>Link https://github.com/Infisical/infisical/issues/2005</li> </ul> <p>The only way I've been able to change the encryption keys so far is to delete the containers, images, and volumes related to infisical in Docker Desktop, and run docker compose up with the desired keys in .env to rebuild the entire system from scratch. I didn't see any other way in documentation or browsing online.</p>"},{"location":"security/infisical/98-tips/#get-stable-releases","title":"Get stable releases","text":"<p>The versioning has changed over time.</p> <ul> <li>First releases using semver are only searcing all tags</li> </ul> <pre><code>curl -s \"https://api.github.com/repos/Infisical/infisical/git/refs/tags\" |  jq -r '.[].ref' | grep -v nightly | grep -v postgre | sed 's|refs/tags/||'\n</code></pre> <ul> <li>Some releases had -postgres suffix. This stopped at v0.147.0 release. See https://github.com/Infisical/infisical/releases/tag/v0.147.0</li> </ul> <pre><code>curl -s \"https://api.github.com/repos/Infisical/infisical/git/refs/tags\" |  jq -r '.[].ref' | grep postgres | sed 's|refs/tags/||'\n</code></pre> <ul> <li>GitHub Releases</li> </ul> <p>Now they use github releases</p> <pre><code>curl -s \"https://api.github.com/repos/Infisical/infisical/releases?per_page=100\" | jq -r '.[].tag_name' | grep -E '^v0\\.[0-9]+\\.[0-9]+$'\n</code></pre>"},{"location":"security/infisical/metrics/","title":"Metrics","text":""},{"location":"security/infisical/metrics/#prometheus-metrics","title":"Prometheus metrics","text":"<p>In order to enable the infisical prometheus metrics we must add the following environment variables to the infisical pod</p> <pre><code>OTEL_TELEMETRY_COLLECTION_ENABLED: \"true\"\nOTEL_EXPORT_TYPE: \"prometheus\"\n</code></pre> <p>This enables the metrics in the port 9464 and /metrics path</p>"},{"location":"security/infisical/metrics/#relevant-metrics","title":"Relevant metrics","text":"<ul> <li>API latency</li> </ul> <pre><code>API_latency_count\nAPI_latency_sum\nAPI_latency_bucket\n</code></pre> <ul> <li>Http server duration:</li> </ul> <p>Measures the duration of inbound HTTP requests</p> <pre><code>http_server_duration_count\nhttp_server_duration_sum\nhttp_server_duration_bucket\n</code></pre> <pre><code>API_errors_count\n</code></pre> <p>API_errors_count tracks the total number of errors that occurred for the specified route, method, and error type.</p> <pre><code>API_errors_sum\n</code></pre> <p>API_errors_sum tracks the cumulative value of the errors.</p> <pre><code>API_errors_bucket\n</code></pre> <p>API_errors_bucket is an histogram that measures the distribution of errors over different time buckets (latency or duration). Each bucket represents the number of errors that occurred within a specific time range (e.g., less than or equal to 5ms, 10ms, etc.). Labels: route: The API route (/api/v3/secrets/raw). method: The HTTP method (GET). type: The type of error (RateLimitError). name: The specific error name (RateLimitExceeded). le: The upper bound of the bucket (e.g., 5ms, 10ms, etc.).</p>"},{"location":"security/infisical/metrics/#some-tips","title":"Some tips","text":"<ul> <li>If you have an old infisical version you must probably need to update to get the metrics</li> <li>The helm chart currently does not support adding the environment variables and pod's port. The environment variables can be provided via the infisical-secrets secret.</li> </ul> <p>https://github.com/Infisical/infisical/issues/3382</p>"},{"location":"security/infisical/update-via-argocd/","title":"How to update infisical with argocd","text":"<p>In order to update infisical self hosted via slack we must do some steps</p>"},{"location":"security/infisical/update-via-argocd/#steps","title":"Steps","text":"<ul> <li> <p>Backup the database Do a backup in the database</p> </li> <li> <p>Disable autosync Disable autosync if enabled in the argocd application</p> </li> <li> <p>Change the values.yaml Change the values.yaml with the new image release.</p> </li> </ul> <p>We can get the infisical releases here: https://github.com/Infisical/infisical/releases</p> <p>And the infisical container releases here: https://hub.docker.com/r/infisical/infisical/tags</p> <ul> <li> <p>Sync the job Sync the job with Force and Replace options and see the log of the job's pod</p> </li> <li> <p>sync the deployment Sync the infisical deployment and see the log of the deployment's pod</p> </li> <li> <p>Enable autosync Enable autosync if needed in the argocd application</p> </li> </ul>"},{"location":"security/infisical/update-via-argocd/#links","title":"Links","text":"<ul> <li> <p>Kubernetes via helm chart https://infisical.com/docs/self-hosting/deployment-options/kubernetes-helm</p> </li> <li> <p>Schema migration https://infisical.com/docs/self-hosting/configuration/schema-migrations</p> </li> </ul>"},{"location":"security/keycloak/99-links/","title":"Links","text":""},{"location":"security/keycloak/99-links/#general-documentation","title":"General documentation","text":"<ul> <li>Guides</li> </ul> <p>https://www.keycloak.org/guides</p> <ul> <li>Documentation</li> </ul> <p>https://www.keycloak.org/documentation</p> <ul> <li>Redhat Build of keycloak</li> </ul> <p>https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/</p>"},{"location":"security/keycloak/99-links/#specific-documentation","title":"Specific documentation","text":"<ul> <li>Server Administration Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/server_admin/</p> <ul> <li>Server Developer Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/server_development/</p> <ul> <li>Authorization Services Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/authorization_services/</p> <ul> <li>Keycloak Admin REST API</li> </ul> <p>https://www.keycloak.org/docs-api/latest/rest-api/index.html</p>"},{"location":"security/keycloak/99-links/#updates-and-releases","title":"Updates and releases","text":"<ul> <li>Release Notes</li> </ul> <p>https://www.keycloak.org/docs/latest/release_notes/</p> <ul> <li>Upgrading Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/upgrading/index.html</p>"},{"location":"security/keycloak/container/","title":"Some links about keycloak in kubernetes and containers","text":""},{"location":"security/keycloak/container/#keycloak-operator","title":"Keycloak operator","text":"<ul> <li> <p>Installation https://www.keycloak.org/operator/installation</p> </li> <li> <p>Basic deployment https://www.keycloak.org/operator/basic-deployment</p> </li> <li> <p>Advanced configuration https://www.keycloak.org/operator/advanced-configuration</p> </li> <li> <p>Using custom Keycloak images https://www.keycloak.org/operator/customizing-keycloak</p> </li> <li> <p>Realm import https://www.keycloak.org/operator/realm-import</p> </li> </ul>"},{"location":"security/keycloak/container/#containers","title":"Containers","text":"<ul> <li> <p>Getting started: Docker https://www.keycloak.org/getting-started/getting-started-docker</p> </li> <li> <p>Getting started: Podman https://www.keycloak.org/getting-started/getting-started-podman</p> </li> <li> <p>Getting started: Kubernetes https://www.keycloak.org/getting-started/getting-started-kube</p> </li> <li> <p>Getting started: Openshift https://www.keycloak.org/getting-started/getting-started-openshift</p> </li> <li> <p>Running Keycloak in a container https://www.keycloak.org/server/containers</p> </li> </ul>"},{"location":"security/keycloak/logging/","title":"Keycloak logging","text":"<p>Two ways to setup a more verbose loggin using the keycloak operator</p>"},{"location":"security/keycloak/logging/#keycloak-logging-handler","title":"Keycloak logging handler","text":"<p>We can choose between these handlers: console (default), file, syslog and gelf</p> <p>Also we can use the environment variable KC_LOG</p> <p>We to change the fields of the template used by the logging system</p> <ul> <li>using the console handler: --log-console-format and KC_LOG_CONSOLE_FORMAT</li> <li>using the file handler: --log-file-format and KC_LOG_FILE_FORMAT</li> </ul> <p>Another options are:</p> <ul> <li>Change the output to json instead of plain</li> <li>Enable color logging using the console handler</li> </ul>"},{"location":"security/keycloak/logging/#keycloak-logging-level","title":"Keycloak logging level","text":"<p>To enable a more verbose logging we can use the \"--log-level\" parameter and choose between several log levels: FATAL, ERROR, WARN, INFO, DEBUG, TRACE, ALL, OFF. The error level can be in lowercase or uppercase.</p> <p>Also we can use the environment variable KC_LOG_LEVEL</p> <p>It is possible to setup a different log level to the default (root) per category, with this format:</p> <pre><code>--log-level=\"&lt;root-level&gt;,&lt;org.category1&gt;:&lt;org.category1-level&gt;\"\n</code></pre>"},{"location":"security/keycloak/logging/#hostname-debug","title":"Hostname debug","text":"<p>For debugging hostname related problems, we can enable a debug feature using --hostname-debug=true Also we can use the environment variable KC_HOSTNAME_DEBUG</p> <p>After enabling it, we can access to MYKEYCLOAKURL/realms/master/hostname-debug to see some hostname information</p>"},{"location":"security/keycloak/logging/#example-in-the-keycloak-operator","title":"Example in the keycloak operator","text":"<pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  additionalOptions:\n    - name: hostname-debug\n      value: \"true\"\n    - name: log-level\n      value: DEBUG\n</code></pre>"},{"location":"security/keycloak/logging/#links","title":"Links","text":"<ul> <li> <p>Configure logging https://www.keycloak.org/server/logging</p> </li> <li> <p>Logging settings https://www.keycloak.org/server/all-config#category-logging</p> </li> <li> <p>Hostname troubleshooting https://www.keycloak.org/server/hostname#_troubleshooting</p> </li> <li> <p>Admin console not loading and hostname related issues #14666 https://github.com/keycloak/keycloak/issues/14666</p> </li> </ul>"},{"location":"security/keycloak/multi-yaml-deploy/","title":"Deploy multiple operators via yaml","text":""},{"location":"security/keycloak/multi-yaml-deploy/#provided-manifests","title":"Provided manifests","text":"<p>Keycloak team offers 3 yaml files:</p> <ul> <li>The keycloak resource crd</li> <li>The keycloakrealmimports crd</li> <li>The operator depllyment and other needed resources</li> </ul>"},{"location":"security/keycloak/multi-yaml-deploy/#the-problems","title":"The problems","text":"<ul> <li> <p>The keycloak operator does not support watching the resources it manages created in all namespaces, so the operator must me deployed in every namespace you create that resources.</p> </li> <li> <p>Updating the crds must be aligned with every operator instance</p> </li> <li> <p>Most of the resources in the operator yaml file are namespaced, but this includes a ClusterRoleBinding binded to the keycloak-operator service account in the keycloak namespace.</p> </li> </ul> <pre><code>subjects:\n  - kind: ServiceAccount\n    name: keycloak-operator\n    namespace: keycloak\n</code></pre> <p>But this gives openshift related permissions, so it can be ignored if you dont use Openshift</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: keycloak-operator-clusterrole\nrules:\n  - apiGroups:\n      - config.openshift.io\n...\n</code></pre>"},{"location":"security/keycloak/multi-yaml-deploy/#the-deployment","title":"The deployment","text":"<p>So we can do this via kustomize</p> <p>The operator-crds folder and its kustomization.yaml file</p> <pre><code>resources:\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/keycloaks.k8s.keycloak.org-v1.yml\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml\n</code></pre> <p>The operator-base folder and its kustomization.yaml file</p> <pre><code>resources:\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/kubernetes.yml\n</code></pre> <p>Our deployment folder and its kustomization.yaml file will deploy the operator in 2 namespaces but the crds once</p> <pre><code>resources:\n  - ../operator-crds-folder-location\n  - ns-1\n  - ns-2\n</code></pre> <p>Inside every namespace folder, we need a kustomization file with a suffix (or prefix). This adds a suffix or prefix to the resource name.</p> <pre><code>namespace: ns-1\nresources:\n  - ../operator-base-folder-location\nnameSuffix: -ns-1\n</code></pre> <pre><code>namespace: ns-2\nresources:\n  - ../operator-base-folder-location\nnameSuffix: -ns-2\n</code></pre> <p>The ClusterRoleBinding created with our prefix is binded to an unexistant service account, but it is openshift related.</p>"},{"location":"security/keycloak/multi-yaml-deploy/#update-the-release","title":"Update the release","text":"<p>In order to update the release we only need to change the urls in the operator-crds and operator-base folders</p>"},{"location":"security/keycloak/start-modes/","title":"Start modes","text":"<p>There are 2 ways to start keycloak, development and production mode</p>"},{"location":"security/keycloak/start-modes/#development-mode","title":"Development mode","text":"<p>In development mode we assume by default:</p> <ul> <li>HTTP is enabled</li> <li>Strict hostname resolution is disabled</li> <li>Cache is set to local (No distributed cache mechanism used for high availability)</li> <li>Theme-caching and template-caching is disabled</li> </ul> <p>If we want to start keycloak in development mode using the binary, we must use this</p> <pre><code>bin/kc.[sh|bat] start-dev\n</code></pre> <p>If we want to start keycloak in development mode using the operator, we must use this</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  unsupported:\n    podTemplate:\n      spec:\n        containers:\n          - name: keycloak\n            args:\n              - start-dev\n</code></pre> <p>In the logs we can read:</p> <pre><code>Hostname settings: Base URL: &lt;unset&gt;, Hostname: &lt;request&gt;, Strict HTTPS: false, Path: &lt;request&gt;, Strict BackChannel: false, Admin URL: &lt;unset&gt;, Admin: &lt;request&gt;, Port: -1, Proxied: true\nRunning the server in development mode. DO NOT use this configuration in production.\n</code></pre>"},{"location":"security/keycloak/start-modes/#production-mode","title":"Production mode","text":"<p>In the production mode we assume by default:</p> <ul> <li>HTTP is disabled as transport layer security (HTTPS) is essential</li> <li>Hostname configuration is expected</li> <li>HTTPS/TLS configuration is expected</li> </ul> <p>If we want to start keycloak in production mode using the binary, we must use this</p> <pre><code>bin/kc.[sh|bat] start\n</code></pre> <p>The production mode is the default mode in the keycloak operator. The minimun setup to make it work is</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  http:\n    tlsSecret: MYSECRET_CONTAINING_CERTIFICATE\n  hostname:\n    hostname: MY.DOMAIN.COM\n</code></pre> <p>If we dont provide this values, we can get some errors</p> <pre><code>Key material not provided to setup HTTPS. Please configure your keys/certificates or start the server in development mode.\n</code></pre>"},{"location":"security/keycloak/start-modes/#production-mode-disabling-tls-and-change-cache-to-local","title":"Production mode disabling tls and change cache to local","text":"<pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  http:\n    httpEnabled: true\n  hostname:\n    strict: false\n  additionalOptions:\n    - name: cache\n      value: local\n</code></pre>"},{"location":"security/keycloak/start-modes/#links","title":"Links","text":"<ul> <li>Starting Keycloak https://www.keycloak.org/server/configuration#_starting_keycloak</li> </ul>"},{"location":"security/keycloak/upgrade/","title":"Upgrade keycloak","text":""},{"location":"security/keycloak/upgrade/#how-to-upgrade-keycloaks-from-minor-releases","title":"How to upgrade keycloaks from minor releases","text":"<ul> <li> <p>Review the migration changes to the new minor release</p> </li> <li> <p>Shutdown keycloak</p> </li> <li> <p>Backup the database</p> </li> </ul> <p>A threshold of 300000 records exists for automatic migration</p>"},{"location":"security/keycloak/upgrade/#upgrade-to-keycloak","title":"Upgrade to keycloak","text":"<p>https://www.keycloak.org/2024/06/keycloak-2500-released</p>"},{"location":"security/keycloak/upgrade/#upgrade-the-keycloak-server","title":"Upgrade the Keycloak server","text":""},{"location":"security/keycloak/upgrade/#upgrade-the-keycloak-adapters","title":"Upgrade the Keycloak adapters","text":"<p>Upgrade the Keycloak Client Libraries (Admin client, Authorization client, Policy enforcer). These are released independently of the Keycloak server and could be typically updated independently of the Keycloak server as the last released version of the client libraries should be compatible with the last released version of the Keycloak server. For more information, see the Upgrading Keycloak Client libraries. Update the server</p>"},{"location":"security/keycloak/upgrade/#from-keycloak-25-to-26","title":"From keycloak 25 to 26","text":"<p>https://www.keycloak.org/docs/26.2.5/upgrading/ https://www.keycloak.org/operator/advanced-configuration https://www.keycloak.org/operator/basic-deployment https://www.keycloak.org/operator/rolling-updates https://www.keycloak.org/server/containers https://www.keycloak.org/server/hostname https://www.keycloak.org/2024/03/keycloak-2400-released https://www.keycloak.org/server/all-config</p> <p>hostname v1</p> <p>The Keycloak Operator has removed the hostname v1 feature, which was deprecated in Keycloak 25 and replaced by hostname v2. If you are using hostname v1, you must migrate to hostname v2. This includes updating your configuration files and Keycloak Operator Custom Resources.</p>"},{"location":"security/keycloak/upgrade/#links","title":"Links","text":"<p>https://www.keycloak.org/docs/latest/upgrading/ https://www.keycloak.org/docs/latest/release_notes/ https://www.keycloak.org/blog-archive https://www.keycloak.org/2024/06/keycloak-2500-released</p> <p>hostname v1</p> <p>The Keycloak Operator has removed the hostname v1 feature, which was deprecated in Keycloak 25 and replaced by hostname v2. If you are using hostname v1, you must migrate to hostname v2. This includes updating your configuration files and Keycloak Operator Custom Resources.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/","title":"Endpoints","text":"<p>Keycloak exposes endpoints across 4 groups. The base URL for each group impacts token issuance, redirect link generation, and OIDC discovery.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#frontend-group","title":"Frontend group","text":"<p>Publicly accessible. Used for browser-based flows and user interaction.</p> Path Description <code>/realms/{realm}/protocol/openid-connect/auth</code> OIDC Authorization / Login <code>/realms/{realm}/protocol/saml</code> SAML SSO <code>/realms/{realm}/protocol/openid-connect/auth/device</code> Device Authorization <code>/realms/{realm}/login-actions/...</code> Consent / Registration actions <code>/realms/{realm}/account/</code> Account management (user self-service) <code>/realms/{realm}/.well-known/openid-configuration</code> OIDC Discovery Document"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#backend-group","title":"Backend group","text":"<p>Programmatic client-to-server communication. Handles token operations without user interaction.</p> Path Description <code>/realms/{realm}/protocol/openid-connect/token</code> Token Endpoint <code>/realms/{realm}/protocol/openid-connect/token/introspect</code> Token Introspection <code>/realms/{realm}/protocol/openid-connect/logout</code> End Session (Logout) <code>/realms/{realm}/protocol/openid-connect/revoke</code> Token Revocation <code>/realms/{realm}/protocol/openid-connect/userinfo</code> Userinfo <code>/realms/{realm}/protocol/openid-connect/certs</code> JWKS URI <code>/realms/{realm}/clients-registrations/...</code> Client Registration <code>/realms/{realm}/protocol/saml/descriptor</code> SAML Descriptor (metadata)"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#administration-group","title":"Administration group","text":"<p>Not exposed publicly.</p> Path Description <code>/admin/</code> Administration Console (web UI) <code>/admin/realms/{realm}/...</code> Admin REST API <code>/resources/</code> Static assets (CSS, JS for admin console)"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#management-interface","title":"Management interface","text":"<p>Dedicated port 9000 (separate from the main server). Health is enabled by default; metrics require opt-in via <code>KC_METRICS_ENABLED</code> or <code>spec.additionalOptions</code>.</p> Path Purpose <code>/health/ready</code> Readiness probe <code>/health/live</code> Liveness probe <code>/health/started</code> Startup probe <code>/metrics</code> Prometheus metrics <p>Port: <code>KC_HTTP_MANAGEMENT_PORT</code> / <code>spec.httpManagement.port</code> (default 9000).</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#exposure-recommendations","title":"Exposure recommendations","text":"<p>Only expose HTTPS (port 8443 via <code>KC_HTTPS_PORT</code>). Do not enable HTTP.</p> Path Exposure Notes <code>/realms/</code> Public Core OIDC/SAML endpoints <code>/resources/</code> Public Static assets for login pages and admin console <code>/js/</code> Public Keycloak JS adapter <code>/admin/</code> Internal only Admin UI and REST API <code>/health</code> Internal only Management interface \u2014 port 9000 <code>/metrics</code> Internal only Management interface \u2014 port 9000"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#admin-console-access","title":"Admin console access","text":"<p>The admin web UI (<code>/admin/</code>) should be exposed on a separate hostname using <code>--hostname-admin</code> / <code>KC_HOSTNAME_ADMIN</code> / <code>spec.hostname.admin</code>. The admin console also needs <code>/resources/</code> for its static assets (already public).</p> <p>Never expose <code>/admin/</code> on the same public hostname as the frontend. Restrict it to a VPN, internal network, or a separate ingress with IP allowlisting.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#internal-pod-calls-via-the-kubernetes-service","title":"Internal pod calls via the Kubernetes service","text":"<p>When a pod calls Keycloak using the internal Kubernetes service DNS name (e.g., <code>keycloak-service.keycloak.svc.cluster.local</code>), the <code>Host</code> header contains that name \u2014 not the configured <code>KC_HOSTNAME</code>.</p> <p>With <code>hostname-strict: true</code> (default), Keycloak rejects these requests. Two solutions:</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#option-1-backchannel-dynamic-resolution-kc_hostname_backchannel_dynamic-spechostnamebackchanneldynamic","title":"Option 1 - backchannel dynamic resolution (<code>KC_HOSTNAME_BACKCHANNEL_DYNAMIC</code> / <code>spec.hostname.backchannelDynamic</code>)","text":"<p>Keycloak accepts requests from any hostname on the backchannel. Issued tokens still carry the public <code>KC_HOSTNAME</code> in the <code>iss</code> claim, so token validation against the JWKS URI is unaffected.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#option-2-split-horizon-dns","title":"Option 2 - split-horizon DNS","text":"<p>Configure in-cluster DNS so the public hostname resolves to the internal service ClusterIP. Pods use the public hostname; <code>Host</code> headers match; tokens are fully consistent. More complex to set up but behaviorally identical to external traffic.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#links","title":"Links","text":"<ul> <li>Configuring the hostname (v2): https://www.keycloak.org/server/hostname</li> <li>Management Interface: https://www.keycloak.org/server/management-interface</li> <li>Configuring a reverse proxy: https://www.keycloak.org/server/reverseproxy</li> <li>Configuring Keycloak for production: https://www.keycloak.org/server/configuration-production</li> </ul>"},{"location":"security/keycloak/endpoint-groups/01-hostname/","title":"Hostname","text":"<p>In order to make Keycloak accessible via the frontend URL, we must configure the hostname option:</p> <pre><code>via cli: --hostname parameter\nvia environment variable: KC_HOSTNAME\nvia operator: spec.hostname.hostname\n</code></pre> <p>The parts of the base URL we dont specify will be resolved dynamically with the request. Some hostname examples:</p> <pre><code>my.keycloak.org  &lt; hostname only\n&lt;https://my.keycloak.org&gt;  &lt; scheme, hostname\n&lt;https://my.keycloak.org:123/auth&gt;  &lt; scheme, hostname, port, path\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#dynamic-resolution-for-frontchannel","title":"Dynamic resolution for frontchannel","text":"<p>The hostname option is mandatory by default because of security reasons and this behaviour is controlled with the following setting:</p> <pre><code>via cli: --hostname-strict parameter\nvia environment variable: KC_HOSTNAME_STRICT\nvia operator: spec.hostname.strict\n</code></pre> <p>This option is enabled by default and disables dynamically resolving the hostname from request headers.</p> <p>It should always be set to true in production, unless your reverse proxy overwrites the Host header. If enabled, the hostname option needs to be specified.</p> <p>If don't want to specify the hostname and make it fully dynamic we must change it to false.</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#dynamic-resolution-for-backchannel","title":"Dynamic resolution for backchannel","text":"<p>It is possible to permit dynamic resolution for backchannel communications, then this baseURL is dynamically resolved based on incoming headers (hostname, scheme, port and context path). This permits applications and clients using an internal URL for communication while maintaining the use of a public URL for frontchannel requests.</p> <p>By default is set to false.</p> <pre><code>via cli: --hostname-backchannel-dynamic parameter\nvia environment variable: KC_HOSTNAME_BACKCHANNEL_DYNAMIC\nvia operator: spec.hostname.backchannelDynamic\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#administration-url","title":"Administration url","text":"<p>We can also use a different base URL for the administration console. This is done with the following setting:</p> <pre><code>via cli: --hostname-admin parameter\nvia environment variable: KC_HOSTNAME_ADMIN\nvia operator: spec.hostname.admin\n</code></pre> <p>This parameter accepts a full url. Example:</p> <p>https://admin.my.keycloak.org:8443</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#administration-rest-api-endpoints","title":"Administration REST API endpoints","text":"<p>This option only applies to the administration console. The Administration REST API endpoints are accesible via the frontend URL specified by the hostname option.</p> <p>If you want to restrict access to the Administration REST API, you need to do it on the reverse proxy level. Administration Console implicitly accesses the API using the URL as specified by the hostname-admin option.</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#troubleshooting","title":"Troubleshooting","text":"<p>It is possible to troubleshoot the hostname configuration with the following setting:</p> <pre><code>via cli: --hostname-debug paramter\nvia environment variable: KC_HOSTNAME_DEBUG\n</code></pre> <p>via operator:</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  additionalOptions:\n    - name: hostname-debug\n      value: \"true\"\n</code></pre> <p>Then the debug site will be available under <code>/realms/{your-realm}/hostname-debug</code></p>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/","title":"Hostname v1 and v2 features","text":"<ul> <li>Until the 25.0.0 release (jun 2024), the only way to configure the hostname is called the hostname-v1 feature.</li> <li>In this release, the default ways was the hostname:v2 feature. If you want to use the hostname-v1 feature instead of v2 feature, you must enable it. They are mutually exclusive.</li> <li>The hostname:v1 option was removed in 26.0.0</li> </ul>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#configure-the-hostname-in-hostnamev1","title":"Configure the hostname in hostname:v1","text":"<pre><code>Using --hostname-url (KC_HOSTNAME_URL)\n\nWe can provide the full url with the --hostname-url parameter or the KC_HOSTNAME_URL environemnt variable using this format:\n\n&lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;/&lt;path&gt;\n\nThis setting is not supported directly in the keycloak operator\n</code></pre> <pre><code>Using --hostname (KC_HOSTNAME) or spec.hostname.hostname in the operator\n\nHere we provide only the hostname (my.domain.com).\n\nNotes about the scheme\n\n\nThe scheme will be https unless you set --hostname-strict-https=false (KC_HOSTNAME_STRICT_HTTPS). This is an undocumented setting\nThis setting is not supported directly in the keycloak operator\n</code></pre> <p>Notes about the port</p> <pre><code>--hostname-port parameter (KC_HOSTNAME_PORT) defines the port number that the Keycloak server is listening on for HTTP or HTTPS traffic. \nThis setting is not supported directly in the keycloak operator\n</code></pre> <p>Notes about the path</p> <pre><code>We can provide --hostname-path (KC_HOSTNAME_PATH) to specify the context path or path prefix for the Keycloak server. This option affects where Keycloak is accessible and how URLs are generated, particularly when deployed behind a reverse proxy. \nExample: If a reverse proxy forwards requests to /keycloak on your Keycloak server, you might set KC_HOSTNAME_PATH=/keycloak to ensure Keycloak's URLs are also prefixed with /keycloak.\n\nThe hostname-path also affects the admin console URL. If you set hostname-path=/keycloak, the admin console will be accessible at your-domain.com/keycloak/admin.\nIn some cases, you might use http-relative-path instead of hostname-path, which specifies the relative path of the HTTP backend without affecting the full hostname.\nThis setting is not supported directly in the keycloak operator\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#hostname-v1-backend","title":"hostname-v1 backend","text":"<p>In hostname-v1, by default, the URLs for backend endpoints are also based on the incoming request. We can change this behaviour with the hostname-strict-backchannel (KC_HOSTNAME_STRICT_BACKCHANNEL). Here the URLs for the backend endpoints are going to be exactly the same as the frontend endpoints.</p> <p>When all applications connected to Keycloak communicate through the public URL, set hostname-strict-backchannel to true. Otherwise, leave this parameter as false (default) to allow client-server communication through a private network.</p>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#hostname-v1-admin-console","title":"hostname-v1 admin console","text":"<p>In hostname-v1, by default, the URLs for administration console are also based on the incoming request. We can restrict access to the administration console using a specific URL using:</p> <pre><code>a host\n\n- parameter: --hostname-admin\n- environment variable: KC_HOSTNAME_ADMIN\n- operator: spec.hostname.admin\n\nor the full url with\n\n- parameter: --hostname-admin-url\n- environment variable: KC_HOSTNAME_ADMIN_URL\n- operator: spec.hostname.adminUrl\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#changed-settings","title":"Changed settings","text":"<p>These are how the settings changed from v1 to v2</p> <p></p>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#links","title":"Links","text":"<ul> <li>Configuring the hostname: (v1) Redhat</li> </ul> <p>https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/24.0/html/server_guide/hostname-</p> <ul> <li>Configuring the hostname (v2)  </li> </ul> <p>https://www.keycloak.org/server/hostname</p> <ul> <li>Upgrade to 25.0.0  </li> </ul> <p>https://www.keycloak.org/docs/25.0.0/upgrading/</p> <ul> <li>Keycloak 26.0.0 released</li> </ul> <p>https://www.keycloak.org/2024/10/keycloak-2600-released</p> <ul> <li>Migrating to 25.0.0</li> </ul> <p>https://www.keycloak.org/docs/latest/upgrading/#migrating-to-25-0-0</p> <ul> <li>All configuration</li> </ul> <p>https://www.keycloak.org/server/all-config</p>"},{"location":"security/openssl/extract-from-pem/","title":"Extract from pem","text":""},{"location":"security/openssl/extract-from-pem/#extract-certificate-from-a-pem-file","title":"Extract certificate from a pem file","text":"<pre><code>openssl x509 -in FILE.pem -out cert1.crt\n</code></pre>"},{"location":"security/openssl/extract-from-pem/#extract-private-key-from-a-pem-file","title":"Extract private key from a pem file","text":"<pre><code>openssl rsa -in FILE.pem -out cert1.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/","title":"Openssl and web certificates","text":""},{"location":"security/openssl/openssl-certs/#web-certificate-and-rfc-6125","title":"Web certificate and RFC 6125","text":"<p>In the RFC 6125 the recommendation is to use the X509v3 Subject Alternative Name (SAN). It includes all the domains and subdomains this certificate will secure. It can also include ip addresses.</p> <pre><code>X509v3 Subject Alternative Name: \n    DNS:my.domain.com\n\nIn the Subject we can setup a Common Name. Examples:\n\n```txt\nSubject: CN = my.domain.com\nSubject: CN = www.domain.com\nSubject: CN = *.domain.com\n</code></pre> <p>As the RFC 6125 says, the SAN is checked first. If SAN does not exists, the CN will be checked. If both are specified, the CN must match an entry in the SAN. But at this point, different clients can have different behaviours.</p>"},{"location":"security/openssl/openssl-certs/#information","title":"Information","text":""},{"location":"security/openssl/openssl-certs/#certificate-information","title":"Certificate information","text":"<pre><code>openssl x509 -noout -text -in 'cerfile.crt'  # PEM format (default)\nopenssl x509 -inform pem -noout -text -in 'cerfile.cer';  # PEM format (default)\nopenssl x509 -inform der -noout -text -in 'cerfile.cer'; # DER format\n</code></pre>"},{"location":"security/openssl/openssl-certs/#checks","title":"Checks","text":""},{"location":"security/openssl/openssl-certs/#private-key-integrity","title":"Private key integrity","text":"<pre><code>openssl rsa -check -noout -in privatekey.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/#modulus-they-must-match","title":"Modulus (they must match)","text":"<pre><code>openssl x509 -noout -modulus -in privatekey.key\nopenssl rsa -noout -modulus -in certificate.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-from-pfx","title":"Extract from pfx","text":""},{"location":"security/openssl/openssl-certs/#extract-the-private-key","title":"Extract the private key","text":"<pre><code>openssl pkcs12 -in [yourfile.pfx] -nocerts -out [tls.key] # encrypted\nopenssl pkcs12 -in [yourfile.pfx] -nocerts -noenc -out [tls.key] # no encrypted\n</code></pre> <p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created). Also, for the \"PEM pass phrase\". This will protecdt the .key generated file. Store this \"PEM pass phrase\"</p> <ul> <li>Decrypt the private key if encrypted</li> </ul> <p>Type the \"PEM pass phrase\"</p> <pre><code>openssl rsa -in [drlive.key] -out [tls-decrypted.key]\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-the-certificate","title":"Extract the certificate","text":"<p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created).</p> <pre><code>openssl pkcs12 -in [yourfile.pfx] -clcerts -nokeys -out [tls.crt]\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-the-ca","title":"Extract the ca","text":"<p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created).</p> <pre><code>openssl pkcs12 -in [yourfile.pfx] -cacerts -nokeys -out [tls.ca]\n</code></pre>"},{"location":"security/protect-git-repos/no-secrets-git/","title":"Protect your git repos from including secrets","text":""},{"location":"security/protect-git-repos/no-secrets-git/#client-side-pre-commit-hook","title":"Client-side pre-commit hook","text":"<p>The pre-commit hook is the first client-side git hook to run when a commit is executed and the best option to check if contains any secret. Every non-zero exit aborts the commit and it can be bypassed using</p> <pre><code>git commit --no-verify\n</code></pre>"},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-implementation-pre-commit-hook","title":"Gitlab implementation (pre-commit hook)","text":"<ul> <li>Client-side secret detection</li> </ul> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/client/index.html</p>"},{"location":"security/protect-git-repos/no-secrets-git/#related-tools","title":"Related tools","text":"<ul> <li>Pre-commit</li> </ul> <p>https://pre-commit.com/</p>"},{"location":"security/protect-git-repos/no-secrets-git/#server-side-and-the-pre-receive-hook","title":"Server-Side and the pre-receive hook","text":"<p>The pre-receive hook is the first server-side git hook to run when a push is executed and the best option to check if contains any secret. Every non-zero exit rejects the push.</p>"},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-implementations-pre-receive-hook","title":"Gitlab implementations (pre-receive hook)","text":"<ul> <li>Gitlab customized pre-receive hook</li> </ul> <p>In Gitlab you can create your own pre-receive hook</p> <p>https://docs.gitlab.com/ee/administration/server_hooks.html</p> <ul> <li>Gitlab Secret push protection</li> </ul> <p>Also in Gitlab, you can use the Secret Push Protection feature, available since 17.2 release.</p> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/secret_push_protection/index.html</p>"},{"location":"security/protect-git-repos/no-secrets-git/#after-the-push","title":"After the push","text":""},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-cicd-implementation","title":"Gitlab CI/CD implementation","text":"<p>You can use the detection of secrets inside a pipeline and protect branches. For example, you can:</p> <ul> <li>Protect a branch</li> </ul> <p>Located in Settings - Repository - Protected branches, prevent direct pushes to important branches and require changes to go through merge requests.</p> <ul> <li>Enable \"Pipelines must succeed\"</li> </ul> <p>Located in Settings - Merge requests - Merge checks and enabling the \"Pipelines must succeed\" option, this ensures that the pipeline must pass before a merge request can be merged.</p> <ul> <li>Scan the code</li> </ul> <p>Use a tool in the pipeline to scan the code and ensure it fails if a secret is detected.</p> <p>Another gitlab related options are:</p> <ul> <li>Pipeline secret detection</li> </ul> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/pipeline/index.html</p> <ul> <li>Gitlab Static Application Security Testing (SAST)</li> </ul> <p>This a gitlab tool to scan your code using Gitlab CI/CD. It supports some programming languages, kubernetes manifests and helm-charts</p> <p>https://docs.gitlab.com/ee/user/application_security/sast/</p>"},{"location":"security/protect-git-repos/no-secrets-git/#tools","title":"Tools","text":"<p>These tools are related with implementing scans in pre-commit hooks, pre-receive hooks and pipelines.</p> Contributors Stars Notes URL Gitleaks 212 22.500 Active https://github.com/gitleaks/gitleaks Trufflehog 163 20.000 Active https://github.com/trufflesecurity/trufflehog Trivy 466 27.600 Active https://github.com/aquasecurity/trivy Talisman 68 2.000 Active https://github.com/thoughtworks/talisman Detect-secrets 71 3.800 Active? https://github.com/Yelp/detect-secrets Git secrets 30 12.400 Awslabs. Unmaintained https://github.com/awslabs/git-secrets Git guardian Platform https://www.gitguardian.com/"},{"location":"security/protect-git-repos/no-secrets-git/#links","title":"Links","text":"<ul> <li> <p>Customizing Git - Git Hooks https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks</p> </li> <li> <p>Gitlab Secret detection https://docs.gitlab.com/ee/user/application_security/secret_detection/ P</p> </li> </ul>"},{"location":"security/protect-git-repos/pre-commit-hooks/","title":"Pre commit hooks","text":""},{"location":"security/protect-git-repos/pre-commit-hooks/#git-hooks","title":"Git hooks","text":"<p>Create the .git/hooks/pre-commit file in your local repo and make it executable</p> <pre><code>cat &lt;&lt; EOF &gt; .git/hooks/pre-commit\n#!/bin/bash\necho \"Scanning commit with trufflehog\"\ntrufflehog git file://. --since-commit HEAD --results=verified,unknown --fail\necho \"Scanning commit with gitleaks\"\ngitleaks git --redact --staged --verbose\nEOF\nchmod u+x .git/hooks/pre-commit\n</code></pre>"},{"location":"security/protect-git-repos/pre-commit-hooks/#sharing-the-git-hooks-with-hookspath","title":"Sharing the git hooks with hooksPath","text":"<p>The git hooks stores in the hooks directory are not pushed to the remote repository. Each developer's hooks remain on their local machine only. This must be executed by all users in the root of all local repositories.</p> <p>An alternative approach is to store the hooks for example in another folder and push it to the repository</p> <p>This changes the path where the hooks are located</p> <pre><code>git config core.hooksPath .githooks # the default value is .git/hooks\n</code></pre>"},{"location":"security/protect-git-repos/pre-commit-hooks/#pre-commit-framework","title":"Pre-commit framework","text":"<p>Another way to share the pre commit hooks is using the pre-commit framework.</p> <p>For this you must :</p> <ul> <li>install this framework</li> <li>configure .pre-commit-config.yaml in the root of your git repo with your pre-commit hooks</li> <li>install the pre-commit scripts in your git repo</li> </ul>"},{"location":"security/protect-git-repos/pre-commit-hooks/#links-and-integrations","title":"Links and integrations","text":"<p>Pre-commit framework</p> <p>https://pre-commit.com/</p> <ul> <li>Trufflehog</li> </ul> <p>https://docs.trufflesecurity.com/pre-commit-hooks</p> <ul> <li>Gitleaks</li> </ul> <p>https://github.com/gitleaks/gitleaks</p> <ul> <li>Talisman</li> </ul> <p>https://github.com/thoughtworks/talisman</p> <ul> <li>Trivy</li> </ul> <p>https://github.com/mxab/pre-commit-trivy</p>"},{"location":"security/protect-git-repos/pre-commit-hooks/#husky","title":"Husky","text":"<p>pending</p> <p>https://typicode.github.io/husky/</p>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/","title":"Ways to scan with trufflehog","text":"<p>If we want to scan git repositories with trufflehog we have some options:</p> <p>This article is based in trufflehog 3.82.13</p>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-filesystem-level","title":"At filesystem level","text":"<p>The following command scans for credentials in a path, and we can use a cloned repo, but probably it is not the best option to scan a git repository because it does not scans the full git history.</p> <pre><code>trufflehog filesystem OPTIONS PATH\n</code></pre>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-git-level","title":"At git level","text":"<p>The following command scans for credentials in a git repository, including all branches and commit history.</p> <pre><code>trufflehog git OPTIONS URL\n</code></pre> <ul> <li>The url can be in https://, file://, or ssh:// format</li> <li>The --branch=BRANCH permits to set an specified branch</li> <li>--max-depth=DEPTH limits the commits to be scanned</li> <li>--since-commit=COMMIT also limits the commits to be scanned</li> </ul>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-provider-level","title":"At provider level","text":"<p>We can scan for credentials in a in github or gitlab repository. It is similar to the git command, but includes additional options in every provider.</p> <pre><code>trufflehog github\n</code></pre> <pre><code>trufflehog gitlab\n</code></pre> <p>There is an experimental github scan</p> <pre><code>trufflehog github-experimental\n</code></pre> <p>In both providers we can:</p> <ul> <li>Configure the github|gitlab server with the --endpoint=https://whatever option</li> <li>Tell trufflehog what repos we want to scan, we can repeat the --repo=REPO option with all the desired repos</li> <li>The authentication can be provided with the --token=TOKEN option or via the GITHUB_TOKEN or GITLAB_TOKEN environment variable</li> </ul> <p>In github there are some specific github options that permit to tell the organization, the member repositories, if include or not forks, if include or not wikis,...</p> <p>Also the output can be in a github actions format.</p>"},{"location":"services/external-dns/98-tips/","title":"Tips","text":""},{"location":"services/external-dns/98-tips/#policy","title":"Policy","text":"<p>We can control how the dns records are syncronized between sources and providers with the policy.</p> <p>Via policy, we can control if the controller can update records when the ingress or services annotations change, or remove records when they are deleted</p> Policy Create Update Remove upsert-only Yes Yes No create-only Yes No No sync Yes Yes Yes <p>upsert-only is the default behaviour</p>"},{"location":"services/external-dns/98-tips/#share-zones-between-clusters","title":"Share zones between clusters","text":"<p>External dns can dinamically create and remove dns entries in the provider when services and ingress resources with certain configurations are added and removed (sync policy)</p> <p>We can also manage the same zone from more than one kubernetes cluster, but this have a potential problem.</p> <p>External secrets by default uses txt dns entries for tracking the ownership of DNS entries. This makes external-dns will only remove entries it manages.</p> <p>This defalt behaviour can be changed changing \"registry\" setting from \"txt\" to \"aws-sd\", \"dynamodb\" or \"noop\".</p> <p>But if we share the same --txt-owner-id between clusters, both controllers will create entries in that zones but both will think they own all records in that zone. This can cause several problems.</p> <p>For AWS Route53: The txtOwnerId should be the Hosted Zone ID (e.g., Z1D633PJN98FT9).</p> <p>For multi-cluster setups with Route53: Since txtOwnerId must be the Hosted Zone ID, sharing the same Route53 zone between multiple external-dns controllers is NOT recommended when using the default \"txt\" registry. All controllers would use the same txtOwnerId, causing ownership conflicts and unpredictable behavior.</p> <p>Recommended approaches for multi-cluster Route53 setups:</p> <ol> <li>Separate hosted zones: Give each cluster its own subdomain zone</li> </ol> <p>```yaml    # Cluster 1    domainFilters: [\"cluster1.example.com\"]    txtOwnerId: Z1D633PJN98FT9</p> <p># Cluster 2    domainFilters: [\"cluster2.example.com\"]    txtOwnerId: Z2E744QKM09GHI    ```</p> <ol> <li>Use aws-sd registry: Switch from \"txt\" to \"aws-sd\" registry</li> </ol> <p><code>yaml    registry: aws-sd</code></p> <ol> <li>Use noop registry: Disable ownership tracking (loses safety features)</li> </ol> <p><code>yaml    registry: noop</code></p>"},{"location":"services/external-dns/aws-route53-pia/","title":"Route53 and Pod identity agent","text":""},{"location":"services/external-dns/aws-route53-pia/#role-and-policies","title":"Role and policies","text":"<p>Create a role called, for example external-dns with this trust policy (trust relationship)</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>And with this permission policy called, for example AllowExternalDNSUpdates</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ListTagsForResources\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>The policy can be more precise for your hosted zone</p>"},{"location":"services/external-dns/aws-route53-pia/#route-53-zone","title":"Route 53 zone","text":"<p>Create a hosted zone in route 53 for your desired (sub)domain</p>"},{"location":"services/external-dns/aws-route53-pia/#deploy-external-dns","title":"Deploy external dns","text":"<p>Deploy the external dns helm chart with this values.yaml file</p> <pre><code>provider:\n  name: aws\ndomainFilters:\n  - yourhostedzone\ntxtOwnerId: Z1D633PJN98FT9  # Your Route53 Hosted Zone ID\nextraArgs: [\"--aws-zone-type=public\"]   # if it is a public zone\n</code></pre> <p>If you use a very old release, pod identity agent can fail</p>"},{"location":"services/external-dns/aws-route53-pia/#configure-the-eks-cluster","title":"Configure the eks cluster","text":"<ul> <li> <p>Under addons, deploy the pod identity agent plugin if not using Eks Auto Mode</p> </li> <li> <p>Under access, create a pod identity association between the external-dns role and the external-dns service account</p> </li> </ul>"},{"location":"services/external-dns/aws-route53-pia/#check-if-it-works","title":"Check if it works","text":"<p>Restart the external-dns deployment and see the logs of the external-dns pod. You must find something like this:</p> <pre><code>\"Applying provider record filter for domains: [yourhostedzone related info ]\"\n\"All records are already up to date\"\n</code></pre>"},{"location":"services/harbor/98-tips/","title":"Tips","text":""},{"location":"services/harbor/98-tips/#default-credentials","title":"Default credentials","text":"<p>We can log in with the following credentials: Username: admin Password: Harbor12345</p>"},{"location":"services/harbor/98-tips/#basic-harbor-settings","title":"Basic Harbor settings","text":"<p>This settings provide a good starting point to configure the defalt VMware Harbor deployment</p> <ul> <li>Consider quota</li> <li>Schedule a scan</li> <li>Configure garbage collection</li> <li>Disable self registration</li> <li>Restrict project creation</li> <li>make robot username unpredictable</li> <li>Disable Webhooks</li> </ul> <p>Under Project Quotas</p> <ul> <li>Consider to enable \"Default quota space per project\"</li> </ul> <p>Under Interrogation Services - Vulnerability</p> <ul> <li>Configure \"Schedule to scan all\"</li> </ul> <p>Under Clean Up</p> <ul> <li>Schedule a garbage collection</li> <li>Enable \"Allow garbage collection on untagged artifacts\"</li> <li>Enable a log rotation</li> </ul> <p>Under Configuration-Authentication</p> <ul> <li>disable \"Allow Self-Registration\"</li> </ul> <p>Under Configuration-System Settings</p> <ul> <li>leave \"Project Creation\" to \"Admin Only\"</li> <li>change the \"Robot Name Prefix\"</li> <li>uncheck \"Webhooks Enabled\"</li> </ul>"},{"location":"services/harbor/98-tips/#consider-this-project-scoped-settings","title":"Consider this project scoped settings","text":"<p>This settings provide a good starting point to configure every vmware harbor project</p> <ul> <li>Avoid public repositories</li> <li>Enabling \"Automatically scan images on push\"</li> <li>Enabling checking of verified images</li> <li>Enabling \"Prevent vulnerable images from running\"</li> <li>Enabling quota</li> </ul>"},{"location":"services/harbor/98-tips/#about-users-and-robot-accounts","title":"About users and robot accounts","text":"<ul> <li>use robot accounts for applications</li> <li>use users for humans</li> <li>give the minimal permissions to them</li> <li>tend to use scoped users/robot accounts</li> <li>create global users/robot accounts only when needed</li> </ul>"},{"location":"services/harbor/api/","title":"Api","text":""},{"location":"services/harbor/api/#creating-projecto-scoped-robot-account","title":"Creating projecto scoped robot account","text":"<p>The only working way to create a project scoped robot account is using an admin account and the /robots api</p> <p>A non admin user or robot account gives an access denied error and the robotv1 (/project/myproject/robots) api seems buggy</p> <p>https://github.com/goharbor/harbor/issues/20692</p>"},{"location":"services/harbor/replication/","title":"Replication","text":""},{"location":"services/harbor/replication/#harbor-to-harbor-oci-replication","title":"Harbor to harbor oci replication","text":"<p>These are the permissions needed to import oci helm charts between harbor instances</p> <pre><code>Artifact: list\nArtifact: Read\nRepository: List\nRepository: Pull\nTag: List\n</code></pre>"},{"location":"services/ingress-nginx/custom-headers/","title":"Add custom headers","text":"<ul> <li> <p>proxy-set-headers for passing a custom list of headers to the upstream server</p> </li> <li> <p>add-headers To pass the custom headers before sending response traffic to the client</p> </li> </ul>"},{"location":"services/ingress-nginx/custom-headers/#easier-option","title":"Easier option","text":""},{"location":"services/ingress-nginx/custom-headers/#harder-option","title":"Harder option","text":"<p>Changes to the custom header config maps do not force a reload of the ingress-nginx-controllers.</p>"},{"location":"services/ingress-nginx/custom-headers/#configure-in-the-controller","title":"Configure in the controller","text":"<p>In the ingress-nginx-controller configmap or similar, we indicate to the controller which is our configmap that will contain our custom headers. This can be done using 2 keys</p>"},{"location":"services/ingress-nginx/custom-headers/#create-the-configmap","title":"Create the configmap","text":""},{"location":"services/ingress-nginx/custom-headers/#restart-the-controller","title":"Restart the controller","text":""},{"location":"services/ingress-nginx/custom-headers/#links","title":"Links","text":"<ul> <li>Custom headers: https://kubernetes.github.io/ingress-nginx/examples/customization/custom-headers/</li> </ul>"},{"location":"services/ingress-nginx/multiple-controllers/","title":"Multiple controllers","text":"<p>You can have more than one ingress controller and ingress class in the same Kubernetes cluster using the nginx ingress controller.</p> <p>To do this, we need to install a new controller, preferably in another namespace. To prevent it from overwriting the previous one, we must add this configuration to the helm chart</p>"},{"location":"services/ingress-nginx/multiple-controllers/#default-values-of-a-helm-chart","title":"Default values of a helm chart","text":"<pre><code>controller:\n  ingressClass: minuevovalor\n  ingressClassResource:\n    name: minuevovalor\n    controllerValue: \"k8s.io/minuevovalor\"\n</code></pre> <p>The default installation uses</p> <pre><code>controller:\n  ingressClass: nginx\n  ingressClassResource:\n    name: nginx\n    controllerValue: \"k8s.io/ingress-nginx\"\n</code></pre> <p>controller.electionID is also a configurable value, which by default takes the controller name and adds the suffix \"-leader\"</p>"},{"location":"services/ingress-nginx/multiple-controllers/#links","title":"Links","text":"<ul> <li>Multiple Ingress controllers https://kubernetes.github.io/ingress-nginx/user-guide/multiple-ingress/</li> </ul>"},{"location":"services/kubeflow/components/","title":"Kubeflow components","text":""},{"location":"services/kubeflow/components/#kubeflow-pipelines","title":"Kubeflow Pipelines","text":"<p>Platform to create machine learning workflows /schedules with containers ML Workflows and Schedules</p> <p>https://www.kubeflow.org/docs/components/pipelines/ https://github.com/kubeflow/pipelines</p>"},{"location":"services/kubeflow/components/#model-registry","title":"Model registry","text":"<p>Model metadata</p> <p>https://www.kubeflow.org/docs/components/model-registry/</p>"},{"location":"services/kubeflow/components/#training-operator","title":"Training Operator","text":"<p>Model Training and Fine-Tuning</p> <p>https://www.kubeflow.org/docs/components/training/ https://github.com/kubeflow/training-operator</p>"},{"location":"services/kubeflow/components/#katib","title":"Katib","text":"<p>Model tuning. Model Optimization and AutoML automated machine learning (AutoML)</p> <p>https://www.kubeflow.org/docs/components/katib/</p>"},{"location":"services/kubeflow/components/#kserve","title":"KServe","text":"<p>Model Serving</p> <p>https://www.kubeflow.org/docs/external-add-ons/kserve/ https://github.com/kserve/kserve https://kserve.github.io/website</p>"},{"location":"services/kubeflow/components/#kubeflow-mpi-operator","title":"Kubeflow MPI Operator","text":"<p>All-Reduce Model Training</p> <p>https://www.kubeflow.org/docs/components/training/user-guides/mpi/ https://github.com/kubeflow/mpi-operator</p>"},{"location":"services/kubeflow/components/#spark-operator","title":"Spark Operator","text":"<p>Data Preparation (run spark applications)</p> <p>https://www.kubeflow.org/docs/scomponents/spark-operator/ https://github.com/kubeflow/spark-operator https://kubeflow.github.io/spark-operator/</p>"},{"location":"services/kubeflow/components/#other","title":"Other","text":""},{"location":"services/kubeflow/components/#kubeflow-notebooks-web-based-development-environments","title":"Kubeflow Notebooks (web-based development environments)","text":"<p>For interactive data exploration and model development, provides a way to run web-based development environments inside your Kubernetes cluster by running them inside Pods.</p> <ul> <li>https://www.kubeflow.org/docs/components/notebooks/</li> </ul>"},{"location":"services/kubeflow/components/#central-dashboard-to-the-kubeflow-ecosystem","title":"Central Dashboard (to the kubeflow ecosystem)","text":"<p>For easy navigation and management with Kubeflow Profiles for access control.</p> <ul> <li>https://www.kubeflow.org/docs/components/central-dash/</li> <li>https://github.com/kubeflow/kubeflow/tree/master/components/centraldashboard</li> </ul>"},{"location":"services/kubeflow/components/#mpi-operator-allreduce-distributed-training-strategy","title":"MPI Operator (allreduce distributed training strategy)","text":"<ul> <li>https://github.com/kubeflow/mpi-operator</li> <li>https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc</li> </ul>"},{"location":"services/kubeflow/components/#istio-service-mesh","title":"Istio (service mesh)","text":"<ul> <li>https://www.kubeflow.org/docs/concepts/multi-tenancy/istio/</li> </ul> <p>with cilium https://docs.cilium.io/en/latest/network/servicemesh/istio/ https://github.com/kubeflow/manifests/issues/2729</p>"},{"location":"services/kubeflow/components/#feast","title":"Feast","text":"<p>https://www.kubeflow.org/docs/external-add-ons/feast/ https://github.com/feast-dev/feast https://feast.dev/</p>"},{"location":"services/kubeflow/components/#elyra","title":"Elyra","text":"<p>https://www.kubeflow.org/docs/external-add-ons/elyra/ https://elyra.readthedocs.io/</p>"},{"location":"services/kubeflow/components/#bento-ml","title":"Bento ML","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/bentoml</p>"},{"location":"services/kubeflow/components/#seldon","title":"Seldon","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/seldon</p>"},{"location":"services/kubeflow/components/#ray","title":"Ray","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/ray</p>"},{"location":"services/kubeflow/components/#additional-tooling","title":"Additional Tooling","text":"<p>Additional tooling for data management (PVC Viewer), visualization (TensorBoards), and more.</p>"},{"location":"services/kubeflow/components/#links","title":"Links","text":"<p>https://www.kubeflow.org/docs/started/introduction/ https://www.kubeflow.org/docs/started/architecture/ https://www.kubeflow.org/docs/releases/kubeflow-1.9/ https://blog.kubeflow.org/ https://github.com/kubeflow/manifests/</p>"},{"location":"services/kubeflow/installation/","title":"How to install kubeflow","text":""},{"location":"services/kubeflow/installation/#install-standalone-components","title":"Install standalone components","text":"<p>There are some kubeflow components that can be deployed as standalone services. They have their own git repository and documentation about how to install it.</p> <p>List:</p> <ul> <li>kserve</li> <li>katib</li> <li>model registry</li> <li>mpi operator</li> <li>pipelines</li> <li>spark operator</li> <li>training operator</li> </ul>"},{"location":"services/kubeflow/installation/#install-the-full-platform-using-distribution","title":"Install the full platform using distribution","text":"<p>https://www.kubeflow.org/docs/distributions/</p>"},{"location":"services/kubeflow/installation/#install-the-full-platform-using-manifests","title":"Install the full platform using manifests","text":"<p>The following repository permits to install the full platfrom using kuberntes manifests</p> <ul> <li> <p>1.9 release https://github.com/kubeflow/manifests/tree/v1.9.0#installation</p> </li> <li> <p>1.8 release https://github.com/kubeflow/manifests/tree/v1.8.1#installation</p> </li> </ul>"},{"location":"services/kubeflow/installation/#links","title":"Links","text":"<ul> <li>Installing Kubeflow https://www.kubeflow.org/docs/started/installing-kubeflow/</li> </ul>"},{"location":"services/kyverno/annotations/","title":"Annotations","text":""},{"location":"services/kyverno/annotations/#policieskyvernoiotitle","title":"policies.kyverno.io/title","text":"<p>Gives a title to the policy</p> <pre><code>policies.kyverno.io/title: Disallow Capabilities\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiodescription","title":"policies.kyverno.io/description","text":"<p>Provides a brief description of what the policy does.</p> <pre><code>policies.kyverno.io/description: &gt;-\n      An ingress resource needs to define an actual host name\n      in order to be valid. This policy ensures that there is a\n      hostname for each rule defined.\n</code></pre> <pre><code>policies.kyverno.io/description: \"Ensure all Pods have resource limits defined.\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioseverity","title":"policies.kyverno.io/severity","text":"<p>Defines the severity level of the policy. Possible values: low, medium, high</p> <pre><code>policies.kyverno.io/severity: medium\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiocategory","title":"policies.kyverno.io/category","text":"<p>Categorizes the policy into a specific group or type. Possible Values: security, compliance, best-practices, etc.</p> <pre><code>policies.kyverno.io/category: security\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiominversion","title":"policies.kyverno.io/minversion","text":""},{"location":"services/kyverno/annotations/#policieskyvernoiosubject","title":"policies.kyverno.io/subject","text":"<p>Specifies the subject of the policy (e.g., Pod, Namespace, Deployment).</p> <pre><code>policies.kyverno.io/subject: Ingress\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiocontrols","title":"policies.kyverno.io/controls","text":"<p>Maps the policy to specific compliance controls (e.g., CIS benchmarks, NIST standards).</p> <pre><code>policies.kyverno.io/controls: \"CIS-1.3.2, NIST-800-53\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioowner","title":"policies.kyverno.io/owner","text":"<p>Identifies the owner or team responsible for the policy.</p> <pre><code>policies.kyverno.io/owner: \"DevSecOps Team\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioscorecard","title":"policies.kyverno.io/scorecard","text":"<p>Indicates whether the policy is part of a compliance scorecard. While primarily used for compliance tracking, this annotation can be integrated into external tools or workflows to enforce compliance checks. Useful in environments where compliance policies are tracked and enforced as part of governance.</p> <pre><code>policies.kyverno.io/scorecard: \"true\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioautogen-controllers","title":"policies.kyverno.io/autogen-controllers","text":"<p>This is not an informative annotation. Specifies which controllers the policy applies to when auto-generating rules.</p> <pre><code>policies.kyverno.io/autogen-controllers: \"Deployment\"\n</code></pre> <pre><code>policies.kyverno.io/autogen-controllers: \"none\"\n</code></pre> <p>See more here</p> <ul> <li>Auto-Gen Rules</li> </ul> <p>https://main.kyverno.io/docs/policy-types/cluster-policy/autogen/</p>"},{"location":"services/kyverno/annotations/#kyvernoiokyverno-version","title":"kyverno.io/kyverno-version","text":"<p>Indicates the version of Kyverno that was used to create or apply the policy.</p> <pre><code>kyverno.io/kyverno-version: 1.6.0\n</code></pre>"},{"location":"services/kyverno/annotations/#kyvernoiokubernetes-version","title":"kyverno.io/kubernetes-version","text":"<p>Indicates the version of Kubernetes that was in use when the policy was created or applied.</p> <pre><code>kyverno.io/kubernetes-version: \"1.22-1.23\"\n</code></pre>"},{"location":"services/rabbitmq/settings/","title":"Settings","text":""},{"location":"services/rabbitmq/settings/#memory","title":"Memory","text":"<p>https://www.rabbitmq.com/docs/memory</p> <p>example:</p> <pre><code>vm_memory_high_watermark.absolute = 450Mi\n</code></pre>"},{"location":"services/rabbitmq/settings/#disk","title":"Disk","text":"<p>https://www.rabbitmq.com/docs/disk-alarms</p> <p>example:</p> <pre><code>disk_free_limit.absolute = 512MB\n</code></pre>"},{"location":"services/rabbitmq/settings/#partition-handling","title":"Partition handling","text":"<p>Strategies:</p> <ul> <li>pause-minority</li> <li>pause-if-all-down mode</li> <li>autoheal</li> <li>ignore</li> </ul> <p>https://www.rabbitmq.com/docs/partitions</p> <pre><code>cluster_partition_handling = autoheal\n</code></pre>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/object-storage/","title":"Self-Hosted Object Storage Solutions","text":"<p>Open source and source-available options for self-hosted S3-compatible object storage.</p> <p>Note: MinIO's open source (Apache 2.0) version is archived. The current MinIO is AGPL-3.0, which restricts commercial use without a commercial license.</p>"},{"location":"storage/object-storage/#kubernetes-native","title":"Kubernetes-Native","text":"<p>Solutions with native Kubernetes integration: operators, CSI drivers, or CNCF project status.</p>"},{"location":"storage/object-storage/#cubefs","title":"CubeFS","text":"<p>License: Apache 2.0 S3-compatible: Yes (native, also POSIX and HDFS) CNCF status: Graduated (January 2025) GitHub: https://github.com/cubefs/cubefs</p> <p>A distributed storage system supporting multiple access protocols: S3, POSIX, and HDFS. Originally developed at JD.com and donated to the CNCF in 2019, it graduated in January 2025. Over 200 organisations run it in production, managing approximately 350 PB of data.</p> <p>Strengths:</p> <ul> <li>Apache 2.0 license</li> <li>Multi-protocol: S3, POSIX, HDFS in a single system</li> <li>Dual storage engines: multi-replica and erasure coding</li> <li>Kubernetes CSI plugin included</li> <li>Multi-tenancy with fine-grained isolation</li> <li>Scales to PB/EB level</li> <li>Strong CNCF governance and security audit</li> </ul> <p>Weaknesses:</p> <ul> <li>Relatively unknown outside Asian cloud/e-commerce companies</li> <li>Operational complexity comparable to Ceph</li> <li>Documentation still maturing for Western audiences</li> </ul> <p>Use when: You need an Apache-licensed, CNCF-graduated storage system with S3, POSIX, and HDFS in one platform, especially for AI/ML or big data workloads alongside Kubernetes.</p>"},{"location":"storage/object-storage/#ceph-rgw","title":"Ceph (RGW)","text":"<p>License: LGPL-2.1 S3-compatible: Yes (via RADOS Gateway) CNCF status: Not a CNCF project \u2014 Rook (its Kubernetes operator) is CNCF Graduated Kubernetes operator: Rook</p> <p>The most battle-tested distributed storage system in the cloud-native ecosystem. Ceph provides object (S3/Swift), block (RBD), and file (CephFS) storage in a single platform.</p> <p>Strengths:</p> <ul> <li>Mature, production-proven at scale</li> <li>Multi-protocol: S3, Swift, NFS, iSCSI, RBD</li> <li>Self-healing and self-managing</li> <li>Rook operator makes Kubernetes deployment straightforward</li> </ul> <p>Weaknesses:</p> <ul> <li>High operational complexity</li> <li>Requires significant resources (minimum 3 nodes recommended)</li> <li>Steep learning curve</li> </ul> <p>Use when: You need a full-featured, production-grade storage platform and have the operational capacity to manage it.</p>"},{"location":"storage/object-storage/#seaweedfs","title":"SeaweedFS","text":"<p>License: Apache 2.0 S3-compatible: Yes CNCF status: Not a CNCF project CSI driver: seaweedfs-csi-driver GitHub: https://github.com/seaweedfs/seaweedfs</p> <p>A distributed file system with S3-compatible API, optimized for storing billions of small files efficiently. Has a dedicated CSI driver for Kubernetes persistent volumes.</p> <p>Strengths:</p> <ul> <li>Apache 2.0 license</li> <li>Excellent performance for small files</li> <li>Low overhead per file (metadata optimization)</li> <li>S3, POSIX (FUSE), HDFS interfaces</li> <li>CSI driver for Kubernetes PVCs</li> <li>Active development</li> </ul> <p>Weaknesses:</p> <ul> <li>Less mature than Ceph for enterprise workloads</li> <li>Smaller ecosystem</li> <li>No CNCF backing</li> </ul> <p>Use when: You need Apache 2.0-licensed S3-compatible storage, especially with many small objects, with Kubernetes PVC support.</p>"},{"location":"storage/object-storage/#general-self-hosted","title":"General Self-Hosted","text":"<p>Solutions designed to run anywhere. Can be deployed on Kubernetes but are not primarily Kubernetes-oriented.</p>"},{"location":"storage/object-storage/#rustfs","title":"RustFS","text":"<p>License: Apache 2.0 S3-compatible: Yes Maturity: Alpha (distributed mode still under testing) GitHub: https://github.com/rustfs/rustfs</p> <p>Written in Rust, explicitly positioned as an Apache 2.0 MinIO replacement. Claims 2.3x throughput of MinIO for 4KB objects. Single binary under 100MB. Has a Helm chart but no operator or CSI driver.</p> <p>Use when: You need an Apache 2.0 MinIO drop-in replacement and are comfortable with alpha software. Watch this project.</p>"},{"location":"storage/object-storage/#garage","title":"Garage","text":"<p>License: AGPL-3.0 S3-compatible: Yes GitHub: https://github.com/deuxfleurs-org/garage</p> <p>Lightweight, distributed object storage written in Rust, designed for geo-distribution across unreliable or heterogeneous nodes. Very low resource footprint. Strong choice for multi-site or edge deployments.</p> <p>Use when: You need geo-distributed object storage across multiple sites with minimal operational overhead. Note the AGPL license.</p>"},{"location":"storage/object-storage/#alarik","title":"Alarik","text":"<p>License: Apache 2.0 S3-compatible: Yes Maturity: Alpha \u2014 do not use in production GitHub: https://github.com/achtungsoftware/alarik</p> <p>Written in Swift (SwiftNIO). No GC pauses (ARC), built-in web console. Single node only, Linux only. Very early stage but worth watching as a lightweight Apache 2.0 option.</p>"},{"location":"storage/object-storage/#openstack-swift","title":"OpenStack Swift","text":"<p>License: Apache 2.0 S3-compatible: Partial (via <code>s3api</code> middleware) Website: https://docs.openstack.org/swift</p> <p>The original OpenStack object storage. Very mature and proven at telco scale. S3 compatibility is not native. Only relevant if you are already operating an OpenStack environment.</p>"},{"location":"storage/object-storage/#zenko-cloudserver-scality","title":"Zenko CloudServer (Scality)","text":"<p>License: Apache 2.0 S3-compatible: Yes (native) GitHub: https://github.com/scality/cloudserver</p> <p>Native S3 API implementation that can proxy to local disk, AWS S3, Azure Blob, or GCS. Lightweight. Suitable for development, testing, or as a routing layer in front of multiple backends. The broader Zenko platform is proprietary.</p>"},{"location":"storage/object-storage/#comparison","title":"Comparison","text":"Solution License S3-Native CNCF Status K8s Integration Complexity Scale CubeFS Apache 2.0 Yes Graduated (2025) CSI + Helm Medium Large (PB+) Ceph (Rook) LGPL-2.1 Yes Rook is Graduated Operator + Helm High Large SeaweedFS Apache 2.0 Yes - CSI + Helm Medium Medium/Large RustFS Apache 2.0 Yes - Helm only (Alpha) Low - (Alpha) Garage AGPL-3.0 Yes - Helm only Low Medium Alarik Apache 2.0 Yes - None (Alpha) Low - (Alpha) OpenStack Swift Apache 2.0 Partial - None High Large Zenko CloudServer Apache 2.0 Yes - Docker/K8s Low Small/Medium"},{"location":"storage/object-storage/#links","title":"Links","text":"<ul> <li>CubeFS documentation</li> <li>CubeFS CNCF graduation announcement</li> <li>Rook documentation</li> <li>SeaweedFS GitHub</li> <li>RustFS GitHub</li> <li>Garage documentation</li> <li>Alarik GitHub</li> <li>OpenStack Swift</li> <li>Zenko CloudServer GitHub</li> <li>CNCF Landscape - Cloud Native Storage</li> </ul>"},{"location":"storage/filestash/info/","title":"Tips","text":""},{"location":"storage/filestash/info/#environment-variables","title":"Environment variables","text":"<p>We can setup 2 environment variables:</p> <ul> <li>ADMIN_PASSWORD stores the admin password credentials in a bcrypt format</li> <li>APPLICATION_URL stores the fqdn (myhost.domain.com)</li> <li>CONFIG_JSON stores the config as a base64 string with the plg_config_env plugin</li> </ul>"},{"location":"storage/filestash/info/#some-web-endpoints","title":"Some web endpoints","text":"<ul> <li>http://domain.com/about</li> <li>http://domain.com/admin/</li> <li>http://domain.com/admin/setup</li> <li>http://domain.com/admin/backend</li> <li>http://domain.com/admin/api/simple-user-management</li> <li>http://domain.com/admin/tty</li> </ul>"},{"location":"storage/filestash/info/#links","title":"Links","text":"<ul> <li>Website</li> </ul> <p>https://www.filestash.app/</p> <ul> <li>Github</li> </ul> <p>https://github.com/mickael-kerjean/filestash</p> <ul> <li>Hardening Guide</li> </ul> <p>https://downloads.filestash.app/upload/hardening-guide.pdf</p>"},{"location":"storage/vmware-csi/tips/","title":"Tips","text":""},{"location":"storage/vmware-csi/tips/#failed-to-attach-disk-the-resource-volume-is-in-use","title":"failed to attach disk. The resource volume is in use","text":"<p>Because of some solved network errors, a container cannot start and keeps in containercreating state</p> <p>We can inspect the pvc and the VolumeAttachment</p> <pre><code>kubectl get pvc\nkubectl get VolumeAttachment | grep MYPVC\nkubectl describe VolumeAttachment OURVOLUMEATTACHMENT\n</code></pre> <p>Here we get that error</p> <pre><code>... failed to attach disk .. The resource 'volume' is in use...\n</code></pre> <p>A solution can be to delete the VolumeAttachment related with our pvc and the finalizer in that VolumeAttachment but that didn't work.</p> <p>Finally I found something was not working in the node. In the Vcenter Server interface in one node where that pod lived, the \"See all disks\" link under \"VM Hardware\" was missing. But in all the other nodes there is a link with all the csi disks attached. The solution was drain that node (the pod started ok in other node), restart the node and uncordon it.</p>"},{"location":"tools/Buildkit/output/","title":"Buildkit outputs","text":"<p>By default, the build result and intermediate cache will only remain internally in BuildKit. An output needs to be specified to retrieve the result.</p> <p>Multiple outputs are supported</p>"},{"location":"tools/Buildkit/output/#imageregistry","title":"Image/Registry","text":"<p>pending</p>"},{"location":"tools/Buildkit/output/#authentication","title":"Authentication","text":"<p>If we want pass credentials to push the image to a repository, the DOCKER_CONFIG controls the directory where the credentials are defined via the config.json file.</p> <p>The default value of this variable is ~/.docker</p>"},{"location":"tools/Buildkit/output/#local-directory","title":"Local directory","text":"<p>Buildkit permit to export the build result (files, directories, binaries or artifacts) directly to a directory on your local filesystem instead of the image itself.</p> <p>For that, Buildkit copies the output files from the build context or build stages into a specified local directory on your machine.</p> <p>This command will place the build output in the ./output directory on your local filesystem.</p> <pre><code>--output type=local,dest=./output\n</code></pre>"},{"location":"tools/Buildkit/output/#docker-tarball","title":"Docker tarball","text":"<p>We can also export it to a tarball.</p> <pre><code>--output type=tar,dest=out.tar\n</code></pre>"},{"location":"tools/Buildkit/output/#oci-tarball","title":"OCI tarball","text":"<p>We can also export it to an OCI tarball</p> <pre><code>--output type=oci,dest=path/to/output.tar\n</code></pre>"},{"location":"tools/Buildkit/output/#containerd-image-store","title":"containerd image store","text":"<p>pending</p>"},{"location":"tools/container-images/98-tips/","title":"Tips","text":""},{"location":"tools/container-images/98-tips/#add-maintainer-to-dockerfile","title":"Add Maintainer to Dockerfile","text":"<p>Use the LABEL instruction</p> <pre><code>LABEL maintainer=\"&lt;your-email@example.com&gt;\"\n</code></pre> <p>Or with more detail:</p> <pre><code>LABEL maintainer=\"Your Name &lt;your-email@example.com&gt;\"\n</code></pre> <p>Note: The old MAINTAINER instruction is deprecated.</p>"},{"location":"tools/container-images/alpine-best-practices/","title":"Alpine Linux Best Practices for Dockerfiles","text":"<p>Alpine Linux is a security-oriented, lightweight Linux distribution that has become the de facto standard for minimal container base images. This guide covers best practices for using Alpine's package manager (apk) in Dockerfiles.</p>"},{"location":"tools/container-images/alpine-best-practices/#key-principles","title":"Key Principles","text":""},{"location":"tools/container-images/alpine-best-practices/#1-always-use-no-cache","title":"1. Always Use <code>--no-cache</code>","text":"<p>The most important flag when installing packages in Alpine.</p> <pre><code>RUN apk add --no-cache curl git nginx\n</code></pre> <p>Why: The <code>--no-cache</code> flag prevents apk from storing the package index locally, reducing image size by approximately 1-3MB. This is essential for keeping container images lean.</p> <p>Without <code>--no-cache</code>:</p> <pre><code># Bad - leaves cache files in the image\nRUN apk add curl\n# Results in ~2-3MB of unnecessary cache data\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-combine-update-and-install-in-a-single-run-command","title":"2. Combine Update and Install in a Single RUN Command","text":"<p>Always update the package index and install packages in the same <code>RUN</code> instruction.</p> <pre><code>RUN apk update &amp;&amp; apk add --no-cache package1 package2\n</code></pre> <p>Why: Each <code>RUN</code> instruction creates a new image layer. Combining operations prevents creating unnecessary layers with outdated package indexes.</p> <p>Anti-pattern:</p> <pre><code># Bad - creates separate layers\nRUN apk update\nRUN apk add --no-cache curl\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-use-virtual-packages-for-build-dependencies","title":"3. Use Virtual Packages for Build Dependencies","text":"<p>When you need build tools that aren't required in the final image, use virtual packages to group and remove them efficiently.</p> <pre><code>RUN apk add --no-cache --virtual .build-deps \\\n    gcc \\\n    musl-dev \\\n    python3-dev \\\n    libffi-dev &amp;&amp; \\\n    pip install --no-cache-dir cryptography &amp;&amp; \\\n    apk del .build-deps\n</code></pre> <p>Why: Virtual packages allow you to install and remove multiple packages as a single unit, ensuring build dependencies don't bloat your final image.</p> <p>Benefits:</p> <ul> <li>Removes all build dependencies in one command</li> <li>Reduces final image size significantly (often 100-300MB savings)</li> <li>Keeps the image clean and minimal</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#4-pin-package-versions-for-reproducibility","title":"4. Pin Package Versions for Reproducibility","text":"<p>Specify exact package versions to ensure consistent builds across time and environments.</p> <pre><code>RUN apk add --no-cache \\\n    nginx=1.24.0-r15 \\\n    curl=8.5.0-r0 \\\n    ca-certificates=20240705-r0\n</code></pre> <p>Why: Without version pinning, your builds may pull different package versions over time, leading to inconsistent behavior or unexpected bugs.</p> <p>Finding versions:</p> <pre><code># Search for available versions\ndocker run --rm alpine:3.19 apk search -e nginx\n\n# Check installed version\ndocker run --rm alpine:3.19 apk info nginx\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#5-combine-related-operations-in-single-layers","title":"5. Combine Related Operations in Single Layers","text":"<p>Group logically related commands together to minimize layers and improve build performance.</p> <pre><code>RUN apk add --no-cache \\\n    python3 \\\n    py3-pip &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt &amp;&amp; \\\n    adduser -D appuser &amp;&amp; \\\n    chown -R appuser:appuser /app\n</code></pre> <p>Why: Fewer layers mean smaller images, faster builds, and better cache utilization.</p>"},{"location":"tools/container-images/alpine-best-practices/#6-clean-up-in-the-same-layer","title":"6. Clean Up in the Same Layer","text":"<p>If not using <code>--no-cache</code>, ensure cleanup happens in the same <code>RUN</code> command.</p> <pre><code>RUN apk update &amp;&amp; \\\n    apk add package &amp;&amp; \\\n    rm -rf /var/cache/apk/*\n</code></pre> <p>Important: However, using <code>--no-cache</code> is preferred as it's cleaner and more reliable.</p>"},{"location":"tools/container-images/alpine-best-practices/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"tools/container-images/alpine-best-practices/#multi-stage-builds-with-alpine","title":"Multi-Stage Builds with Alpine","text":"<p>Use Alpine in build stages to keep final images minimal.</p> <pre><code># Build stage\nFROM alpine:3.19 AS builder\nRUN apk add --no-cache --virtual .build-deps \\\n    go \\\n    git \\\n    musl-dev &amp;&amp; \\\n    go build -o /app main.go &amp;&amp; \\\n    apk del .build-deps\n\n# Final stage\nFROM alpine:3.19\nRUN apk add --no-cache ca-certificates\nCOPY --from=builder /app /app\nENTRYPOINT [\"/app\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#installing-from-edge-repository","title":"Installing from Edge Repository","text":"<p>Sometimes you need newer packages from Alpine's edge repository.</p> <pre><code>RUN apk add --no-cache \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/edge/community \\\n    package-name\n</code></pre> <p>Caution: Edge packages are less stable. Pin versions when using edge repositories.</p>"},{"location":"tools/container-images/alpine-best-practices/#adding-multiple-repositories","title":"Adding Multiple Repositories","text":"<pre><code>RUN apk add --no-cache \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/v3.19/main \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/v3.19/community \\\n    package1 package2\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":""},{"location":"tools/container-images/alpine-best-practices/#1-dont-run-update-in-a-separate-layer","title":"1. Don't Run Update in a Separate Layer","text":"<pre><code># Bad\nRUN apk update\nRUN apk add --no-cache curl\n\n# Good\nRUN apk update &amp;&amp; apk add --no-cache curl\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-dont-use-apk-upgrade-in-production-images","title":"2. Don't Use <code>apk upgrade</code> in Production Images","text":"<pre><code># Bad - unpredictable and breaks reproducibility\nRUN apk upgrade\n\n# Good - pin specific versions instead\nRUN apk add --no-cache nginx=1.24.0-r15\n</code></pre> <p>Why: Using <code>apk upgrade</code> makes builds non-reproducible and can introduce breaking changes unexpectedly.</p>"},{"location":"tools/container-images/alpine-best-practices/#3-dont-install-unnecessary-packages","title":"3. Don't Install Unnecessary Packages","text":"<pre><code># Bad - bash adds ~10MB and increases attack surface\nRUN apk add --no-cache bash curl wget git vim nano\n\n# Good - minimal installation\nRUN apk add --no-cache curl\n</code></pre> <p>Common unnecessary packages:</p> <ul> <li><code>bash</code> (use Alpine's default <code>sh</code> when possible)</li> <li><code>vim</code>, <code>nano</code> (not needed in production containers)</li> <li><code>wget</code> (if you already have <code>curl</code>)</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#4-dont-forget-to-remove-build-dependencies","title":"4. Don't Forget to Remove Build Dependencies","text":"<pre><code># Bad - leaves 200MB+ of build tools\nRUN apk add --no-cache gcc musl-dev &amp;&amp; \\\n    pip install package\n\n# Good - removes build tools\nRUN apk add --no-cache --virtual .build-deps gcc musl-dev &amp;&amp; \\\n    pip install package &amp;&amp; \\\n    apk del .build-deps\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#package-equivalence-guide","title":"Package Equivalence Guide","text":"<p>When migrating from Debian/Ubuntu to Alpine, package names often differ:</p> Debian/Ubuntu Alpine Notes <code>build-essential</code> <code>build-base</code> Includes gcc, make, libc-dev <code>python3-dev</code> <code>python3-dev</code> Same name <code>libssl-dev</code> <code>openssl-dev</code> Different prefix <code>libpq-dev</code> <code>postgresql-dev</code> Different name <code>default-libmysqlclient-dev</code> <code>mariadb-dev</code> MySQL client <code>libffi-dev</code> <code>libffi-dev</code> Same name <code>libjpeg-dev</code> <code>jpeg-dev</code> Simplified name <code>libpng-dev</code> <code>libpng-dev</code> Same name <p>Finding packages:</p> <pre><code># Search for packages\ndocker run --rm alpine:3.19 apk search keyword\n\n# Get package info\ndocker run --rm alpine:3.19 apk info package-name\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#real-world-examples","title":"Real-World Examples","text":""},{"location":"tools/container-images/alpine-best-practices/#python-application","title":"Python Application","text":"<pre><code>FROM alpine:3.19\n\n# Install runtime dependencies\nRUN apk add --no-cache \\\n    python3 \\\n    py3-pip \\\n    ca-certificates\n\n# Install build dependencies, build, then remove them\nCOPY requirements.txt .\nRUN apk add --no-cache --virtual .build-deps \\\n    gcc \\\n    musl-dev \\\n    python3-dev \\\n    libffi-dev \\\n    openssl-dev &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt &amp;&amp; \\\n    apk del .build-deps\n\nCOPY . /app\nWORKDIR /app\n\nCMD [\"python3\", \"app.py\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#nodejs-application","title":"Node.js Application","text":"<pre><code>FROM alpine:3.19\n\n# Install Node.js and npm\nRUN apk add --no-cache \\\n    nodejs=20.11.0-r0 \\\n    npm=10.2.5-r0\n\nWORKDIR /app\n\n# Install dependencies\nCOPY package*.json ./\nRUN npm ci --only=production &amp;&amp; \\\n    npm cache clean --force\n\nCOPY . .\n\nUSER node\nCMD [\"node\", \"server.js\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#go-application-multi-stage","title":"Go Application (Multi-stage)","text":"<pre><code># Build stage\nFROM alpine:3.19 AS builder\nRUN apk add --no-cache go git\nWORKDIR /build\nCOPY . .\nRUN go build -ldflags=\"-w -s\" -o app .\n\n# Final stage\nFROM alpine:3.19\nRUN apk add --no-cache ca-certificates\nCOPY --from=builder /build/app /usr/local/bin/app\nUSER nobody\nENTRYPOINT [\"/usr/local/bin/app\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#security-considerations","title":"Security Considerations","text":""},{"location":"tools/container-images/alpine-best-practices/#1-keep-base-image-updated","title":"1. Keep Base Image Updated","text":"<p>Regularly update your base image version:</p> <pre><code># Pin to specific version for reproducibility\nFROM alpine:3.19\n\n# Or use latest minor version (receives security updates)\nFROM alpine:3\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-run-as-non-root-user","title":"2. Run as Non-Root User","text":"<pre><code>RUN adduser -D -u 1000 appuser\nUSER appuser\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-install-only-required-packages","title":"3. Install Only Required Packages","text":"<p>Each package increases the attack surface. Audit your dependencies regularly.</p>"},{"location":"tools/container-images/alpine-best-practices/#4-use-specific-package-versions","title":"4. Use Specific Package Versions","text":"<p>Pin versions to avoid supply chain attacks through malicious package updates.</p>"},{"location":"tools/container-images/alpine-best-practices/#performance-tips","title":"Performance Tips","text":""},{"location":"tools/container-images/alpine-best-practices/#1-order-layers-by-change-frequency","title":"1. Order Layers by Change Frequency","text":"<p>Put frequently changing instructions last to maximize cache hits:</p> <pre><code>FROM alpine:3.19\n\n# Rarely changes - cached\nRUN apk add --no-cache python3 py3-pip\n\n# Changes occasionally - cached if requirements unchanged\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Changes frequently - rebuilt often\nCOPY . /app\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-use-buildkit-for-better-caching","title":"2. Use BuildKit for Better Caching","text":"<pre><code>DOCKER_BUILDKIT=1 docker build -t myapp .\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-leverage-multi-stage-builds","title":"3. Leverage Multi-Stage Builds","text":"<p>Keep build tools in separate stages to minimize final image size.</p>"},{"location":"tools/container-images/alpine-best-practices/#useful-apk-commands","title":"Useful apk Commands","text":"<pre><code># Update package index\napk update\n\n# Install package\napk add package-name\n\n# Install without caching\napk add --no-cache package-name\n\n# Install specific version\napk add package-name=1.2.3-r0\n\n# Search for packages\napk search keyword\n\n# Show package info\napk info package-name\n\n# List installed packages\napk list --installed\n\n# Remove package\napk del package-name\n\n# Remove virtual package group\napk del .build-deps\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#links-and-resources","title":"Links and Resources","text":"<ul> <li>Alpine Linux Official Website</li> <li>Alpine Linux Docker Hub</li> <li>Alpine Linux Packages</li> <li>Alpine Linux Wiki</li> <li>apk-tools Documentation</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#when-not-to-use-alpine","title":"When Not to Use Alpine","text":"<p>While Alpine is excellent for most use cases, consider alternatives when:</p> <ol> <li>Binary compatibility issues - Your application requires glibc (Alpine uses musl libc)</li> <li>Performance critical - Some applications show performance degradation with musl (2x slowdowns reported in edge cases)</li> <li>Limited package availability - The package you need isn't available in Alpine repositories</li> <li>Team familiarity - Your team is more comfortable with Debian/Ubuntu ecosystems</li> </ol> <p>In these cases, consider Debian Slim, Ubuntu, or distroless images instead.</p>"},{"location":"tools/container-images/build-container-images/","title":"Build container images","text":""},{"location":"tools/container-images/build-container-images/#docker-build","title":"docker build","text":"<p>What: The classic Docker CLI command to build images. Engine: Uses the legacy Docker image builder by default, but can use BuildKit if enabled. Features:</p> <ul> <li>Basic Dockerfile support</li> <li>single-platform builds</li> <li>limited caching.</li> </ul> <pre><code>docker build -t myimage .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/using_docker_build/</p>"},{"location":"tools/container-images/build-container-images/#docker-buildx","title":"docker buildx","text":"<p>What: An advanced Docker CLI plugin for building images. Engine: Uses BuildKit under the hood. Features:</p> <ul> <li>Multi-platform builds (e.g., build for amd64 and arm64 in one command)</li> <li>Advanced caching (local, registry, inline)</li> <li>Output to multiple formats (Docker, OCI, tar, etc.)</li> <li>Build secrets, better performance</li> </ul> <pre><code>docker buildx build --platform linux/amd64,linux/arm64 -t myimage .\n</code></pre>"},{"location":"tools/container-images/build-container-images/#buildkit","title":"buildkit","text":"<p>What: The next-generation image builder for Docker, designed for speed and flexibility. Engine: Can be used standalone or as the backend for docker build and docker buildx. Features:</p> <ul> <li>Parallel build steps</li> <li>Advanced caching</li> <li>Build secrets</li> <li>Improved performance</li> </ul> <p>Usage: Enabled in Docker with DOCKER_BUILDKIT=1 docker build ... Used natively by docker buildx</p> <pre><code>DOCKER_BUILDKIT=1 docker build .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/using_buildkit/</p>"},{"location":"tools/container-images/build-container-images/#buildah","title":"buildah","text":"<p>What: A Red Hat-sponsored, daemonless tool for building OCI and Docker images. Engine: Standalone, does not require the Docker daemon. Features:</p> <ul> <li>Scriptable, fine-grained control over image layers</li> <li>Rootless builds</li> <li>Integrates well with Podman and OpenShift</li> <li>No need for a running Docker daemon</li> </ul> <pre><code>buildah bud -t myimage .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/buildah_rootless_multi_arch/</p>"},{"location":"tools/container-images/build-container-images/#podman","title":"podman","text":""},{"location":"tools/container-images/build-container-images/#kaniko","title":"kaniko","text":"<p>The project has no mantainers</p>"},{"location":"tools/container-images/distroless-images/","title":"Distroless images","text":"<p>Distroless images are minimal container images that contain only the application and its runtime dependencies, without including package managers, shells, or other standard Linux distribution utilities.</p>"},{"location":"tools/container-images/distroless-images/#key-benefits","title":"Key Benefits","text":"<ul> <li>Reduced Attack Surface: No shell or package manager means fewer vulnerabilities to exploit</li> <li>Smaller Image Size: Only essential runtime components are included</li> <li>Improved Security: Minimizes the potential for supply chain attacks</li> <li>Better Compliance: Easier to audit and maintain due to fewer components</li> </ul>"},{"location":"tools/container-images/distroless-images/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Production deployments where security is critical</li> <li>Microservices and cloud-native applications</li> <li>Containerized applications that don't require debugging tools in production</li> </ul>"},{"location":"tools/container-images/distroless-images/#popular-distroless-and-minimal-base-images","title":"Popular Distroless and Minimal Base Images","text":""},{"location":"tools/container-images/distroless-images/#true-distroless-images-no-package-manager","title":"True Distroless Images (No Package Manager)","text":"<p>These images do not include package managers in the final image, providing maximum security by eliminating the ability to install additional packages at runtime.</p>"},{"location":"tools/container-images/distroless-images/#google-distroless","title":"Google Distroless","text":"<p>Primarily maintained by Google and available at <code>gcr.io/distroless/</code>.</p> <p>Key Features:</p> <ul> <li>No shell or package manager</li> <li>Supports multiple runtimes: static binaries, Java, Python, Node.js, .NET</li> <li>Based on Debian</li> </ul> <p>Size: ~110MB for multi-stage builds with distroless (vs 848MB without optimization)</p> <p>Links:</p> <ul> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#chainguard-images-wolfi-based","title":"Chainguard Images (Wolfi-based)","text":"<p>Built on Wolfi, a Linux distribution designed for cloud workloads with zero known CVEs.</p> <p>Key Features:</p> <ul> <li>Uses glibc (not musl like Alpine)</li> <li>Constantly rebuilt with latest sources</li> <li>Includes SBOMs for every image</li> <li>No kernel (container runtime only)</li> <li>No package manager in runtime image (apk only during build)</li> </ul> <p>Available at: <code>cgr.dev/chainguard/</code></p> <p>Links:</p> <ul> <li>Chainguard Images GitHub</li> <li>Wolfi GitHub</li> <li>Official Website</li> <li>Image Directory</li> <li>Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#ubuntu-chiseled","title":"Ubuntu Chiseled","text":"<p>Distroless images built from Ubuntu packages using Chisel.</p> <p>Key Features:</p> <ul> <li>Uses glibc</li> <li>No shell or package manager in final image</li> <li>Only minimal required dependencies included</li> <li>Carefully sliced packages</li> </ul> <p>Available at: <code>ubuntu/</code> on Docker Hub</p> <p>Links:</p> <ul> <li>Official Page</li> <li>Chisel Documentation</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#busybox","title":"BusyBox","text":"<p>Single compact executable with simplified Linux tools.</p> <p>Key Features:</p> <ul> <li>Multiple libc flavors: musl, glibc, uclibc</li> <li>Includes basic utilities (file archiving, process manipulation, etc.)</li> <li>No package manager</li> <li>Extremely minimal</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#scratch","title":"Scratch","text":"<p>Docker's most minimal base - literally empty.</p> <p>Key Features:</p> <ul> <li>Contains nothing at all</li> <li>Only suitable for static binaries</li> <li>Smallest possible image</li> </ul> <p>Links:</p> <ul> <li>Docker Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#minimal-images-with-package-managers","title":"Minimal Images with Package Managers","text":"<p>These images include package managers, allowing installation of additional packages at runtime. They offer more flexibility but a larger attack surface compared to true distroless images.</p>"},{"location":"tools/container-images/distroless-images/#alpine-linux","title":"Alpine Linux","text":"<p>Popular minimal Linux distribution (~5MB base image).</p> <p>Key Features:</p> <ul> <li>Uses musl libc (not glibc)</li> <li>BusyBox tool suite</li> <li>apk package manager included</li> <li>Very small size</li> </ul> <p>Considerations:</p> <ul> <li>Some applications require glibc and won't work with musl</li> <li>Performance degradation possible (2x slowdown not uncommon)</li> <li>Limited package availability compared to Debian/Ubuntu</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#debian-slim","title":"Debian Slim","text":"<p>Pared-down Debian with commonly needed tools removed.</p> <p>Key Features:</p> <ul> <li>Uses glibc</li> <li>apt package manager included</li> <li>Good balance between size and functionality</li> <li>~74MB (vs 118MB for full Debian)</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#red-hat-ubi-minimal","title":"Red Hat UBI Minimal","text":"<p>RHEL-based minimal image for enterprise environments.</p> <p>Key Features:</p> <ul> <li>Based on RHEL packages</li> <li>microdnf package manager (scaled-down DNF)</li> <li>Strong security focus and timely updates</li> <li>~92MB on disk, 32MB compressed</li> </ul> <p>Considerations:</p> <ul> <li>Limited to curated Red Hat packages</li> <li>Best for Red Hat ecosystem (OpenShift, RHEL)</li> <li>Subscription required for non-UBI packages</li> </ul> <p>Links:</p> <ul> <li>Red Hat Catalog</li> <li>UBI FAQ</li> <li>Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#comparison-summary","title":"Comparison Summary","text":""},{"location":"tools/container-images/distroless-images/#true-distroless-images-no-runtime-package-manager","title":"True Distroless Images (No Runtime Package Manager)","text":"Image Maintainer Size Package Manager libc Use Case Scratch Docker 0 MB None None Static binaries only BusyBox BusyBox Project ~5 MB None Various Basic utilities needed Chainguard Chainguard ~20-80 MB None (apk build only) glibc Supply chain security Ubuntu Chiseled Canonical ~20-50 MB None glibc Ubuntu ecosystem Distroless Google ~25-100 MB None glibc Production, high security"},{"location":"tools/container-images/distroless-images/#images-with-package-managers","title":"Images with Package Managers","text":"Image Maintainer Size Package Manager libc Use Case Alpine Alpine Linux ~5 MB apk musl General purpose, size-critical Debian Slim Debian Project ~74 MB apt glibc More tooling, Debian packages UBI Minimal Red Hat ~92 MB microdnf glibc Enterprise/Red Hat ecosystem"},{"location":"tools/container-images/distroless-images/#use-cases-for-distroless-images","title":"Use Cases for Distroless Images","text":"<ul> <li>Pure static binaries - Use scratch (most minimal, zero attack surface)</li> <li>Static binaries needing ca-certificates - Using distroless/static variant</li> <li>Precompiled Java JAR/WAR files - Using Java runtime variants</li> <li>Precompiled Python applications - Using Python runtime variant</li> <li>Precompiled Node.js applications - Using Node.js runtime variants</li> <li>Precompiled .NET applications - Using .NET runtime variants</li> <li>Dynamic binaries requiring glibc - Using base or glibc-dynamic variants</li> <li>C/C++ applications - Using cc variant (includes libgcc, libstdc++)</li> <li>Go binaries with CGO enabled - Using base variant (includes glibc and SSL libraries)</li> </ul>"},{"location":"tools/container-images/distroless-images/#specific-distroless-implementations","title":"Specific Distroless Implementations","text":""},{"location":"tools/container-images/distroless-images/#liberica-runtime-container-bellsoft","title":"Liberica Runtime Container (BellSoft)","text":"<p>Optimized Java runtime containers from BellSoft, available as both JDK and JRE variants.</p> <p>Available at: <code>bellsoft/liberica-runtime-container</code></p> <p>Links:</p> <ul> <li>Docker Hub</li> <li>BellSoft Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#amazon-corretto","title":"Amazon Corretto","text":"<p>No-cost, production-ready distribution of OpenJDK from Amazon Web Services.</p> <p>Available at: <code>amazoncorretto</code></p> <p>Links:</p> <ul> <li>Docker Hub</li> <li>ECR Public Gallery</li> <li>Official Website</li> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#multi-stage-build-pattern","title":"Multi-Stage Build Pattern","text":"<p>Distroless images are typically used in multi-stage builds where dependencies are installed in a full-featured image, then only the application and runtime are copied to the distroless final image:</p> <pre><code># Build stage\nFROM golang:1.21 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\n# Final stage\nFROM gcr.io/distroless/static-debian12\nCOPY --from=builder /app/myapp /myapp\nENTRYPOINT [\"/myapp\"]\n</code></pre>"},{"location":"tools/container-images/distroless-images/#important-note-on-entrypoint","title":"Important Note on ENTRYPOINT","text":"<p>Distroless images without a shell require ENTRYPOINT to be specified in exec form (JSON array), not shell form:</p> <pre><code># Correct - exec form\nENTRYPOINT [\"/myapp\", \"--flag\"]\n\n# Wrong - shell form (requires /bin/sh)\nENTRYPOINT /myapp --flag\n</code></pre>"},{"location":"tools/git/98-tips-remote-branches/","title":"Remote branches tips","text":""},{"location":"tools/git/98-tips-remote-branches/#get-remote-branches","title":"Get remote branches","text":"<p>To get the remote branches we must using git fetch</p> <pre><code>git fetch\n</code></pre> <p>This gets the remote branches from the default remote of the current branch. If the current branch tracks origin/main, it fetches updates from origin. If there's no tracking branch configured, it defaults to the remote named origin if it exists.</p> <p>To see what remote your current branch tracks:</p> <pre><code>git branch -vv\n</code></pre> <p>We can also specify the remote</p> <pre><code>git fetch origin\n</code></pre> <p>Get the from all remotes</p> <pre><code>git fetch --all\n</code></pre>"},{"location":"tools/git/98-tips-remote-branches/#about-git-fetch-and-git-pull","title":"About git fetch and git pull","text":"<p>Git fetch only downloads commits, files, and refs from remote but doesn't modify the working directory</p> <ul> <li>Updates remote-tracking branches (e.g., origin/main)</li> <li>Safe operation - won't affect your current work</li> <li>Lets you review changes before integrating</li> </ul> <pre><code>git fetch origin\ngit log origin/main  # Review what's new\ngit merge origin/main  # Merge when ready\n</code></pre> <p>Use fetch when you want to see what changed before integrating.</p> <p>Git pull does git fetch + git merge (or rebase)</p> <ul> <li>Downloads remote changes AND merges them into your current branch</li> <li>Modifies your working directory immediately</li> <li>Can cause merge conflicts if you have local changes</li> </ul> <pre><code>  git fetch\n  git diff main origin/main # See what's different before pulling\n  git pull  # Now merge. git pull --all from all remotes\n</code></pre> <p>Use pull when you're ready to update immediately.</p>"},{"location":"tools/git/98-tips-remote-branches/#local-references","title":"Local References","text":"<p>To see local references to remote branches we can use</p> <pre><code>git branch --remotes # -r\n</code></pre> <p>If we want to remove local references to remote branches that they do not exists, we can use the --prune parameter</p> <pre><code>git fetch --all --prune\n</code></pre>"},{"location":"tools/git/98-tips/","title":"Tips","text":""},{"location":"tools/git/98-tips/#how-to-delete-the-git-history","title":"How to delete the git history","text":"<p>Checkout/create orphan branch (this branch won't show in git branch command):</p> <pre><code>git checkout --orphan latest_branch\n</code></pre> <p>See what you need to include</p> <pre><code>git status\ngit add -A # if you want to add all\ngit commit -am \"clean history\"\n</code></pre> <p>Delete main branch</p> <pre><code>git branch -D main\n</code></pre> <p>Rename the current branch to main</p> <pre><code>git branch -m main\n</code></pre> <p>See the remotes and push changes This ensures you don't overwrite other people's work</p> <pre><code>git remote  -v\ngit push --force-with-lease REMOTE main\n</code></pre>"},{"location":"tools/git/98-tips/#gitlab-no-allowed-to-force-push","title":"Gitlab: no allowed to force push","text":"<p>If your are pushing to gitlab and get this error \"You are not allowed to force push code to a protected branch on this project\"</p> <p>Enable force push in the repository under Settings  &gt; Repository &gt; Protected branches</p> <p>See the branch and enable \"Allowed to force push\" and repeat the push. After the push, disable it</p>"},{"location":"tools/git/99-links/","title":"Links","text":"<ul> <li>Git Reference</li> </ul> <p>https://git-scm.com/docs</p> <ul> <li>Git book</li> </ul> <p>https://git-scm.com/book/en/v2</p> <ul> <li>Git External Links</li> </ul> <p>https://git-scm.com/doc/ext</p>"},{"location":"tools/git/authentication/","title":"Authentication with remotes","text":"<p>There are 2 ways to authenticate to a git repository depending on how the remote is configured.</p>"},{"location":"tools/git/authentication/#git-remote-management","title":"Git remote management","text":"<pre><code>git config --get remote.origin.url                                        # check current URL\ngit remote set-url origin git@provider.com:organization/repository.git    # switch to SSH\ngit remote set-url origin https://provider.com/organization/repository.git # switch to HTTPS\ngit remote add upstream https://provider.com/organization/repository.git  # add a new remote\n</code></pre>"},{"location":"tools/git/authentication/#via-ssh","title":"Via SSH","text":"<p>The remote uses this format:</p> <pre><code>git@provider.com:organization/repository.git\n</code></pre> <p>This authentication mode relies on a SSH key loaded in memory (<code>ssh-agent</code> or similar) and imported in the provider's profile.</p> <p>Generate a key, load it, and add the public key (<code>~/.ssh/id_ed25519.pub</code>) to the provider profile:</p> <pre><code>ssh-keygen -t ed25519 -C \"your-email@example.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\nssh -T git@github.com  # test the connection\n</code></pre> <p>To use different keys per host, configure <code>~/.ssh/config</code>:</p> <pre><code>Host github.com\n  IdentityFile ~/.ssh/id_ed25519_github\nHost gitlab.com\n  IdentityFile ~/.ssh/id_ed25519_gitlab\n</code></pre>"},{"location":"tools/git/authentication/#via-https","title":"Via HTTPS","text":"<p>The remote uses this format:</p> <pre><code>https://provider.com/organization/repository.git\n</code></pre> <p>In this case we need to provide username and password/token in our operations.</p>"},{"location":"tools/git/authentication/#token-based-authentication","title":"Token-based authentication","text":"<p>Most providers require or recommend tokens instead of passwords:</p> <ul> <li>GitHub: Password auth is not supported. Use a Personal Access Token (PAT) created at https://github.com/settings/tokens</li> <li>GitLab: Use a Personal Access Token created at <code>Settings &gt; Access Tokens</code></li> <li>Bitbucket: Use an App Password created at <code>Personal settings &gt; App passwords</code></li> </ul>"},{"location":"tools/git/authentication/#embedding-credentials-in-the-url","title":"Embedding credentials in the URL","text":"<p>It is possible to embed the token in the remote URL (not recommended for shared machines):</p> <pre><code>git remote set-url origin https://&lt;username&gt;:&lt;token&gt;@provider.com/org/repo.git\n</code></pre>"},{"location":"tools/git/authentication/#using-netrc","title":"Using .netrc","text":"<p>Credentials can be stored in <code>~/.netrc</code> (Linux/macOS) or <code>~/_netrc</code> (Windows). Ensure <code>chmod 600 ~/.netrc</code>:</p> <pre><code>machine github.com\nlogin &lt;username&gt;\npassword &lt;token&gt;\n</code></pre>"},{"location":"tools/git/authentication/#credentials-helper-https-only","title":"Credentials helper (HTTPS only)","text":"<p>To avoid being asked for credentials on every HTTPS operation, use a credential helper.</p>"},{"location":"tools/git/authentication/#built-in-helpers","title":"Built-in helpers","text":"<pre><code>git config --global credential.helper 'cache --timeout=3600'  # cache in memory (default 900s)\ngit config --global credential.helper store                    # store in ~/.git-credentials\n</code></pre> <p>Warning: The <code>store</code> helper saves credentials in plain text. Use it only on trusted machines.</p>"},{"location":"tools/git/authentication/#platform-specific-helpers","title":"Platform-specific helpers","text":"<p>These helpers integrate with the OS-level keystore for secure persistent storage:</p> Platform Helper Backend Linux <code>git-credential-libsecret</code> GNOME Keyring / KWallet via libsecret macOS <code>git-credential-osxkeychain</code> macOS Keychain (included with Xcode CLI tools) Windows <code>git-credential-wincred</code> Windows Credential Manager <p>Configure them:</p> <pre><code># macOS\ngit config --global credential.helper osxkeychain\n# Windows\ngit config --global credential.helper wincred\n</code></pre> <p>On Ubuntu/Debian, <code>git-credential-libsecret</code> is not pre-built. Build it from git contrib sources:</p> <pre><code>sudo apt-get install -y make gcc libsecret-1-0 libsecret-1-dev libglib2.0-dev\nsudo make --directory=/usr/share/doc/git/contrib/credential/libsecret\ngit config --global credential.helper \\\n  /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n</code></pre> <p>On Fedora/RHEL, install the <code>git-credential-libsecret</code> package directly.</p>"},{"location":"tools/git/authentication/#git-credential-manager-gcm","title":"Git Credential Manager (GCM)","text":"<p>Cross-platform alternative that supports OAuth, PATs, and MFA. Recommended over the platform-specific helpers above.</p> <p>https://github.com/git-ecosystem/git-credential-manager/</p> <pre><code>git config --global credential.helper manager\n</code></pre>"},{"location":"tools/git/authentication/#provider-clis","title":"Provider CLIs","text":"<p>Both GitHub and GitLab CLIs can act as credential helpers for their respective platforms:</p> <pre><code># GitHub CLI\ngh auth login &amp;&amp; gh auth setup-git\n# GitLab CLI (glab)\nglab auth login &amp;&amp; glab auth setup-git\n</code></pre>"},{"location":"tools/git/authentication/#links","title":"Links","text":"<ul> <li>Git credentials</li> <li>GitHub: Set up Git</li> <li>GitHub: About remote repositories</li> <li>GitHub: Connecting to GitHub with SSH</li> <li>GitHub: Caching your GitHub credentials in Git</li> <li>GitLab: Personal access tokens</li> </ul>"},{"location":"tools/git/move-between-places/","title":"Move between places","text":""},{"location":"tools/git/move-between-places/#working-directory","title":"Working directory","text":"<p>The working directory or working tree is the directory that containes the files we are working with.</p> <p>Inside the working directory we can find tracked an untracked files.</p>"},{"location":"tools/git/move-between-places/#tracked-files","title":"Tracked files","text":"<p>Tracked files are files that git knows about, they were in the last snapshot. They can be:</p> <ul> <li>unmodified</li> </ul> <p>No changed made from the last snapshot</p> <ul> <li>modified</li> </ul> <p>With pending changes from the last snapshot</p> <ul> <li>staged</li> </ul> <p>Added to the staging area</p> <p>the git status command show the different status of a file.</p>"},{"location":"tools/git/move-between-places/#untracked-files","title":"Untracked files","text":"<p>Untracked files are files in the working directory not included in the last snapshot and they are not in the staging area.</p>"},{"location":"tools/git/move-between-places/#staging-area-or-index","title":"Staging area or index","text":""},{"location":"tools/git/move-between-places/#head","title":"HEAD","text":""},{"location":"tools/git/move-between-places/#movements","title":"Movements","text":"<p>We can move files from one state to another</p> From to Command untracked or modified file staging area git add staging area next snapshot git commit modified file unmodified file git restore (--worktree) staging area untracked or modified file git restore --staged or git reset HEAD last snapshot untracked git rm --cached <p>checkout</p>"},{"location":"tools/git/move-between-places/#links","title":"Links","text":"<ul> <li>Recording Changes to the Repository</li> </ul> <p>https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository</p> <ul> <li>Undoing Things</li> </ul> <p>https://git-scm.com/book/en/v2/Git-Basics-Undoing-Things</p> <ul> <li>Reset Demystified https://git-scm.com/book/en/v2/Git-Tools-Reset-Demystified</li> </ul> <p>https://git-scm.com/docs/git-add https://git-scm.com/docs/git-reset https://git-scm.com/docs/git-checkout https://git-scm.com/docs/git-restore</p>"},{"location":"tools/jq/tips/","title":"Tips","text":""},{"location":"tools/jq/tips/#remove-a-prefix-from-an-string","title":"remove a prefix from an string","text":"<p>Using the ltrimstr function</p> <pre><code>export KRO_VERSION=$(curl -sL \\\n    https://api.github.com/repos/kro-run/kro/releases/latest | \\\n    jq -r '.tag_name | ltrimstr(\"v\")'\n  )\n</code></pre>"},{"location":"tools/terraform/97-errors/","title":"Errors","text":""},{"location":"tools/terraform/97-errors/#index-value-required","title":"Index value required","text":"<pre><code>Error: Index value required \u2502 \u2502 on line 1: \u2502 (source code not available) \u2502 \u2502 Index brackets must contain either a literal number or a literal string.\n</code></pre> <p>We must use single quotes in the resource 'RESOURCE'</p>"},{"location":"tools/terraform/init/","title":"Init","text":""},{"location":"tools/terraform/init/#working-with-different-environments-and-backend","title":"Working with different environments and backend","text":"<p>If we share the code between different environments and backends, we need to run this:</p> <pre><code>terraform init -backend-config=path/to/backend-config.tfvars\n</code></pre> <ul> <li>When switching between environments with different backends</li> <li>When changing backend settings (e.g., switching S3 buckets, Azure storage accounts, or vSphere datastores).</li> <li>When initializing a new environment with a different backend configuration.</li> <li>Whenever you want to reconfigure the backend (for example, after editing backend config files).</li> </ul> <p>You do not need to run -backend-config if only changing variables in your .tfvars files that are unrelated to the backend. Use it only for backend-specific changes.</p>"},{"location":"tools/vscode/features/","title":"Features","text":""},{"location":"tools/vscode/features/#linting","title":"Linting","text":"<p>Linting refers to the process of analyzing your code for potential errors, bugs, stylistic errors, and other issues. It helps improve code quality and maintain consistency across your codebase.</p> <p>Key Features of Linting in VS Code:</p> <ul> <li>Error Detection: Linters can detect syntax errors, runtime errors, and logical errors in your code.</li> <li>Code Style Enforcement: Linters enforce coding standards and style guidelines, ensuring that your code adheres to best practices.</li> <li>Real-time Feedback: As you write code, linters provide immediate feedback, highlighting issues directly in the editor.</li> <li>Fix Suggestions: Many linters offer suggestions for fixing detected issues, and some can even automatically apply fixes.</li> </ul> <p>In order to enable linting in VS Code we need to install an extension via the VS Code Marketplace that provides a linter tool (linter) for our programming language.</p> <p>Here we have a list of linters</p> <p>https://marketplace.visualstudio.com/search?target=VSCode&amp;category=Linters</p>"},{"location":"tools/vscode/features/#formatting","title":"Formatting","text":""},{"location":"tools/vscode/features/#snippet","title":"Snippet","text":"<p>https://code.visualstudio.com/docs/editor/userdefinedsnippets</p>"},{"location":"tools/vscode/features/#intellisense-and-code-completion","title":"Intellisense and Code completion","text":"<p>editor.quickSuggestions editor.tabCompletion</p> <p>https://code.visualstudio.com/docs/editor/intellisense</p> <p>Code completion is an vscode feature that provides suggestions and auto-completions to our code.</p> <p>Code completion is part of a wider feature called intellisense.</p> <p>Key Features of Code Completion in VS Code:</p> <ul> <li> <p>Auto-Suggestions: As you type, VS Code suggests possible completions for your code based on the context. This includes variable names, function names, class names, and more.</p> </li> <li> <p>Snippet Insertion: VS Code can insert code snippets, which are predefined templates for common code structures. This helps speed up coding by reducing repetitive typing.</p> </li> <li> <p>Parameter Info: When you type a function call, VS Code shows the function's parameters and their types, helping you understand what arguments are required.</p> </li> <li> <p>Quick Info: Hovering over a symbol provides additional information, such as documentation comments, type information, and more.</p> </li> <li> <p>Member Lists: When you type a dot after an object, VS Code shows a list of available members (methods, properties, etc.) for that object.</p> </li> </ul> <p>How to Use Code Completion in VS Code:</p> <ul> <li>Typing: Simply start typing, and VS Code will automatically show suggestions based on the context.</li> <li>Trigger Suggestions: You can manually trigger suggestions by pressing Ctrl + Space.</li> <li>Accepting Suggestions: Use the Tab or Enter key to accept a suggestion from the list.</li> </ul>"},{"location":"tools/vscode/kubernetes-resources/","title":"Kubernetes syntax","text":"<p>How to enable systax for kubernetes resources (and other yaml files)</p>"},{"location":"tools/vscode/kubernetes-resources/#install-and-enable-the-yaml-extension","title":"Install and enable the yaml extension","text":"<p>Install the redhat yaml extension</p> <p>https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml</p> <p>And associate the yaml files to it in the settings.json vscode file (at user or workspace level)</p> <pre><code>    \"files.associations\": {\n        \"*.yaml\": \"yaml\"\n    },\n</code></pre>"},{"location":"tools/vscode/kubernetes-resources/#yaml-schemas","title":"Yaml schemas","text":"<p>We can associate an url or local file containing a json schema to files with the yaml.schemas setting.</p> <pre><code>\"yaml.schemas\": {\n    \"kubernetes\": \"*.yaml\"\n},\n</code></pre> <p>But this will not recognize CRDs. For that we need to get the json schema for that CRDs and associate them with files using, for example, patterns.</p>"},{"location":"tools/vscode/kubernetes-resources/#where-to-get-the-crds","title":"Where to get the CRDs","text":"<p>A typical place to search some CRDs are</p> <ul> <li>JSON schema store (very limited)</li> </ul> <p>https://www.schemastore.org/</p> <ul> <li>yannh kubernetes schemas</li> </ul> <p>This repository contains updated schemas for vanilla kubernetes resources and it is related with the kuberconform utility</p> <p>https://github.com/yannh/kubernetes-json-schema</p> <ul> <li>Datree's CRDs catalog</li> </ul> <p>Big online CRDs catalog</p> <p>https://github.com/datreeio/CRDs-catalog/</p> <ul> <li>CRDs extractor</li> </ul> <p>Datree offers a very interesting script called crd extractor that it search all the crds in a kubernetes cluster and it stores them as json under ~/.datree/crdSchemas/</p> <p>https://github.com/datreeio/CRDs-catalog/releases/latest/download/crd-extractor.zip</p> <ul> <li>Official schema</li> </ul> <p>Sometimes the developer offers a public json schema of their CRDs</p>"},{"location":"tools/vscode/kubernetes-resources/#examples","title":"Examples","text":"<p>Here we can see how to associate public or downloaded json schemas with file names.</p> <pre><code>    \"yaml.schemas\": {\n        // Gitlab ci\n        \"https://gitlab.com/gitlab-org/gitlab/-/raw/master/app/assets/javascripts/editor/schema/ci.json\": [\".gitlab-ci.yml\"],\n        // MKdocs\n        \"https://json.schemastore.org/mkdocs-1.6.json\": [\"mkdocs.yml\"],\n        // Helm\n        \"https://json.schemastore.org/chart-lock.json\" : \"Chart.lock\",\n        \"https://json.schemastore.org/chart.json\": \"Chart.yaml\",\n        // Argocd\n        \"https://raw.githubusercontent.com/datreeio/CRDs-catalog/refs/heads/main/argoproj.io/application_v1alpha1.json\": [\"argocd-app-*.yaml\"],\n        \"file:///home/myuser/.datree/crdSchemas/argoproj.io/applicationset_v1alpha1.json\": [\"argocd-appset-*.yaml\"],\n        \"file:///home/myuser/.datree/crdSchemas/argoproj.io/appproject_v1alpha1.json\": [\"argocd-proj-*.yaml\"],\n    },\n</code></pre> <p>I prefer this way over the kubernetes microsoft extension</p>"},{"location":"tools/vscode/shortcuts/","title":"Some shortcuts","text":""},{"location":"tools/vscode/shortcuts/#in-markdown","title":"In markdown","text":""},{"location":"tools/vscode/shortcuts/#ctrlshifto","title":"Ctrl+Shift+O","text":"<p>Fast access to sections</p>"},{"location":"tools/vscode/shortcuts/#ctrlshiftv","title":"Ctrl+Shift+V","text":"<p>Document preview</p>"},{"location":"tools/vscode/shortcuts/#ctrlspace","title":"Ctrl+Space","text":"<p>Show suggestions</p>"},{"location":"tools/vscode/yaml/","title":"Yaml","text":"<p>https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml</p>"},{"location":"tools/vscode/yaml/#schemas","title":"schemas","text":"<p>yaml.schemas: Helps you associate schemas with files in a glob pattern yaml.schemaStore.enable: When set to true, the YAML language server will pull in all available schemas from JSON Schema Store yaml.schemaStore.url: URL of a schema store catalog to use when downloading schemas.</p>"},{"location":"tools/vscode/yaml/#completion","title":"completion","text":"<p>yaml.completion: Enable/disable autocompletion</p>"},{"location":"vibe-sreing/","title":"Intro to vibe sreing","text":""},{"location":"vibe-sreing/#ai-agents-assistants","title":"Ai Agents / Assistants","text":"<p>List of AI agents</p> Agent Company URL Claude Code Terminal based Anthropic https://www.anthropic.com/claude-code Gemini CLI Terminal based Google Codex Terminal based OpenAi https://openai.com/codex/ Github copilot cli Terminal based Microsoft https://github.com/features/copilot/cli Windsurf (formerly codeium) IDE https://windsurf.com/ Vscode + Github copilot agent mode IDE Microsoft https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode Cursor IDE https://cursor.com/ Vscodium N/A https://vscodium.com/"},{"location":"vibe-sreing/#web-interfaces","title":"Web interfaces","text":"Company URL Claude Anthropic ChatGPT OpenAi https://chatgpt.com/"},{"location":"vibe-sreing/#large-language-models","title":"Large language models","text":"Family Name Latest Release Company Website Sonnet 4.5 Anthropic https://www.anthropic.com Opus 4.1 Anthropic https://www.anthropic.com Haiku 3.5 Anthropic https://www.anthropic.com GPT GPT-5 OpenAI https://openai.com o-series o4-mini OpenAI https://openai.com Gemini 2.5 Google https://ai.google.dev Llama 4 Meta https://ai.meta.com Magistral Medium Mistral AI https://mistral.ai Medium 3 Mistral AI https://mistral.ai Small 3.1 Mistral AI https://mistral.ai Qwen 3 Alibaba Cloud https://qwenlm.github.io V-series V3.2 DeepSeek https://www.deepseek.com R-series R1-0528 DeepSeek https://www.deepseek.com Grok 4 Fast xAI https://x.ai"},{"location":"vibe-sreing/vscode-extensions/","title":"VS code Extensions","text":"<ul> <li>Claude code</li> </ul> <p>Official VS Code extension bringing Claude's AI capabilities directly into the editor.</p> <p>https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code</p> <ul> <li>Github Copilot</li> </ul> <p>AI pair programmer providing code suggestions and completions based on context and comments.</p> <p>https://code.visualstudio.com/docs/copilot/overview https://marketplace.visualstudio.com/items?itemName=GitHub.copilot</p> <ul> <li>Kilo code</li> </ul> <p>AI coding assistant focused on enterprise security and compliance requirements.</p> <p>https://kilocode.ai/</p> <ul> <li>Windsurf plugin</li> </ul> <p>AI-powered coding assistant with advanced context understanding and multi-file operations.</p> <p>https://marketplace.visualstudio.com/items?itemName=Codeium.codeium</p> <ul> <li>Amazon Q for VScode</li> </ul> <p>AWS's AI assistant for developers with deep integration into AWS services and documentation.</p> <p>https://aws.amazon.com/q/developer/ https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.amazon-q-vscode</p> <ul> <li>Tabnine</li> </ul> <p>AI code completion tool trained on open source code with privacy-focused options.</p> <p>https://www.tabnine.com/</p> <ul> <li>Augment</li> </ul> <p>AI coding assistant with focus on code quality and team collaboration features.</p> <p>https://marketplace.visualstudio.com/items?itemName=augment.vscode-augment</p>"},{"location":"vibe-sreing/claude-code/00-index/","title":"Intro","text":""},{"location":"vibe-sreing/claude-code/01-files/","title":"Files","text":""},{"location":"vibe-sreing/claude-code/01-files/#scopes","title":"Scopes","text":"<p>Claude Code uses four scope tiers that determine where config applies and who it is shared with.</p> Scope Location Who it affects Shared? Managed System dirs (<code>/etc/claude-code/</code> on Linux/WSL) All users on the machine IT-deployed User <code>~/.claude/</code> You, all projects No Project <code>.claude/</code> in repo All collaborators Yes (committed to git) Local <code>.claude/*.local.*</code> in repo You, this repo only No (auto-gitignored)"},{"location":"vibe-sreing/claude-code/01-files/#managed","title":"Managed","text":"<p>Organization-wide enforcement by IT/DevOps. Highest precedence \u2014 cannot be overridden. Used for security policies, compliance, and model restrictions.</p> <ul> <li>Linux/WSL: <code>/etc/claude-code/</code></li> <li>macOS: <code>/Library/Application Support/ClaudeCode/</code></li> </ul>"},{"location":"vibe-sreing/claude-code/01-files/#user","title":"User","text":"<p>Personal preferences applied across all projects. Not shared with anyone. Best for personal tools, themes, and API configuration.</p>"},{"location":"vibe-sreing/claude-code/01-files/#project","title":"Project","text":"<p>Committed to source control and shared with all collaborators. Best for team permissions, hooks, and shared tooling.</p>"},{"location":"vibe-sreing/claude-code/01-files/#local","title":"Local","text":"<p>Personal machine-specific overrides within a project. Automatically gitignored. Good for testing configurations or machine-specific settings before sharing with the team.</p>"},{"location":"vibe-sreing/claude-code/01-files/#files-by-scope","title":"Files by Scope","text":""},{"location":"vibe-sreing/claude-code/01-files/#settings","title":"Settings","text":"File Scope <code>/etc/claude-code/managed-settings.json</code> Managed <code>~/.claude/settings.json</code> User <code>.claude/settings.json</code> Project <code>.claude/settings.local.json</code> Local"},{"location":"vibe-sreing/claude-code/01-files/#memory-instructions-claudemd","title":"Memory / Instructions (CLAUDE.md)","text":"File Scope <code>~/.claude/CLAUDE.md</code> User <code>CLAUDE.md</code> or <code>.claude/CLAUDE.md</code> Project <code>.claude/CLAUDE.local.md</code> Local <p>Loaded at startup to provide custom instructions and context.</p>"},{"location":"vibe-sreing/claude-code/01-files/#mcp-servers","title":"MCP Servers","text":"File Scope <code>~/.claude.json</code> User (also stores OAuth, preferences, per-project state) <code>.mcp.json</code> Project <code>/etc/claude-code/managed-mcp.json</code> Managed"},{"location":"vibe-sreing/claude-code/01-files/#subagents","title":"Subagents","text":"Path Scope <code>~/.claude/agents/</code> User <code>.claude/agents/</code> Project <p>Markdown files with YAML frontmatter defining specialized agents.</p>"},{"location":"vibe-sreing/claude-code/01-files/#precedence-highest-to-lowest","title":"Precedence (highest to lowest)","text":"<ol> <li>Managed \u2014 cannot be overridden</li> <li>Command line arguments</li> <li>Local (<code>.claude/settings.local.json</code>)</li> <li>Project (<code>.claude/settings.json</code>)</li> <li>User (<code>~/.claude/settings.json</code>)</li> </ol> <p>Settings are merged across scopes, not replaced. Lower-precedence values still apply for settings not overridden at a higher scope.</p>"},{"location":"vibe-sreing/claude-code/01-files/#key-settings-fields","title":"Key Settings Fields","text":"<pre><code>{\n  \"$schema\": \"https://json.schemastore.org/claude-code-settings.json\",\n  \"model\": \"opus\",\n  \"effortLevel\": \"high\",\n  \"availableModels\": [\"sonnet\", \"haiku\"],\n  \"permissions\": {\n    \"allow\": [\"Bash(npm run lint)\", \"Read(~/.zshrc)\"],\n    \"deny\": [\"Bash(curl *)\", \"Read(./.env)\"],\n    \"ask\": [\"Bash(git push *)\"],\n    \"additionalDirectories\": [\"../docs/\"],\n    \"defaultMode\": \"acceptEdits\"\n  },\n  \"env\": {\n    \"MY_VAR\": \"value\"\n  },\n  \"hooks\": {},\n  \"cleanupPeriodDays\": 30,\n  \"language\": \"english\",\n  \"statusLine\": {\"type\": \"command\", \"command\": \"~/.claude/statusline.sh\"},\n  \"sandbox\": {\n    \"enabled\": true,\n    \"network\": {\n      \"allowedDomains\": [\"github.com\"]\n    }\n  }\n}\n</code></pre>"},{"location":"vibe-sreing/claude-code/01-files/#permissions-evaluation-order","title":"Permissions evaluation order","text":"<p><code>deny</code> \u2192 <code>ask</code> \u2192 <code>allow</code> (first match wins)</p>"},{"location":"vibe-sreing/claude-code/01-files/#backups","title":"Backups","text":"<p>Claude Code automatically creates timestamped backups of configuration files, retaining the 5 most recent.</p>"},{"location":"vibe-sreing/claude-code/01-files/#links","title":"Links","text":"<ul> <li>Settings reference</li> <li>Memory files (CLAUDE.md)</li> <li>Subagents</li> <li>MCP servers</li> </ul>"},{"location":"vibe-sreing/claude-code/02-models/","title":"Models","text":"<p>Claude Code supports model selection via aliases or full model names.</p>"},{"location":"vibe-sreing/claude-code/02-models/#model-aliases","title":"Model Aliases","text":"Alias Behavior <code>default</code> Recommended model for your account type <code>sonnet</code> Latest Sonnet (currently 4.6) for daily coding tasks <code>opus</code> Latest Opus (currently 4.6) for complex reasoning <code>haiku</code> Fast and efficient for simple tasks <code>sonnet[1m]</code> Sonnet with 1M token context window <code>opusplan</code> Opus during plan mode, Sonnet for execution <p>Aliases always point to the latest version. To pin to a specific version, use the full model name (e.g., <code>claude-opus-4-6</code>).</p>"},{"location":"vibe-sreing/claude-code/02-models/#switching-models","title":"Switching Models","text":"<p>Four ways to set the model, in order of priority:</p> <ol> <li>During session: <code>/model &lt;alias|name&gt;</code></li> <li>At startup: <code>claude --model &lt;alias|name&gt;</code></li> <li>Environment variable: <code>ANTHROPIC_MODEL=&lt;alias|name&gt;</code></li> <li>Settings file: <code>\"model\": \"opus\"</code> in <code>~/.claude/settings.json</code></li> </ol>"},{"location":"vibe-sreing/claude-code/02-models/#default-model-by-subscription","title":"Default Model by Subscription","text":"User type Default model Max, Team Premium, Pro Opus 4.6 Pay-as-you-go (API) Sonnet 4.5 <p>Claude Code may automatically fall back to Sonnet if you hit a usage threshold with Opus.</p>"},{"location":"vibe-sreing/claude-code/02-models/#opusplan","title":"opusplan","text":"<p>Uses Opus during plan mode for complex reasoning, then switches to Sonnet for code execution. Best of both worlds: reasoning quality + execution efficiency.</p>"},{"location":"vibe-sreing/claude-code/02-models/#effort-level-opus-46","title":"Effort Level (Opus 4.6)","text":"<p>Controls adaptive reasoning depth. Three levels: low, medium, high (default).</p> <ul> <li>In <code>/model</code>: use arrow keys on the effort slider</li> <li>Environment variable: <code>CLAUDE_CODE_EFFORT_LEVEL=low|medium|high</code></li> <li>Settings file: <code>\"effortLevel\": \"high\"</code></li> </ul>"},{"location":"vibe-sreing/claude-code/02-models/#extended-context-1m-tokens","title":"Extended Context (1M tokens)","text":"<p>Use the <code>[1m]</code> suffix for long sessions:</p> <pre><code>/model sonnet[1m]\n/model claude-sonnet-4-6[1m]\n</code></pre> <p>Opus 4.6 1M context is only for API and pay-as-you-go users. Not available for Pro/Max/Teams/Enterprise subscribers.</p>"},{"location":"vibe-sreing/claude-code/02-models/#environment-variables","title":"Environment Variables","text":"Variable Description <code>ANTHROPIC_MODEL</code> Override model selection <code>CLAUDE_CODE_EFFORT_LEVEL</code> Effort level: <code>low</code>, <code>medium</code>, or <code>high</code> <code>ANTHROPIC_DEFAULT_OPUS_MODEL</code> Full model name for the <code>opus</code> alias <code>ANTHROPIC_DEFAULT_SONNET_MODEL</code> Full model name for the <code>sonnet</code> alias <code>ANTHROPIC_DEFAULT_HAIKU_MODEL</code> Full model name for the <code>haiku</code> alias and background tasks <code>CLAUDE_CODE_SUBAGENT_MODEL</code> Model for subagents <p><code>ANTHROPIC_SMALL_FAST_MODEL</code> is deprecated in favor of <code>ANTHROPIC_DEFAULT_HAIKU_MODEL</code>.</p>"},{"location":"vibe-sreing/claude-code/02-models/#prompt-caching","title":"Prompt Caching","text":"<p>Enabled by default to reduce costs. Disable globally or per model tier:</p> Variable Description <code>DISABLE_PROMPT_CACHING</code> Set to <code>1</code> to disable for all models <code>DISABLE_PROMPT_CACHING_HAIKU</code> Set to <code>1</code> to disable for Haiku only <code>DISABLE_PROMPT_CACHING_SONNET</code> Set to <code>1</code> to disable for Sonnet only <code>DISABLE_PROMPT_CACHING_OPUS</code> Set to <code>1</code> to disable for Opus only"},{"location":"vibe-sreing/claude-code/02-models/#enterprise-model-restrictions","title":"Enterprise Model Restrictions","text":"<p>Admins can restrict available models via <code>availableModels</code> in managed settings. The <code>model</code> field sets the explicit override:</p> <pre><code>{\n  \"availableModels\": [\"sonnet\", \"haiku\"],\n  \"model\": \"sonnet\"\n}\n</code></pre>"},{"location":"vibe-sreing/claude-code/02-models/#links","title":"Links","text":"<ul> <li>Model configuration</li> <li>Models overview</li> <li>Choosing the right model</li> <li>Claude Code model configuration (support article)</li> </ul>"},{"location":"vibe-sreing/claude-code/98-tips/","title":"Tips","text":""},{"location":"vibe-sreing/claude-code/98-tips/#use-claude-code-to-setup-claude-code","title":"Use claude code to setup claude code","text":"<p>You can use claude code itself to manage thinks like:</p> <ul> <li>settings.json</li> <li>agents</li> <li>custom slash commands</li> <li>agent skills</li> </ul>"},{"location":"vibe-sreing/claude-code/98-tips/#clear-context","title":"Clear context","text":"<p>Use the /clear command frequently between tasks and conversations to reset the context window.</p>"},{"location":"vibe-sreing/claude-code/98-tips/#share-context","title":"Share context","text":"<p>Share CLAUDE.md context pushing them to the git repository</p>"},{"location":"vibe-sreing/claude-code/98-tips/#vs-code-problems","title":"VS code problems","text":"<p>Use mcp__ide__getDiagnostics to fix problems detected in VScode problems tab</p>"},{"location":"vibe-sreing/claude-code/98-tips/#use-commands","title":"Use commands","text":"<p>Use commands for common tasks</p>"},{"location":"vibe-sreing/claude-code/98-tips/#working-with-conversations","title":"Working with conversations","text":"<p>Continue the last conversation</p> <pre><code>claude -c\nclaude --continue\n</code></pre> <p>Open an interactive menu to choose from last conversations</p> <pre><code>claude -r\nclaude --resume\n</code></pre> <p>From claude prompt, this makes the same</p> <pre><code>/resume\n</code></pre> <p>Choose a conversation to resume</p> <pre><code>claude -r conversationid\nclaude --resume  conversationid\n</code></pre>"},{"location":"vibe-sreing/claude-code/99-links/","title":"Links","text":""},{"location":"vibe-sreing/claude-code/99-links/#official","title":"Official","text":"<ul> <li>Claude code official doc</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/overview</p>"},{"location":"vibe-sreing/claude-code/99-links/#community","title":"Community","text":"<ul> <li>Claudelog</li> </ul> <p>https://claudelog.com/</p> <ul> <li>Awesome claude code</li> </ul> <p>https://github.com/hesreallyhim/awesome-claude-code</p> <ul> <li>Awesome claude</li> </ul> <p>https://github.com/alvinunreal/awesome-claude</p> <ul> <li>Claude Code Commands</li> </ul> <p>https://claudecodecommands.directory/</p> <ul> <li>Claudepro directory</li> </ul> <p>https://claudepro.directory/</p>"},{"location":"vibe-sreing/claude-code/shortcuts/","title":"Shortcuts","text":""},{"location":"vibe-sreing/claude-code/shortcuts/#keyboard-shortcuts","title":"Keyboard shortcuts","text":"Shortcuts Description ! Shortcut to run bash commands directly # Shortcut to add to memory @ Mention a file \\ + ENTER Multiline input (quick escape, all terminals) ALT+CTRL+K Mention a focused file Attach a file CTRL+B Move bash command to background CTRL+C Cancel current input or generation CTRL+D Exit Claude Code session CTRL+J Line feed character (multiline input) CTRL+L Clear terminal screen CTRL+R Reverse search command history ESC + ESC Rewind code/conversation SHIFT+ENTER Multiline input (after /terminal-setup) SHIFT+TAB Cycle through modes: Default \u2192 Auto-Accept (\u23f5\u23f5) \u2192 Plan Mode (\u23f8) \u2192 Default... TAB Toggle extended thinking Up/Down arrows Navigate command history"},{"location":"vibe-sreing/claude-code/shortcuts/#links","title":"Links","text":"<ul> <li>Interactive mode</li> </ul> <p>https://docs.claude.com/en/docs/claude-code/interactive-mode</p>"},{"location":"vibe-sreing/claude-code/vscode/","title":"Visual Studio Code","text":"<p>There is an official Claude Code VSCode extension available in the marketplace.</p>"},{"location":"vibe-sreing/claude-code/vscode/#tips","title":"Tips","text":""},{"location":"vibe-sreing/claude-code/vscode/#mcp-settings-file","title":"MCP settings file","text":"<ul> <li> <p>The VS Code Extension uses ~/.config/Code/User/mcp.json file to configure the mcp servers</p> </li> <li> <p>The Claude Code CLI users ~/.claude.json (the large history file)</p> </li> <li> <p>Enabling enableAllProjectMcpServers at claude code settings, automatically approves all MCP servers defined in project .mcp.json</p> </li> </ul>"},{"location":"vibe-sreing/claude-code/vscode/#auto-installation","title":"Auto installation","text":"<p>The /config command permits to enable|disable the autoinstallation of the claude code IDE extension, vscode in this case. When you run claude under a vscode terminal, will will install the extension if it is enabled</p>"},{"location":"vibe-sreing/claude-code/vscode/#check-the-integration-with-vscode","title":"Check the integration with vscode","text":"<p>With the extension installed, this command manages the integration into vscode a show the status</p> <pre><code>/ide\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#open-the-extension","title":"Open the extension","text":"<p>Executing claude under a vscode terminal does not opens the extension. In order to open the extension you can:</p> <ul> <li> <p>Click the Claude Code icon</p> </li> <li> <p>Use the Ctrl+Esc shortcut</p> </li> </ul> <p>There is a vscode setting that changes this behaviour and make this icon open the vscode terminal, not the extension tab</p> <pre><code>\"claude-code.useTerminal\": true\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#add-multiline-input","title":"Add multiline input","text":"<p>We can configure the vscode terminal to support multiline prompts, this is, when in the prompt, inserts a \"\\\" and jumps to a new line.</p> <p>This is done with the following builtin command:</p> <pre><code>/terminal-setup\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#add-opened-tab-to-context","title":"Add opened tab to context","text":"<p>We can add an opened tab (file) to a claude code prompt as context (using #), focusing the tab and pressing</p> <pre><code>ALT + CTRL + k\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#links","title":"Links","text":"<ul> <li>Visual Studio Code</li> </ul> <p>https://docs.claude.com/en/docs/claude-code/vs-code</p> <ul> <li>VS Code Extension (Beta)</li> </ul> <p>https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code</p>"},{"location":"vibe-sreing/claude-code/concepts/01-context/","title":"Context and memory","text":"<p>Giving a good context to claude code is one of the most important things to accomplish</p> <p>We can give context to claude using different ways.</p>"},{"location":"vibe-sreing/claude-code/concepts/01-context/#context-via-claudemd","title":"Context via CLAUDE.md","text":"<p>When you run Claude in a directory Claude uses that directory as the current working context. It can access files within that directory (and subdirectories) and it looks for any CLAUDE.md files to gain additional context, but these are optional.</p> <p>That files are pulled when starting a conversation and they can include any relevant information we can give to claude code:</p> <ul> <li>What is our project about, including environments, releases</li> <li>How we want organize the code, branches,...</li> <li>What utilities and commands we use</li> <li>What we want to test</li> <li>Code style guidelines</li> <li>Testing instructions</li> <li>and so on</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/01-context/#locations","title":"Locations","text":"<p>When we launch claude code in a directory, it search CLAUDE.md files in some locations:</p> <ul> <li>Project memory (./CLAUDE.md)</li> </ul> <p>In the directory where claude was launched</p> <ul> <li>User memory (~/.claude/CLAUDE.md)</li> </ul> <p>We can include here some personal preferences</p> <ul> <li>Parent (git)</li> </ul> <p>If the directory is part of a git repository, in parent directories up to the root of the repo</p> <ul> <li>Subdirectories</li> </ul> <p>If there are CLAUDE.md files in subdirectories, they are only included when Claude reads files in those subtrees, not at claude launch</p> <p>Using CLAUDE.local.md files is deprecated. Use imports instead</p>"},{"location":"vibe-sreing/claude-code/concepts/01-context/#tips","title":"Tips","text":"<p>The /init command reads the current and create a CLAUDE.md file The /memory command permits to edit the user and current directory CLAUDE.md files</p>"},{"location":"vibe-sreing/claude-code/concepts/01-context/#other-ways-to-add-context","title":"Other ways to add context","text":"<ul> <li> <p>The # key adds context to memory and it will incorporated to the CLAUDE.md file</p> </li> <li> <p>The current selection/tab in the IDE is automatically shared with Claude Code context</p> </li> <li> <p>Diagnostic sharing</p> </li> </ul> <p>Diagnostic errors (lint, syntax, etc.) from the IDE are automatically shared with Claude as you work</p>"},{"location":"vibe-sreing/claude-code/concepts/01-context/#links","title":"Links","text":"<ul> <li>Manage Claude's memory</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/memory</p> <ul> <li>Outdated (warning)</li> </ul> <p>https://www.anthropic.com/engineering/claude-code-best-practices</p>"},{"location":"vibe-sreing/claude-code/concepts/02-project/","title":"Project","text":""},{"location":"vibe-sreing/claude-code/concepts/02-project/#what-is-a-claude-code-project","title":"What is a Claude Code Project?","text":"<p>A Claude Code project is simply any directory where you've launched the <code>claude</code> command or are interacting with Claude Code. There's no special \"project file\" or configuration required to designate a directory as a Claude Code project.</p> <p>When you run Claude in a directory:</p> <ul> <li>Claude establishes that directory as the current working context</li> <li>It can access files within that directory and subdirectories</li> <li>It looks for CLAUDE.md files to gain additional context (optional)</li> <li>Claude becomes aware of your entire project structure</li> </ul> <p>The project concept in Claude Code is more about providing a working context than a formal technical structure with required configuration files.</p>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#getting-started","title":"Getting Started","text":"<p>To start using Claude Code in your project, simply navigate to your project directory and run the <code>claude</code> command. Claude Code will then be able to help you:</p> <ul> <li>Build features from descriptions</li> <li>Debug issues</li> <li>Navigate your codebase</li> <li>Automate various development tasks</li> <li>Find up-to-date information about your project</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#memory-and-context-management","title":"Memory and Context Management","text":"<p>Claude Code manages memory and context through a hierarchical structure with three levels:</p>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#memory-types","title":"Memory Types","text":"<ol> <li>Enterprise Policy (Organization-wide)</li> <li>Stored in system-level CLAUDE.md files</li> <li>Shared with all users in the organization</li> <li> <p>Highest precedence</p> </li> <li> <p>Project Memory (Team-shared)</p> </li> <li>Stored in <code>./CLAUDE.md</code> or <code>./.claude/CLAUDE.md</code></li> <li>Contains project-specific instructions: architecture, coding standards, common workflows</li> <li> <p>Shared with team members via source control</p> </li> <li> <p>User Memory (Personal)</p> </li> <li>Stored in <code>~/.claude/CLAUDE.md</code></li> <li>Contains personal preferences</li> <li>Applies across all projects for an individual</li> </ol>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#memory-loading-behavior","title":"Memory Loading Behavior","text":"<ul> <li>Memory files are automatically loaded when Claude Code launches</li> <li>Higher-level memories take precedence over lower-level ones</li> <li>The system recursively discovers memories from the current working directory upwards</li> <li>You can import additional memory files using <code>@path/to/import</code> syntax</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#project-configuration","title":"Project Configuration","text":""},{"location":"vibe-sreing/claude-code/concepts/02-project/#project-specific-settings","title":"Project-Specific Settings","text":"<p>Projects can have their own Claude Code settings and configurations stored in <code>.claude/</code> directories:</p> <ul> <li>Project-specific subagents: <code>.claude/agents/</code></li> <li>Project memory files: <code>.claude/CLAUDE.md</code></li> <li>Custom slash commands</li> <li>Hooks and integrations</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#directory-structure-notes","title":"Directory Structure Notes","text":"<ul> <li>The folders under <code>~/.claude/projects/</code> can be safely deleted if you no longer need those projects</li> <li>This directory is a custom organization structure, not required by Claude Code</li> <li>You can organize your projects however works best for your workflow</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/02-project/#team-collaboration","title":"Team Collaboration","text":"<p>For Team and Enterprise plans, Claude Code projects can be shared among team members with premium seats, enabling:</p> <ul> <li>Collaborative development workflows</li> <li>Shared project memory and context</li> <li>Team-wide coding standards and practices</li> <li>Consistent development patterns across the team</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/03-hooks/","title":"Hooks","text":"<p>Hooks are automated scripts that intercept and modify various stages of Claude's interaction with tools and the development environment. They're configured in settings files like <code>~/.claude/settings.json</code>.</p> <p>Hook Events:</p> <ul> <li>PreToolUse: Before a tool is used</li> <li>PostToolUse: After a tool completes</li> <li>UserPromptSubmit: When a user submits a prompt</li> <li>SessionStart/End: At session initialization or conclusion</li> </ul> <p>Capabilities:</p> <ul> <li>Validate tool usage and control permissions</li> <li>Add context to interactions automatically</li> <li>Log operations for auditing</li> <li>Perform security checks</li> </ul> <p>Important Notes:</p> <ul> <li>Hooks execute shell commands automatically (60-second default timeout)</li> <li>Users are solely responsible for configured commands</li> <li>Hooks run in parallel</li> <li>Environment variables like <code>CLAUDE_PROJECT_DIR</code> are available</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/","title":"Commands, Agents and Skills","text":"<p>Claude Code provides three mechanisms to extend its behavior: commands, agents, and skills. They serve different purposes and have different invocation models.</p>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#quick-comparison","title":"Quick comparison","text":"Aspect Commands Agents Skills Location <code>.claude/commands/</code> <code>.claude/agents/</code> <code>.claude/skills/</code> File format <code>&lt;namespace&gt;/&lt;name&gt;.md</code> <code>&lt;name&gt;.md</code> <code>&lt;name&gt;/SKILL.md</code> Invocation <code>/namespace:name</code> (user) Automatic via Task tool <code>/namespace:name</code> or automatic Execution Inline prompt injection Forked subagent Inline or forked context Metadata None Frontmatter Frontmatter Auto-trigger No Yes (based on description) Conditional Model override No Yes Yes Status Deprecated Active Active"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#commands-legacy","title":"Commands (legacy)","text":"<p>Commands are the original extension mechanism. Simple markdown files that inject a prompt when invoked via <code>/namespace:name</code>. They live under <code>.claude/commands/&lt;namespace&gt;/&lt;name&gt;.md</code> and can be defined per user (<code>~/.claude/commands/</code>) or per project (<code>.claude/commands/</code>).</p> <ul> <li>No frontmatter or metadata support</li> <li>Always run inline in the current conversation context</li> <li>Cannot be auto-triggered</li> <li>Superseded by skills</li> </ul> <p>Claude Code includes some built-in commands</p>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#agents","title":"Agents","text":"<p>Agents (subagents) are specialized AI assistants that run as background workers via the <code>Task</code> tool. They are spawned automatically by the orchestrator based on pattern matching against their description. Manage them with the <code>/agents</code> command.</p> <ul> <li><code>name</code>: Identifier used when spawning via Task tool</li> <li><code>description</code>: Determines when the orchestrator auto-spawns the agent</li> <li><code>model</code>: Override which model runs the agent (haiku, sonnet, opus)</li> </ul> <p>The task tool runs a sub-agent to handle complex, multi-step tasks and delegate work</p>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#agent-key-characteristics","title":"Agent key characteristics","text":"<ul> <li>Always run in a forked context (separate from main conversation)</li> <li>Triggered automatically based on description matching</li> <li>Cannot be invoked directly via slash command</li> <li>Useful for proactive, always-on behaviors (e.g., lint every markdown file)</li> <li>Each agent gets its own fresh context, keeping the main conversation clean</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#skills","title":"Skills","text":"<p>Skills are the current extension format, replacing commands with richer capabilities. They combine slash command invocation with optional auto-triggering and metadata. Each skill requires a <code>SKILL.md</code> file with a name (64 char max) and description (1024 char max).</p>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#skill-architecture-three-content-levels","title":"Skill architecture - three content levels","text":"<ol> <li>Metadata (always loaded, ~100 tokens): Lightweight description of purpose and trigger conditions</li> <li>Instructions (loaded when triggered, &lt;5,000 tokens): Procedural knowledge and step-by-step workflows</li> <li>Resources and code (loaded as needed): Optional scripts and reference materials executed via bash</li> </ol>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#skill-frontmatter-options","title":"Skill frontmatter options","text":"<ul> <li><code>name</code>: Slash command name (format <code>namespace:command</code>)</li> <li><code>description</code>: Purpose and auto-trigger conditions</li> <li><code>disable-model-invocation</code>: When <code>true</code>, only manual <code>/name</code> invocation works</li> <li><code>context</code>: <code>fork</code> runs in separate context, omit for inline</li> <li><code>model</code>: Override model for this skill</li> <li><code>argument-hint</code>: Shows usage hint in command palette</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#skill-key-characteristics","title":"Skill key characteristics","text":"<ul> <li>Invokable via <code>/namespace:name</code> like legacy commands</li> <li>Support auto-triggering (unless <code>disable-model-invocation: true</code>)</li> <li>Can run inline or forked depending on <code>context</code> setting</li> <li>Support arguments and model overrides</li> <li>Can include resource files alongside <code>SKILL.md</code></li> <li>Available in Claude Code, Claude API, and Claude.ai (with limitations)</li> <li>No network access by default, sandboxed execution</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#when-to-use-what","title":"When to use what","text":"Use case Mechanism Proactive background behavior (always-on) Agent User-invoked workflow with <code>/command</code> Skill Auto-triggered + manually invokable Skill with <code>disable-model-invocation: false</code> Strictly manual invocation only Skill with <code>disable-model-invocation: true</code> Legacy compatibility Command (migrate to skill)"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#choosing-between-agent-and-skill","title":"Choosing between agent and skill","text":"<p>The decision comes down to how the functionality gets triggered:</p> <ul> <li>Agent: When you want something that runs automatically without you asking. The orchestrator reads the description and spawns it whenever the situation matches.</li> <li>Skill: When you want something you invoke explicitly with <code>/name</code>, or when you need arguments, context mode control, or command palette visibility.</li> </ul> Agent Skill Runs in Always forked (isolated context) Inline or forked (<code>context: fork</code>) Accepts arguments No Yes (<code>argument-hint</code>) User can invoke directly No Yes (<code>/name</code>) Appears in command palette No Yes <p>Rule of thumb:</p> <ul> <li>If you would forget to invoke it manually, make it an agent</li> <li>If you want to control when it runs, make it a skill with <code>disable-model-invocation: true</code></li> <li>If you need both, create both pointing to the same logic</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#coexistence","title":"Coexistence","text":"<p>Agents and skills can coexist for the same functionality. A common pattern is:</p> <ul> <li>Agent handles automatic triggering (orchestrator spawns it via Task tool)</li> <li>Skill provides the <code>/command</code> entry point for manual invocation</li> </ul> <p>Both can share the same prompt content and model configuration.</p>"},{"location":"vibe-sreing/claude-code/concepts/04-commands-agents-skills/#links","title":"Links","text":"<ul> <li>Slash commands</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/slash-commands</p> <ul> <li>Agent skills overview</li> </ul> <p>https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview</p> <ul> <li>Equipping agents for the real world with Agent Skills</li> </ul> <p>https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills</p>"},{"location":"vibe-sreing/claude-code/concepts/05-mcp-servers/","title":"MCP servers","text":"<p>Model Context Protocol (MCP) is a standardized protocol that allows Claude Code to connect with external tools, services, and data sources.</p> <p>Purpose:</p> <ul> <li>Connect Claude to hundreds of tools and data sources</li> <li>Enable complex interactions across different platforms</li> <li>Extend Claude Code's capabilities beyond built-in tools</li> </ul> <p>Server Types:</p> <ol> <li>Remote HTTP servers</li> <li>Remote SSE servers</li> <li>Local stdio servers</li> </ol> <p>Installation Scopes:</p> <ul> <li>Local: Project-specific, private configuration</li> <li>Project: Shared team configuration</li> <li>User: Available across multiple projects</li> </ul> <p>Key Features:</p> <ul> <li>OAuth 2.0 authentication support</li> <li>Environment variable expansion</li> <li>Resource referencing via @ mentions</li> <li>Slash commands for quick actions</li> </ul> <p>Example Use Cases:</p> <ul> <li>Fetch Sentry error logs</li> <li>Review GitHub pull requests</li> <li>Query PostgreSQL databases</li> <li>Create Jira tickets</li> <li>Integrate with monitoring tools</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/","title":"Tools","text":"<p>Tools are what make Claude Code agentic. Without tools, Claude can only respond with text. With tools, Claude can act: read code, edit files, run commands, search the web, and interact with external services.</p> <p>Each tool use returns information that feeds back into the agentic loop, informing Claude's next decision.</p>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#the-agentic-loop","title":"The agentic loop","text":"<p>When you give Claude a task, it works through three phases: gather context, take action, and verify results. Claude uses tools throughout all phases, chaining dozens of actions together and course-correcting along the way.</p> <p>You can interrupt at any point to steer Claude in a different direction.</p>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#built-in-tools","title":"Built-in tools","text":""},{"location":"vibe-sreing/claude-code/concepts/06-tools/#file-operations","title":"File operations","text":"Tool Description Permission Read Read file contents No Edit Targeted edits to specific files Yes Write Create or overwrite files Yes NotebookEdit Edit Jupyter notebook cells Yes"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#search","title":"Search","text":"Tool Description Permission Glob Find files by pattern matching (e.g., <code>**/*.yaml</code>) No Grep Search file contents with regex No"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#execution","title":"Execution","text":"Tool Description Permission Bash Execute shell commands in your environment Yes"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#web","title":"Web","text":"Tool Description Permission WebSearch Search the web for information No WebFetch Fetch and process content from a URL No"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#orchestration","title":"Orchestration","text":"Tool Description Permission Task Spawn subagents for delegated work No TaskOutput Retrieve output from background tasks No AskUserQuestion Ask the user for clarification No ExitPlanMode Exit plan mode and start implementing Yes"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#extending-tools","title":"Extending tools","text":"<p>The built-in tools are the foundation. Claude Code can be extended with additional tools through:</p> <ul> <li>MCP servers: Connect external services (databases, APIs, custom tools) via the Model Context Protocol. See MCP servers</li> <li>Skills: Domain-specific workflows that orchestrate tool usage. See Agent skills</li> <li>Subagents: Delegated workers with their own tool access and context. See Subagents</li> <li>Hooks: Shell commands that run automatically on tool events. See Hooks</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#permissions","title":"Permissions","text":"<p>Tools that modify state (Edit, Write, Bash) require permission. Claude asks before using them unless you pre-approve in <code>settings.json</code>.</p>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#permission-modes","title":"Permission modes","text":"<p>Cycle through modes with <code>Shift+Tab</code>:</p> <ul> <li>Default: Claude asks before file edits and shell commands</li> <li>Auto-accept edits: Claude edits files without asking, still asks for commands</li> <li>Plan mode: Read-only tools only, creates a plan for approval</li> </ul>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#permission-rules-in-settings","title":"Permission rules in settings","text":"<pre><code>{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(kubectl get:*)\",\n      \"Bash(npm run *)\",\n      \"Read\"\n    ],\n    \"deny\": [\n      \"Bash(kubectl delete:*)\",\n      \"Read(./.env)\"\n    ]\n  }\n}\n</code></pre> <p>Rules support glob patterns for command arguments. Deny rules take precedence over allow rules.</p>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#how-claude-chooses-tools","title":"How Claude chooses tools","text":"<p>Claude selects tools based on the prompt and what it learns at each step. For example, when asked to \"fix the failing tests\", Claude might:</p> <ol> <li>Bash - Run the test suite to see failures</li> <li>Read - Read the error output and relevant source files</li> <li>Edit - Fix the code</li> <li>Bash - Run tests again to verify</li> </ol> <p>Each tool use returns information that informs the next step. This is the agentic loop in action.</p>"},{"location":"vibe-sreing/claude-code/concepts/06-tools/#links","title":"Links","text":"<ul> <li>How Claude Code works</li> </ul> <p>https://code.claude.com/docs/en/how-claude-code-works</p> <ul> <li>Settings and permissions</li> </ul> <p>https://code.claude.com/docs/en/settings</p>"},{"location":"vibe-sreing/mcp/99-links/","title":"Links","text":"<ul> <li>Model Context Protocol</li> </ul> <p>https://modelcontextprotocol.io/</p> <ul> <li>Pulse MCP</li> </ul> <p>https://www.pulsemcp.com/</p> <ul> <li>Visual studio code MCP servers</li> </ul> <p>Model Context Protocol servers enabling AI assistants to securely access external tools and data.</p> <p>https://code.visualstudio.com/mcp</p> <ul> <li>MCP List from modelcontextprotocol.io</li> </ul> <p>https://github.com/modelcontextprotocol/servers</p> <ul> <li>mcpservers.org</li> </ul> <p>https://mcpservers.org/</p>"},{"location":"vibe-sreing/mcp/aws/","title":"AWS Official MCP servers","text":""},{"location":"vibe-sreing/mcp/aws/#overview","title":"Overview","text":"<p>AWS MCP (Model Context Protocol) Servers are specialized servers developed by AWS Labs that enhance AI applications' interactions with AWS services. These servers provide AI assistants like Claude with contextual access to AWS documentation, guidance, and best practices, enabling more accurate and efficient cloud-native development through a standardized client-server architecture.</p> <p>https://awslabs.github.io/mcp/</p>"},{"location":"vibe-sreing/mcp/aws/#key-benefits","title":"Key Benefits","text":"<ul> <li>Improved Output Quality: Provides relevant, up-to-date AWS information to AI assistants</li> <li>Latest Documentation: Access to current AWS documentation and service capabilities</li> <li>Workflow Automation: Enables automation of complex AWS-related workflows</li> <li>Domain Expertise: Delivers specialized knowledge about AWS services and best practices</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#server-categories","title":"Server Categories","text":"<p>AWS Labs provides MCP servers organized into the following categories:</p> <ol> <li>Documentation: Access to AWS documentation and knowledge bases</li> <li>Infrastructure &amp; Deployment: Tools for infrastructure management and deployment</li> <li>AI &amp; Machine Learning: Integration with AWS AI/ML services</li> <li>Data &amp; Analytics: Data processing and analytics capabilities</li> <li>Developer Tools &amp; Support: Development and debugging support</li> <li>Integration &amp; Messaging: Event-driven and messaging services</li> <li>Cost &amp; Operations: Cost management and operational tools</li> <li>Healthcare &amp; Lifesciences: Industry-specific services</li> </ol>"},{"location":"vibe-sreing/mcp/aws/#notable-mcp-servers","title":"Notable MCP Servers","text":"<ul> <li>AWS API MCP Server: Direct AWS API interaction</li> <li>AWS Documentation MCP Server: AWS documentation access</li> <li>AWS Knowledge MCP Server: AWS best practices and guidance</li> <li>Bedrock Knowledge Bases Retrieval MCP Server: Knowledge base integration</li> <li>Service-Specific Servers: DynamoDB, EKS, Lambda, S3, and more</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#deployment-options","title":"Deployment Options","text":"<ul> <li>Local Servers: Best for development, testing, and offline work</li> <li>Remote Servers: Ideal for team collaboration, scalability, and automatic updates</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#use-cases","title":"Use Cases","text":"<ul> <li>Cloud-native application development with AI assistance</li> <li>Infrastructure as Code (IaC) with intelligent AWS service recommendations</li> <li>Automated AWS resource management and optimization</li> <li>Context-aware troubleshooting and debugging</li> <li>Best practices guidance for AWS architectures</li> </ul> <p>These servers enable AI assistants to provide intelligent, context-aware support for AWS-related tasks, improving development velocity and code quality.</p>"},{"location":"vibe-sreing/mcp/aws/#add-at-user-scope-in-claude-code","title":"Add at user scope in claude code","text":"<pre><code>claude mcp add --transport stdio --scope user awsdoc -e AWS_DOCUMENTATION_PARTITION=aws -- uvx awslabs.aws-documentation-mcp-server@latest\n</code></pre> <pre><code>claude mcp add --transport stdio --scope user awseks -- uvx awslabs.eks-mcp-server@latest --allow-sensitive-data-access\n</code></pre>"},{"location":"vibe-sreing/mcp/context7/","title":"Context7","text":""},{"location":"vibe-sreing/mcp/context7/#overview","title":"Overview","text":"<p>Context7 is an MCP (Model Context Protocol) server developed by Upstash that provides up-to-date, version-specific code documentation directly into AI coding assistants' context. It solves the common problem of AI models using outdated or generic library information by fetching real-time, accurate code examples and documentation.</p>"},{"location":"vibe-sreing/mcp/context7/#key-features","title":"Key Features","text":"<ul> <li>Version-Specific Documentation: Provides current documentation for specific library versions</li> <li>Real-Time Context: Fetches up-to-date code examples and API references</li> <li>Eliminates Hallucinations: Prevents AI-generated outdated or incorrect APIs</li> <li>Multi-Platform Support: Works with Cursor, VS Code, Claude Code, and other AI coding platforms</li> <li>Flexible Deployment: Supports both remote and local server connections</li> <li>Rate Limiting: Optional API key for higher rate limits and private repository access</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#how-it-works","title":"How It Works","text":"<ol> <li>User adds \"use context7\" to their prompt in supported AI coding platforms</li> <li>The system fetches current, version-specific documentation for the requested library or framework</li> <li>AI assistant receives accurate, up-to-date information for code generation</li> </ol>"},{"location":"vibe-sreing/mcp/context7/#benefits","title":"Benefits","text":"<ul> <li>More accurate AI-assisted coding with current documentation</li> <li>Reduces debugging time from outdated API usage</li> <li>Access to latest library features and best practices</li> <li>Eliminates need to manually search for documentation</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#requirements","title":"Requirements","text":"<ul> <li>Node.js v18.0.0 or higher</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#claude-code","title":"Claude code","text":"<p>Claude Code Remote Server Connection</p> <pre><code>claude mcp add --transport http context7 &lt;https://mcp.context7.com/mcp&gt; --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n</code></pre> <p>Claude Code Local Server Connection</p> <pre><code>claude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n</code></pre>"},{"location":"vibe-sreing/mcp/context7/#vscode","title":"VSCODE","text":"<p>VS Code Remote Server Connection</p> <pre><code>\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n</code></pre> <p>VS Code Local Server Connection</p> <pre><code>\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n</code></pre>"},{"location":"vibe-sreing/mcp/context7/#links","title":"Links","text":"<ul> <li>Context7 website</li> </ul> <p>https://context7.com/</p> <ul> <li>Context7 github</li> </ul> <p>https://github.com/upstash/context7</p>"},{"location":"vibe-sreing/mcp/taskmaster/","title":"Taskmaster","text":""},{"location":"vibe-sreing/mcp/taskmaster/#overview","title":"Overview","text":"<p>Taskmaster is an open-source MCP (Model Context Protocol) server that functions as a project manager for AI assistants like Claude. It helps AI agents manage complex projects by breaking them down into manageable, trackable tasks.</p>"},{"location":"vibe-sreing/mcp/taskmaster/#key-features","title":"Key Features","text":"<ul> <li>Task Decomposition: Automatically breaks down complex projects into smaller, actionable tasks</li> <li>One-shot Completion: Helps AI agents complete tasks efficiently in single iterations</li> <li>Context Management: Prevents context overload by maintaining focus on specific tasks</li> <li>Code Protection: Avoids disrupting existing code through structured task execution</li> <li>Progress Tracking: Keeps AI agents on track throughout multi-step projects</li> </ul>"},{"location":"vibe-sreing/mcp/taskmaster/#use-cases","title":"Use Cases","text":"<ul> <li>Managing ambitious, multi-component projects</li> <li>Coordinating AI-driven development workflows</li> <li>Preventing scope creep during implementation</li> <li>Maintaining project structure across long conversations</li> </ul>"},{"location":"vibe-sreing/mcp/taskmaster/#availability","title":"Availability","text":"<ul> <li>Completely free and open source</li> <li>Requires user to provide their own API keys</li> <li>Available at: https://www.task-master.dev/</li> </ul> <p>Taskmaster MCP enables more systematic and organized collaboration between users and AI assistants, particularly valuable for complex software engineering projects that require careful planning and execution.</p>"}]}