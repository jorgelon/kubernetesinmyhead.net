{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kubernetesinmyhead.net","text":"<p>These are my notes about some kubernetes related technologies</p>"},{"location":"#conventions","title":"Conventions","text":"<p>98-tips.md 99-links.md</p>"},{"location":"CI-CD/argo-events/filtering/","title":"Filters","text":"<ul> <li>Filtering in eventsources</li> </ul> <p>https://argoproj.github.io/argo-events/eventsources/filtering/</p> <ul> <li>Filtering in sensors</li> </ul> <p>https://argoproj.github.io/argo-events/sensors/filters/intro/</p>"},{"location":"CI-CD/argo-events/parameters-to-workflow/","title":"Pass parameters to workflows","text":"<p>We can trigger an argo workflow with argo events and pass parameters</p> <pre><code>    - template:\n        name: argo-workflow-trigger\n        conditions: mycondition\n        policy:\n          k8s:\n            labels:\n              workflows.argoproj.io/phase: Succeeded\n        k8s:  # I have found some problems using \"argoWorkflow:\" here\n          operation: create\n          ...\n          parameters:\n            - src:\n                dependencyName: mydependency\n                dataKey: body.fistparameter\n              dest: spec.arguments.parameters.0.value ## this will be the first parameter in the workflow\n            - src:\n                dependencyName: mydependency\n                dataKey: body.another\n              dest: spec.arguments.parameters.1.value ## this will be the second parameter in the workflow\n            - src:\n                dependencyName: mydependency\n                dataKey: body.mytitle\n              dest: metadata.annotations.workflows\\.argoproj.io\\/title # we can escape characters\n            - src:\n                dependencyName: mydependency\n                dataKey: body.mydescription\n              dest: metadata.annotations.workflows\\.argoproj\\.io\\/description # we can escape characters\n\n</code></pre>"},{"location":"CI-CD/argo-events/service-accounts/","title":"Service accounts","text":""},{"location":"CI-CD/argo-events/service-accounts/#for-eventsources","title":"For eventsources","text":"<p>The service account of an eventsource can be specified in the spec.template.serviceAccountName field of the eventsource but it does not requires special permissions. In addition to that it can be interesting to create a new one and not use the default one.</p> <p>The exception is the \"resource\" eventsource. In that case you must give that service account the list and watch permissions to that resource being watched.</p>"},{"location":"CI-CD/argo-events/service-accounts/#for-sensors","title":"For sensors","text":"<p>The service account of a sensor can be specified in the spec.template.serviceAccountName field of the eventsource but it does not requires special permissions. In addition to that it can be interesting to create a new one and not use the default one.</p> <p>The exceptions are the k8s trigger adn the argoWorkflow trigger.</p>"},{"location":"CI-CD/argo-events/service-accounts/#argo-workflow-trigger","title":"Argo workflow trigger","text":"<p>To submit a workflow, the service account of the sensor needs create and list rbac permissions. To resubmit, retry, resume or suspend a workflow, the service account of the sensor needs \"update\" and \"get\" rbac permissions.</p> <p>Here it is a non tuned rbac permssions https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/sensor-rbac.yaml</p> <p>!!! Note \"This rbac permissions are not related with what the workflow will do. Only for create, retry,... For that see service accounts in argo workflows</p>"},{"location":"CI-CD/argo-events/service-accounts/#k8s-resource-trigger","title":"K8s resource trigger","text":"<p>To create a kubernetes resource, the service account of the sensor needs \"create\" rbac permissions for that resource.</p>"},{"location":"CI-CD/argo-events/service-accounts/#links","title":"Links","text":"<ul> <li>Service Accounts in argo events https://argoproj.github.io/argo-events/service-accounts/</li> </ul>"},{"location":"CI-CD/argo-workflows/labels-annotations/","title":"Labels and annotations","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#labels","title":"Labels","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#workflow-creator","title":"Workflow creator","text":"<p>https://argo-workflows.readthedocs.io/en/stable/workflow-creator/</p>"},{"location":"CI-CD/argo-workflows/labels-annotations/#configmap","title":"Configmap","text":"<ul> <li>Executor plugin</li> </ul> <pre><code>workflows.argoproj.io/configmap-type: ExecutorPlugin\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/stable/executor_plugins/</p> <ul> <li>Memoization and cache</li> </ul> <pre><code>workflows.argoproj.io/configmap-type: Cache\n</code></pre> <ul> <li>Parameter</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/memoization/</p> <pre><code>workflows.argoproj.io/configmap-type: Parameter\n</code></pre>"},{"location":"CI-CD/argo-workflows/labels-annotations/#annotations","title":"Annotations","text":""},{"location":"CI-CD/argo-workflows/labels-annotations/#title-and-description","title":"Title and description","text":"<pre><code>workflows.argoproj.io/title: \"My title\"\n</code></pre> <p>defaults to metadata.name if not specified</p> <pre><code>workflows.argoproj.io/description: \"SuperDuperProject\"\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/stable/title-and-description/</p>"},{"location":"CI-CD/argo-workflows/retries/","title":"retryStrategy","text":"<p>retryStrategy permits to control retries and it ca be defined at 2 levels</p> <ul> <li>at workflow level (spec.retryStrategy) affects all templates in the workflow</li> <li>in every template describes how to retry a template when it fails</li> </ul> <p>In the retryStrategy we have some options</p>"},{"location":"CI-CD/argo-workflows/retries/#retrypolicy-and-expression","title":"retryPolicy and expression","text":"<p>Both are re-evaluated after each attempt. For example, if you set retryPolicy: OnFailure and your first attempt produces a failure then a retry will be attempted. If the second attempt produces an error, then another attempt will not be made. The expression result will be logical and with the retryPolicy. Both must be true to retry.</p>"},{"location":"CI-CD/argo-workflows/retries/#expression","title":"expression","text":"<p>Expression is a condition expression for when a node will be retried. If it evaluates to false, the node will not be retried and the retry strategy will be ignored. If expression evaluates to false, the step will not be retried.</p> <p>This variables are available:</p> <ul> <li>lastRetry.exitCode: The exit code of the last retry, or \"-1\" if not available</li> <li>lastRetry.status: The phase of the last retry: Error, Failed</li> <li>lastRetry.duration: The duration of the last retry, in seconds</li> <li>lastRetry.message: The message output from the last retry (available from version 3.5)</li> </ul>"},{"location":"CI-CD/argo-workflows/retries/#retrypolicy","title":"retryPolicy","text":"<p>RetryPolicy is a policy of NodePhase statuses that will be retried. Here we choose what failures type to retry.</p> <ul> <li>Always</li> </ul> <p>Retry all failed steps</p> <ul> <li>OnFailure</li> </ul> <p>Retry steps whose main container is marked as failed in Kubernetes</p> <ul> <li>OnError</li> </ul> <p>Retry steps that encounter Argo controller errors, or whose init or wait containers fail</p> <ul> <li>OnTransientError</li> </ul> <p>Retry steps that encounter errors defined as transient, or errors matching the TRANSIENT_ERROR_PATTERN environment variable. Available in version 3.0 and later.</p> <p>The retryPolicy applies even if you also specify an expression, but in version 3.5 or later the default policy means the expression makes the decision unless you explicitly specify a policy.</p> <p>About the default retryPolicy</p> <p>The default retryPolicy is OnFailure, except in version 3.5 or later when an expression is also supplied, when it is Always</p> <p></p>"},{"location":"CI-CD/argo-workflows/retries/#limit","title":"limit","text":"<p>Limit is the maximum number of retry attempts when retrying a container. It does not include the original container; the maximum number of total attempts will be <code>limit + 1</code>.</p>"},{"location":"CI-CD/argo-workflows/retries/#affinity","title":"affinity","text":"<p>Affinity prevents running workflow's step on the same host</p> <p>nodeAntiAffinity</p>"},{"location":"CI-CD/argo-workflows/retries/#backoff","title":"backoff","text":"<p>Backoff is a backoff strategy. You can configure the delay between retries with backoff. See example for usage.</p> <ul> <li> <p>duration</p> </li> <li> <p>maxDuration</p> </li> <li> <p>cap</p> </li> <li> <p>factor (int)</p> </li> </ul> <p>duration, maxDuration and cap are strings taken as unit members (by default seconds). Example: \"4\". Could also be a Duration, e.g.: \"2m\", \"6h\"</p>"},{"location":"CI-CD/argo-workflows/retries/#links","title":"Links","text":"<ul> <li>Retries</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/retries/</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/","title":"Service accounts and rbac","text":""},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#argo-workflows","title":"Argo workflows","text":"<p>All the pods in a workflow use a service account. This service account can be specified with spec.serviceAccountName in the workflow. If not, uses the \"default\" service account (not recommended).</p> <p>If using the argo cli, we can achieve that with the --serviceaccount NAME parameter</p> <pre><code>argo submit --serviceaccount myserviceaccount myworkflow\n</code></pre> <p>The minimum permissions to run workflows in newest releases (since 3.4) is:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: executor\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtaskresults\n    verbs:\n      - create\n      - patch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: executor\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: executor\nsubjects:\n  - kind: ServiceAccount\n    name: executor\n    namespace: argo\n</code></pre> <p>Depending of what the workflow has to do, it will neccesary to give that service account additional permissions via kubernetes rbac. For example, if the workflow creates pods, it will need the verb \"create\" in pods resource.</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#using-the-http-template","title":"Using the http template","text":"<p>See the http template doc</p>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#workflow-links","title":"Workflow links","text":"<ul> <li> <p>Argo workflows service accounts https://argo-workflows.readthedocs.io/en/stable/service-accounts/</p> </li> <li> <p>Argo workflows rbac https://argo-workflows.readthedocs.io/en/stable/workflow-rbac/</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/service-accounts-rbac/#in-an-argo-events-sensor","title":"In an Argo events sensor","text":"<p>If you want to create a workflow using an argo events sensor, you can pass the name of the service account (and other arguments) to the argo cli with the field \"args\" in addition to \"source\", \"parameters\" and \"operation\".</p> <ul> <li>Argo workflows trigger spec in Argo events https://github.com/argoproj/argo-events/blob/master/api/sensor.md#argoproj.io/v1alpha1.ArgoWorkflowTrigger</li> </ul>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/","title":"Timeouts and self cleaning","text":""},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-timeout-activedeadlineseconds","title":"Workflow Timeout (activeDeadlineSeconds)","text":"<p>The maximum time allowed (timeout) for a workflow is configured with the spec.activeDeadlineSeconds fields. After this number of seconds, the workflow is terminated.</p> <p>Changing this value to zero in a running workflow will terminate it</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  activeDeadlineSeconds: 10\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-auto-deletion-ttlstrategy","title":"Workflow auto deletion (TTLStrategy)","text":"<p>We can control the self deletion of a finished workflow with some spec.TTLStrategy fields:</p> <ul> <li>secondsAfterCompletion</li> </ul> <p>Number of seconds the workflow we will be maintained after completion</p> <ul> <li>secondsAfterFailure</li> </ul> <p>Number of seconds the workflow we will be maintained after failure</p> <ul> <li>secondsAfterSuccess</li> </ul> <p>Number of seconds the workflow we will be maintained after Succeeded</p> <p>If we configure all 3, secondsAfterFailure and secondsAfterSucces have precedence</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#pod-auto-deletion-podgc","title":"Pod auto deletion (PodGC)","text":"<p>With PodGC we can configure when to delete the completed pods.</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#strategy","title":"Strategy","text":"<p>We must choose an strategy:</p> <ul> <li>\"OnPodCompletion\" deletes the pods when the pods ends (including failures)</li> <li>\"OnPodSuccess\"  deletes the pods when the pods ends successfully</li> <li>\"OnWorkflowCompletion\" deletes the pods when the workflow ends</li> <li>\"OnWorkflowSuccess\" deletes the pods when the workflow ends successfully</li> <li>No settings means no deletion will occur.</li> </ul>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#deletedelayduration","title":"deleteDelayDuration","text":"<p>spec.PodGC.deleteDelayDuration is an string field where we can specify the time to wait until the pods in the GC queue will be deleted.</p> <p>The default value is 5s. A zero (value) will delete the pods immediately</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#labelselector","title":"labelSelector","text":"<p>With labelSelector we can se filter using labels what pods will be deleted</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: pod-gc-strategy-\nspec:\n  entrypoint: pod-gc-strategy\n  podGC:\n    strategy: OnPodSuccess\n    deleteDelayDuration: 30s\n    labelSelector:\n      matchLabels:\n        should-be-deleted: \"true\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#workflow-defaults-at-controller-level","title":"Workflow defaults at controller level","text":"<p>We can configure al these the default values at controller level in the workflow-controller-configmap configMap</p> <pre><code>  workflowDefaults: |\n    spec:\n      activeDeadlineSeconds: 1200\n      ttlStrategy:\n        secondsAfterCompletion: 86400\n        secondsAfterFailure: 86400\n        secondsAfterSuccess: 28800\n      podGC:\n        strategy: OnPodCompletion\n        deleteDelayDuration: 86400s\n</code></pre>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#cronworkflow-history-limits","title":"CronWorkflow history limits","text":"<p>We can control the number of successful jobs to mantain with spec.successfulJobsHistoryLimit</p> <p>The default value is 3</p> <p>And also the failed ones with spec.failedJobsHistoryLimit</p> <p>The default value is 1</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#template-defaults","title":"Template defaults","text":"<p>pending</p>"},{"location":"CI-CD/argo-workflows/timeouts-cleaning/#links","title":"Links","text":"<ul> <li> <p>Cost optimization https://argo-workflows.readthedocs.io/en/stable/cost-optimisation/#limit-the-total-number-of-workflows-and-pods</p> </li> <li> <p>Timeouts https://argo-workflows.readthedocs.io/en/stable/walk-through/timeouts/</p> </li> <li> <p>Default workflow spec https://argo-workflows.readthedocs.io/en/stable/default-workflow-specs/</p> </li> <li> <p>Template defaults https://argo-workflows.readthedocs.io/en/stable/template-defaults/</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/variables-intro/","title":"Variables: Intro","text":"<p>In Argo Workflows there 2 kinds of template tag, or ways to call a variable,</p>"},{"location":"CI-CD/argo-workflows/variables-intro/#simple","title":"Simple","text":"<p>The simple way is using this format</p> <pre><code>{{variable}}\n</code></pre> <p>There is simple substitution between the variable and the value</p> <p>The recommended way is not to leave spaces between the brackets</p>"},{"location":"CI-CD/argo-workflows/variables-intro/#expression","title":"Expression","text":"<p>But we can call the variable using an expression, with this format</p> <pre><code>{{=variable}}\n</code></pre> <p>In this case the value of the variable is the result of evaluating the tag as an expression.</p> <p>There are some different things we can do using the expr language. In this example we extract data from a json</p> <pre><code>jsonpath(inputs.parameters.json, '$.some.path')\n</code></pre> <p>If we hyphens in the tag we can have unexpected error. This can be related with parameters or steps. To solve it we can rename the parameter or step, or reference them by indexing into the parameter or step map.</p> <pre><code>inputs.parameters['my-param'] or steps['my-step'].outputs.result\n</code></pre> <p>In this example we can parse a json a using the key \"password\" as a parameter</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: test\nspec:\n  entrypoint: main\n  templates:\n    - name: http-post\n      http:\n        url: \"https://{{workflow.parameters.harborUrl}}/api/v2.0/robots\"\n        method: POST\n    - name: echo\n      inputs:\n        parameters:\n          - name: password\n      container:\n        image: docker.io/alpine\n        command: [\"echo\"]\n        args: \"{{inputs.parameters.username}}\"\n    - name: main\n      steps:\n        - - name: makeapicall\n            template: http-post\n        - - name: deploy-credentials\n            template: write-secret\n            arguments:\n              parameters:\n                - name: password\n                  value: \"{{=jsonpath(steps.makeapicall.outputs.result, '$.password')}}\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/variables-intro/#links","title":"Links","text":"<ul> <li>Workflow Variables</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/variables/</p> <ul> <li>Expr Lang</li> </ul> <p>https://expr-lang.org/docs/language-definition</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/","title":"Define parameters","text":""},{"location":"CI-CD/argo-workflows/arguments/1-define/#global-parameters-arguments","title":"Global parameters (arguments)","text":"<p>We can define global parameters at workflow level</p> <ul> <li>They are located inside arguments section</li> <li>When the workflow is called fully (not calling a template) They and they are passed to the entrypoint template.</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: myworkflow\nspec:\n  arguments:\n    parameters:\n      - name: param1\n      - name: param2\n</code></pre> <p>They can be used inside the templates as variables :</p> <pre><code>\"{{workflow.parameters.param1}}\"\n\"{{workflow.parameters.param2}}\"\n</code></pre> <p>!!! note If we want to provide values to a global parameter, we must pass them there (spec.arguments.parameter.parameter.value)</p> <p>\"{{workflow.parameters.json}}\" is also a variable with all the parameters as a json string</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/#local-scoped-parameters-inputs","title":"Local scoped parameters (inputs)","text":"<p>We can also define parameters at template level as inputs. They are local scoped parameters.</p> <p>A template defines inputs which are then provided by template callers (such as steps, dag, or even a workflow).</p> <p>As inputs</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  name: myworkflow\nspec:\n  templates:\n    - name: mytemplate\n      inputs:\n        parameters:\n          - name: param1\n          - name: param2\n</code></pre> <p>They can be used inside the templates as:</p> <pre><code>\"{{inputs.parameters.param1}}\"\n\"{{inputs.parameters.param2}}\"\n</code></pre> <p>!!! note If we want to provide values to a local parameter, we can make it using template caller (dag, steps) level, input level and workflow level. See this</p> <p>\"{{inputs.parameters.json}}\" is also a variable with all the parameters as a json string</p>"},{"location":"CI-CD/argo-workflows/arguments/1-define/#notes-and-suggestions","title":"Notes and suggestions","text":"<ul> <li> <p>In containerset, container and script templates, inputs and outputs can only be loaded a saved from a template called main.</p> </li> <li> <p>Because there are 3 ways to call One suggestion is to define a parameter that can be used in more than one template in both places, at spec level (argument) and at template level (inputs)</p> </li> </ul>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/","title":"Resolve parameters","text":"<p>When a parameter is used in a task or template, the value is resolved in the following order of precedence:</p> <ul> <li>Task/Step-Level arguments: If a parameter is passed to a task/step using arguments, this value takes precedence.</li> <li>Template inputs: If no task-level arguments are provided, the value defined in the template's inputs is used.</li> <li>Global arguments: If neither task-level arguments nor template inputs provide a value, the global arguments value is used.</li> </ul> <p>Let's see this WorkflowTemplate. It has 2 templates: main (entrypoint) and whalesay</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: test-params\nspec:\n  entrypoint: main\n  arguments:\n    parameters:\n      - name: message\n        value: \"A\" # Third in precedence, if provided and calling the main template\n  templates:\n    - name: main\n      inputs:\n        parameters:\n          - name: message\n            value: \"B\" # Second in precedence, if provided and calling the main template\n      dag:\n        tasks:\n          - name: whalesay\n            template: whalesay\n            arguments:\n              parameters:\n                - name: message\n                  value: \"C\" # First in precedence, if provided and calling the main template\n    - name: whalesay\n      inputs:\n        parameters:\n          - name: message\n            value: \"D\" # whalesay param\n      container:\n        image: docker/whalesay:latest\n        command: [cowsay]\n        args: [\"{{inputs.parameters.message}}\"]\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-the-whole-tamplate","title":"Calling the whole tamplate","text":"<p>If we create a workflow calling the whole template without choosing the entrypoint</p> <ul> <li>the workflow will not have entrypoint</li> <li>the printed value will be A</li> </ul> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-the-main-template","title":"Calling the main template","text":"<p>If we create a workflow calling the main template</p> <ul> <li>the entrypoint will be main</li> <li>the printed value will be C</li> </ul> <p>The order will be C &gt; B &gt; A</p> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/2-resolve/#calling-whalesay","title":"Calling whalesay","text":"<p>If we we create a workflow calling the whalesay template</p> <ul> <li>the entrypoint will be whalesay</li> <li>D will have preference over A. D will be the value.</li> </ul> <p>The order will be D &gt; A</p> <p>If both are not provided, the workflow will have an error</p> <p></p>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/","title":"Parameters in secrets and configmaps","text":""},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#parameters-in-a-configmap","title":"Parameters in a configmap","text":"<p>If we want to get the parameters from a configmap we must label that configmap with this label</p> <pre><code>workflows.argoproj.io/configmap-type: Parameter\n</code></pre> <p>Then we can consume it using \"valueFrom\" \"configMapKeyRef\"</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: arguments-parameters-from-configmap-\nspec:\n  entrypoint: print-message-from-configmap\n  templates:\n  - name: print-message-from-configmap\n    inputs:\n      parameters:\n      - name: message\n        valueFrom:\n          configMapKeyRef:\n            name: simple-parameters\n            key: msg\n    container:\n      image: busybox\n      command: [\"echo\"]\n      args: [\"{{inputs.parameters.message}}\"]\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#parameters-in-a-secret","title":"Parameters in a secret","text":"<p>But if we want to store that parameter in a kubernetes secret we must use it:</p> <ul> <li>As an environment variable</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: print-secrets\n  templates:\n  - name: print-secrets\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: ['\n        echo \"secret from env: $MYSECRETPASSWORD\"\n      ']\n      env:\n      - name: MYSECRETPASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: my-secret\n            key: mypassword\n</code></pre> <ul> <li>As a volume</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secret-example-\nspec:\n  entrypoint: print-secrets\n  volumes:\n  - name: my-secret-vol\n    secret:\n      secretName: my-secret\n  templates:\n  - name: print-secrets\n    container:\n      image: alpine:3.7\n      command: [sh, -c]\n      args: ['\n        echo \"secret from file: `cat /secret/mountpath/mypassword`\"\n      ']\n      volumeMounts:\n      - name: my-secret-vol\n        mountPath: \"/secret/mountpath\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/arguments/configmaps-secrets/#links","title":"Links","text":"<p>https://argo-workflows.readthedocs.io/en/stable/walk-through/secrets/</p>"},{"location":"CI-CD/argo-workflows/arguments/links/","title":"Links","text":"<ul> <li>Parameters  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/walk-through/parameters/</p> <ul> <li>Parameters spec  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/fields/#parameter</p> <ul> <li>Inputs  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/workflow-inputs/</p> <ul> <li>Intermediate parameters</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/intermediate-inputs/</p> <ul> <li>Parameters in workflow templates  </li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/workflow-templates/</p> <ul> <li>Workflow Variables</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/variables/</p>"},{"location":"CI-CD/argo-workflows/artifacts/key-only/","title":"Key only artifact","text":"<p>A key only artifact is an input or output artifact where we only specifiy the key, without the name of the bucket or credentials.</p> <p>With this, we can do a simpler workflow moving that settings to a an artifact repository ref</p> <ul> <li>Key-Only Artifacts</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/key-only-artifacts/</p> <ul> <li>Artifact Repository Ref</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/artifact-repository-ref/</p>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/","title":"Ways to call a (cluster)workflowtemplate","text":""},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#call-the-whole-template-with-workflowtemplateref","title":"Call the whole template with workflowTemplateRef","text":"<p>We can create a workflow specifying a (cluster)workflowtemplate to be launched using spec.workflowTemplateRef. We can also provider parameters where that they will be passed to the entrypoint.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: myworkflow-\nspec:\n  workflowTemplateRef:\n    name: myclusterworkflowtemplate\n    clusterScope: true # this calls a clusterworkflowtemplate. If false or ommited (default), it calls a workflowtemplate\n  arguments:\n    parameters:\n      - name: message\n        value: \"Hello, world!\"\n</code></pre> <p>No template inside the the (cluster)workflowtemplate is selected. The default one will be used and it will receive the parameters.</p> <p>In the UI</p> <p></p> <p>We cannot use spec.templates if we are using spec.workflowTemplateRef. This throws an error</p> <pre><code>Templates is invalid field in spec if workflow referred WorkflowTemplate reference\n</code></pre>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#from-a-task-or-step-with-templateref","title":"From a task or step with templateRef","text":"<p>We can invoke a (cluster)workflowtemplate from a task or step defined in a dag or steps template using templateRef. In this case we must choose what template will be chosen form the (cluster)workflowtemplate as entrypoint.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: myworkflow-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: main\n            templateRef:\n              name: myclusterworkflowtemplate # this calls a clusterworkflowtemplate. If false or ommited (default), it calls a workflowtemplate\n              template: whalesay # choose the desired template from the (cluster)workflowtemplate\n              clusterScope: true\n            arguments:\n              parameters:\n                - name: message\n                  value: \"Hello from task\"\n</code></pre> <p>In the UI</p> <p></p>"},{"location":"CI-CD/argo-workflows/cluster-workflow-templates/1-call/#precedence","title":"Precedence","text":"<p>A workflow can call a (cluster)workflowTemplate and this (cluster)workflowTemplate can call other (cluster)workflowTemplate(s).</p> <p>For example:</p> <pre><code>Workflow &gt; (cluster)workflowTemplate A &gt; (cluster)workflowTemplate B\n</code></pre> <p>Argo Workflows handles nested template calls by dynamically expanding and composing templates at runtime.</p> <ul> <li> <p>When submitting the workflow, Argo workflows resolves (cluster)workflowTemplate A and then (cluster)workflowTemplate B. This continues until all template references are resolved into concrete steps.</p> </li> <li> <p>Parameters, volumes, and other configurations cascade down through template calls. So parameters, volumes and other metadatas at higher levels take precedence with lower levels if defined in both. With ServiceAccount, the most specific serviceAccount definition wins</p> </li> <li> <p>The result is a single, expanded Workflow object with all steps defined concretely. Once expanded, the final Workflow doesn't change even if source templates are modified</p> </li> </ul> <pre><code># Workflow (highest precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"v1.0.0\"  # This wins\n    workflowTemplateRef:\n      name: deploy-template\n\n  # WorkflowTemplate (middle precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"latest\"   # Overridden by Workflow\n      - name: namespace\n        value: \"staging\"  # This wins (not defined in Workflow)\n    templates:\n    - name: main\n      templateRef:\n        name: build-template\n        template: build\n\n  # WorkflowTemplate 2 (lowest precedence)\n  spec:\n    arguments:\n      parameters:\n      - name: image-tag\n        value: \"dev\"      # Overridden\n      - name: namespace\n        value: \"default\"  # Overridden\n      - name: registry\n        value: \"harbor.local\" # This wins (not defined upstream)\n\n  Result: image-tag: \"v1.0.0\", namespace: \"staging\", registry: \"harbor.local\"\n</code></pre>"},{"location":"CI-CD/argo-workflows/templates/0-templates/","title":"Templates","text":"<p>The templates are functions than can be called in argo workflows. There are several template types, and they are defined here:</p> <pre><code>spec:\n  templates:\n    - name: my-template\n      TYPEOFTEMPLATE:\n    - name: my-template2\n      TYPEOFTEMPLATE:\n    - name: my-template3\n      TYPEOFTEMPLATE:\n    - name: my-template4\n      TYPEOFTEMPLATE:\n    ...\n</code></pre> <p>There is a possible confussion with terms. A template is something like a function defined in a workflow, workflowtemplate or clusterworkflowtemplate. But a WorkflowTemplate or ClusterWorkflowTemplate is a kubernetes CRD acting like a base to create workflows. They include templates inside and other several fields.</p>"},{"location":"CI-CD/argo-workflows/templates/0-templates/#list-of-template-types","title":"List of template types","text":""},{"location":"CI-CD/argo-workflows/templates/0-templates/#template-callers","title":"Template callers","text":"<p>There are 2 special template types called template callers or template invocators. They invoke templates, workflowtemplates or clusterworkflowtemplates.</p> <ul> <li>Steps</li> </ul> <p>In the \"steps\" template caller you can define a list of tasks to be executed sequentially or in parallel. Also another options are available.</p> <ul> <li>Dag</li> </ul> <p>The \"dag\" template invocator executes other normal templates using dependencies between them</p>"},{"location":"CI-CD/argo-workflows/templates/0-templates/#other-templates","title":"Other templates","text":"<ul> <li>Container</li> </ul> <p>The most simple template type. It defines a container image with command and args like in a kubernetes pod.</p> <ul> <li>Script</li> </ul> <p>Same as container but it add a \"source\" field where you can define a script to be executed. The result is saved in an variable.</p> <ul> <li>Containerset</li> </ul> <p>It defines some containers to be executed in the same pod. An important thing is that they can share empty-dir volumes.</p> <ul> <li>Resource</li> </ul> <p>This template permits to do actions in kubernetes resources (get, create, apply, delete, replace, or patch resources on your cluster)</p> <ul> <li> <p>Http This template does a http call to an endpoint</p> </li> <li> <p>Data</p> </li> </ul> <p>This template permits to transform a source of data.</p> <ul> <li>Suspend</li> </ul> <p>Permits to suspend the execution of the workflow. It can be resumed manually or after a defined duration.</p>"},{"location":"CI-CD/argo-workflows/templates/http/","title":"Http","text":"<p>The http argo workflows template permit to do some http requests.</p>"},{"location":"CI-CD/argo-workflows/templates/http/#rbac-permissions","title":"Rbac Permissions","text":"<p>The http template uses the argo agent, not the workflow controller. When a workflow that uses the argo agent is created, a WorkflowTaskSet is created so we have to give additional permissions:</p> <ul> <li>to the service account specified in the workflow (default sa: default but change it is recommended)</li> <li>to the service account of the workflow controller (default sa: argo)</li> </ul> <p>The needed permissions are</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor-http\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets/status\n    verbs:\n      - patch\n</code></pre> <p>More info about this</p> <ul> <li>https://raw.githubusercontent.com/argoproj/argo-workflows/refs/heads/main/manifests/quick-start/base/agent-role.yaml</li> <li>https://github.com/argoproj/argo-workflows/issues/13770</li> <li>https://github.com/argoproj/argo-workflows/issues/10340</li> <li>https://argo-workflows.readthedocs.io/en/stable/upgrading/#06d4bf76f-fix-reduce-agent-permissions-fixes-7986-7987</li> </ul>"},{"location":"CI-CD/argo-workflows/templates/http/#secret","title":"Secret","text":"<p>Another requirement is to create a secret as described here</p> <p>https://github.com/argoproj/argo-workflows/issues/10340</p>"},{"location":"CI-CD/argo-workflows/templates/http/#executor-http-service-account-permissions","title":"executor-http service account permissions","text":"<p>If we create a service account called executor-http to execute the http templates, we can use this to give the minimum permissions. If you use another non http templates you will have to give that service account more permissions</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: executor-http\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: executor-http\nrules:\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - argoproj.io\n    resources:\n      - workflowtasksets/status\n    verbs:\n      - patch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: executor-http\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: executor-http\nsubjects:\n  - kind: ServiceAccount\n    name: executor-http\n    namespace: argo\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  annotations:\n    kubernetes.io/service-account.name: executor-http\n  name: executor-http.service-account-token\ntype: kubernetes.io/service-account-token\n</code></pre>"},{"location":"CI-CD/argo-workflows/templates/http/#options","title":"Options","text":"<p>Inside the http template we can use some configurations:</p> <ul> <li>url: to pass the url of the endpoint. With insecureSkipVerify we can ignore not known certificates</li> <li>body and bodyFrom: to pass the body</li> <li>headers: array with http headers we want to pass (name, value and valueFrom)</li> <li>method: the method of the request</li> <li>successCondition: what make the request successfull</li> <li>timeoutSeconds: timeout of the request (default 30s)</li> </ul>"},{"location":"CI-CD/argo-workflows/templates/http/#outputs","title":"Outputs","text":"<p>outputs.result in an HTTP template stores the response body HTTP templates capture the response body in the result parameter if the body is non-empty. For HTTP templates, result captures the response body. It is accessible from the outputs map: outputs.result.</p>"},{"location":"CI-CD/argo-workflows/templates/http/#variables","title":"Variables","text":"<p>Only available for successCondition</p> <pre><code>request.method: (string)\nrequest.url: (string)\nrequest.body: (string)\nrequest.headers: map\nresponse.statusCode: (int)\nresponse.body: (string)\nresponse.headers: map\n</code></pre> <p>https://argo-workflows.readthedocs.io/en/latest/variables/#http-templates</p>"},{"location":"CI-CD/argo-workflows/templates/http/#bugs-missing-features","title":"Bugs | Missing features","text":"<ul> <li>Workflow-controller was unable to obtain node</li> </ul> <p>https://github.com/argoproj/argo-workflows/issues/13847</p> <ul> <li>Enable PodGC for the agent pod</li> </ul> <p>https://github.com/argoproj/argo-workflows/issues/12692</p>"},{"location":"CI-CD/argo-workflows/templates/http/#links","title":"Links","text":"<ul> <li>HTTP Template</li> </ul> <p>https://argo-workflows.readthedocs.io/en/latest/http-template/</p> <ul> <li>HTTP spec</li> </ul> <p>https://argo-workflows.readthedocs.io/en/stable/fields/#http</p>"},{"location":"CI-CD/gitlab-ci-cd/98-tips/","title":"Tips","text":""},{"location":"CI-CD/gitlab-ci-cd/98-tips/#a-cicd-variable-contains-a","title":"A CI/CD variable contains a $","text":"<p>If a gitlab cicd variable contains a $, we can escape it addin another $ in the value</p> <pre><code>asljfrower$34onamdgflg &gt; asljfrower$$34onamdgflg\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/98-tips/#credentials-to-pull-image","title":"Credentials to pull image","text":"<ul> <li> <p>create a credential with pull permissions in your registry</p> </li> <li> <p>Add a masked variable to the CI/CD with this format</p> </li> </ul> <pre><code>name: DOCKER_AUTH_CONFIG\nvalue: {\"auths\":{\"FQDN\":{\"username\":\"your-username\",\"password\":\"your-password\"}}}\n</code></pre> <p>And thats it. GitLab will automatically use these credentials to authenticate with your registry. The authentication happens at the GitLab Runner level, not within your job container.</p> <ul> <li>Run your CI/CD jobs in Docker containers</li> </ul> <p>https://docs.gitlab.com/ci/docker/using_docker_images/</p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/","title":"Protect uploading secrets to main","text":""},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#protect-the-main-branch","title":"Protect the main branch","text":"<p>Go to Settings - Repository - Protected branches and configure that nobody can push to the main branch. In \"Allowed to merge\" leave the roles you permit to merge.</p> <p></p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#force-pass-the-pipeline","title":"Force pass the pipeline","text":"<p>Go to Settings - Merge requests - Merge requests and enable \"Pipelines must succeed\"</p> <p></p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#configure-the-pipeline-to-scan-the-pushes","title":"Configure the pipeline to scan the pushes","text":"<p>Configure the pipeline to scan the merge requests in the .gitlab-ci.yml file</p> <p>Example</p> <pre><code>stages:\n  - security\ntrufflehog-git:\n  stage: security\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n  image:\n    name: docker.io/trufflesecurity/trufflehog:3.82.1\n    entrypoint: [\"/bin/sh\", \"-c\"]\n  script:\n    - trufflehog --fail git \"$CI_REPOSITORY_URL\"\n</code></pre>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#test","title":"Test","text":"<p>In order to test it, push a new branch with changes and do a merge request to main</p>"},{"location":"CI-CD/gitlab-ci-cd/protect-merges-from-secrets/#links","title":"Links","text":"<ul> <li>Merge request pipelines</li> </ul> <p>https://docs.gitlab.com/ee/ci/pipelines/merge_request_pipelines.html</p> <ul> <li>CI_PIPELINE_SOURCE predefined variable</li> </ul> <p>https://docs.gitlab.com/ee/ci/jobs/job_rules.html#ci_pipeline_source-predefined-variable</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/","title":"Stages and needs","text":"<p>The stages permit to group gitlab CI/CD jobs</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#defining-the-stages","title":"Defining the stages","text":"<p>The default stages are</p> <ul> <li>.pre</li> <li>build</li> <li>test</li> <li>deploy</li> <li>.post</li> </ul> <p>They can be redefined in the .gitlab-ci.yml file</p> <pre><code>stages:\n  - mystage1\n  - mystage2\n  - mystage3\n</code></pre> <p>If a stage is defined but no jobs use it, the stage is not visible in the pipeline</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#defining-the-stage-of-a-job","title":"Defining the stage of a job","text":"<p>We define inside a job the stage belongs to.</p> <pre><code>myjob:\n  stage: mystage1\n</code></pre> <p>If no stage is defined, the job uses the test stage by default</p>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#how-the-stages-run","title":"How the stages run","text":"<ul> <li>If a pipeline contains only jobs in the .pre or .post stages, it does not run. There must be at least one other job in a different stage.</li> <li>All the jobs in the same stage runs in parallel</li> <li>When all jobs in an stage succeed, the next stage jobs start.</li> <li>When all stages succeed, the pipeline is marked as passed</li> <li>If any job fails, the pipeline is marked as failed and jobs in later stages do not start. Jobs in the current stage are not stopped and continue to run.</li> </ul>"},{"location":"CI-CD/gitlab-ci-cd/stages-needs/#needs","title":"Needs","text":"<p>Pending</p> <p>https://docs.gitlab.com/ci/yaml/#needs</p>"},{"location":"CI-CD/reloader/00-intro/","title":"Intro","text":"<p>Reloader permits to restart some kubernetes resources when some defined configmaps or secrets has changed.</p> <p>It Works with these kubernetes resources:</p> <ul> <li>Deployments</li> <li>Daemonsets</li> <li>Statefulsets</li> <li>DeploymentConfigs (from openshift, needs to be enabled via isOpenshift: true)</li> <li>Rollouts (from Argo rollouts, needs to be enabled via isArgoRollouts: true)</li> </ul>"},{"location":"CI-CD/reloader/00-intro/#how-it-works","title":"How it works","text":"<p>Reloader tracks the kubernetes resources configured. When a secret or configmap is updated, reloader triggers a restart of the resource.</p> <p>For this, it watches the data section of the secret</p> <p>There are 2 reload strategies here. The default one (env-vars) creates an environment variable in the restarted pods. The \"annotations\" mode add an annotation \"reloader.stakater.com/last-reloaded-from\" in the pods (via template spec)</p>"},{"location":"CI-CD/reloader/00-intro/#more-info","title":"More info","text":"<ul> <li> <p>Github https://github.com/stakater/Reloader</p> </li> <li> <p>Github docs https://github.com/stakater/Reloader/tree/master/docs</p> </li> </ul>"},{"location":"CI-CD/reloader/98-tips/","title":"Tips","text":""},{"location":"CI-CD/reloader/98-tips/#working-with-argo-rollouts","title":"Working with Argo Rollouts","text":"<p>We must enable this feature. In the helm chart:</p> <pre><code>reloader:\n  isArgoRollouts: true\n</code></pre> <p>Then we must annotate the Argo Rollout resource. See annotations</p> <p>Using workloadRef crashes the operator. See this bug: https://github.com/stakater/Reloader/issues/751</p>"},{"location":"CI-CD/reloader/annotations/","title":"Annotations","text":"<p>The method to configure how we control how reloader restarts the workloads is via resource annotations.</p> <p>There are 3 ways to the reload</p>"},{"location":"CI-CD/reloader/annotations/#automatic-reload","title":"Automatic reload","text":"<p>This is the default behaviour. We have this 3 options: if any secret inside the workload or configmap changes, or if only a configmap or secret changes.</p> <p>It is possible to use a custom annotations with a reloader controller parameter</p> Annotation Behaviour Custom annotation parameter reloader.stakater.com/auto: \"true\" reload if a secret or configmap changes --auto-annotation reloader.stakater.com/auto: \"false\" disables the reload in the workload --auto-annotation configmap.reloader.stakater.com/auto: \"true\" reload if a configmap changes --configmap-auto-annotation secret.reloader.stakater.com/auto: \"true\" reload if a secret changes --secret-auto-annotation <ul> <li>reloader.stakater.com/auto and reloader.stakater.com/search cannot be used together. the auto annotation takes precedence.</li> <li>If both configmap.reloader.stakater.com/auto and secret.reloader.stakater.com/auto are used, only one needs to be true to trigger a reload.</li> </ul> <p>Enabling --auto-reload-all in the controller makes all workloads treated as reloader.stakater.com/auto: \"true\" unless they have reloader.stakater.com/auto: \"false\"</p>"},{"location":"CI-CD/reloader/annotations/#giving-the-name-of-the-resource","title":"Giving the name of the resource","text":"<p>We can be more specific giving the name(s) of the secret(s) or configmap(s) that must trigger the reload. Multiple configmaps or secrets can be specified, comma separated</p> <p>It is possible to use a custom annotations with a reloader controller parameter</p> Annotation Behaviour Custom annotation parameter configmap.reloader.stakater.com/reload: \"NAME_OF_THE_CONFIGMAP\" reload if specified configmap changes --configmap-annotation secret.reloader.stakater.com/reload: \"NAME_OF_THE_SECRET\" reload if specified secret changes --secret-annotation"},{"location":"CI-CD/reloader/annotations/#search-and-match-restart","title":"Search and match restart","text":"<p>Another way to control the reload is using a two way annotation</p> <p>If we annotate the workload with this</p> <pre><code>reloader.stakater.com/search: \"true\"\n</code></pre> <p>... reload will trigger a reload if the configmap or secrets that the workload includes have the following annotation</p> <pre><code>reloader.stakater.com/match: \"true\"\n</code></pre> <p>It is possible the override this annotation with the --auto-search-annotation flag</p>"},{"location":"CI-CD/reloader/annotations/#other-annotations","title":"Other annotations","text":"Annotation Where Behaviour reloader.stakater.com/ignore: \"true\" CM/Secret The resource will not trigger reloads reloader.stakater.com/rollout-strategy: \"rollout\" Workload A rollout is triggered patching the template reloader.stakater.com/rollout-strategy: \"restart\" Workload The pods are deleted without patching the template deployment.reloader.stakater.com/pause-period: \"5m\" Workload Pause rollouts for a deployment for a specified duration"},{"location":"CI-CD/renovate/1-phases/","title":"Renovate phases","text":""},{"location":"CI-CD/renovate/1-phases/#configurations","title":"Configurations","text":"<p>The first step is to merge the configurations. From more important to less:</p> <pre><code>cli &gt; env &gt; file &gt; default\n</code></pre>"},{"location":"CI-CD/renovate/1-phases/#cloning","title":"Cloning","text":"<p>Then the platform module interacts with the source control platform and clones the list of configured repositories.</p> <p>There are some supported platforms we can configure, such as azure, bitbucket, github, gitlab, gitea,...</p> <p>More information about platforms here:</p> <p>https://docs.renovatebot.com/modules/platform/</p>"},{"location":"CI-CD/renovate/1-phases/#vulnerabilities","title":"Vulnerabilities","text":"<p>Next there is check for vulnerabilities.</p> <p>Is it possible to only update dependencies if vulnerabilities have been detected (security:only-security-updates)</p>"},{"location":"CI-CD/renovate/1-phases/#extract-dependencies-with-package-managers","title":"Extract dependencies with package managers","text":"<p>Then the manager module looks for files based on their name and extracts the dependencies. It assigns a datasource to each extracted package file or dependency. The datasource tells Renovate how to search for new versions.</p> <p>Example: The gitlabci manager finds a dependency: python:3.10-alpine which has the docker datasource</p> <p>Some package managers are ansible, helm-values, argocd,... and it is possible to reconfigure the file match.</p> <p>More info about the package managers module here:</p> <p>https://docs.renovatebot.com/modules/manager/</p>"},{"location":"CI-CD/renovate/1-phases/#look-up-updates","title":"Look up updates","text":"<p>Then the datasource module looks for available versions of the dependency looking up registries.</p> <p>Example: The docker datasource looks for versions and finds: [python:3.9,python:3.9-alpine,python:3.10,python:3.10-alpine,python:3.11,python:3.11-alpine]</p> <p>Some datasources are docker, github-releases, helm, ruby-gems</p> <p>More info about the datasources module here:</p> <p>https://docs.renovatebot.com/modules/datasource/</p>"},{"location":"CI-CD/renovate/1-phases/#versioning","title":"Versioning","text":"<p>Once we have located available versions, the versioning module will use a scheme to perform sorting and filtering of results.</p> <p>Example: The docker versioning returns python:3.11-alpine, because that version is compatible with python:3.10-alpine</p> <p>It is usually recommended to configure the versioning because the default way can fail in some scenarios.</p> <p>More info about versioning module here:</p> <p>https://docs.renovatebot.com/modules/versioning/</p>"},{"location":"CI-CD/renovate/1-phases/#write-updates","title":"Write updates","text":"<p>Once the updates has been chosen, the changes are pushed to the repository depending how it is configured</p>"},{"location":"CI-CD/renovate/1-phases/#links","title":"Links","text":"<ul> <li>How renovate works https://docs.renovatebot.com/key-concepts/how-renovate-works/</li> </ul>"},{"location":"CI-CD/renovate/self-hosted-configurations/host-rules-from-environment/","title":"Host rules from environment variables","text":"<p>There is a setting that permits to detect host rules from environemnt variables:</p> <pre><code>via cli: --detect-host-rules-from-env\nvia env: RENOVATE_DETECT_HOST_RULES_FROM_ENV\n</code></pre> <p>This setting by default is disabled but, enabling it, permits to configure host rules with variables.</p> <p>How that rule is detected? Renovate search for this syntax:</p> <pre><code>RENOVATE_DATASOURCENAME_DOMAIN/SUBDOMAIN_FIELD\n</code></pre> <p>for example</p> <pre><code>RENOVATE_DOCKER_DOCKER_IO_USERNAME\nRENOVATE_DOCKER_DOCKER_IO_PASSWORD\n</code></pre> <p>Notes:</p> <ul> <li>The RENOVATE_ is optional, but the documentation says it will be required in the future</li> <li>Only domains/subdomains are supported. Nothing like protocols (https://,...).</li> <li>The field name can be: TOKEN, USERNAME, PASSWORD, HTTPSPRIVATEKEY, HTTPSCERTIFICATE, HTTPSCERTIFICATEAUTHORITY</li> <li>Hyphens (-) in datasource or host name must be replaced with double underscores (__).</li> <li>Periods (.) in host names must be replaced with a single underscore (_).</li> </ul>"},{"location":"database/cloudnative-pg/98-tips/","title":"Tips","text":""},{"location":"database/cloudnative-pg/98-tips/#minimal-cluster","title":"Minimal cluster","text":"<p>This is the minimal cluster spec</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: sinbootstrap\nspec:\n  storage:\n    storageClass: standard\n    size: 1Gi\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#disable-non-ssl-connections","title":"Disable non ssl connections","text":"<p>If we want to deny all non ssl connections to the cluster, we can add this sections to the cluster</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgre\nspec:\n  postgresql:\n    pg_hba:\n      - hostssl all all all scram-sha-256\n      - hostnossl all all all reject\n</code></pre> <p>This setting add 2 rules between some fixed rules (system rules) and the default rule that permits both ssl and non ssl connections. The fist rule permit ssl connections via password and the second one denies non ssl connections.</p> <p>The postgresql documentation says \"The first record with a matching connection type, client address, requested database, and user name is used to perform authentication.\"</p> <p>Some tips:</p> <ul> <li>We can do better rules specifying users, databases and hosts</li> <li>The controller applies these rules without restarting the pods</li> <li>We can get the current rules with this</li> </ul> <pre><code>select pg_reload_conf();\ntable pg_hba_file_rules;\n</code></pre> <p>https://www.postgresql.org/docs/current/auth-pg-hba-conf.html</p>"},{"location":"database/cloudnative-pg/98-tips/#recreate-all-the-cluster-nodes","title":"Recreate all the cluster nodes","text":"<ul> <li>Destroy 2 replicas</li> </ul> <pre><code>kubectl cnpg destroy MYCLUSTER ONE-REPLICA\nkubectl cnpg destroy MYCLUSTER ANOTHER-REPLICA\n</code></pre> <p>Once they are ok, promote a replica to be primary</p> <pre><code>kubectl cnpg promote MYCLUSTER ONE-REPLICA\n</code></pre> <p>Once is prometed, destroy the older primary</p> <pre><code>kubectl cnpg destroy MYCLUSTER OLD-PRIMARY\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#info-about-primary-replicas","title":"Info about primary replicas","text":"<p>Show the nodes where the replicas are located</p> <pre><code>kubectl get pod -A -l cnpg.io/instanceRole=primary -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,NODE:.spec.nodeName\n</code></pre>"},{"location":"database/cloudnative-pg/98-tips/#get-an-sql-session","title":"get an sql session","text":"<pre><code>kubectl cnpg psql mycluster\n</code></pre> <pre><code>SELECT timeline_id FROM pg_control_checkpoint();\n</code></pre>"},{"location":"database/cloudnative-pg/99-links/","title":"Links","text":"<ul> <li>CloudNativePG official site</li> </ul> <p>https://cloudnative-pg.io/</p> <ul> <li>CloudNativePG github</li> </ul> <p>https://github.com/cloudnative-pg/cloudnative-pg</p> <ul> <li>Gabriele Bartolini blog</li> </ul> <p>https://www.gabrielebartolini.it/</p>"},{"location":"database/cloudnative-pg/errors/","title":"Errors","text":""},{"location":"database/cloudnative-pg/errors/#timeline-servers-history-checkpoint-error","title":"Timeline | server's history | checkpoint error","text":"<p>Sympthoms:</p> <p>The operator is trying to create a replica but it can't and the new instance fails with this error</p> <pre><code>...\nrequested timeline XXX is not a child of this server's history\nLatest checkpoint is at YYY on timeline XXX, but in the history of the requested timeline, the server forked off from that timeline at YYY\n</code></pre> <p>Destroying the failing replicas and forcing to create a new one does not solve the problem.</p> <p>Cause:</p> <p>There is difference between the primary instance and the backups</p> <p>Workaround:</p> <ul> <li>Do a manual backup not with the operator</li> <li>Change the cluster to 1 instance. Probably needs a manual destroy of the failing instances</li> <li>Change where the backups will be stored. We must change the backup section selecting another bucket o changing the serverName. The goal is to start an empty backup folder.</li> <li>Do a manual backup using the operator and check it is working.</li> <li>Also check the wal files are being written in the barmanObjectStore</li> <li>Change the instances to the desired number</li> </ul> <p>I have tried this using barmanObjectStore based backup.</p>"},{"location":"database/cloudnative-pg/errors/#wal-file-not-found-in-the-recovery-object-store","title":"WAL file not found in the recovery object store\"","text":"<ul> <li>See the logs in every cnpg instance</li> <li>Check the name of the failing .history file and see the .history files in the /var/lib/postgresql/wal/pg_wal/ directory of the instances</li> <li>Ensure you have the proper IRSA permissions</li> <li>Try to recreate all the instances with destroy | promote actions</li> </ul>"},{"location":"database/cloudnative-pg/errors/#the-token-included-in-the-request-has-no-service-account-role-association-for-it","title":"The token included in the request has no service account role association for it","text":"<pre><code>ERROR: Barman cloud backup delete exception: Error when retrieving credentials from container-role: Error retrieving metadata: Received non 200 response 404 from container metadata:\n...\n(ResourceNotFoundException): The token included in the request has no service account role association for it., fault: client\\n\\n\",\"error\":\"exit status 4\"\n\n</code></pre> <p>This can be caused because there were some changes in the IRSA authentication (iam role, annotation,..) To solve it, restart the cluster</p>"},{"location":"database/cloudnative-pg/errors/#error-calling-the-headbucket-operation","title":"Error calling the HeadBucket operation","text":"<pre><code>ERROR: Barman cloud WAL archiver exception: An error occurred (403) when calling the HeadBucket operation: Forbidden\"\n</code></pre> <p>This is an AWS IAM permissions issue. Probably you need to add \"s3:ListBucket\" Action permissions to the bucket itself.</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": \"arn:aws:s3:::BUCKETNAME\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"database/cloudnative-pg/errors/#http-communication-issue-error","title":"\"HTTP communication issue\" error","text":"<p>Restart the controller</p>"},{"location":"database/cloudnative-pg/errors/#a-replica-cannot-be-created","title":"A replica cannot be created","text":"<p>If we get errors like</p> <pre><code>\"requested timeline XXX is not a child of this server's history\"\n\"Latest checkpoint is at XXX on timeline XXX, but in the history of the requested timeline, the server forked off from that timeline at YYY.\"\n</code></pre> <p>and only the primary is up. We can:</p> <ul> <li>Do a manual backup via pgdump of every database</li> <li>Leave the cluster with only 1 replica and no backup section</li> <li>Rename the s3 folder or use a different serverName in the backup section.</li> <li>Enable the backup section and do a backup via the kubectl cnpg plugin</li> <li>If it works, increase the replicas to 3</li> </ul>"},{"location":"database/cloudnative-pg/errors/#using-the-csi-driver-nfs","title":"Using the csi driver NFS","text":"<ul> <li>You can probably need to give more permissions</li> <li>spec.postgresUID and spec.postgresGID (default 26) in the cluster resource definition gives you more possibilities</li> <li>Don't use the subDir parameter</li> </ul> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-postgre\nparameters:\n  mountPermissions: \"0777\"\n...\n</code></pre> <p>This avoids some permission errors and other like:</p> <pre><code>controller with name instance-cluster already exists. Controller names must be unique to avoid multiple controllers reporting to the same metric\n</code></pre> <pre><code>stale NFS file handle\n</code></pre> <pre><code>This is an old primary instance in a new cluster without backup\n</code></pre>"},{"location":"database/cloudnative-pg/pdb/","title":"CloudnativePG, PDB and draining nodes","text":"<p>When a node is cordoned, Kubernetes creates a taint on the node and marks it as unschedulable. This prevents new pods from being scheduled on the node.</p> <p>We can see the taint in a cordoned node</p> <pre><code>spec:\n  taints:\n  - effect: NoSchedule\n    key: node.kubernetes.io/unschedulable\n  unschedulable: true\n</code></pre> <p>By default, Cloudnative PG creates a pod disruption budget with no allowed disruptions to protect the primary instance. If the node where the primary instance is cordoned, the operator will try to find a replica in another node and then promote it to primary. Once the promotion has been done, the former primary can be evicted with a node drain.</p> <p>It is easy to check this behaviour. Simply cordon the node where the replica is, and see the logs in the controller. We can get this kind of messages.</p> <pre><code>Primary is running on an unschedulable node, will try switching over\n</code></pre> <p>If all the replicas are in not ready nodes, the pdb will continue blocking the drain operations.</p> <pre><code>Current primary is running on unschedulable node, but there are no valid candidates\n</code></pre> <p>In that situation, we must move a replica to another node an let the controller do a promotion, for example, with a restart with cnpg cli. A good practice can be to setup the anti-affinity in the cluster resource to ensure every replica in deployed in different nodes.</p>"},{"location":"database/cloudnative-pg/pdb/#disabling","title":"Disabling","text":"<p>That pdb creation can be disabled via spec.enablePDB in the cluster resource. This feature is available since the v1.23.0 release.</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: mycluster\nspec:\n  enablePDB: false\n</code></pre>"},{"location":"database/cloudnative-pg/pdb/#karpenter","title":"Karpenter","text":"<p>There is a problem with karpenter and this behaviour. Karpenter knows there is pdb in the node with no allowed disruption and the node will not be disrupted.</p> <p>You can get events like this</p> <pre><code>DisruptionBlocked\nCannot disrupt Node: pdb \"mynamespace/mycluster-primary\" prevents pod evictions\n</code></pre> <p>In order to permit a node to be disrupted by karpenter you must move the primary instaces to another nodes.</p> <p>In production environents:</p> <ul> <li>Use pod antiaffinity to have the cnpg instances in different nodes</li> <li>Rotate all the primary instances to a single node to permit a more optimized karpenter consolidation</li> <li>After the consolidation, rotate some of that primary instances to another nodes.</li> </ul> <p>See what nodes cannot be disrupted</p> <pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node | grep pdb\n</code></pre> <p>See the cluster information (needs the kubectl cnpg plugin installed)</p> <pre><code>kubectl get clusters --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t\"}{.status.currentPrimary}{\"\\n\"}{end}' | while read namespace cluster primary; do\n    node=$(kubectl get pod $primary -n $namespace -o jsonpath='{.spec.nodeName}')\n    echo \"############### CLUSTER $cluster ###############\"\n    kubectl cnpg status $cluster -n $namespace | grep -A 6 \"Instances status\"\n    nodepool=$(kubectl get node $node -o jsonpath='{.metadata.labels.karpenter\\.sh/nodepool}')\n    echo \"&gt;&gt;&gt;&gt; The cluster is in the nodepool $nodepool. Showing nodes:\"\n    kubectl get nodes -l karpenter\\.sh/nodepool=$nodepool\n    echo \"&gt;&gt;&gt;&gt; Promotion command: &lt;kubectl cnpg -n $namespace promote $cluster REPLICAID&gt;\"\n    echo \"##############################\"\ndone\n</code></pre>"},{"location":"database/cloudnative-pg/postgre-images/","title":"PostgreSQL operand Images","text":"<p>There are 2 ways to define the postgresql (operand) image we want to use in a CloudNative-PG cluster:</p> <ul> <li>using spec.imageName</li> </ul> <p>Here we configure the url of the docker image</p> <ul> <li>using spec.imageCatalogRef</li> </ul> <p>Here we can select a version from an existing ImageCatalog or ClusterImageCatalog</p>"},{"location":"database/cloudnative-pg/postgre-images/#oficial-postgresql-images","title":"Oficial postgresql images","text":"<p>CloudNative-PG builds and provides some postgresql images ready to be used</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgresql-images","title":"Postgresql images","text":"<ul> <li>Github repo</li> </ul> <p>https://github.com/cloudnative-pg/postgres-containers</p> <ul> <li>Registry</li> </ul> <p>https://github.com/cloudnative-pg/postgres-containers/pkgs/container/postgresql</p> <p>Here we can find 3 image types:</p> <ul> <li>Minimal</li> </ul> <p>They include APT PostgreSQL packages from PostgreSQL Global Development Group (PGDG).</p> <ul> <li>standard</li> </ul> <p>They include pgaudit, Postgres Failover Slots, pgvector, all locales and LLVM JIT support until 18 release</p> <ul> <li>system (Deprecated)</li> </ul> <p>They are based on standard images. Include Barman Cloud binaries for backup operations and they will be removed when in-core Barman Cloud support is phased out</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgresql-clusterimagecatalogs","title":"Postgresql ClusterImageCatalogs","text":"<p>All the current provided ClusterImageCatalogs located are here:</p> <p>https://github.com/cloudnative-pg/artifacts/tree/main/image-catalogs</p> <p>They include the standard, minimal and system images</p> <p>Also there are some legacy catalogs here:</p> <p>https://github.com/cloudnative-pg/postgres-containers/tree/main/Debian</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgis-images","title":"Postgis images","text":"<p>CloudNative-PG also builds and provides some postgis images. But the plan is to stop offering postgis image once PostgreSQL 17 reaches end of life (November 2029).</p> <p>\"Starting with PostgreSQL 18, the extension_control_path GUC will allow PostGIS to be mounted as a separate image volume, removing the need for dedicated PostGIS container images.\"</p> <ul> <li>Github repo</li> </ul> <p>https://github.com/cloudnative-pg/postgis-containers</p> <ul> <li>Registry</li> </ul> <p>https://github.com/cloudnative-pg/postgis-containers/pkgs/container/postgis</p> <p>Here we can find 2 image types:</p> <ul> <li>standard</li> </ul> <p>Without Barman Cloud</p> <ul> <li>system (Deprecated)</li> </ul> <p>with Barman Cloud</p>"},{"location":"database/cloudnative-pg/postgre-images/#postgis-clusterimagecatalogs","title":"Postgis ClusterImageCatalogs","text":"<p>All the current provided ClusterImageCatalogs located are here:</p> <p>They include the standard and system images</p> <p>https://github.com/cloudnative-pg/postgis-containers/tree/main/image-catalogs</p>"},{"location":"database/cloudnative-pg/postgre-images/#important-notes","title":"Important notes","text":""},{"location":"database/cloudnative-pg/postgre-images/#migration-path","title":"Migration Path","text":"<ul> <li>Move to the the backup barman plugin</li> <li>Avoid using system images. Move to standard or minimal.</li> <li>Avoid using system or legacy ClusterImageCatalogs. Move to standard or minimal.</li> </ul>"},{"location":"database/cloudnative-pg/postgre-images/#default-release","title":"Default release","text":"<p>The default release of postgresql offered bye the the operator is the \"latest available minor version of the latest stable major version supported by the PostgreSQL Community\". The best practice in production is to use an specific image, better with the SHA256 digest</p>"},{"location":"database/cloudnative-pg/postgre-images/#custom-image","title":"Custom image","text":"<p>It is possible to build your own custom images</p>"},{"location":"database/cloudnative-pg/postgre-images/#operator-image","title":"Operator image","text":"<p>We can also override the container image of the operator (cloudnative-pg) changing the image of the cnpg operator deployment</p> <p>The releases can be found here:</p> <p>https://github.com/cloudnative-pg/cloudnative-pg/pkgs/container/cloudnative-pg</p> <p>the value of the OPERATOR_IMAGE_NAME will be applied in the sidecar of every instance of the cluster</p>"},{"location":"database/cloudnative-pg/production/","title":"CNPG in production","text":""},{"location":"database/cloudnative-pg/production/#deployment","title":"Deployment","text":"<ul> <li>Use gitops tools (argocd, flux,...) to control the deployment</li> <li>Use gitops tools like external-secrets operator to control the credentials</li> <li>You can enable the spec.monitoring.enablePodMonitor setting and setup a monitoring and alerting system</li> </ul>"},{"location":"database/cloudnative-pg/production/#configuration","title":"Configuration","text":"<ul> <li>Always setup a backup section in our clusters and review the status of the backups</li> <li>Try not to enable spec.enableSuperuserAccess. You can create additional roles with the needed permissions.</li> <li>Configure the primaryUpdateStrategy</li> <li>Define the resources (requests and limits in the cluster)</li> <li>Give the postgresql pods a higher priority class</li> <li>Leave spec.enablePDB enabled (default)</li> <li>Use odd replicas (3, 5, ...)</li> <li>Configure the affinity section to distribute the instances in nodes</li> <li>Consider to use dedicated and/or performance nodes in the the postgresql instances</li> </ul>"},{"location":"database/cloudnative-pg/production/#karpenter-and-cluster-autoescaler","title":"Karpenter and cluster autoescaler","text":"<p>Until 1.26 release, cloudnative-pg only detects a node is being drained if detects via the node.kubernetes.io/unschedulable taint</p> <p>Since 1.26 release, cloudnative-pg detects a node is being drained with these taints:</p> <ul> <li>node.kubernetes.io/unschedulable</li> <li>ToBeDeletedByClusterAutoscaler</li> <li>karpenter.sh/disrupted</li> <li>karpenter.sh/disruption</li> </ul> <p>When karpenter and cluster autoscaler taints the node, the controller knows the node will be delete and it can initiate a failover</p>"},{"location":"database/cloudnative-pg/rolling-update/","title":"Rolling update a cluster","text":""},{"location":"database/cloudnative-pg/rolling-update/#reasons","title":"Reasons","text":"<p>There are several reasons we can do some changes in a cloudnative pg cluster that requires a rolling update in the cluster, this is, recreate the postgresql pods with the new settings:</p> <ul> <li>Updates in the operator (*)</li> <li>Changes in the spec.image field or in the image catalog</li> <li>Changes in spec.resources</li> <li>Changes in the postgresql configuration</li> <li>Changes in the size of the persistent volume</li> </ul> <p>(*) There is a way to not trigger a rolling update when the operator is updated called \"In-place updates of the instance manager\". But it is not a clean way to do it.</p> <p>When a rolling update is triggered, first the operator upgrades the replicas, but we can configure how the primary instance will be updated.</p>"},{"location":"database/cloudnative-pg/rolling-update/#primaryupdatestrategy","title":"primaryUpdateStrategy","text":"<p>spec.primaryUpdateStrategy defines if we want to control the update the of primary instance.</p> <ul> <li> <p>unsupervised (default) The update is automatic based in the spec.primaryUpdateMethod field (see below)</p> </li> <li> <p>supervised This is the manual update to the primary and suspends the update of the primary. In order to continue we can manually do the switchover or the restart of the primary.</p> </li> </ul>"},{"location":"database/cloudnative-pg/rolling-update/#primaryupdatemethod","title":"primaryUpdateMethod","text":"<p>spec.primaryUpdateMethod defines how we want to update the primary instance and it is applied when the primaryUpdateStrategy is \"unsupervised\". We have 2 options here:</p> <ul> <li> <p>restart (default) This restarts the primary replica</p> </li> <li> <p>switchover</p> </li> </ul> <p>A switchover operation is triggered. In the switchover operation the former primary will be shut down. The spec.switchoverDelay can be expressed in seconds as the time to give to the primary to shutdown gracefully and archive the wal files. The default value is 3600 (1h).</p> <ul> <li>RTO (recovery time objective) is the time between the failure and when the service is up again.</li> <li>RPO (recovery point objective) is more related with the amount of data loss</li> </ul> <p>A lower spec.switchoverDelay gives priority to reduce the time (RTO) and a higher value reduces the risk of data loss (RPO).</p> <p>Then, the most aligned replica is promoted as the new primary.</p> <p>Again this value is a decision to take depending of several reasons like the environment or workload. In all cases a rolling update causes a service loss.</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/","title":"Migration to barman cloud plugin","text":""},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#install-the-barman-cloud-plugin","title":"Install the barman cloud plugin","text":"<p>Follow https://cloudnative-pg.io/plugin-barman-cloud/docs/installation/</p> <p>Requirements</p> <ul> <li>CloudNativePG version 1.26 or later. 1.27 has some improvements</li> <li>cert-manager</li> </ul>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#create-an-objectstore","title":"Create an ObjectStore","text":"<p>We must translate the spec.backup.barmanObjectStore section of the cluster to a new ObjectStore resource, under spec.configuration.</p> <p>serverName must be empty here. It must be configured in the the plugins section in the cluster</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#migration","title":"Migration","text":"<p>Here we must do some changes at once</p> <ul> <li> <p>Remove the spec.backup.barmanObjectStore section and spec.backup.retentionPolicy if it was defined. Also remove the entire spec.backup section if it is now empty</p> </li> <li> <p>Configure the spec.plugin section</p> </li> </ul> <p>parameters: barmanObjectName and serverName if needed</p> <ul> <li>Add barman-cloud.cloudnative-pg.io to the plugins list, as described in Configuring WAL archiving</li> </ul> <p>This change restarts the cluster pods</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#migration-test","title":"Migration test","text":"<ul> <li> <p>Check the wal backup is working</p> </li> <li> <p>Create a manual backup</p> </li> </ul> <pre><code>kubectl cnpg backup MYCLUSTER --method=plugin --plugin-name=barman-cloud.cloudnative-pg.io\n</code></pre>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#update-the-scheduled-backup","title":"Update the scheduled backup","text":"<p>If it works, update the scheduled backup to use the plugin</p> <p>changing</p> <pre><code>    method: barmanObjectStore\n</code></pre> <p>for</p> <pre><code>    method: plugin\n        pluginConfiguration:\n        name: barman-cloud.cloudnative-pg.io\n</code></pre>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#change-the-images","title":"Change the images","text":"<p>Finally don't use legacy or system images and migrate to minimal or standard images</p> <p>See here for more info</p>"},{"location":"database/cloudnative-pg/backup/migration-to-plugin/#links","title":"Links","text":"<ul> <li>Migrating from Built-in CloudNativePG Backup</li> </ul> <p>https://cloudnative-pg.io/plugin-barman-cloud/docs/migration/</p> <ul> <li>[Bug]: Missing prometheus metrics</li> </ul> <p>After migrating to the barman cloud plugin, some Prometheus metrics related to backup monitoring may not be available or properly exposed. This affects monitoring dashboards that rely on backup-specific metrics to track backup success/failure rates and timing.</p> <p>https://github.com/cloudnative-pg/cloudnative-pg/issues/7812</p>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/","title":"From remote cnpg","text":"<p>We can create a new cluster from an existing cnpg cluster.</p>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#requirements","title":"Requirements","text":"<p>In the source and destination cluster we need the same:</p> <ul> <li>the same major PostgreSQL release with imageName or imageCatalogRef</li> <li>the same hardware architecture</li> <li>the same tablespaces</li> </ul> <p>We also need</p> <ul> <li>enough max_wal_senders</li> <li>network connectivity between them</li> </ul>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#importing-the-streaming_replica-creds","title":"Importing the streaming_replica creds","text":"<p>We will use the streaming_replica role in the source cluster.</p> <p>In order to get the credentials there, go to the namespace where the source cluster is located an export the secret that ends with \"-replication\"</p> <pre><code>kubectl get secret OMMITED-replication -o yaml\n</code></pre> <p>Then clean it and leave it this way</p> <pre><code>apiVersion: v1\ndata:\n  tls.crt: OMMITED\n  tls.key: OMMITED\nkind: Secret\nmetadata:\n  name: OMMITED-replication\ntype: kubernetes.io/tls\n</code></pre> <p>Go to the namespace where the new cluster will be created and import the secret</p> <pre><code>kubectl apply -f mysecret.yaml\n</code></pre>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#the-new-cluster","title":"The new cluster","text":"<p>Then create the new cluster with this basic settings</p> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: cnpg\nspec:\n  imageCatalogRef: # or imageName\n    apiGroup: postgresql.cnpg.io\n    kind: ClusterImageCatalog\n    major: 15 # must be the same major PostgreSQL\n    name: postgresql\n  externalClusters:\n    - name: my-remote-cluster # descriptive name\n      connectionParameters:\n        host: my-remote-host # host or ip\n        user: streaming_replica\n        sslmode: require\n      sslKey:\n        name: OMMITED-replication\n        key: tls.key\n      sslCert:\n        name: OMMITED-replication\n        key: tls.crt\n  bootstrap:\n    pg_basebackup:\n      source: my-remote-cluster\n      # Next settings if we can create a database, owner and assign credentials to that user\n      database: desired-db\n      owner: db-owner-name\n      secret:\n        name: desired-db-secret\n</code></pre>"},{"location":"database/cloudnative-pg/bootstrap/pg_basebackup-remote-cnpg/#links","title":"Links","text":"<ul> <li>Bootstrap from a live cluster (pg_basebackup)</li> </ul> <p>https://cloudnative-pg.io/documentation/1.26/bootstrap/#requirements</p> <ul> <li>SSL Support</li> </ul> <p>https://www.postgresql.org/docs/current/libpq-ssl.html</p>"},{"location":"database/cloudnative-pg/bootstrap/recovery-backup/","title":"Recovery from a backup","text":""},{"location":"database/cloudnative-pg/bootstrap/recovery-backup/#recreate-the-cluster-from-a-backup-object","title":"Recreate the cluster from a backup object","text":"<p>If we have a backup section and working backups in our cluster, the easiest way recreate a failing cnpg cluster is using a backup object.</p> <ul> <li>Choose the desired backup</li> </ul> <p>To see our backups in our namespace</p> <pre><code>kubectl get backup\n</code></pre> <ul> <li>Delete the cluster</li> </ul> <p>Then delete the cluster</p> <pre><code>kubectl delete cluster MYCLUSTER\n</code></pre> <ul> <li>Change the cluster definition</li> </ul> <p>Configure the desired backup name changing the bootstrap section</p> <pre><code>spec:\n  bootstrap:\n    recovery:\n      backup:\n        name: MYWORKINGBACKUP\n</code></pre> <ul> <li>Change where the new backups will be stored</li> </ul> <p>Change the destination of the new backups. The recovery from backup fails is cnpg find a non empty folder. I think it is a good practice to start this new cluster storing the data in a new empty folder.</p> <pre><code>spec:\n  backup:\n    barmanObjectStore:\n      serverName: ANOTHERFOLDER\n</code></pre> <ul> <li>Apply the new cluster</li> </ul> <p>Finally apply the new cluster definition</p>"},{"location":"database/cloudnative-pg/metrics/cnpg/","title":"CNPG metrics","text":""},{"location":"database/cloudnative-pg/metrics/cnpg/#cnpg_backends_total","title":"cnpg_backends_total","text":"<p>This metric total tracks how many client connections (backend) are in a PostgreSQL database instance managed by CloudNative-PG. This includes the active, idle, idle in trasaction and other connection states</p> <p>It's derived from PostgreSQL's internal connection tracking and is essential for monitoring database connection health and It's equivalent to SELECT count(*) FROM pg_stat_activity, which counts all entries in   the activity table regardless of state.</p>"},{"location":"database/cloudnative-pg/metrics/cnpg/#cnpg_backends_waiting_total","title":"cnpg_backends_waiting_total","text":"<p>Counts the total number of backend connections that are waiting (measured in seconds).</p> <p>The metric helps identify when PostgreSQL backends are stuck waiting for resources or query completion, which can indicate database performance problems.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/","title":"Dashboard and alerts","text":""},{"location":"database/cloudnative-pg/metrics/dashboard/#official-dashboard","title":"Official dashboard","text":"<p>The official cnpg grafana dashboard is located here</p> <p>https://github.com/cloudnative-pg/grafana-dashboards/blob/main/charts/cluster/grafana-dashboard.json</p> <p>Note: This metric list is based on the current dashboard version. The required metrics may change when the dashboard is updated. Always verify against the latest dashboard version.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#required-metrics","title":"Required Metrics","text":"<p>To make the official CNPG Grafana dashboard work, you need to ensure the following metrics are available in your Prometheus instance:</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#cnpg-operator-and-cluster-metrics","title":"CNPG Operator and Cluster Metrics","text":"<p>These metrics are provided by CloudNative-PG operator and PostgreSQL clusters:</p> <pre><code>cnpg_pg_replication_streaming_replicas\ncnpg_pg_replication_is_wal_receiver_up\ncnpg_pg_replication_lag\ncnpg_pg_stat_replication_write_lag_seconds\ncnpg_pg_stat_replication_flush_lag_seconds\ncnpg_pg_stat_replication_replay_lag_seconds\ncnpg_pg_postmaster_start_time\ncnpg_pg_stat_database_xact_commit\ncnpg_pg_stat_database_xact_rollback\ncnpg_backends_total\ncnpg_pg_settings_setting\ncnpg_pg_replication_in_recovery\ncnpg_pg_stat_archiver_seconds_since_last_archival\ncnpg_collector_last_available_backup_timestamp\ncnpg_collector_postgres_version\ncnpg_pg_database_size_bytes\ncnpg_collector_first_recoverability_point\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#kube-state-metrics","title":"Kube State Metrics","text":"<p>These metrics are provided by kube-state-metrics:</p> <pre><code>kube_pod_container_resource_requests\nkube_pod_status_ready\nkube_pod_container_status_ready\nkube_pod_info\nkube_node_labels\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#kubelet-metrics","title":"Kubelet Metrics","text":"<p>These metrics are exposed by the Kubelet:</p> <pre><code>kubelet_volume_stats_available_bytes\nkubelet_volume_stats_capacity_bytes\nkubelet_volume_stats_inodes_used\nkubelet_volume_stats_inodes\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#cadvisor-metrics","title":"cAdvisor Metrics","text":"<p>These metrics are provided by cAdvisor (part of Kubelet):</p> <pre><code>container_memory_working_set_bytes\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#controller-runtime-metrics","title":"Controller Runtime Metrics","text":"<p>These metrics are provided by the controller-runtime library (used by CNPG operator):</p> <pre><code>controller_runtime_reconcile_total\n</code></pre>"},{"location":"database/cloudnative-pg/metrics/dashboard/#recording-rules","title":"Recording Rules","text":"<p>The dashboard requires this recording rule to be created:</p> <pre><code>node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate\n</code></pre> <p>This rule aggregates CPU usage rates by node, namespace, pod, and container dimensions.</p> <p>Required source metric: The recording rule depends on <code>container_cpu_usage_seconds_total</code> which is provided by cAdvisor (part of Kubelet). This metric must be available in your Prometheus instance for the recording rule to work.</p>"},{"location":"database/cloudnative-pg/metrics/dashboard/#alerts","title":"Alerts","text":"<ul> <li>Default provided alerts</li> </ul> <p>https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/refs/heads/main/docs/src/samples/monitoring/alerts.yaml</p>"},{"location":"database/postgresql/98-tips/","title":"Tips","text":""},{"location":"database/postgresql/98-tips/#list-all-databases","title":"List all databases","text":"<pre><code>SELECT datname FROM pg_database WHERE datistemplate = false;\n</code></pre>"},{"location":"database/postgresql/98-tips/#list-roles","title":"List roles","text":"<pre><code>SELECT rolname FROM pg_roles;\nSELECT * FROM pg_roles;\n</code></pre>"},{"location":"database/postgresql/98-tips/#list-roles-and-attributes","title":"List roles and attributes","text":"<pre><code>SELECT r.rolname, r.rolsuper, r.rolinherit,\n  r.rolcreaterole, r.rolcreatedb, r.rolcanlogin,\n  r.rolconnlimit, r.rolvaliduntil,\n  ARRAY(SELECT b.rolname\n        FROM pg_catalog.pg_auth_members m\n        JOIN pg_catalog.pg_roles b ON (m.roleid = b.oid)\n        WHERE m.member = r.oid) as memberof\n, r.rolreplication\n, r.rolbypassrls\nFROM pg_catalog.pg_roles r\nWHERE r.rolname !~ '^pg_'\nORDER BY 1;\n</code></pre>"},{"location":"database/postgresql/98-tips/#get-the-database-owner","title":"Get the database owner","text":"<pre><code>SELECT d.datname as \"Name\",\npg_catalog.pg_get_userbyid(d.datdba) as \"Owner\"\nFROM pg_catalog.pg_database d\nWHERE d.datname = 'tmb'\nORDER BY 1;\n</code></pre>"},{"location":"database/postgresql/98-tips/#create-role-with-password-and-ddbb-with-owner","title":"Create role with password and DDBB with owner","text":"<pre><code>CREATE ROLE owner WITH LOGIN PASSWORD 'whatever';\nCREATE DATABASE database WITH OWNER = 'owner';\n</code></pre>"},{"location":"database/postgresql/98-tips/#delete-roluser","title":"Delete rol/user","text":"<pre><code>REASSIGN OWNED BY grafanareader TO postgres;\nDROP OWNED BY grafanareader;\nDROP USER grafanareader;\n</code></pre>"},{"location":"database/postgresql/98-tips/#view-the-default-privileges-in-a-specific-schema","title":"View the default privileges in a specific schema","text":"<p>To view the default privileges in a specific schema in PostgreSQL, you can query the pg_default_acl system catalog. This catalog contains information about the default access control lists (ACLs) for objects created in the database.</p> <p>Query to Get Default Privileges Here is a query to retrieve the default privileges for the tmb schema:</p> <p>Explanation</p> <ul> <li>pg_default_acl: This catalog contains the default ACLs for objects created in the database.</li> <li>pg_namespace: This catalog contains information about schemas.</li> <li>pg_roles: This catalog contains information about roles.</li> <li>defaclobjtype: The type of object the default ACL applies to (e.g., r for tables, S for sequences, f for functions).</li> <li>defaclacl: The default ACLs for the specified object type.</li> </ul> <pre><code>SELECT\n    n.nspname AS schema_name,\n    r.rolname AS role_name,\n    CASE d.defaclobjtype\n        WHEN 'r' THEN 'TABLE'\n        WHEN 'S' THEN 'SEQUENCE'\n        WHEN 'f' THEN 'FUNCTION'\n        ELSE d.defaclobjtype\n    END AS object_type,\n    d.defaclacl AS default_privileges\nFROM\n    pg_default_acl d\nJOIN\n    pg_namespace n ON n.oid = d.defaclnamespace\nJOIN\n    pg_roles r ON r.oid = d.defaclrole\nWHERE\n    n.nspname = 'myschema';\n</code></pre>"},{"location":"database/postgresql/extensions/","title":"Tips: extensions","text":"<p>To determine where a PostgreSQL extension is installed, you can query the pg_extension system catalog. This catalog contains information about all the extensions installed in the current database, including the schema in which each extension is installed.</p> <p>SQL Query to Find Extension Installation Schema Here is a query to list all extensions installed in the current database along with the schema in which each extension is installed:</p> <pre><code>SELECT\n    e.extname AS extension_name,\n    n.nspname AS schema_name\nFROM\n    pg_extension e\nJOIN\n    pg_namespace n ON e.extnamespace = n.oid\nORDER BY\n    extension_name;\n</code></pre> <p>Explanation</p> <ul> <li>pg_extension: This catalog contains information about all the extensions installed in the current database.</li> <li>pg_namespace: This catalog contains information about schemas.</li> <li>extname: The name of the extension.</li> <li>extnamespace: The OID of the schema where the extension is installed.</li> <li>nspname: The name of the schema.</li> </ul>"},{"location":"deployment/argo-rollouts/2-ways/","title":"2 Ways to create a rollout","text":"<p>There are 2 ways to create a rollout in argo rollouts</p>"},{"location":"deployment/argo-rollouts/2-ways/#single-resource","title":"Single resource","text":"<p>The first way needs to create a single rollout resource (excluding the service, ingress,..) that includes the logic of the rollout and the logic of the deployment via spec.template.</p> <p>This way does not create a deployment resource.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: rollout\n  template:\n    metadata:\n      labels:\n        app: rollout\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n  strategy:\n    blueGreen:\n      activeService: rollout\n      autoPromotionEnabled: false\n</code></pre>"},{"location":"deployment/argo-rollouts/2-ways/#separate-rollout-and-deployment","title":"Separate rollout and deployment","text":"<p>The second one is with 2 workload resources. First you create the deployment as your wish. Then you create a rollout resource without spec.template but using spec.workloadRef referencing the existing deployment.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 0 # we usually want 0 replicas here. see below\n  selector:\n    matchLabels:\n      app: deployment\n  template:\n    metadata:\n      labels:\n        app: deployment\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n</code></pre> <p>The point to define 0 replicas in the deployment is because the rollout has its own spec.replicas field. If we leave empty (1) or more replicas in the spec.replicas field of the deployment, this will deploy them in addition to the replicas managed by the rollout. And usually this is not a desired behaviour.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: deployment\nspec:\n  replicas: 3 # it is better to control them here\n  selector:\n    matchLabels:\n      app: deployment\n  workloadRef: \n    apiVersion: apps/v1\n    kind: Deployment\n    name: deployment\n  strategy:\n    blueGreen:\n      activeService: deployment\n      autoPromotionEnabled: false\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/","title":"Analysis","text":"<p>Analysis is argo workflows are tests that can be launched in a kubernetes cluster, typically inside a Rollout or using Kargo promotions, but they can be used without referencing them in that applications</p> <p>That analysis make some queries to systems like prometheus, Datadog,... Also supports web queries and executing a kubernetes job.</p> <p>We can template that analysis using AnalysisTemplate or ClusterAnalysisTemplate kubernetes resources. And they are instanciated via an AnalysisRun resource.</p>"},{"location":"deployment/argo-rollouts/analysis/#analysis-spec","title":"Analysis Spec","text":"<p>When defining an AnalysisTemplate, ClusterAnalysisTemplate or AnalysisRun we have this fields</p>"},{"location":"deployment/argo-rollouts/analysis/#specmetrics","title":"spec.metrics","text":"<p>This is where we define the queries|tests|measurements via the following fields</p> <ul> <li>name</li> </ul> <p>The name we give to the test, query or measurement</p> <ul> <li>provider</li> </ul> <p>It includes the query|test itself and provider configuration. There are some supported providers like prometheus, Datadog, CloudWatch, InfluxDB, Web, Job, Graphite,...</p> <ul> <li>initialDelay</li> </ul> <p>It adds a delay to the test execution. Example: 30s, 5m,...</p> <ul> <li>count and interval</li> </ul> <p>Count is the number of times we want to repeat the test, query or measurement and interval is the time to wait between tests. Example: 15s</p>"},{"location":"deployment/argo-rollouts/analysis/#test-query-or-measurement-result","title":"Test, query or measurement result","text":"<p>The query|test|measurement itself is done against a provider, and we can consider it as successful or failed with the SuccessCondition and failureCondition settings.</p> <p>Sometimes the query|test|measurement cannot be evaluated as successful or failed and they are considered as an inconclusive result. This could happen due to missing data, timeouts, or other issues that prevent the metric from being evaluated. One example of how analysis runs could become Inconclusive, is when a metric defines no success or failure conditions. They also can</p>"},{"location":"deployment/argo-rollouts/analysis/#handling-success-results","title":"Handling Success results","text":"<p>consecutiveSuccessLimit define the required consecutive number of successes to consider the analysis to succeed</p> <p>consecutiveSuccessLimit default value is 0 (disabled) and it is available since v1.8 release</p>"},{"location":"deployment/argo-rollouts/analysis/#handling-error-results","title":"Handling Error results","text":"<ul> <li>With failureLimit we can define the maximum number of test errors we want to tolerate.</li> </ul> <p>The default value of failureLimit is 0 so no failures are tolerated. To disable we can set it to \"-1\". failureLimit has precedence over consecutiveSuccessLimit. Also failureLimit or consecutiveSuccessLimit are not reached, the test (measurement) is considered as inconclusive.</p> <ul> <li>consecutiveErrorLimit defines the maximum number of consecutive errors that are allowed for a metric before the analysis is considered to have failed.</li> </ul>"},{"location":"deployment/argo-rollouts/analysis/#handling-inconclusive-results","title":"Handling inconclusive results","text":"<p>InconclusiveLimit sets a threshold for how many inconclusive results are acceptable during an analysis. If the number of inconclusive results exceeds this limit, the analysis is marked as failed. If inconclusiveLimit is not specified, the default behavior is to allow unlimited inconclusive results, meaning the analysis will not fail due to inconclusive results</p>"},{"location":"deployment/argo-rollouts/analysis/#example","title":"Example","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisRun\nmetadata:\n  generateName: test-\n  namespace: argocd\nspec:\n  metrics:\n    - name: argocd-app-health-sync  # name of the measurement\n      initialDelay: 30s # wait 30 seconds to start doing queries\n      count: 15 # 15 times\n      interval: 10s # every 10 seconds. Mix this when the metric is updated\n      provider:\n        prometheus:\n          address: \"http://prometheus-operated.monitoring:9090\"\n          query: |\n            argocd_app_info{name=\"my-argocd-app\",health_status=\"Healthy\", sync_status=\"Synced\"}\n      successCondition: len(result) == 1 &amp;&amp; result[0] == 1  # only 1 result with value 1\n      failureCondition: len(result) == 0 || result[0] != 1 # empty array or result not 1\n      failureLimit: 3 # tolerate 3 errors max\n      consecutiveErrorLimit: 3 # tolerate 3 consecutive errors max\n      consecutiveSuccessLimit: 8 # its ok with 8 consecutiveSuccessLimit\n      inconclusiveLimit: 2 # tolerate 2 inconclusive results\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specargs","title":"spec.args","text":"<p>Inside a template we can define arguments</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: mytemplate\nspec:\n  args:\n  - name: service-name\n  - name: prometheus-port\n</code></pre> <p>And they can used later as variables in the query</p> <pre><code>{{args.service-name}}\n{{args.prometheus-port}}\"\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specttlstrategy","title":"spec.ttlStrategy","text":"<p>ttlStrategy permits to control the the lifetime of an analysis run and delete them after a period of time. If this field is unset, the analysis controller will not delete them and they must be deleted manually or via other garbage collection policies (e.g. successfulRunHistoryLimit and unsuccessfulRunHistoryLimit).</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisRun\nspec:\n  ...\n  ttlStrategy:\n    secondsAfterCompletion: 3600\n    secondsAfterSuccess: 1800\n    secondsAfterFailure: 1800\n</code></pre>"},{"location":"deployment/argo-rollouts/analysis/#specterminate","title":"spec.terminate","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#specmeasurementretention","title":"spec.measurementRetention","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#specdryrun","title":"spec.dryRun","text":"<p>pending</p>"},{"location":"deployment/argo-rollouts/analysis/#links","title":"Links","text":"<ul> <li>Analysis &amp; Progressive Delivery</li> </ul> <p>https://argoproj.github.io/argo-rollouts/features/analysis/</p> <ul> <li>Argo Rollouts FAQ</li> </ul> <p>https://argoproj.github.io/argo-rollouts/FAQ/</p> <ul> <li>Kargo Analysis Templates Reference</li> </ul> <p>https://docs.kargo.io/user-guide/reference-docs/analysis-templates/</p>"},{"location":"deployment/cluster-api/98-tips/","title":"Tips","text":"<ul> <li>Get cluster api related crds</li> </ul> <pre><code>kubectl api-resources  | grep -i x-k8s.io\n</code></pre> <ul> <li>Get all supported providers</li> </ul> <pre><code>clusterctl config repositories\n</code></pre>"},{"location":"deployment/cluster-api/99-links/","title":"Links","text":"<ul> <li>Root official documentation book</li> </ul> <p>https://cluster-api-operator.sigs.k8s.io/</p> <ul> <li>And source</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api-operator/tree/main/docs/book</p> <ul> <li>Github</li> </ul> <p>https://github.com/kubernetes-sigs/cluster-api/</p>"},{"location":"deployment/cluster-api/99-links/#cluster-api-provider-vsphere","title":"Cluster Api Provider Vsphere","text":"<ul> <li> <p>https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/</p> </li> <li> <p>https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/main/docs/getting_started.md</p> </li> <li> <p>https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/releases/</p> </li> </ul> <p>Also includes ovas</p>"},{"location":"deployment/cluster-api/baremetal-providers/","title":"Bare Metal Infrastructure Providers","text":"<p>Cluster API provides several infrastructure providers designed specifically for bare metal environments or that support bare metal deployments. This document explores the available options and their characteristics.</p>"},{"location":"deployment/cluster-api/baremetal-providers/#overview","title":"Overview","text":"<p>Bare metal infrastructure providers enable Kubernetes cluster provisioning on physical servers without requiring a virtualization layer or cloud provider. These providers handle hardware lifecycle management, OS provisioning, and Kubernetes node bootstrapping.</p>"},{"location":"deployment/cluster-api/baremetal-providers/#dedicated-bare-metal-providers","title":"Dedicated Bare Metal Providers","text":""},{"location":"deployment/cluster-api/baremetal-providers/#metal3","title":"Metal3","text":"<p>The most mature and widely adopted bare metal provider for Cluster API.</p> <p>Key Features:</p> <ul> <li>Uses OpenStack Ironic for bare metal provisioning</li> <li>Supports hardware introspection and discovery</li> <li>Handles firmware and BIOS configuration</li> <li>Provides power management capabilities (IPMI, Redfish)</li> <li>Network boot support (PXE, iPXE)</li> </ul> <p>Architecture:</p> <ul> <li>Leverages BareMetalHost custom resources</li> <li>Integrates with Metal3-io ecosystem</li> <li>Supports multiple boot methods</li> </ul> <p>Use Cases:</p> <ul> <li>On-premises data centers</li> <li>Edge computing deployments</li> <li>Environments requiring hardware-level control</li> </ul> <p>Get started:</p> <pre><code>clusterctl init --infrastructure metal3\n</code></pre> <p>Links:</p> <ul> <li>https://github.com/metal3-io/cluster-api-provider-metal3</li> <li>https://metal3.io/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#tinkerbell","title":"Tinkerbell","text":"<p>A bare metal provisioning framework designed for scalability and flexibility.</p> <p>Key Features:</p> <ul> <li>Workflow-based provisioning system</li> <li>Microservices architecture</li> <li>Hardware data management</li> <li>DHCP and TFTP services included</li> <li>Custom workflow definitions</li> </ul> <p>Architecture:</p> <ul> <li>Workflow engine (temporal-based)</li> <li>Hardware database (Hegel)</li> <li>Network boot services (Boots)</li> <li>Image serving (Registry)</li> </ul> <p>Use Cases:</p> <ul> <li>Modern cloud-native bare metal environments</li> <li>Organizations needing customizable provisioning workflows</li> <li>Multi-tenant bare metal platforms</li> </ul> <p>Get started:</p> <pre><code>clusterctl init --infrastructure tinkerbell\n</code></pre> <p>Links:</p> <ul> <li>https://github.com/tinkerbell/cluster-api-provider-tinkerbell</li> <li>https://tinkerbell.org/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#sidero-community-maintained","title":"Sidero (Community Maintained)","text":"<p>Note: Sidero Labs is no longer actively developing Sidero Metal. The project is now community-maintained. For new deployments, Sidero Labs recommends using Omni as the alternative.</p> <p>A bare metal provider built specifically for Talos Linux.</p> <p>Key Features:</p> <ul> <li>Talos Linux native integration</li> <li>API-driven bare metal management</li> <li>Automatic hardware discovery</li> <li>Secure boot support</li> <li>Immutable infrastructure approach</li> </ul> <p>Architecture:</p> <ul> <li>Sidero Metal platform</li> <li>Integration with Talos Control Plane provider</li> <li>IPMI/Redfish for power management</li> </ul> <p>Status:</p> <ul> <li>No longer actively developed by Sidero Labs</li> <li>Community-maintained (Slack support available)</li> <li>Supports Kubernetes v1.34 and Talos v1.11</li> <li>Recommended to use Omni for new deployments</li> </ul> <p>Links:</p> <ul> <li>https://github.com/siderolabs/sidero</li> <li>https://www.sidero.dev/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#omni-sidero-labs-alternative","title":"Omni (Sidero Labs Alternative)","text":"<p>The successor to Sidero from Sidero Labs, providing a SaaS platform for Talos Linux cluster management.</p> <p>Key Features:</p> <ul> <li>Managed Talos Linux platform</li> <li>Multi-cluster management</li> <li>Works with bare metal, cloud, and edge</li> <li>Web-based UI and API</li> <li>Secure by default (WireGuard-based)</li> <li>GitOps integration</li> </ul> <p>Architecture:</p> <ul> <li>SaaS control plane (self-hosted option available)</li> <li>Lightweight agents on managed nodes</li> <li>No PXE boot infrastructure requirements</li> <li>Works with existing bare metal</li> </ul> <p>Use Cases:</p> <ul> <li>Organizations using Talos Linux</li> <li>Multi-environment deployments (bare metal + cloud)</li> <li>Simplified cluster lifecycle management</li> <li>Edge computing scenarios</li> </ul> <p>Note: Omni is not a traditional Cluster API provider but rather a complete cluster management platform that can work alongside or instead of Cluster API.</p> <p>Links:</p> <ul> <li>https://github.com/siderolabs/omni</li> <li>https://omni.siderolabs.com/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#hivelocity","title":"Hivelocity","text":"<p>A commercial bare metal cloud provider with Cluster API integration.</p> <p>Key Features:</p> <ul> <li>Managed bare metal infrastructure</li> <li>Global data center presence</li> <li>API-driven provisioning</li> <li>Network configuration management</li> <li>Storage options</li> </ul> <p>Use Cases:</p> <ul> <li>Organizations needing managed bare metal</li> <li>Multi-region bare metal deployments</li> <li>Hybrid cloud scenarios</li> </ul> <p>Links:</p> <ul> <li>https://github.com/hivelocity/cluster-api-provider-hivelocity</li> <li>https://www.hivelocity.net/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#flexiblehybrid-providers","title":"Flexible/Hybrid Providers","text":""},{"location":"deployment/cluster-api/baremetal-providers/#byoh-bring-your-own-host","title":"BYOH (Bring Your Own Host)","text":"<p>The most flexible option for existing bare metal infrastructure.</p> <p>Key Features:</p> <ul> <li>SSH-based host registration</li> <li>No special hardware requirements</li> <li>Works with any Linux distribution</li> <li>Agent-based approach</li> <li>Supports existing infrastructure</li> </ul> <p>Architecture:</p> <ul> <li>Lightweight host agent</li> <li>SSH connectivity</li> <li>No PXE boot required</li> <li>No BMC/IPMI requirements</li> </ul> <p>Use Cases:</p> <ul> <li>Legacy bare metal infrastructure</li> <li>Heterogeneous environments</li> <li>Quick proof-of-concept deployments</li> <li>Environments without IPMI/Redfish</li> </ul> <p>Get started:</p> <pre><code>clusterctl init --infrastructure byoh\n</code></pre> <p>Links:</p> <ul> <li>https://github.com/kubernetes-sigs/cluster-api-provider-bringyourownhost</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#maas-metal-as-a-service","title":"MAAS (Metal as a Service)","text":"<p>Canonical's bare metal provisioning solution.</p> <p>Key Features:</p> <ul> <li>Comprehensive hardware management</li> <li>Network configuration automation</li> <li>Storage configuration</li> <li>Integration with Juju</li> <li>Web UI and API</li> </ul> <p>Architecture:</p> <ul> <li>Central MAAS server</li> <li>Rack and region controllers</li> <li>PXE boot infrastructure</li> <li>Image management</li> </ul> <p>Use Cases:</p> <ul> <li>Ubuntu-centric environments</li> <li>Organizations using Canonical stack</li> <li>Large-scale bare metal deployments</li> </ul> <p>Links:</p> <ul> <li>https://github.com/spectrocloud/cluster-api-provider-maas</li> <li>https://maas.io/</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#k0smotron-remotemachine","title":"k0smotron RemoteMachine","text":"<p>SSH-based remote machine provisioning.</p> <p>Key Features:</p> <ul> <li>SSH connectivity</li> <li>k0s distribution focus</li> <li>Simple deployment model</li> <li>No agent required on target</li> </ul> <p>Use Cases:</p> <ul> <li>k0s Kubernetes distributions</li> <li>Simple bare metal scenarios</li> <li>Remote server provisioning</li> </ul> <p>Links:</p> <ul> <li>https://github.com/k0sproject/k0smotron</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#provider-comparison","title":"Provider Comparison","text":"Provider Maturity Complexity Hardware Discovery Boot Method Special Requirements Metal3 High Medium-High Yes PXE/iPXE IPMI/Redfish Tinkerbell Medium Medium Yes PXE DHCP/TFTP infrastructure Sidero Medium (Community) Medium Yes PXE Talos Linux (not actively developed) Omni Medium Low No Agent-based Talos Linux, Omni account Hivelocity Medium Low N/A Managed Hivelocity account BYOH Medium Low No N/A SSH access MAAS High Medium-High Yes PXE MAAS server k0smotron Low-Medium Low No N/A SSH access"},{"location":"deployment/cluster-api/baremetal-providers/#choosing-a-provider","title":"Choosing a Provider","text":""},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-metal3","title":"When to use Metal3","text":"<ul> <li>Standard bare metal with IPMI/Redfish</li> <li>Need hardware introspection</li> <li>Want mature, community-supported solution</li> <li>OpenStack Ironic familiarity</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-tinkerbell","title":"When to use Tinkerbell","text":"<ul> <li>Need flexible, workflow-based provisioning</li> <li>Modern microservices architecture preference</li> <li>Custom provisioning requirements</li> <li>Building bare metal platform</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-omni","title":"When to use Omni","text":"<ul> <li>Standardizing on Talos Linux</li> <li>Want managed cluster lifecycle</li> <li>Multi-cluster, multi-environment deployments</li> <li>Prefer SaaS or simplified operations</li> <li>Security is primary concern</li> <li>Need GitOps integration</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-sidero","title":"When to use Sidero","text":"<ul> <li>Existing Sidero deployments (migration not required)</li> <li>Community support is acceptable</li> <li>Cannot use Omni (on-premises only, no SaaS)</li> <li>For new deployments: Consider Omni, Metal3, or BYOH instead</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-byoh","title":"When to use BYOH","text":"<ul> <li>Working with existing infrastructure</li> <li>No IPMI/Redfish available</li> <li>Quick testing/POC</li> <li>Heterogeneous hardware</li> <li>SSH access only</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#when-to-use-maas","title":"When to use MAAS","text":"<ul> <li>Ubuntu environment</li> <li>Need comprehensive hardware lifecycle</li> <li>Want integrated networking/storage</li> <li>Canonical ecosystem</li> </ul>"},{"location":"deployment/cluster-api/baremetal-providers/#getting-provider-information","title":"Getting Provider Information","text":"<p>List all available infrastructure providers:</p> <pre><code>clusterctl config repositories | grep InfrastructureProvider\n</code></pre> <p>Get specific provider versions:</p> <pre><code>clusterctl config repositories | grep InfrastructureProvider | grep metal3\n</code></pre>"},{"location":"deployment/cluster-api/baremetal-providers/#common-requirements","title":"Common Requirements","text":"<p>Most bare metal providers share these requirements:</p> <ol> <li>Network Infrastructure</li> <li>DHCP server (for PXE-based providers)</li> <li>TFTP server (for network boot)</li> <li>DNS resolution</li> <li> <p>Network connectivity to BMC interfaces</p> </li> <li> <p>Management Network</p> </li> <li>Separate management network for BMC/IPMI</li> <li>Out-of-band management access</li> <li> <p>Network segmentation</p> </li> <li> <p>Storage</p> </li> <li>Boot images repository</li> <li>OS image storage</li> <li> <p>Container registry access</p> </li> <li> <p>Credentials</p> </li> <li>BMC/IPMI credentials</li> <li>SSH keys</li> <li>API tokens (for cloud bare metal)</li> </ol>"},{"location":"deployment/cluster-api/baremetal-providers/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with BYOH for testing, migrate to full-featured providers for production</li> <li>Network Planning: Design network topology before deployment</li> <li>Credential Management: Use secrets management for BMC credentials</li> <li>Image Management: Maintain updated OS images</li> <li>Monitoring: Implement hardware monitoring (temperature, power, etc.)</li> <li>Documentation: Document hardware inventory and network topology</li> </ol>"},{"location":"deployment/cluster-api/baremetal-providers/#additional-resources","title":"Additional Resources","text":"<ul> <li>Cluster API Providers Reference</li> <li>Cluster API Book</li> <li>Metal3 Documentation</li> <li>Tinkerbell Documentation</li> <li>Omni Documentation</li> <li>Talos Linux Documentation</li> </ul>"},{"location":"deployment/cluster-api/concepts/","title":"Concepts","text":""},{"location":"deployment/cluster-api/concepts/#coreprovider","title":"CoreProvider","text":"<p>A component responsible for providing the fundamental building blocks of the Cluster API. It defines and implements the main Cluster API resources such as Clusters, Machines, and MachineSets, and manages their lifecycle. This includes:</p> <p>Defining the main Cluster API resources and their schemas. Implementing the logic for creating, updating, and deleting these resources. Managing the overall lifecycle of Clusters, Machines, and MachineSets. Providing the base upon which other providers like BootstrapProvider and InfrastructureProvider build.</p> <p>Only the cluster-api CoreProvider is available</p>"},{"location":"deployment/cluster-api/concepts/#bootstrapprovider","title":"BootstrapProvider","text":"<p>A component responsible for turning a server into a Kubernetes node as well as for:</p> <ul> <li>Generating the cluster certificates, if not otherwise specified</li> <li>Initializing the control plane, and gating the creation of other nodes until it is complete</li> <li>Joining control plane and worker nodes to the cluster</li> </ul> <p>Get all:</p> <pre><code>clusterctl config repositories | grep BootstrapProvider\n</code></pre> <p>Examples: kubeadm, talos, microk8s, rke2,...</p>"},{"location":"deployment/cluster-api/concepts/#controlplaneprovider","title":"ControlPlaneProvider","text":"<p>A component responsible for managing the control plane of a Kubernetes cluster. This includes:</p> <p>Provisioning the control plane nodes. Managing the lifecycle of the control plane, including upgrades and scaling.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep ControlPlaneProvider\n</code></pre> <p>Examples: kubeadm, talos, microk8s, rke2,...</p>"},{"location":"deployment/cluster-api/concepts/#infrastructureprovider","title":"InfrastructureProvider","text":"<p>A component responsible for the provisioning of infrastructure/computational resources required by the Cluster or by Machines (e.g. VMs, networking, etc.). For example, cloud Infrastructure Providers include AWS, Azure, and Google, and bare metal Infrastructure Providers include VMware, MAAS, and metal3.io.</p> <p>Examples: aws, azure, gcp, harvester, vsphere, metal3, vcluster, openstack, docker, byoh, ...</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep InfrastructureProvider\n</code></pre>"},{"location":"deployment/cluster-api/concepts/#ipamprovider","title":"IPAMProvider","text":"<p>A component that manages pools of IP addresses using Kubernetes resources. It serves as a reference implementation for IPAM providers, but can also be used as a simple replacement for DHCP.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep IPAMProvider\n</code></pre> <p>Examples: in-cluster, nutanix</p>"},{"location":"deployment/cluster-api/concepts/#runtimeextensionprovider","title":"RuntimeExtensionProvider","text":"<p>Get all:</p> <pre><code>clusterctl config repositories | grep RuntimeExtensionProvider\n</code></pre> <p>Examples: nutanix</p>"},{"location":"deployment/cluster-api/concepts/#addonprovider","title":"AddonProvider","text":"<p>A component that extends the functionality of Cluster API by providing a solution for managing the installation, configuration, upgrade, and deletion of Cluster add-ons using Helm charts.</p> <p>Get all:</p> <pre><code>clusterctl config repositories | grep AddonProvider\n</code></pre>"},{"location":"deployment/cluster-api/deployment-operator/","title":"Deployment using operator","text":"<p>This example uses kubeadm and vsphere provider</p>"},{"location":"deployment/cluster-api/deployment-operator/#deploy-cert-manager","title":"Deploy cert-manager","text":"<p>It is a requirement</p>"},{"location":"deployment/cluster-api/deployment-operator/#deploy-the-operator","title":"Deploy the operator","text":"<p>Using the operator-components.yaml file from  https://github.com/kubernetes-sigs/cluster-api-operator/releases</p> <p>This deploys capi-operator-system namespace and the namespaced crds for the providers</p>"},{"location":"deployment/cluster-api/deployment-operator/#coreprovider-kubeadm","title":"CoreProvider kubeadm","text":"<p>Choose the release from https://github.com/kubernetes-sigs/cluster-api/releases The deployed manifest name is core-components.yaml and includes the capi-system namespace and the following</p> <ul> <li>Cluster</li> <li>ClusterClass</li> <li>ClusterResourceSet</li> <li>ClusterResourceSetBinding</li> <li>ExtensionConfig</li> <li>IPAddress</li> <li>IPAddressClaim</li> <li>Machine</li> <li>MachineDeployment</li> <li>MachineDrainRule</li> <li>MachineHealthCheck</li> <li>MachinePool</li> <li>MachineSet</li> </ul> <p>For example:</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: CoreProvider\nmetadata:\n  name: cluster-api\nspec:\n  version: v1.11.1\n</code></pre>"},{"location":"deployment/cluster-api/deployment-operator/#bootstrapprovider-kubeadm","title":"BootstrapProvider kubeadm","text":"<p>Use the same release as CoreProvider The deployed manifest is bootstrap-components.yaml and in kubedm includes the following CRDs:</p> <ul> <li>KubeadmConfig</li> <li>KubeadmConfigTemplate</li> </ul> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: BootstrapProvider\nmetadata:\n  name: kubeadm\nspec:\n  version: v1.11.1\n</code></pre>"},{"location":"deployment/cluster-api/deployment-operator/#controlplaneprovider","title":"ControlPlaneProvider","text":"<p>Use the same release as CoreProvider The deployed manifest is control-plane-components.yaml and includes the following CRDs</p> <ul> <li>KubeadmControlPlane</li> <li>KubeadmControlPlaneTemplate</li> </ul> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: ControlPlaneProvider\nmetadata:\n  name: kubeadm\nspec:\n  version: v1.11.1\n</code></pre>"},{"location":"deployment/cluster-api/deployment-operator/#infraestructureprovider-vsphere","title":"InfraestructureProvider vsphere","text":"<p>We need a vsphere-settings secret with credentials and settings. See the release and configuration options in https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/</p> <pre><code>apiVersion: operator.cluster.x-k8s.io/v1alpha2\nkind: InfrastructureProvider\nmetadata:\n  name: vsphere\nspec:\n  version: v1.14.0\n  configSecret:\n    name: vsphere-settings\n</code></pre> <p>This adds the following cluster scoped crds</p> <ul> <li>VSphereClusterIdentity</li> <li>VSphereDeploymentZone</li> <li>VSphereFailureDomain</li> </ul> <p>and namespace scoped crds</p> <ul> <li>VSphereCluster</li> <li>VSphereClusterTemplate</li> <li>VSphereMachine</li> <li>VSphereMachineTemplate</li> <li>VSphereVM</li> </ul>"},{"location":"deployment/crossplane/providers-and-mrap/","title":"Crossplane Providers and Managed Resource Activation Policies","text":""},{"location":"deployment/crossplane/providers-and-mrap/#overview","title":"Overview","text":"<p>This guide explains the relationship between Providers and Managed Resource Activation Policies (MRAP) in Crossplane v2, and how to configure them effectively.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#understanding-providers","title":"Understanding Providers","text":""},{"location":"deployment/crossplane/providers-and-mrap/#what-are-providers","title":"What Are Providers?","text":"<p>Providers are packages that extend Crossplane with the ability to manage external resources (AWS, GCP, Azure, Kubernetes, etc.). Each provider:</p> <ul> <li>Installs Custom Resource Definitions (CRDs) for managed resources</li> <li>Runs a controller that reconciles those resources</li> <li>Manages the lifecycle of cloud infrastructure</li> </ul>"},{"location":"deployment/crossplane/providers-and-mrap/#provider-types","title":"Provider Types","text":"<p>Monolithic Providers (legacy):</p> <ul> <li>Single package containing all resources for a cloud provider</li> <li>Example: <code>provider-aws</code> (includes S3, EC2, RDS, etc.)</li> <li>Larger footprint, slower updates</li> </ul> <p>Family Providers (modular):</p> <ul> <li>Split into smaller, focused packages</li> <li>Example: <code>provider-aws-s3</code>, <code>provider-aws-ec2</code>, <code>provider-aws-rds</code></li> <li>Install only what you need</li> <li>Better performance and faster updates</li> </ul>"},{"location":"deployment/crossplane/providers-and-mrap/#installing-providers","title":"Installing Providers","text":"<p>Providers are installed using the <code>Provider</code> resource:</p> <pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-aws-s3\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws-s3:v2.0.0\n</code></pre> <p>When a provider is installed:</p> <ol> <li>Crossplane downloads the package</li> <li>CRDs are created in the cluster</li> <li>Provider controller is deployed</li> <li>Provider becomes available for use</li> </ol> <p>Check provider status:</p> <pre><code>kubectl get providers\nkubectl get providerrevisions\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#understanding-managed-resource-activation-policies-mrap","title":"Understanding Managed Resource Activation Policies (MRAP)","text":""},{"location":"deployment/crossplane/providers-and-mrap/#what-are-mraps","title":"What Are MRAPs?","text":"<p>MRAPs are Crossplane v2 resources that control which managed resource types Crossplane actively reconciles.</p> <p>Key concept: Installing a provider creates CRDs, but MRAP determines if Crossplane watches and reconciles resources of those types.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#why-mraps-exist","title":"Why MRAPs Exist","text":"<p>Performance: Limit reconciliation to only the resources you actually use Security: Explicit control over what resource types can be managed Multi-tenancy: Different namespaces can have different activation policies Scalability: Reduce overhead in clusters with many providers installed</p>"},{"location":"deployment/crossplane/providers-and-mrap/#the-default-catch-all-mrap","title":"The Default Catch-All MRAP","text":"<p>After upgrading to Crossplane v2, a default catch-all MRAP may be created that activates all managed resource types.</p> <p>Problem with catch-all:</p> <ul> <li>Reconciles every CRD, even unused ones</li> <li>Poor performance at scale</li> <li>Less explicit control</li> <li>Higher resource consumption</li> </ul> <p>Best practice: Delete the default catch-all and create specific MRAPs.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#creating-specific-mraps","title":"Creating Specific MRAPs","text":"<p>Create targeted activation policies for only the resource types you use:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-storage-resources\nspec:\n  activate:\n    - buckets.s3.aws.upbound.io          # v1 cluster-scoped\n    - buckets.s3.aws.m.upbound.io        # v2 namespaced\n    - bucketpolicies.s3.aws.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#provider-mrap-workflow","title":"Provider + MRAP Workflow","text":"<p>The complete flow for using managed resources:</p> <pre><code>1. Install Provider\n   \u2514\u2500&gt; Creates CRDs (e.g., buckets.s3.aws.upbound.io)\n\n2. Create MRAP\n   \u2514\u2500&gt; Activates reconciliation for specific CRD types\n\n3. Deploy ProviderConfig\n   \u2514\u2500&gt; Configures credentials for the provider\n\n4. Create Managed Resources\n   \u2514\u2500&gt; Crossplane reconciles them (because MRAP activates them)\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#installation-example","title":"Installation Example","text":""},{"location":"deployment/crossplane/providers-and-mrap/#step-1-install-the-provider","title":"Step 1: Install the Provider","text":"<pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-aws-s3\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws-s3:v2.0.0\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-2-create-mrap","title":"Step 2: Create MRAP","text":"<pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-s3-activation\nspec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-3-configure-provider-credentials","title":"Step 3: Configure Provider Credentials","text":"<pre><code>apiVersion: aws.m.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-config\n  namespace: production\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      key: credentials\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#step-4-create-managed-resource","title":"Step 4: Create Managed Resource","text":"<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  name: my-bucket\n  namespace: production\nspec:\n  forProvider:\n    region: us-east-1\n  providerConfigRef:\n    name: aws-config\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#mrap-best-practices","title":"MRAP Best Practices","text":""},{"location":"deployment/crossplane/providers-and-mrap/#1-be-specific","title":"1. Be Specific","text":"<p>Only activate resource types you actually use:</p> <p>Bad (activates everything):</p> <pre><code>spec:\n  activate:\n    - \"*\"  # Avoid this\n</code></pre> <p>Good (explicit list):</p> <pre><code>spec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - instances.ec2.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#2-include-both-v1-and-v2-during-migration","title":"2. Include Both v1 and v2 During Migration","text":"<p>During the v1 to v2 migration period, include both cluster-scoped and namespaced versions:</p> <pre><code>spec:\n  activate:\n    - buckets.s3.aws.upbound.io      # v1 (cluster-scoped)\n    - buckets.s3.aws.m.upbound.io    # v2 (namespaced)\n</code></pre> <p>Once migration is complete, remove v1 entries.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#3-organize-by-function","title":"3. Organize by Function","text":"<p>Create separate MRAPs for different resource groups:</p> <pre><code>---\napiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: storage-resources\nspec:\n  activate:\n    - buckets.s3.aws.m.upbound.io\n    - bucketpolicies.s3.aws.m.upbound.io\n---\napiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: compute-resources\nspec:\n  activate:\n    - instances.ec2.aws.m.upbound.io\n    - securitygroups.ec2.aws.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#4-document-your-activation-policies","title":"4. Document Your Activation Policies","text":"<p>Maintain clear documentation of which MRAPs exist and why, so teams know what resources are available.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#managing-multiple-providers","title":"Managing Multiple Providers","text":"<p>When using multiple providers (AWS, GCP, Azure), create organized MRAPs:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: multi-cloud-storage\nspec:\n  activate:\n    # AWS S3\n    - buckets.s3.aws.m.upbound.io\n    # GCP Storage\n    - buckets.storage.gcp.m.upbound.io\n    # Azure Storage\n    - accounts.storage.azure.m.upbound.io\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/crossplane/providers-and-mrap/#resources-not-reconciling","title":"Resources Not Reconciling","text":"<p>Problem: Created a managed resource but it's not being reconciled</p> <p>Solutions:</p> <ol> <li>Check if provider is installed and healthy: <code>kubectl get providers</code></li> <li>Verify MRAP includes the resource type:    <code>kubectl get managedresourceactivationpolicy -o yaml</code></li> <li>Check provider logs for errors</li> </ol>"},{"location":"deployment/crossplane/providers-and-mrap/#mrap-not-found","title":"MRAP Not Found","text":"<p>Problem: Error about missing MRAP after upgrade</p> <p>Solution: Create an MRAP for the resource types you're using. In v2, MRAPs are required.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#too-many-resources-activated","title":"Too Many Resources Activated","text":"<p>Problem: Performance issues, high memory usage</p> <p>Solution: Review MRAPs and remove unused resource types from activation lists.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#checking-active-resources","title":"Checking Active Resources","text":"<p>To see which resource types are activated:</p> <pre><code>kubectl get managedresourceactivationpolicy -o yaml\n</code></pre> <p>To list all CRDs installed by providers:</p> <pre><code>kubectl get crd | grep -E '(aws|gcp|azure).*upbound'\n</code></pre>"},{"location":"deployment/crossplane/providers-and-mrap/#migration-considerations","title":"Migration Considerations","text":"<p>When migrating from v1 to v2:</p> <ol> <li>Providers continue working - Existing providers don't need    immediate changes</li> <li>MRAPs are new - You must create MRAPs in v2 for reconciliation    to work</li> <li>Both versions coexist - Include both v1 and v2 resource types    in MRAPs during migration</li> <li>Cleanup after migration - Remove v1 resource types from MRAPs    once migration is complete</li> </ol>"},{"location":"deployment/crossplane/providers-and-mrap/#summary","title":"Summary","text":"Component Purpose Required? Provider Installs CRDs and controllers Yes MRAP Activates reconciliation for resource types Yes (v2) ProviderConfig Configures credentials Yes Managed Resource Actual infrastructure to create Yes <p>Key takeaway: Providers install capabilities, MRAPs activate them, ProviderConfigs configure them, and Managed Resources use them.</p>"},{"location":"deployment/crossplane/providers-and-mrap/#references","title":"References","text":"<ul> <li>Crossplane Provider Documentation</li> <li>Managed Resource Activation Policies</li> <li>Upbound Marketplace</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/","title":"Upgrading Crossplane from v1 to v2","text":""},{"location":"deployment/crossplane/upgrading-to-v2/#overview","title":"Overview","text":"<p>This guide covers the complete upgrade process from Crossplane v1 to v2, with a focus on environments using standalone managed resources (not Compositions).</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#prerequisites","title":"Prerequisites","text":"<p>Before upgrading to Crossplane v2, ensure:</p> <ol> <li>Running Crossplane v1.20 - Upgrade to v1.20 first if on an earlier version</li> <li>Remove deprecated features:</li> <li>Native patch and transform compositions (replaced by composition functions)</li> <li>ControllerConfig type (replaced by DeploymentRuntimeConfig)</li> <li>External secret stores (no longer supported)</li> <li>Default registry flags (use fully qualified image names)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#whats-new-in-v2","title":"What's New in v2","text":"<ul> <li>Namespaced managed resources - Resources are now namespace-scoped instead of cluster-scoped</li> <li>Managed Resource Activation Policies (MRAP) - Control which resources Crossplane reconciles</li> <li>Composition functions - More flexible composition approach</li> <li>Improved multi-tenancy - Better isolation with namespace-scoped resources</li> <li>Breaking changes - Some v1 features removed</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#upgrade-process","title":"Upgrade Process","text":""},{"location":"deployment/crossplane/upgrading-to-v2/#step-1-pre-upgrade-assessment","title":"Step 1: Pre-Upgrade Assessment","text":"<ol> <li>Verify Crossplane version (check that you're running v1.20)</li> <li>Inventory your resources (list all managed resources and providers)</li> <li>Check for deprecated features:</li> <li>Review compositions for native patch and transform</li> <li>Check for ControllerConfig resources</li> <li>Verify no external secret stores are configured</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-2-upgrade-crossplane-core","title":"Step 2: Upgrade Crossplane Core","text":"<ol> <li>Upgrade to v2</li> <li>Verify the upgrade (check deployment and logs)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-3-upgrade-provider-packages","title":"Step 3: Upgrade Provider Packages","text":"<ol> <li>Update provider manifests</li> </ol> <p>Ensure providers use fully qualified image names and v2-compatible versions:</p> <p><code>yaml    apiVersion: pkg.crossplane.io/v1    kind: Provider    metadata:      name: provider-aws-s3    spec:      package: xpkg.upbound.io/upbound/provider-aws-s3:v2</code></p> <ol> <li>Deploy updated providers</li> </ol> <p>Apply the updated provider manifests.</p> <ol> <li>Verify provider health</li> </ol> <p>Check that providers are healthy and running:</p> <p><code>bash    kubectl get providers    kubectl get providerrevisions</code></p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-4-verify-and-test","title":"Step 4: Verify and Test","text":"<ol> <li>Check that existing cluster-scoped managed resources continue working</li> <li>Verify providers are healthy and running</li> <li>Test that existing infrastructure remains functional</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#complete-upgrade-checklist","title":"Complete Upgrade Checklist","text":"<ul> <li>[ ] Verify running Crossplane v1.20</li> <li>[ ] Inventory all managed resources and providers</li> <li>[ ] Remove deprecated features (ControllerConfig, external secret stores, etc.)</li> <li>[ ] Upgrade Crossplane core to v2</li> <li>[ ] Upgrade provider packages to v2-compatible versions</li> <li>[ ] Verify all existing resources continue working</li> <li>[ ] Test existing infrastructure functionality</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#best-practices","title":"Best Practices","text":"<ol> <li>Test in staging first - Perform the upgrade in a non-production environment</li> <li>Gradual migration - Migrate resources incrementally, not all at once</li> <li>Keep both versions during transition - Maintain both cluster-scoped and namespaced resources during migration</li> <li>Use orphan deletion policy - Set <code>deletionPolicy: Orphan</code> for safety during migration</li> <li>Monitor closely - Watch logs and metrics during and after the upgrade</li> <li>Document your process - Keep detailed notes of your specific migration steps</li> <li>Backup configurations - Export all resource definitions before upgrading</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#day-2-operations","title":"Day 2 Operations","text":"<p>After successfully upgrading Crossplane to v2, you can begin adopting v2 features at your own pace. Existing v1 cluster-scoped resources continue working indefinitely alongside new v2 namespaced resources.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#understanding-coexistence","title":"Understanding Coexistence","text":"<p>Crossplane v2 supports both cluster-scoped (v1) and namespaced (v2) resources simultaneously. This allows you to:</p> <ul> <li>Run v2 without immediately migrating existing resources</li> <li>Test namespaced resources while keeping production on cluster-scoped resources</li> <li>Migrate resources gradually over time</li> <li>Maintain both resource types indefinitely if needed</li> </ul>"},{"location":"deployment/crossplane/upgrading-to-v2/#configure-managed-resource-activation-policies-mrap","title":"Configure Managed Resource Activation Policies (MRAP)","text":"<p>In Crossplane v2, you must explicitly specify which managed resources should be reconciled using MRAPs.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-1-review-default-mrap","title":"Step 1: Review Default MRAP","text":"<p>By default, Crossplane v2 may create a catch-all MRAP that activates all resource types. Review existing policies:</p> <pre><code>kubectl get managedresourceactivationpolicy -o yaml\n</code></pre>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-2-create-targeted-mraps","title":"Step 2: Create Targeted MRAPs","text":"<p>Create activation policies for your specific resource types:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1alpha1\nkind: ManagedResourceActivationPolicy\nmetadata:\n  name: aws-resources\nspec:\n  activate:\n    - buckets.s3.aws.upbound.io        # v1 cluster-scoped\n    - buckets.s3.aws.m.upbound.io      # v2 namespaced\n    - instances.ec2.aws.upbound.io\n    - instances.ec2.aws.m.upbound.io\n</code></pre> <p>Important: Include both cluster-scoped (<code>.aws.upbound.io</code>) and namespaced (<code>.aws.m.upbound.io</code>) resource types during the migration period.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#step-3-delete-default-catch-all-optional","title":"Step 3: Delete Default Catch-All (Optional)","text":"<p>For better performance and security, delete the default catch-all MRAP after creating specific policies.</p> <p>See the Providers and MRAP guide for more details.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#migrating-to-namespaced-managed-resources","title":"Migrating to Namespaced Managed Resources","text":"<p>When ready, you can migrate existing cluster-scoped resources to namespaced versions.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#understanding-the-resource-changes","title":"Understanding the Resource Changes","text":"<ul> <li>v1 (cluster-scoped): <code>s3.aws.upbound.io/v1beta2</code></li> <li>v2 (namespaced): <code>s3.aws.m.upbound.io/v1beta1</code></li> </ul> <p>The <code>.m.</code> indicates a namespaced managed resource.</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#migration-strategies","title":"Migration Strategies","text":"<p>Choose based on your resource type and tolerance for downtime:</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#option-a-create-first-migration","title":"Option A: Create-First Migration","text":"<p>Use when duplicate resources are acceptable or downtime is tolerable.</p> <ol> <li>Deploy the new namespaced resource</li> <li>Wait for the resource to become ready</li> <li>Update application references (ConfigMaps, Secrets, etc.)</li> <li>Delete the old cluster-scoped resource</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#option-b-orphan-and-adopt-migration","title":"Option B: Orphan-and-Adopt Migration","text":"<p>Use for resources with globally unique names (like S3 buckets) or zero-downtime requirements.</p> <ol> <li>Set <code>deletionPolicy: Orphan</code> on the old resource</li> <li>Delete the old Crossplane resource (cloud resource remains)</li> <li>Deploy the new namespaced resource with the same cloud resource name</li> <li>Verify the resource was adopted (check events and status)</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#migrate-providerconfigs","title":"Migrate ProviderConfigs","text":"<p>ProviderConfigs also need to be namespaced:</p> <p>Before (v1)</p> <pre><code>apiVersion: aws.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-provider\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      namespace: crossplane-system\n      key: credentials\n</code></pre> <p>After (v2)</p> <pre><code>apiVersion: aws.m.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: aws-provider\n  namespace: production-infrastructure\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      name: aws-credentials\n      key: credentials\n</code></pre> <p>Note: The secret reference no longer needs a namespace when ProviderConfig is namespaced (it uses the same namespace).</p>"},{"location":"deployment/crossplane/upgrading-to-v2/#create-namespace-structure","title":"Create Namespace Structure","text":"<p>Plan and create namespaces for your managed resources:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production-infrastructure\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: staging-infrastructure\n</code></pre>"},{"location":"deployment/crossplane/upgrading-to-v2/#exploring-other-v2-features","title":"Exploring Other v2 Features","text":"<p>Beyond namespaced resources, consider exploring:</p> <ol> <li>Composition functions - More powerful than patch and transform</li> <li>Multi-tenancy - Use namespaces to isolate teams/environments</li> <li>Refined MRAP policies - Optimize which resources are actively reconciled</li> <li>Updated monitoring - Adjust dashboards for namespace-scoped resources</li> </ol>"},{"location":"deployment/crossplane/upgrading-to-v2/#references","title":"References","text":"<ul> <li>Crossplane v2 Upgrade Guide</li> <li>Managed Resource Activation Policies</li> <li>Upbound Provider Documentation</li> </ul>"},{"location":"git/detached-head/","title":"Detached head","text":"<p>A detached HEAD is a Git state where HEAD points directly to a specific commit instead of pointing to a branch reference.</p> <ul> <li>Normal state</li> </ul> <pre><code>git checkout branch\n</code></pre> <pre><code>HEAD -&gt; main -&gt; commit abc123\nHEAD points to main, which points to a commit.\n</code></pre> <ul> <li>Detached HEAD:</li> </ul> <pre><code>git checkout commitid\n</code></pre> <pre><code>HEAD -&gt; commit abc123\nHEAD points directly to a commit, not through a branch.\n</code></pre>"},{"location":"git/detached-head/#notes","title":"Notes","text":"<ul> <li> <p>CI/CD pipelines often checkout specific commits (like GitLab does with git checkout 35ff590b)</p> </li> <li> <p>If you want to push the branch, you can get this error</p> </li> </ul> <pre><code>error: src refspec MYBRANCH does not match any\nerror: failed to push some refs to 'MYREPO'\n</code></pre> <p>This is because git looks for a local branch named MYBRANCH to push. In detached HEAD state, there's no local main branch reference, so it fails.</p> <p>Solution: Using git push origin HEAD:MYBRANCH pushes whatever HEAD points to (the commit) directly to the remote main branch</p>"},{"location":"gitops/get-raw-yaml/","title":"Ways to get a raw yaml","text":""},{"location":"gitops/get-raw-yaml/#from-kustomize","title":"From kustomize","text":"<p>If using kustomize based specifications, we can use</p> <pre><code>kustomize build\nkubectl kustomize\n</code></pre>"},{"location":"gitops/get-raw-yaml/#from-helm","title":"From helm","text":"<p>If using native helm, we can use</p> <pre><code>helm template\n</code></pre> <p>If using helmCharts inside a kustomization yaml, we wan use</p> <pre><code>kustomize build --enable-helm\nkubectl kustomize --enable-helm\n</code></pre> <p>https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/helmcharts/</p> <p>It has some limitations and it will be deprecated in favour of KRM functions</p> <p>https://github.com/kubernetes-sigs/kustomize/issues/4401</p>"},{"location":"gitops/get-raw-yaml/#using-krm-kpt-functions","title":"Using KRM (kpt) functions","text":"<p>KRM functions are pluggable, reusable scripts or containers that automate the transformation, validation, and generation of Kubernetes YAML resources, enabling powerful and flexible configuration workflows.</p> <p>However, support in kustomize and kubectl kustomize is still experimental or alpha, and may not be recommended for production use yet. The ecosystem is growing, but you should check the documentation and maturity of the specific tool and function you plan to use.</p> <p>https://kpt.dev/book/02-concepts/03-functions</p>"},{"location":"gitops/get-raw-yaml/#gitops","title":"Gitops","text":""},{"location":"gitops/get-raw-yaml/#from-an-argocd-applcation","title":"From an argocd applcation","text":"<p>We can use</p> <pre><code>argocd app manifest &lt;app-name&gt;\n</code></pre>"},{"location":"gitops/get-raw-yaml/#fluxcd","title":"Fluxcd","text":"<p>FluxCD\u2019s source-controller can fetch and output raw manifests from Git, Helm, or OCI sources.</p>"},{"location":"gitops/argocd/argocd-secret-eso/","title":"argocd-secret via extenalsecret","text":""},{"location":"gitops/argocd/argocd-secret-eso/#intro","title":"Intro","text":"<p>The argocd-secret kubernetes secret stores some information self managed by argocd like:</p> <pre><code>admin.password\nadmin.passwordMtime\nserver.secretkey\ntls.crt\ntls.key\n</code></pre> <p>It also supports other settings related with the webhooks token</p> <pre><code>webhook.github.secret: \nwebhook.gitlab.secret: \nwebhook.bitbucket.uuid: \nwebhook.bitbucketserver.secret: \nwebhook.gogs.secret: \n</code></pre> <p>and also additional users/accounts related info</p> <p>We can get that info with</p> <pre><code>kubectl get secret argocd-secret -o yaml --show-managed-fields\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#the-problem","title":"The problem","text":"<p>The problem with this is that argocd is the owner of those fields. If we want to patch this content, mixing that selfmanaged fields with other controllers, like external-secrets-operator, we can get some unexpected results, like the deletion of the managed fields</p> <p></p> <p>If the deletion occurs, those fields can be recreated restarting the argocd-server server</p> <pre><code>kubectl rollout restart deployment argocd-server -n argocd\n</code></pre> <p>Also,the annotation argocd.argoproj.io/tracking-id can be change all the time between:</p> <pre><code>myapp:external-secrets.io/ExternalSecret:argocd/argocd-secret\nmyapp:/Secret:argocd/argocd-secret\n</code></pre> <p>Making the owner of that field different at every change</p> <ul> <li>argocd-controller</li> <li>externalsecrets.external-secrets.io/argocd-secret</li> </ul>"},{"location":"gitops/argocd/argocd-secret-eso/#solution-1-via-externalsecret","title":"Solution 1: Via ExternalSecret","text":"<ul> <li>argocd.argoproj.io/compare-options: IgnoreExtraneous</li> </ul> <p>Because the owner changes all the time, this ignores that frequent out of sync only.</p> <ul> <li> <p>argocd.argoproj.io/sync-options: Prune=false</p> </li> <li> <p>creationPolicy: Merge</p> </li> </ul> <p>The external secrets operator does not create the secret. It only merges the values with an existing secret</p> <ul> <li>deletionPolicy: Merge</li> </ul> <p>If the secret is deleted from the provider, external secrets operator simply removes the keys from the secret, not the secret itself</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: argocd-secret\nspec:\n  data:\n    - remoteRef:\n        key: secret/argocd-webhook-token\n      secretKey: webhook.github.secret\n  target:\n    creationPolicy: Merge\n    deletionPolicy: Merge\n    template:\n      metadata:\n        annotations:\n          argocd.argoproj.io/sync-options: Prune=false\n          argocd.argoproj.io/compare-options: IgnoreExtraneous\n...\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#solution-2-move-our-setting-to-another-externalsecret","title":"Solution 2: Move our setting to another (external)secret","text":"<p>Argocd permits to put the value of a configmap or secret in our own custom secret. This permits to let external secrets operator to manage that external secret without touch the original one.</p> <ul> <li>Create our custom secret</li> </ul> <p>First we have to create an (external)secret with our value</p> <p>It must have the app.kubernetes.io/part-of: argocd label</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: argocd-custom-secret\nspec:\n  data:\n    - remoteRef:\n        key: secret/argocd-webhook-token \n      secretKey: webhook.github.secret\n  target:\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/part-of: argocd\n...\n</code></pre> <ul> <li>Link the original place with the custom secret</li> </ul> <p>Then we go to the original place where that value is configured an make a link to our new secret with the following format:</p> <pre><code>KEY: $CUSTOM-SECRET-NAME:KEY-IN-CUSTOM-SECRET\n</code></pre> <p>For example</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-secret\ntype: Opaque\nstringData:\nwebhook.github.secret: $argocd-custom-secret:webhook.github.secret\n</code></pre> <ul> <li>Restart workload</li> </ul> <p>Depending of what workload and setting is, we can probably need to restart the workload. For example</p> <pre><code>kubectl rollout restart deployment argocd-server -n argocd\n</code></pre>"},{"location":"gitops/argocd/argocd-secret-eso/#notes","title":"Notes","text":"<ul> <li> <p>This solution puts $argocd-custom-secret:webhook.github.secret as the value in the beginning. The restart changes it with the value in the custom secret.</p> </li> <li> <p>If we are doing the link with a kustomize patch and it is not applied, we must probably delete the secret first. This removes the self managed data including admin password and certificates, and the argocd needs to be restarted to recreate them</p> </li> </ul>"},{"location":"gitops/argocd/settings-stable/","title":"Settings table","text":""},{"location":"gitops/argocd/settings-stable/#list-of-argocd-settings","title":"List of argocd settings","text":"<pre><code>AppC = application controller\nServer = argocd server\nAppSC = applicationset controller\nRepo = repo server\n</code></pre> Variable Setting in the configmap configmap AppC Server AppSC Repo REDIS_PASSWORD argocd-redis x x x ARGOCD_CONTROLLER_REPLICAS hardcoded x ARGOCD_RECONCILIATION_TIMEOUT timeout.reconciliation argocd-cm x x ARGOCD_HARD_RECONCILIATION_TIMEOUT timeout.hard.reconciliation argocd-cm x ARGOCD_RECONCILIATION_JITTER timeout.reconciliation.jitter argocd-cm x ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS controller.repo.error.grace.period.seconds argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH argocd-cmd-params-cm x ARGOCD_APP_STATE_CACHE_EXPIRATION argocd-cmd-params-cm x REDIS_SERVER argocd-cmd-params-cm x x x REDIS_COMPRESSION argocd-cmd-params-cm x x x REDISDB argocd-cmd-params-cm x x x ARGOCD_DEFAULT_CACHE_EXPIRATION argocd-cmd-params-cm x x x ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS argocd-cmd-params-cm x ARGOCD_APPLICATION_NAMESPACES argocd-cmd-params-cm x ARGOCD_CONTROLLER_SHARDING_ALGORITHM argocd-cmd-params-cm x ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT argocd-cmd-params-cm x ARGOCD_K8SCLIENT_RETRY_MAX argocd-cmd-params-cm x x ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF argocd-cmd-params-cm x x ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF argocd-cmd-params-cm x ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT argocd-cmd-params-cm x ARGOCD_HYDRATOR_ENABLED argocd-cmd-params-cm x x ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING argocd-cmd-params-cm x ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL argocd-cmd-params-cm x KUBECACHEDIR hardcoded x ARGOCD_SERVER_INSECURE argocd-cmd-params-cm x ARGOCD_SERVER_BASEHREF argocd-cmd-params-cm x ARGOCD_SERVER_ROOTPATH argocd-cmd-params-cm x ARGOCD_SERVER_LOGFORMAT argocd-cmd-params-cm x ARGOCD_SERVER_LOG_LEVEL argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER argocd-cmd-params-cm x ARGOCD_SERVER_DISABLE_AUTH argocd-cmd-params-cm x ARGOCD_SERVER_ENABLE_GZIP argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS argocd-cmd-params-cm x ARGOCD_SERVER_X_FRAME_OPTIONS argocd-cmd-params-cm x ARGOCD_SERVER_CONTENT_SECURITY_POLICY argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_SERVER_REPO_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER_PLAINTEXT argocd-cmd-params-cm x ARGOCD_SERVER_DEX_SERVER_STRICT_TLS argocd-cmd-params-cm x ARGOCD_TLS_MIN_VERSION argocd-cmd-params-cm x x ARGOCD_TLS_MAX_VERSION argocd-cmd-params-cm x x ARGOCD_TLS_CIPHERS argocd-cmd-params-cm x x ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_OIDC_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_LOGIN_ATTEMPTS_EXPIRATION argocd-cmd-params-cm x ARGOCD_SERVER_STATIC_ASSETS argocd-cmd-params-cm x ARGOCD_APP_STATE_CACHE_EXPIRATION argocd-cmd-params-cm x ARGOCD_MAX_COOKIE_NUMBER argocd-cmd-params-cm x ARGOCD_SERVER_LISTEN_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_METRICS_LISTEN_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_ADDRESS argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_INSECURE argocd-cmd-params-cm x ARGOCD_SERVER_OTLP_HEADERS argocd-cmd-params-cm x ARGOCD_APPLICATION_NAMESPACES argocd-cmd-params-cm x ARGOCD_SERVER_ENABLE_PROXY_EXTENSION argocd-cmd-params-cm x ARGOCD_API_CONTENT_TYPES argocd-cmd-params-cm x ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS argocd-cmd-params-cm x ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS argocd-cmd-params-cm x"},{"location":"gitops/argocd/applicationset/templating/","title":"Templating","text":"<p>There are 2 ways to define the application template that an applicationset will create:</p> <ul> <li>Fast template</li> <li>Go template + Sprig function library</li> </ul>"},{"location":"gitops/argocd/applicationset/templating/#fast-template","title":"Fast template","text":"<p>By default, an argocd applicationset uses fastemplate as template engine to define an application.</p> <p>According the argocd documentation, it will be deprecated. https://github.com/argoproj/argo-cd/issues/10858</p>"},{"location":"gitops/argocd/applicationset/templating/#go-text-template","title":"Go text template","text":"<p>It will replace fast template and it must be activated in the applicationset definition with goTemplate: true. The go text template is more powerful than the fast template and provides some functions</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: my appset\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n</code></pre> <p>In addition to the go template, also the Sprig function library can be used with goTemplate enabled</p>"},{"location":"gitops/argocd/applicationset/templating/#gotemplateoptions","title":"goTemplateOptions","text":"<p>It is possible to add some goTemplateOptions The recommendeed goTemplateOptions is [\"missingkey=error\"]. This makes the applicationset fail is a parameter defined in the template does not exist.</p>"},{"location":"gitops/argocd/applicationset/templating/#template-patch","title":"Template Patch","text":"<p>pending</p>"},{"location":"gitops/argocd/applicationset/templating/#links","title":"Links","text":"<ul> <li>Argocd Templates</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Template/</p> <ul> <li>Argocd Go Template</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/GoTemplate/</p> <ul> <li>fasttemplate</li> </ul> <p>https://github.com/valyala/fasttemplate</p> <ul> <li>Go text template</li> </ul> <p>https://pkg.go.dev/text/template</p> <ul> <li>Sprig function library</li> </ul> <p>https://masterminds.github.io/sprig/</p>"},{"location":"gitops/argocd/clusters/add-a-cluster/","title":"Add a cluster","text":"<p>We want to update the credentials for a kubernetes cluster added to argocd.</p> <p>This has been tested with a kubeadm cluster (not a eks cluster or similar)</p> <p>We will have</p> <ul> <li>The kubeconfig file of the argocd main instance that will receive the new cluster</li> <li>The kubeconfig file of the argocd instance we want to add to the main instance</li> </ul>"},{"location":"gitops/argocd/clusters/add-a-cluster/#login-to-the-argocd-main-instance","title":"Login to the argocd main instance","text":"<p>For that we need:</p> <pre><code>argocd login --grpc-web MY-ARGOCD-SERVER # Log in our argocd server\nargocd login --sso --grpc-web MY-ARGOCD-SERVER # or via sso\n</code></pre>"},{"location":"gitops/argocd/clusters/add-a-cluster/#context","title":"Context","text":"<p>Get the context for the cluster we want to add</p> <pre><code>KUBECONFIG=PATH_TO_KUBECONFIG kubectl config get-contexts\n</code></pre>"},{"location":"gitops/argocd/clusters/add-a-cluster/#add-a-temporary-cluster","title":"Add a temporary cluster","text":"<p>Then add the cluster with a temporary name</p> <pre><code>argocd cluster add MYCONTEXT --kubeconfig PATH_TO_KUBECONFIG --name CLUSTER_NAME --project MY_PROJECT --cluster-endpoint kubeconfig --grpc-web \n</code></pre> <p>This creates</p> <ul> <li>a ServiceAccount \"argocd-manager\"</li> <li>a ClusterRole \"argocd-manager-role\"</li> <li>a ClusterRoleBinding \"argocd-manager-role-binding\"</li> <li>a Created bearer token secret for ServiceAccount \"argocd-manager\"</li> </ul>"},{"location":"gitops/argocd/clusters/add-a-cluster/#notes","title":"Notes","text":"<p>The temporary name permits to explore the new secret created in the argocd namespace and get the config key that stores the cluster configuration</p> <p>Copying that config to an existing cluster fixes the following error:</p> <pre><code>the server has asked for the client to provide credentials\n</code></pre>"},{"location":"gitops/argocd/deletion/00-index/","title":"Deletion","text":"<p>There are several ways to disable automatic deletion of things in argocd</p>"},{"location":"gitops/argocd/deletion/00-index/#settings-table","title":"Settings table","text":"Setting Level Goal finalizer ApplicationSet Permits to enable foreground/background deletion of Application spec.syncPolicy.applicationsSync ApplicationSet / Controller Prevent an/all ApplicationSet delete applications spec.syncPolicy.preserveResourcesOnDeletion ApplicationSet Does not delete Application resources whe the Applicationset is deleted finalizer Application Non-cascade does not delete Application resources when the Application is deleted spec.syncPolicy.automated Application If ommited, disables applying new changes spec.syncPolicy.automated.prune Application \"False\" disables pruning resources not defined in the target state dry-run mode Controller Prevent all ApplicationSet doing actions argocd.argoproj.io/sync-options: Delete=false Resource argocd.argoproj.io/sync-options: Prune=false Resource"},{"location":"gitops/argocd/deletion/10-application/","title":"Application deletion","text":""},{"location":"gitops/argocd/deletion/10-application/#finalizers","title":"Finalizers","text":"<p>A kubernetes finalizer gives the responsability to a controller to prevent resource deletion. In argocd we can specify a finalizer in an Application via metadata.finalizers field.</p> <p>In this case, the  finalizer configures how the kubernetes resources defined in the application will be deleted.</p>"},{"location":"gitops/argocd/deletion/10-application/#cascade-deletion","title":"Cascade deletion","text":"<p>Configuring a finalizer in an Application enables the cascade deletion. The application is not deleted is not deleted inmediately. Kubernetes marks the resource for deletion, but delegates the deletion of the child resources to the Argocd controller, that performs cleanup before allowing the resource to be deleted.</p> <p>There are 2 ways to perform a cascade deletion, also called propagation policies:</p>"},{"location":"gitops/argocd/deletion/10-application/#foreground-cascade-propagation-policy","title":"foreground cascade propagation policy","text":"<p>This is a synchronous deletion. The deletion of the Application is locked until all child resources are successfully deleted, when the controller removes the finalizer. Slower, but this does a clean an ordered deletion of child resources.</p> <p>The foreground cascade deletion finalizer is resources-finalizer.argocd.argoproj.io</p> <pre><code>metadata:\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\n</code></pre>"},{"location":"gitops/argocd/deletion/10-application/#background-cascade-propagation-policy","title":"background cascade propagation policy","text":"<p>This is an asynchronous deletion. The Argocd controller initiates the deletion of the child resources in the background, but the deletion of the Application is not locked. It is a faster but may leave orphaned resources if deletion fails.</p> <p>The background cascade deletion finalizer is resources-finalizer.argocd.argoproj.io/background</p> <pre><code>metadata:\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io/background\n</code></pre>"},{"location":"gitops/argocd/deletion/10-application/#non-cascading-orphan-deletion","title":"Non cascading (orphan) deletion","text":"<p>When deleting Application with no finalizer, no resources will be deleted, only the Application</p> <p>This can be useful:</p> <ul> <li>when we want to keep resources running but manage them differently</li> <li>moving resources between Application or ApplicationSet without downtime</li> <li>remove the Application but don't want to risk deleting critical resources</li> </ul> <p>The default finalizer for an Application is foreground cascade deletion</p> <ul> <li>The ApplicationSet has no default finalizer</li> </ul>"},{"location":"gitops/argocd/deletion/10-application/#application-manual-deletion","title":"Application manual deletion","text":"<p>Via argocd binary</p> <pre><code># foreground\nargocd app delete APPNAME \nargocd app delete APPNAME --cascade # or\nargocd app delete APPNAME --cascade --propagation-policy foreground # or\n# background\nargocd app delete APPNAME --cascade --propagation-policy background # or\n# orphan\nargocd app delete APPNAME --cascade=false\n</code></pre> <p>Via kubectl</p> <pre><code># foreground\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io\"]}}' --type merge\nkubectl delete app APPNAME\n# background\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io/background\"]}}' --type merge\nkubectl delete app APPNAME\n# orphan\nkubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": null}}' --type merge\nkubectl delete app APPNAME\n</code></pre>"},{"location":"gitops/argocd/deletion/20-applicationset/","title":"ApplicationSet","text":""},{"location":"gitops/argocd/deletion/20-applicationset/#relation-between-the-the-applicationset-and-generated-applications","title":"Relation between the the ApplicationSet and generated Applications","text":"<p>The generated Applications by an ApplicationSet have:</p> <ul> <li>In .metadata.ownerReferences, a reference to the ApplicationSet as owner</li> <li>In .metadata.finalizers a resources-finalizer.argocd.argoproj.io finalizer if the ApplicationSet has .syncPolicy.preserveResourcesOnDeletion as false</li> <li>By default they have the resources-finalizer.argocd.argoproj.io finalizer</li> </ul>"},{"location":"gitops/argocd/deletion/20-applicationset/#deleting-an-applicationset","title":"Deleting an ApplicationSet","text":"<p>When an ApplicationSet is deleted, this will occur in order</p> <ul> <li> <p>the ApplicationSet is deleted</p> </li> <li> <p>the generated Applications are deleted (because of the owner reference)</p> </li> <li> <p>the deployed resources created in that Application are deleted</p> </li> </ul> <p>There are 3 ways to control how this deletion is done via finalizers</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#default-no-finalizer","title":"Default (no finalizer)","text":"<p>By default an ApplicationSet has not finalizer. This means the argocd applicationset controller will not manage the deletion of the ApplicationSet. It will be done using kubernetes garbage collector.</p> <ul> <li>Nothing blocks or delays its deletion. The ApplicationSet is deleted inmediately</li> <li>This performs a cascade deletion of the Applications and resources, because of the owner reference</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/</p> <p>If we want to delete an ApplicationSet resource, while preventing Applications (and their deployed resources) from being deleted, we can use a non-cascading delete:</p> <pre><code>kubectl delete ApplicationSet (NAME) --cascade=orphan\n</code></pre>"},{"location":"gitops/argocd/deletion/20-applicationset/#using-argocd-finalizer","title":"Using argocd finalizer","text":"<p>We can add a finalizer an ApplicationSet. This makes the applicationset controller responsible to manage how the Applications and resources are deleted</p> <ul> <li>Foreground</li> </ul> <p>The foreground finalizer blocks deletion until all Applications are deleted and ensures complete and ordered cleanup.</p> <ul> <li>Background</li> </ul> <p>The background finalizer initiates the deletion in the background. Faster, but may leave resources if deletion of child resources fails.</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#preserve-applications-resources","title":"Preserve Application's resources","text":"<p>If we want to preserve the deletion of the Application's resources we can enable spec.syncPolicy.preserveResourcesOnDeletion in the ApplicationSet.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  syncPolicy:\n    preserveResourcesOnDeletion: true\n</code></pre> <p>This removes the resources-finalizer.argocd.argoproj.io finalizer in the generated Applications so a non cascading (orphan) deletion is performed in the Application</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#applicationset-permissions","title":"ApplicationSet permissions","text":"<p>We can configure if an ApplicationSet can create, update and delete its discovered applications.</p> <p>This is different than when an ApplicationSet is deleted.</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#dry-run","title":"Dry run","text":"<p>At controller level we can disable all modifications that the ApplicationSets can do in Application in an argocd instance.</p> <p>This is done enabling the dryrun mode (--dryrun parameter) via the data.ApplicationSetcontroller.dryrun key in the argocd-cmd-params-cm ConfigMap</p>"},{"location":"gitops/argocd/deletion/20-applicationset/#policy-at-applicationset-level","title":"Policy at ApplicationSet level","text":"<p>Also we can control the individual actions an ApplicationSet can to in its applications via the spec.syncPolicy.applicationsSync setting with the following values:</p> Action create update delete sync (default) \u2611 \u2611 \u2611 create-only \u2611 \u2612 \u2612 create-delete \u2611 \u2612 \u2611 create-update \u2611 \u2611 \u2612 dry run mode \u2612 \u2612 \u2612 <p>So a to prevent that deletion we can use, for example</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  syncPolicy:\n    applicationsSync: create-update\n</code></pre> <p>This also can be configured at ApplicationSet controller level via --policy parameter. This setting exists in the data.ApplicationSetcontroller.policy key in the argocd-cmd-params-cm ConfigMap. This setting takes precedence over all ApplicationSets configuration although we can change this behaviour via data.ApplicationSetcontroller.enable.policy.override in the argocd-cmd-params-cm ConfigMap</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/","title":"Safe changes to an Applicationset","text":"<p>Changing an Applicationset can be dangerous because by default it can create and remove applications. So this is a workaround to avoid disgusting situations</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-disable-autosync-in-parent-application","title":"Preparation: Disable autosync in parent Application","text":"<p>!!! danger \"This is critical\"</p> <pre><code>If the Applicationset itself is deployed using gitops, disable autosync (at least, the prune option) in the application that manages our Applicationset.\n</code></pre>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-disable-autosync-in-generated-applications","title":"Preparation: Disable autosync in generated Applications","text":"<p>This is optional, depending of the changes. We can disable autoSync in the generated Applications (spec.template.spec.syncPolicy)</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#preparation-prevent-applications-being-deleted","title":"Preparation: Prevent applications being deleted","text":"<p>If your have modified things in the the generators, this can potentially delete applications.</p> <p>We can control this adding temporarily the spec.syncPolicy.applicationsSync: create-update setting to our Applicationset. This prevents our Applicationset from deleting applications.</p> <p>But we have a problem here. How to identify what applications will be deleted when we remove the create-update setting from the Applicationset?. We can add a version label to the generated applications in the current (old) Applicationset.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Applicationset\nmetadata:\n  name: myappset\nspec:\n  syncPolicy:\n    applicationsSync: create-update\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/version: v0.0.1\n</code></pre> <p>Commit and sync this changes to the current Applicationset</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#add-your-changes","title":"Add your changes","text":"<p>Next step is to add the changes:</p> <ul> <li>The changes you really want to do to the Applicationset</li> <li>Increase the version label to v0.0.2</li> </ul> <p>Commit and push the changes. The changes to the Applicationset will not be applied because we have disable autoSync</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#check-name","title":"Check name","text":"<p>With our Applicationset with pending changes:</p> <ul> <li>Explore the differences between both versions</li> <li>Check the Applicationset's name does not change and the old one will not be deleted (pruned).</li> </ul> <p>!!! danger \"\"</p> <pre><code>The default behaviour when deleting an Applicationset is delete the Applicationset itself and all the generated applications and resources.\n</code></pre>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#sync-your-changes","title":"Sync your changes","text":"<p>If you are ok with the changes sync the Applicationset resource</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#check-if-it-will-delete-applications","title":"Check if it will delete Applications","text":"<p>The new changes are propagated to the Applications. We must check the labels in the Applications generated by our Applicationset. The Applications with v0.0.2 are ok, but if any has v0.0.1, they will be delete when removing applicationsSync</p> <p>!!! warning \"\"     Remember to check the label only in the applications that this Applicationset generates</p>"},{"location":"gitops/argocd/deletion/21-safe-changes-appset/#remove-protections","title":"Remove protections","text":"<p>If all generated Applications are ok and with v0.0.2, we can</p> <ul> <li>remove the applicationsSync option</li> <li>enable autoSyn in the application that manages our Applicationset</li> </ul>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/","title":"Safe changes to Resources","text":""},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#remove-from-argocd-not-from-the-cluster","title":"Remove from argocd, not from the cluster","text":"<p>If we want to remove a kubernetes resources from an argocd Application we can make it in different ways.</p>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#with-the-tracking-method","title":"With the tracking method","text":"<p>When argocd manages a kubernetes marks it to know it is being managed.</p> <p>Since argocd 3.0 the default tracking method is adding an annotation to the resource</p> <pre><code>argocd.argoproj.io/tracking-id\n</code></pre> <p>Until 3.0 the default tracking method was adding the app.kubernetes.io/instance label, and that can cause some errors because it is a well known label included in some applications. In this case it is a good practice to change the tracking method or changing the label.</p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/resource_tracking/</p> <p>STEPS</p> <ul> <li> <p>First of all the more secure starting point is to disable autoSync in the Application or, at least, selfHeal to false</p> </li> <li> <p>Then we only need to delete that tracking method from the resource, for example the annotation, but autosync and self heal will be add it. So we need</p> </li> <li> <p>Next we can edit that resource and remove the tracking annotation. The resource will be shown out of sync. This can be done via kubectl or argocd web interface.</p> </li> <li> <p>Then we push the changes to git where we declare we dont want the resource in that Application. Refresh the Application and the resource will dissapear but it will not be deleted from the cluster</p> </li> </ul>"},{"location":"gitops/argocd/deletion/41-safe-changes-resources/#with-orphan-deletion","title":"With orphan deletion","text":"<p>This is faster but it is also potentially less secure because implies a deletion.</p> <ul> <li> <p>First of all disable autosync in the application</p> </li> <li> <p>Then remove the resource from the argocd ui using Non-cascading (Orphan) Delete. The resource will be shown out of sync.</p> </li> </ul> <p> </p> <ul> <li>Then we push the changes to git where we declare we dont want the resource in that Application. Refresh the Application and the resource will dissapear but it will not be deleted from the cluster</li> </ul>"},{"location":"gitops/argocd/deletion/99-links/","title":"Links","text":"<ul> <li>Controlling if/when the ApplicationSet controller modifies Application resources</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/ApplicationSet/Controlling-Resource-Modification/</p> <ul> <li>Application Pruning &amp; Resource Deletion</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/ApplicationSet/Application-Deletion/</p> <ul> <li>Automated Sync Policy</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p> <ul> <li>Sync Options</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/</p> <ul> <li>App Deletion</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/app_deletion/</p> <ul> <li>Everything You Ever Wanted to Know About Deletion and Argo CD Finalizers but Were Afraid to Ask</li> </ul> <p>https://codefresh.io/blog/argocd-application-deletion-finalizers/</p>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/","title":"ApplicationSet Not Found UI Error","text":""},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#problem-description","title":"Problem Description","text":"<p>After upgrading ArgoCD from 2.x to 3.x, non admin users with project-scoped permissions see this error in the UI:</p> <pre><code>Unable to load data: ApplicationSet XXX not found in any namespace\n</code></pre>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#root-cause","title":"Root Cause","text":"<p>In argocd 3.0 a RBAC change was introduced.</p> <p>Prior to argocd v3.0, policies granting update and delete to applications also applied to sub-resources (like ApplicationSet Starting with v3.0, update and delete actions only apply to the application itself. ArgoCD 3.0+ disabled permission inheritance by default, requiring explicit permissions for each resource type.</p> <p>The ArgoCD UI tries to load ApplicationSet information when displaying applications (for context/breadcrumbs), but users lack the necessary permissions.</p> <p>Notes</p> <ul> <li>This is a UI display issue, not a functional problem</li> <li>Admin users are not affected (they have global permissions)</li> </ul>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#solutions","title":"Solutions","text":"<ul> <li>Restore old behaviour</li> </ul> <p>It is possible to enable the previous behaviour with this setting in the argocd-cm configmap</p> <pre><code>server.rbac.disableApplicationFineGrainedRBACInheritance: \"false\"\n</code></pre> <p>\u26a0\ufe0f Warning: This affects all RBAC inheritance behavior, not just ApplicationSets.</p> <ul> <li>Add Explicit ApplicationSet Permissions (Recommended)</li> </ul> <p>Another option is to give minimal ApplicationSet permissions</p> <pre><code>p, proj:MYPROJECT:MYROLE, applicationsets, get, PROJECT/*, allow\n</code></pre> <p>Testing</p> <pre><code>argocd account can-i get applicationsets 'PROJECT/*'\n</code></pre>"},{"location":"gitops/argocd/errors/applicationset-not-found-ui-error/#related-issues","title":"Related Issues","text":"<ul> <li>GitHub Issue #23571: ApplicationSet not found Toast</li> <li>ArgoCD 3.0 Migration Guide</li> <li>ApplicationSet Security Documentation</li> </ul>"},{"location":"gitops/argocd/errors/cache-key-missing/","title":"cache: key is missing","text":"<p>After an update we get this error in the web interface</p> <pre><code>error getting cached app managed resources: cache: key is missing\n</code></pre> <p>It seems the solution is restarting the argocd-application-controller</p> <pre><code>kubectl rollout restart statefulset -n argocd argocd-application-controller\n</code></pre>"},{"location":"gitops/argocd/errors/update-to-2.12/","title":"Update to 2.12 breaks appset","text":"<p>Until argocd 2.11 there should be a relation between the repository url defined in the repository secret and the repoURL configured in the application and applicationset.</p> <p>Since the 2.12 release there is another restriction. If you have a repository with a project configured, only the applications that belongs to that project can use the repository. So</p> <p>But there is another problem related with the Git generator in applicationsets. The applicationset cannot belong to a project, so all the repositories used by that applicationsets must not have a project defined, or they will fail</p> <p>Links</p> <ul> <li>v2.11 to 2.12</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/upgrading/2.11-2.12/</p> <ul> <li>2.12.x throws could not read Username for 'https://gitlab.com' from the UI</li> </ul> <p>https://github.com/argoproj/argo-cd/issues/19585</p> <ul> <li>rpc error: code = Internal desc</li> </ul> <p>https://github.com/argoproj/argo-cd/issues/19174</p> <p>If you delete the project from the repositories and the errors in the appset don't dissapear, you can force a restart  with kubectl -n argocd rollout restart sts,deploy</p>"},{"location":"gitops/argocd/errors/update-to-2.12/#cache-key-is-missing","title":"cache: key is missing","text":"<p>After an update we get this error in the web interface</p> <pre><code>error getting cached app managed resources: cache: key is missing\n</code></pre> <p>It seems the solution is restarting the argocd-application-controller</p> <pre><code>kubectl rollout restart statefulset -n argocd argocd-application-controller\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/","title":"Change admin user","text":"<p>Argocd documentation recommends to disable the admin user.</p>"},{"location":"gitops/argocd/login-credentials/disable-admin/#new-user","title":"New user","text":"<p>First we will create a new user that will have adminpermissions</p>"},{"location":"gitops/argocd/login-credentials/disable-admin/#create-the-new-user","title":"Create the new user","text":"<p>To add the user we must add it in the argocd-cm ConfigMap. The user will have login permissions.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  accounts.myuser: login\n  accounts.myuser.enabled: \"true\"\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#make-it-administrator","title":"Make it administrator","text":"<p>There is a predefined role called admin that our user will receive. Will will only remove all default permissions.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.default: \"\"\n  policy.csv: |\n    g, myuser, role:admin\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#assign-password","title":"Assign password","text":"<p>Get the value of the default admin password</p> <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 --decode; echo\n</code></pre> <p>Login to our argocd instance</p> <pre><code>argocd login \"fqdn of the server\"\n</code></pre> <p>And change give the new user a password</p> <pre><code>argocd account update-password --account myuser --grpc-web\n</code></pre> <p>Test login with the new user</p> <pre><code>argocd logout \"fqdn of the server\"\nargocd login \"fqdn of the server\"\n</code></pre>"},{"location":"gitops/argocd/login-credentials/disable-admin/#disable-admin-user","title":"Disable admin user","text":"<p>Change the default admin password (optional)</p> <pre><code>argocd account update-password --account admin --grpc-web\n</code></pre> <p>Disable the admin user</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  admin.enabled: \"false\"\n</code></pre> <p>And delete delete the original secret</p> <p>```shell kubectl -n argocd delete secret argocd-initial-admin-secret</p>"},{"location":"gitops/argocd/notifications/0-index/","title":"Argocd Notifications","text":""},{"location":"gitops/argocd/notifications/0-index/#notification-services","title":"Notification services","text":"<p>Argocd supports several notification services:</p> <ul> <li>Alertmanager</li> <li>AWS SQS</li> <li>Email</li> <li>GitHub</li> <li>Google Chat</li> <li>Grafana</li> <li>Mattermost</li> <li>NewRelic</li> <li>Opsgenie</li> <li>Overview</li> <li>PagerDuty</li> <li>PagerDuty V2</li> <li>Pushover</li> <li>Rocket.Chat</li> <li>Slack</li> <li>Teams</li> <li>Telegram</li> <li>Webex Teams</li> <li>Webhook</li> </ul>"},{"location":"gitops/argocd/notifications/0-index/#install-the-notifications-catalog","title":"Install the notifications catalog","text":"<p>We can customize the notifications catalog, but we can use the official provided one. The stable url of this manifest is located here:</p> <pre><code>https://raw.githubusercontent.com/argoproj/argo-cd/stable/notifications_catalog/install.yaml\n</code></pre>"},{"location":"gitops/argocd/notifications/0-index/#setup-subscriptions","title":"Setup subscriptions","text":"<p>There are 2 places where to setup what notifications to send</p>"},{"location":"gitops/argocd/notifications/0-index/#default-subscriptions","title":"Default subscriptions","text":"<p>The default subscriptions are defined in in the argocd-notifications-cm configMap in the data.subscriptions field.</p> <p>This settings sends notifications when all the applications have the health degraded to the \"MYRECIPIENT\" destination.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  subscriptions: |\n    - recipients:\n      - MYRECIPIENT\n      triggers:\n      - on-health-degraded\n</code></pre> <p>In this configmap we also configure that recipient for the notification service we want to use</p>"},{"location":"gitops/argocd/notifications/0-index/#at-application-level","title":"At application level","text":"<p>At application level we can choose what events we want to receive adding an annotation to the application using this format</p> <pre><code>notifications.argoproj.io/subscribe.&lt;trigger&gt;.&lt;service&gt;: &lt;recipient&gt;\n</code></pre> <p>The trigger is the event. We have this default triggers</p> <ul> <li>on-created</li> <li>on-deleted</li> <li>on-deployed</li> <li>on-health-degraded</li> <li>on-sync-failed</li> <li>on-sync-running</li> <li>on-sync-status-unknown</li> <li>on-sync-succeeded</li> </ul> <p>The service is the notification service or provider.</p> <p>Finally the recipient is configured in the argocd-notifications-cm configMap. We can separate more than one recipient separated with \";\"</p> <p>An example to send when the application is has the unknown status via telegram to the sre and developers recipient is adding this annotation to the application:</p> <pre><code>notifications.argoproj.io/subscribe.on-sync-status-unknown.telegram: sre;developers\n</code></pre>"},{"location":"gitops/argocd/notifications/0-index/#links","title":"Links","text":"<ul> <li>Notifications Overview</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/</p> <ul> <li>Notification subscriptions</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/subscriptions/</p> <ul> <li>Triggers</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/triggers/</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/","title":"Send notifications to Microsoft Teams","text":""},{"location":"gitops/argocd/notifications/1-microsoft-teams/#create-the-webhoook","title":"Create the webhoook","text":"<p>First we need to create a webhook in Microsoft Teams.</p> <p>Choose the desired channel and go to \"Manage channel\" &gt; \"Conectors\" &gt; \"Edit\" and add an incoming webhook. At the end we get and Url we must copy and configure in argocd</p> <p>We can configure a custom image for this webhook</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#configure-argocd-notifications-cm-configmap","title":"Configure argocd-notifications-cm configmap","text":"<p>In this configmap we can setup the recipients, the destinations. If we want to add a chanel called argocdNotify, the configmap can be</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  service.teams: |\n    recipientUrls:\n      argocdNotify: $channel-teams-url\n</code></pre> <p>Do not change $channel-teams-url with the former url</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#configure-argocd-notifications-cm-secret","title":"Configure argocd-notifications-cm secret","text":"<p>Argocd will search the url in the argocd-notifications-cm secret</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-notifications-cm\nstringData:\n  channel-teams-url: MYWEBHOOKURL\n</code></pre> <p>Don't forget to setup subscriptions</p>"},{"location":"gitops/argocd/notifications/1-microsoft-teams/#links","title":"Links","text":"<ul> <li>Teams</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/teams/</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/","title":"Reconciliation","text":""},{"location":"gitops/argocd/reconciliation/00-reconciliation/#application-sources","title":"Application sources","text":"<p>Argocd permits to define 3 repository types (Application sources) and we can use them to define the desired state of the cluster:</p> <ul> <li>Git repositories</li> <li>Helm repositories</li> <li>Oci registries</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#what-is-the-reconciliation-process","title":"What is the reconciliation process","text":"<p>The reconcilation is a discovery process where some tasks are done:</p> <ul> <li>argocd-repo-server checks if there are changes in the application sources (and update the stored cache)</li> <li>argocd-repo-server generates the final manifests (desired state)</li> <li>argocd-application-controller checks if there are differences with the live state (drift detection)</li> </ul> <p>If there are any Git changes, Argo CD will only update applications with the auto-sync setting enabled</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#argocd-repo-server-operations","title":"argocd-repo-server operations","text":""},{"location":"gitops/argocd/reconciliation/00-reconciliation/#recheck-repository-and-store-in-cache","title":"Recheck repository and store in cache","text":"<p>argocd-repo-server checks every certain time if there are changes in the Application source (repository) and stores the most recent one in the redis cache</p> <p>The frequency is controlled this way:</p> <pre><code>CLI parameter: --revision-cache-expiration\nDefault: 3m\nSetting: timeout.reconciliation in argocd-cm ConfigMAp\nENV: ARGOCD_RECONCILIATION_TIMEOUT\n</code></pre> <ul> <li>In a git source checks if the latest commit SHA for the desired branch/tag has changed</li> <li>In a helm source, checks for the index.yaml file</li> <li>In an OCi repository checks the tag list</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#render-manifests-and-store-in-cache","title":"Render manifests and store in cache","text":"<p>Using that Application source cache, the argocd-repo-server:</p> <ul> <li>Generates the final manifests that represents the desired (target) state using the  the proper tool (kustomize, helm template, ...)</li> <li>Stores them in the redis cache.</li> </ul> <p>This is done for every application and returns the generated manifests to the application-controller.</p> <p>It is possible to control the expiration of that cache with this:</p> <pre><code>CLI parameter: --repo-cache-expiration  \nDefault: 24h\nSetting: reposerver.repo.cache.expiration  in argocd-cmd-params-cm ConfigMAp\nENV: ARGOCD_REPO_CACHE_EXPIRATION\n</code></pre> <p>There is an additional  per application tunning that makes a new commit be ignored and consider our cache as valid, so no new manifests generation will be launched. This is done via argocd.argoproj.io/manifest-generate-paths annotation, that tells argocd the paths in the git repo must change to trigger a new manifest generation in our application.</p> <ul> <li>Repo Server Cache Code</li> </ul> <p>https://github.com/argoproj/argo-cd/blob/master/reposerver/cache/cache.go</p> <ul> <li>Command line</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/server-commands/argocd-repo-server/</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#drift-detection-application-controller","title":"Drift detection (application controller)","text":"<p>The application-controller compares that final manifests (desired/target state) with the live (real) state in the cluster in order to detect drift.</p> <ul> <li>If a drift is detected, the application is consideres \"Out of Sync\"</li> <li>Applying the desired state is part of the Sync process, not the reconciliation process and it can triggered automatically enabl AutoSync</li> </ul> <p>In the application controller, the ARGOCD_RECONCILIATION_TIMEOUT (timeout.reconciliation in the argocd-cm ConfigMap) controls how often the application-controller checks all applications for drift, regardless of whether any changes occurred. This is:</p> <ul> <li>If no timeout.reconciliation is configured, the default value is 120s</li> <li>Valid values are duration strings (5m, 3h, 5d,..)</li> <li>A zero value disables this reconciliation operation</li> <li>The argocd-repo-server and argocd-application-controller must be restarted to apply this setting</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#jitter","title":"Jitter","text":"<p>There are some situations where the reconciliation operation can be delayed because of a large number of applications. We can give more extra time to the refresh operation via the timeout.reconciliation.jitter setting.</p> <ul> <li>Valid values are duration strings (5m, 3h, 5d,..)</li> <li>A zero value disables the jitter</li> <li>The default value is 60 seconds</li> </ul> <p>Example: if the sync timeout is 3 minutes and the jitter is 1 minute, then the actual timeout will be between 3 and 4 minutes. The default timeout.reconciliation and timeout.reconciliation.jitter values makes the maximum period to be 3 minutes</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#hard-refresh","title":"Hard refresh","text":"<p>It is possible to ignore the cached application source when doing a refresh of an application. This is the hard refresh and it forces the repo server to regenerate the application source cache again.</p> <p>This can be useful in some situations:</p> <ul> <li>Helm charts with code changes but same version</li> <li>External secrets (Vault, etc.) that don't trigger Git changes</li> <li>Registry updates where chart digest changes but version stays same</li> <li>Cache corruption or stuck sync states</li> </ul> <p>Hard refresh is an expensive operation as it rebuilds everything from scratch.</p> <p>By default argocd doesn't do a hard refresh but we can trigger a hard refresh manually.</p> <ul> <li>Via argocd ui (refresh + hard button)</li> <li>Using the argocd cli (argocd app get --hard-refresh)</li> <li>Via argocd.argoproj.io/refresh=hard annotation to the application</li> <li>Using the api</li> </ul> <p>We can also configure a periodic hard refresh. This is done at controller level with the timeout.hard.reconciliation setting in the argocd-cm Configmap.</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#other-ways-to-trigger-an-application-refresh","title":"Other ways to trigger an Application refresh","text":"<ul> <li>Manually via argocd ui (refresh button)</li> <li>Using the argocd cli (argocd app get --refresh)</li> <li>Via a webhook (a change in the source notifies a change to argocd)</li> <li>Via argocd.argoproj.io/refresh=normal annotation to the application</li> <li>Using the api</li> <li>Undocummented behaviour accessing applications the web UI.</li> </ul>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#event-driven-automatic-refresh","title":"Event driven automatic refresh","text":"<p>There is another mecanism where argocd application controller triggers a refresh.</p> <p>Application-controller watches Kubernetes resources using watch APIs. When a resource's resourceVersion changes, triggers immediate reconciliation. We can see in the logs strings like \"Requesting app refresh caused by object update\". This can cause high CPU with frequently-changing resources and it can be avoid enabling resource.ignoreResourceUpdatesEnabled in the argocd-cm ConfigMap, enabled by default since argocd 3.0</p>"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#refresh-settings-table","title":"Refresh settings table","text":"Concept Default Environment variable argocd-cm ConfigMap Binary Normal refresh 120s ARGOCD_RECONCILIATION_TIMEOUT timeout.reconciliation --app-resync Jitter 60s ARGOCD_RECONCILIATION_JITTER timeout.reconciliation.jitter --app-resync-jitter Hard refresh ARGOCD_HARD_RECONCILIATION_TIMEOUT timeout.hard.reconciliation --app-hard-resync"},{"location":"gitops/argocd/reconciliation/00-reconciliation/#application-controller-references","title":"Application controller references","text":"<ul> <li>Application Controller Code</li> </ul> <p>https://github.com/argoproj/argo-cd/blob/master/cmd/argocd-application-controller/commands/argocd_application_controller.go</p> <ul> <li>Command line</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/server-commands/argocd-application-controller/</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/","title":"Tune reconciliation","text":""},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#timeoutreconciliation","title":"timeout.reconciliation","text":"<p>When deploying many Applications, the reconciliation default values can cause high cpu / memory consuption. To avoid it we can increase the timeout.reconciliation value to a greater value (12h, 24h) or disabling it (0)</p> <p>Changing this value needs to restart the Application controller and the repo server</p> <p>If you set timeout.reconciliation to 0 or a big value, then Argo CD will stop polling Git repositories automatically at every 3 minutes so it is recommended to use alternative methods such as webhooks, for example, when a new commit is pushed to a git repository to tell the reposerver to recheck if there are changes</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#setting-the-application-webhook","title":"Setting the Application webhook","text":"<p>If the git repository does not trigger Applications using Applicationsets, expose only the argocd api (argocd server) and create a webhook for https://myargocd/api/webhook</p> <pre><code>service: argocd-server\nport: 443\nname: https\n</code></pre> <p>What this Application webhook does:</p> <ul> <li>It triggers a normal refresh to the Applications</li> <li>Uses intelligent filtering: Only triggers refresh if the webhook event matches the application's source repository and if the changed files match the application's refresh paths (if configured). See manifest-generate-paths for more tunning.</li> <li>This is defined in webhook.go and in application_annotations.go.</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#setting-the-applicationset-webhook","title":"Setting the Applicationset webhook","text":"<p>If the git repository triggers Applications using Applicationsets, also expose the Applicationset controller and create a webhook for https://Applicationseturl/api/webhook</p> <pre><code>service: argocd-applicationset-controller\nport: 7000\nname: webhook\n</code></pre> <p>What this Applicationset webhook does:</p> <ul> <li>It makes an annotation argocd.argoproj.io/application-set-refresh to the Applicationset.</li> <li>This is defined in webhook.go and common.go</li> <li>This annotation makes the applicationset re-evaluate generators (like Git files generator).</li> </ul> <p>Links</p> <ul> <li>https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/webhook/</li> <li>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git/#webhook-configuration</li> <li>See more info here</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#manifest-generate-paths","title":"manifest-generate-paths","text":"<p>Another great setting is using manifest-generate-paths. We can tell argocd Applications to only trigger a refresh is the changes are detected in one or more paths of the github repository.</p> <p>This can be done adding the argocd.argoproj.io/manifest-generate-paths label to the Applications</p> <pre><code>argocd.argoproj.io/manifest-generate-paths: .\n</code></pre> <p>This only will trigger a refresh when the changes are detected in the path the Application has been declared. We can configure more paths comma separated.</p> <p>This does not work for helm registries</p>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#selective-sync-applyoutofsynconly","title":"Selective Sync (ApplyOutOfSyncOnly)","text":"<p>It is possible to reduce the calls to the kubernetes api server made by argocd syncing only the changed objects. This is called Selective Sync</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\n...\nspec:\n  syncPolicy:\n    syncOptions:\n    - ApplyOutOfSyncOnly=true\n</code></pre> <p>But this has some counterparts</p> <ul> <li>The sync operation is not recorded in the history</li> <li>The rollback is not possible</li> <li>The hooks (not the git webhooks) dont run</li> </ul>"},{"location":"gitops/argocd/reconciliation/01-tune-reconciliation/#links","title":"Links","text":"<ul> <li>High Availability</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/high_availability/</p> <ul> <li>Reconcile Optimization</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/reconcile/</p> <ul> <li>Selective Sync</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/selective_sync/</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/","title":"Gitlab webhook","text":"<p>Now we will argocd make a refresh when a push event is received in the gitlab repository. This is very useful when increasing or disabling timeout.reconciliation</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#gitlab-side","title":"Gitlab side","text":"<p>Go to your repository in Settings -  Webhooks and create a webhook with this options.</p> <pre><code>URL: https://YOUR_ARGOCD_INSTANCE_URL/api/webhook  (we need it exposed via ingress)\nSecret Token: use a password generator to define a token\nTrigger. Choose push events and configure the branch where your argocd manifests are located\nSSL verification: If your argocd instace has a known certificate, let it enabled\n</code></pre> <p></p> <p></p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#argocd-side-custom-secret","title":"Argocd side (custom secret)","text":"<p>By defaut argocd-secret is used, but we will use a custom secret to store this setting. If you don't want to create a new secret, it is posibble to directly configure the value of the token in the webhook.gitlab.secret key of the argocd-secret Secret. I propose this way because of some problems merging an externalsecret with the existing argocd-secret Secret</p> <ul> <li>Configure the argocd-secret to tell the webhook.gitlab.secret will be in another secret</li> </ul> <pre><code>apiVersion: v1\nstringData:\n  webhook.gitlab.secret: $argocd-custom-secrets:webhook.gitlab.secret\nkind: Secret\nmetadata:\n  name: argocd-secret\ntype: Opaque\n</code></pre> <p>And the create a secret:</p> <ul> <li>called argocd-custom-secrets</li> <li>label it with app.kubernetes.io/part-of: argocd</li> <li>include the webhook.gitlab.secret key with the token</li> </ul> <p>Now will link the webhook.gitlab.secret with the recently created secret. This is done setting this line in the argocd-secret Secret</p> <p>Finally test the hook in the the place where we created the webhook instance with a push test</p> <p></p> <p>It must have connectivity between gitlab and argocd</p>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#same-for-the-applicationset-controller","title":"Same for the Applicationset controller","text":"<p>We can do the same for the applicationsets, specially relevant if we:</p> <ul> <li>have increased the default 3m value of applicationsetcontroller.requeue.after setting in the argocd-cmd-params-cm ConfigMap</li> <li>make it at Applicationset level with requeueAfterSeconds</li> </ul> <p>Steps</p> <ul> <li>Create a new ingress that exposes the webhook port of the argocd-applicationset-controller pod</li> <li>Create a new webhook in gitlab the same way, now with the url https://THE_NEW_INGRESS/api/webhook</li> </ul>"},{"location":"gitops/argocd/reconciliation/gitlab-webhook/#links","title":"Links","text":"<ul> <li>Git Webhook Configuration</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/webhook/</p> <ul> <li>Git Generator</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git/</p>"},{"location":"gitops/argocd/sync/00-sync/","title":"Sync","text":"<p>Argocd in their documentation defines 2 states for an argocd application:</p> <ul> <li>The target state, as the desired state of the application.</li> </ul> <p>We define this state via files in git repositories</p> <ul> <li>The live state</li> </ul> <p>The real state is how the application is the kubernetes cluster</p> <p>Sync is the argocd process that applies the manifests in the cluster.</p>"},{"location":"gitops/argocd/sync/00-sync/#sync-status","title":"Sync status","text":"<p>We have 3 possible sync statuses</p> <ul> <li>Synced</li> </ul> <p>The target state and the live state are the same. Everything is ok.</p> <p></p> <ul> <li>OutOfSync</li> </ul> <p>There are differences between the target state and the live state. Sometimes a Sync is needed to move this state to Synced</p> <p></p> <ul> <li>Unknown</li> </ul> <p>There is a problem with the Sync process</p>"},{"location":"gitops/argocd/sync/00-sync/#sync-process","title":"Sync process","text":"<p>The sync process or sync stage is the operation that applies the manifests of an application to the kubernetes cluster</p> <p>It cant be executed:</p> <ul> <li> <p>Using the web interface</p> </li> <li> <p>Using the argocd cli</p> </li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/commands/argocd_app_sync/</p> <ul> <li>Using kubectl</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-kubectl/</p> <ul> <li>In an automated way</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p> <p>See the other links in this section for more information</p>"},{"location":"gitops/argocd/sync/98-tips-external-secret/","title":"External-secret OutOfSync","text":"<p>Sometimes we can get an argocd Application OutOfSync because an external-secret is OutOfSync with some differences in this places:</p> <ul> <li>conversionStrategy</li> <li>decodingStrategy</li> <li>decodingStrategy</li> </ul> <p>... and we did not define that values.</p> <p>The root cause is that External Secrets Operator acts as a mutating admission controller, modifying resources after they're applied. It adds default values for conversionStrategy, decodingStrategy, and metadataPolicy fields when they're not explicitly specified in the ExternalSecret manifest. This creates a drift between your Git source (without these fields) and the live cluster state (with default values added), causing ArgoCD to show the resources as OutOfSync</p>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-1-ignoredifferences","title":"Solution 1: ignoreDifferences","text":"<p>We can ignore that fields at controller level. This is not the best option because we are ignoring some fields in the resource.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\ndata:\n  resource.customizations.ignoreDifferences.external-secrets.io_ExternalSecret: |\n      jqPathExpressions:\n      - '.spec.data[]?.remoteRef.conversionStrategy'\n      - '.spec.data[]?.remoteRef.decodingStrategy'\n      - '.spec.data[]?.remoteRef.metadataPolicy'\n      - '.spec.dataFrom[]?.extract.conversionStrategy'\n      - '.spec.dataFrom[]?.extract.decodingStrategy'\n      - '.spec.dataFrom[]?.extract.metadataPolicy'\n      - '.spec.dataFrom[]?.find.conversionStrategy'\n      - '.spec.dataFrom[]?.find.decodingStrategy'\n      - '.spec.dataFrom[]?.find.metadataPolicy'\n</code></pre> <p>This can also be configured at Application level with spec.ignoreDifferences. It must be configured in all drifted Applications</p> <p>We also neeed to ignore that differences when syncing</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-options: RespectIgnoreDifferences=true\n</code></pre>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-2-add-default-values","title":"Solution 2: add default values","text":"<p>Another option is to add to the external-secret the default values the external secret operator adds. For example</p> <pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nspec:\n  data:\n  - secretKey: mykey\n    remoteRef:\n      key: mykey\n      conversionStrategy: Default\n      decodingStrategy: None\n      metadataPolicy: None\n</code></pre>"},{"location":"gitops/argocd/sync/98-tips-external-secret/#solution-3-use-server-side-apply","title":"Solution 3: use Server Side Apply","text":"<p>Another option is to use Server Side Apply. With this:</p> <ul> <li>ArgoCD declares ownership of only the fields it manages</li> <li>Other controllers can own and modify their own fields (like external secrets operator)</li> <li>Kubernetes merges changes from multiple sources without conflicts</li> </ul> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-options: ServerSideApply=true\nspec:\n  ...\n</code></pre> <p>Note: I did not make this work for now.</p>"},{"location":"gitops/argocd/sync/98-tips/","title":"Argocd Tips","text":""},{"location":"gitops/argocd/sync/98-tips/#treat-something-as-string-in-jsonpointers","title":"Treat some/thing as string in jsonPointers","text":"<p>We want to exclude a path like this using ignoreDifferences and jsonPointers</p> <pre><code>/spec/template/metadata/annotations/checksum/secret-jobservice\n</code></pre> <p>But the key to exclude is checksum/secret-jobservice and not secret-jobservice inside checksum</p> <p>For this we must escape th \"/\" with \"~1\"</p> <pre><code>      ignoreDifferences:\n        - group: apps\n          kind: Deployment\n          name: harbor-core\n          jsonPointers:\n            - /spec/template/metadata/annotations/checksum~1secret-jobservice\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/","title":"ServerSideApply","text":"<p>There are 2 ways to apply manifests in kubernetes. Client-Side and Server-Side</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#client-side-apply-in-kubernetes","title":"Client-Side Apply in kubernetes","text":"<p>By default kubectl apply uses the traditional client-side way.</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#server-side-apply-in-kubernetes","title":"Server-Side Apply in kubernetes","text":"<ul> <li>Server-Side</li> </ul> <p>It became GA in kubernetes 1.22 as a new object merge algorithm, as well as tracking of field ownership, running on the Kubernetes API server.</p> <p>Server-Side Apply allows multiple \"managers\" (e.g., ArgoCD, kubectl, other controllers) to declaratively manage different parts of a resource's configuration. Each manager owns specific fields in the resource's manifest. The Kubernetes API server tracks which manager owns which fields in the resource's spec. This is called field ownership. When a manager applies changes, only the fields it owns are updated, leaving fields owned by other managers untouched. If two managers attempt to modify the same field, the API server detects the conflict and rejects the change unless explicitly forced.</p> <p>To see what \"manager\" controls what fields we can use this:</p> <pre><code>kubectl get RESOURCE RESOURCENAME --show-managed-fields -o yaml\n</code></pre>"},{"location":"gitops/argocd/sync/ServerSideApply/#serversideapply-in-argocd","title":"ServerSideApply in argocd","text":"<p>When to enable it</p> <ul> <li> <p>When the resource exceeds the 262144 bytes allowed in the annotations. This error can be found in some big CRDs.</p> </li> <li> <p>When Patching of existing resources on the cluster that are not fully managed by Argo CD.</p> </li> <li> <p>Use a more declarative approach, which tracks a user's field management, rather than a user's last applied state.</p> </li> <li> <p>Improved Merge Behavior:</p> </li> </ul> <p>Server-Side Apply ensures that changes made by other tools or controllers (e.g., Horizontal Pod Autoscaler, custom controllers) are not overwritten by ArgoCD unless explicitly managed by ArgoCD. This is particularly useful in scenarios where multiple tools manage the same resource.</p> <ul> <li>Conflict Detection:</li> </ul> <p>ArgoCD will detect and report conflicts if another manager modifies fields that ArgoCD is trying to manage. This prevents accidental overwrites and ensures better collaboration between tools.</p> <ul> <li>Declarative Ownership:</li> </ul> <p>ArgoCD explicitly declares ownership of specific fields in the resource manifest. This makes it easier to understand which tool is responsible for managing which parts of a resource.</p> <ul> <li>Compatibility with Kubernetes Features:</li> </ul> <p>Server-Side Apply is required for certain Kubernetes features, such as managing CRDs (Custom Resource Definitions) or advanced controllers that rely on field ownership.</p> <ul> <li>Potential for Conflicts:</li> </ul> <p>If not carefully managed, enabling Server-Side Apply can lead to conflicts when multiple tools or users attempt to manage the same fields in a resource. Performance:</p> <p>Server-Side Apply shifts the responsibility of merging changes to the Kubernetes API server, which can reduce the load on ArgoCD but may slightly increase the load on the API server.</p>"},{"location":"gitops/argocd/sync/ServerSideApply/#enable-serversideapply-in-argocd","title":"Enable ServerSideApply in argocd","text":"<ul> <li>At Application controller level</li> </ul> <p>We can enable server side apply with the param --server-side-diff-enabled in the argocd-application-controller, or with the following setting in the argocd-cmd-params-cm configMap</p> <pre><code>controller.diff.server.side: true # false by default\n</code></pre> <p>Enables the server side diff feature at the application controller level. Diff calculation will be done by running a server side apply dryrun (when diff cache is unavailable)</p> <ul> <li>At application level</li> </ul>"},{"location":"gitops/argocd/sync/ServerSideApply/#links","title":"Links","text":"<ul> <li>Server-Side Apply</li> </ul> <p>https://kubernetes.io/docs/reference/using-api/server-side-apply/</p> <ul> <li>Server-Side Apply support for ArgoCD (proposal)</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/proposals/server-side-apply/</p> <ul> <li>ServerSideApply argocd sync option</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/#server-side-apply</p>"},{"location":"gitops/argocd/sync/resource-hooks/","title":"Resource Hooks","text":"<p>Resource hooks are ways to execute some actions at certain moments of the sync operation. They are typically used in resources like Pods, Jobs and Workflows (from Argo workflows)</p> <p>For this we only need to add the following anotation to the resource:</p> <pre><code>argocd.argoproj.io/hook: HOOK\n</code></pre> <p>The HOOK can be:</p> <ul> <li>PreSync</li> </ul> <p>It will run in the PreSync phase</p> <ul> <li>Sync</li> </ul> <p>It will run in the Sync phase</p> <ul> <li>PostSync</li> </ul> <p>It will run in the PostSync phase</p> <ul> <li>SyncFail</li> </ul> <p>It will run if a sync operation fails</p> <ul> <li>PostDelete</li> </ul> <p>It will run  after all Application resources are deleted</p> <ul> <li>Skip</li> </ul> <p>Argocd will skip the application of the manifest</p>"},{"location":"gitops/argocd/sync/resource-hooks/#important-notes","title":"Important notes","text":"<ul> <li> <p>Hooks are not run during selective sync</p> </li> <li> <p>Multiple hooks can be specified as a comma separated list</p> </li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#deletion-policy","title":"Deletion policy","text":"<p>We can also define when to delete the hook with the following annotation:</p> <pre><code>argocd.argoproj.io/hook-delete-policy: POLICY\n</code></pre> <p>Where the policy can be:</p> <ul> <li>HookSucceeded</li> </ul> <p>The hook is deleted after the hook ends ok</p> <ul> <li>HookFailed</li> </ul> <p>The hook is deleted is it fails</p> <ul> <li>BeforeHookCreation (default)</li> </ul> <p>The hook is deleted before the new one is created. It exits to be used with named hooks</p> <p>Multiple hook delete policies can be specified as a comma separated list.</p>"},{"location":"gitops/argocd/sync/resource-hooks/#named-hooks","title":"Named hooks","text":"<p>If the Resource hook have a name, it is considered a named hook and it will be only created once. If we want to create it at every sync, we have 2 options:</p> <ul> <li>use generateName instead of name</li> <li>use BeforeHookCreation as deletion policy</li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#jobs-and-workflows-from-argo-workflows","title":"Jobs and workflows (from Argo workflows)","text":"<p>A job has the field ttlSecondsAfterFinished and a workflow (from Argo Workflows) have ttlStrategy. Both properties offer autoclean after some time.</p> <p>Using this fields can cause an OutOfSync state when the deletion comes. Using deletion hooks instead of ttlSecondsAfterFinished and ttlStrategy avoids this situation.</p>"},{"location":"gitops/argocd/sync/resource-hooks/#argocd-cannot-delete-a-resource-hook","title":"Argocd cannot delete a resource hook","text":"<p>In some situations I have detected argocd cannot delete a resource hook in a job with</p> <pre><code>argocd.argoproj.io/hook: Sync\nargocd.argoproj.io/hook-delete-policy: BeforeHookCreation\n</code></pre> <p>In the application controller</p> <pre><code>deleted resource batch/Job ... reason=ResourceDeleted type=Normal\n</code></pre> <p>But the resource hooks keeps in a Pending Deletion state</p> <p>There are some entries that suggest there is a bug here</p> <ul> <li>https://github.com/argoproj/argo-cd/issues/14929</li> <li>https://github.com/argoproj/gitops-engine/pull/461</li> <li>https://github.com/argoproj/argo-cd/issues/16446</li> </ul>"},{"location":"gitops/argocd/sync/resource-hooks/#links","title":"Links","text":"<ul> <li>Resource Hooks</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/resource_hooks/</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/","title":"Control retries and timeouts","text":""},{"location":"gitops/argocd/sync/syncPolicy-retry/#control-retries-in-syncpolicy","title":"Control retries in syncPolicy","text":"<p>We can control the retries when a sync operation starts. We can configure it with spec.syncPolicy.retry at application definition level or as a parameter with the argocd binary</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#retry-limit","title":"Retry limit","text":"<p>The number of failed retries is configured with the retry limit option. If a sync attempt fails, ArgoCD will automatically retry the sync up to the number of times defined by the retry.limit If the value in less than 0, ArgoCD will retry indefinitely until the sync succeeds or is manually stopped.</p>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#backoff","title":"Backoff","text":"<p>With retry.backoff we can configure the delay between sync attempts with the possibility to increase the time between every failed attempt to avoid overwhelming the system or external resources.</p> <ul> <li> <p>duration is the delay between attempts (default 5s)</p> </li> <li> <p>factor multiplies the delay (duration) after each failed retry (default 2)</p> </li> <li> <p>maxDuration is the max delay between retry attempts (default 3m0s)</p> </li> </ul>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#example-in-the-application-spec","title":"Example in the application spec","text":"<pre><code>3 retries\n1st retry delay: 10 seconds\n2st retry delay: 30 seconds\n3st retry delay: 60 seconds\n</code></pre> <p>... (but never exceeding 5 minutes between retries)</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\nspec:\n  syncPolicy:\n    retry:\n      limit: 3\n      backoff:\n        duration: 10\n        factor: 3 \n        maxDuration: 5m\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#related-argocd-app-sync-parameters","title":"Related argocd app sync parameters","text":"<pre><code>--retry-limit\n--retry-backoff-duration\n--retry-backoff-factor\n--retry-backoff-max-duration\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-retry/#setup-a-timeout-for-app-sync","title":"Setup a timeout for app sync","text":"<p>It is possible, and probably recommended to configure a timeout in seconds when an application starts a sync operation.</p> <p>It is configured at controller level, and sets the maximum time allowed for a single sync operation to complete before it is considered failed.</p> <p>This is done with the controller.sync.timeout.seconds setting in the argocd-cmd-params-cm configmap</p> <pre><code>controller.sync.timeout.seconds: \"1800\" # 30 minutes\n</code></pre> <p>The default value is \"0\", no timeout</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/","title":"Sync and compare options","text":""},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#sync-options","title":"Sync options","text":"<p>Argocd offer several sync options to configure the Sync process.</p> <p>This options can be configured at application level, resource level, or both</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#at-application-level","title":"At application level","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\nspec:\n  syncPolicy:\n    syncOptions:\n    - OPTION=VALUE\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#at-resource-level","title":"At resource level","text":"<p>At resource level is made using an annotation in the resource. Multiple options can be configured comma separated.</p> <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-options: OPTION1=VALUE,OPTION2=VALUE\n</code></pre> <p>Multiple Sync Options can be configured</p>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#list-of-sync-options","title":"List of sync options","text":"Explanation ApplyOutOfSyncOnly Only sync OutOfSync resources. Specially relevant in big applications or low resource api servers CreateNamespace Create the spec.destination.namespace namespace. Useful with spec.syncPolicy.managedNamespaceMetadata Delete Configures how a resource will be deleted after the application is deleted FailOnSharedResource Makes the sync process fail if a resource is managed by other argocd application Force Force=true,Replace=true deletes and recreate a resource at every sync Prune Prune resources not declared in the target state but they exist in the application PruneLast Moves the prune process at the end of the sync operation PrunePropagationPolicy Permits to choose the garbage collection (foreground,background or orphan) Replace Converts kubectl apply operation in a kubectl replace or kubectl create operation RespectIgnoreDifferences Used spec.ignoreDifferences also in the sync process. Don't sync/correct that fields ServerSideApply With true, we pass --server-side=true parameter to kubectl SkipDryRunOnMissingResource Ignores if a resource to be created does not have its CRD Validate With false, we pass --validate=false to kubectl"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#notes","title":"Notes","text":"<ul> <li>Replace=true takes precedence over ServerSideApply=true.</li> <li>Prune=false Prevents a resource from being pruned</li> <li>Prune=confirm Requires manual confirmation before pruning</li> <li>Delete=false Don't delete the resource from the cluster during app deletion</li> <li>Delete=confirm Requires manual confirmation before deletion</li> <li>RespectIgnoreDifferences sync option is only effective when the resource is already created in the cluster. If the Application is being created and no live state exists, the desired state is applied as-is</li> </ul>"},{"location":"gitops/argocd/sync/syncPolicy-syncOptions/#links","title":"Links","text":"<ul> <li>Sync Options</li> </ul> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/sync-options/</p> <ul> <li>Garbage Collection</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/","title":"Automated sync","text":"<p>By default, after a reconciliation/refresh, an Application does not make and automatic sync when it detects differences between the target and the live state. So the differences between them are not applied in an automated way. For that we need to enable the autosync feature in the Application (or Applicationset)</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#enabling-autosync","title":"Enabling autosync","text":"<p>We can enable autosync with this setting in the Application resource</p> <pre><code>spec:\n  syncPolicy:\n    automated: {}\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#autosync-options","title":"autosync options","text":"<p>There some options here to configure the automated sync.</p> <ul> <li>prune</li> </ul> <p>Enables automatic deletion of resources that they are not defined in the Application but they were. The default value is false.</p> <ul> <li>allowEmpty</li> </ul> <p>This setting is disabled by default and prevents a pruning operation can remove all the resources in the Application.</p> <ul> <li>selfHeal</li> </ul> <p>When a change in detected in the kubernetes cluster that generates a drift, argocd by default ignores it. Enabling selfHeal triggers a new sync.</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#toggling-autosync","title":"Toggling autosync","text":"<p>Since argocd 3.1 we can also enable/disable for an Application resource using a new feature:</p> <pre><code>spec:\n  syncPolicy:\n    automated:\n      enabled: true # or false\n</code></pre>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#some-notes-about-this-new-feature","title":"Some notes about this new feature","text":"<ul> <li> <p>Setting this value to false, disables autosync but permits to configure prune, allowEmpty and selfHeal</p> </li> <li> <p>This setting is currently not working because of a bug https://github.com/argoproj/argo-cd/issues/24171</p> </li> <li> <p>This setting has no effect in Applications managed by ApplicationSets.</p> </li> </ul> <p>To enable/disable autosync in an Application managed by ApplicationSet without changing the setting in all generated Applications, we have some options. For example we can use templatePatch or ignoring that field in the ApplicationSet and then changing manually the desired value in our Application</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nspec:\n  ignoreApplicationDifferences:\n    - jsonPointers:\n        - /spec/syncPolicy/automated/enabled\n</code></pre> <p>This permits enabling and disabling the autosync manually and directly in the Application resource but loosing gitops control of desired state.</p>"},{"location":"gitops/argocd/sync/syncPolicyautomated/#links","title":"Links","text":"<p>https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/","title":"GitRepository","text":"<p>It produces an artifact for a git repository revision. This is a tarball artifact with the fetched data.</p> <p>When defining a GitRepository, we can specify some options</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#repo-settings","title":"Repo settings","text":"<p>We can configure</p> <ul> <li>the url of the repository (spec.url)</li> <li>the proxy, if needed (spec.proxySecretRef)</li> <li>the credentials, if needed (spec.secretRef)</li> <li>the provider. It can be generic(default), github or azure (spec.provider)</li> </ul>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#what-to-include-in-the-tarball","title":"What to include in the tarball","text":"<ul> <li>sparseCheckout</li> </ul> <p>With spec.sparseCheckout we can specify a list of directories we want to be the only included in the tarball</p> <ul> <li>ignore</li> </ul> <p>With spec.ignore we can we can specify a list of directories that they will be excluded in the tarball Use with caution because there is default exclusion list</p>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#checkout-specref","title":"Checkout (spec.ref)","text":"<ul> <li>the commit sha</li> <li>the name of the reference</li> <li>a SemVer tag expression</li> <li>a tag</li> <li>a branch (default master)</li> </ul> <p>What takes precedecens follows this order</p> <pre><code>commit &gt; name &gt; semver &gt; tag &gt; branch\n</code></pre>"},{"location":"gitops/fluxcd/source-controller/gitrepository/#repo-operations","title":"Repo operations","text":"<ul> <li>the checkout frequency (spec.interval)</li> <li>the timeout (spec.timeout)</li> <li>if we want to stop the reconciliation (spec.suspend)</li> <li>if we want to verify the commit (spec.verify)</li> <li>if we want to initialize and fetch all Git submodules recursively when cloning the repository (spec.recurseSubmodules)</li> <li>if we want to include artifacts from another GitRepository (spec.include)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/gitrepositories/</p> <p>and the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.GitRepository</p>"},{"location":"gitops/fluxcd/source-controller/helmchart/","title":"HelmChart","text":"<p>A helm chart downloads a helm chart and packages as a tarball</p>"},{"location":"gitops/fluxcd/source-controller/helmchart/#configuration","title":"Configuration","text":""},{"location":"gitops/fluxcd/source-controller/helmchart/#source","title":"Source","text":"<ul> <li>We can define the source of the chart with spec.sourceRef</li> <li>The chart name or path of the sourceRef (spec, chart)</li> <li>The list of value files, as a relative path in the sourceRef (spec.valuesFiles)</li> <li>If some declared value files don't exist, we can ignore the error (spec.ignoreMissingValuesFiles)</li> </ul> <p>If the sourceRef is a HelmRepository</p> <ul> <li>The version (spec.version)</li> </ul>"},{"location":"gitops/fluxcd/source-controller/helmchart/#operations","title":"Operations","text":"<ul> <li>The frequency to check the spec.sourceRef for updates (spec.interval)</li> <li>If we want to create a new version with a new ChartVersion or Revision (spec.reconcileStrategy)</li> </ul> <p>for a HelmRepository, it will be ChartVersion for a GitRepository or bucket, it will be Revision</p> <ul> <li>If we want to suspend the reconciliation (spec.suspend)</li> <li>If we want to verify the chart (spec.verify)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/helmcharts/</p> <p>And the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.HelmChart</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/","title":"HelmRepository","text":"<p>There are 2 HelmRepository types</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#helm-https-repository","title":"Helm HTTP/S repository","text":"<p>Defines a Source to produce an Artifact for a Helm repository index YAML (index.yaml)</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#oci-helm-repository","title":"OCI Helm repository","text":"<p>Defines a source that does not produce an Artifact. It\u2019s a data container to store the information about the OCI repository that can be used by HelmChart to access OCI Helm charts.</p> <p>Because of that, an oci helm repository dont have ready or status</p>"},{"location":"gitops/fluxcd/source-controller/helmrepository/#configuration","title":"Configuration","text":"<p>We can configure some options about the repository</p> <ul> <li>the url (spec.url) and type between default or oci (spec.type)</li> <li>the credentials, if needed (spec.secretRef and spec.passCredentials)</li> <li>The client certificates (spec.certSecretRef)</li> </ul> <p>For default repositories</p> <ul> <li>the checkout frequency (spec.interval)</li> </ul> <p>For oci repositories</p> <ul> <li>If we want permit non-tls connections to an oci repository (spec.insecure)</li> <li>The oci provider, between aws, azure, gcp or generic (spec.provider)</li> </ul> <p>And about the operations</p> <ul> <li>the timeout (spec.timeout)</li> <li>if we want to stop the reconciliation (spec.suspend)</li> </ul> <p>More info here</p> <p>https://fluxcd.io/flux/components/source/helmrepositories/</p> <p>and the spec here</p> <p>https://fluxcd.io/flux/components/source/api/v1/#source.toolkit.fluxcd.io/v1.HelmRepository</p>"},{"location":"kubernetes/98-tips/","title":"Tips","text":""},{"location":"kubernetes/98-tips/#how-to-use-an-environment-variable-as-an-arguement","title":"How to use an environment variable as an arguement","text":"<p>We must use this format</p> <pre><code>$(VAR_NAME)\n</code></pre> <p>For example</p> <pre><code>args:\n- echo \n- $(VAR_NAME)\nenv:\n- name: VAR_NAME\n  value: myvalue\n</code></pre>"},{"location":"kubernetes/99-links/","title":"Links","text":"<ul> <li>Official website</li> </ul> <p>https://kubernetes.io/</p> <ul> <li>Kubernetes Failure Stories</li> </ul> <p>https://k8s.af/</p>"},{"location":"kubernetes/compat/","title":"Kubernetes compatibilities and requirements","text":""},{"location":"kubernetes/compat/#keywords","title":"Keywords","text":"<ul> <li>Compatibility Matrix</li> <li>System Requirements</li> <li>Supported Releases</li> </ul>"},{"location":"kubernetes/compat/#cilium","title":"Cilium","text":"<p>Relation with kubernetes releases, linux distributions, linux kernel and others</p> <p>https://docs.cilium.io/en/stable/network/kubernetes/compatibility/ https://docs.cilium.io/en/stable/operations/system_requirements/</p>"},{"location":"kubernetes/compat/#karpenter","title":"Karpenter","text":"<p>https://karpenter.sh/docs/upgrading/compatibility/</p>"},{"location":"kubernetes/compat/#kyverno","title":"Kyverno","text":"<p>Relation with kubernetes releases</p> <p>https://kyverno.io/docs/installation/</p>"},{"location":"kubernetes/compat/#argocd","title":"Argocd","text":"<p>Tested kubernetes versions</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/</p>"},{"location":"kubernetes/compat/#jaeger-operator","title":"Jaeger Operator","text":"<p>https://github.com/jaegertracing/jaeger-operator/blob/main/COMPATIBILITY.md</p>"},{"location":"kubernetes/hack-dns/","title":"Hack dns","text":""},{"location":"kubernetes/hack-dns/#redirect-internal-service-to-external-fqdn","title":"Redirect internal service to external fqdn","text":"<p>We can redirect the dns resolution of an internal service to an external fqdn</p> <p>We can achieve this using an external name service. With the following example, \"my-service\" will return \"my.fqdn.url\"</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  type: ExternalName\n  externalName: my.fqdn.url\n</code></pre>"},{"location":"kubernetes/hack-dns/#redirect-external-fqdn-to-internal-service","title":"Redirect external fqdn to internal service","text":"<p>We can do that changing coredns configuration. For example this line redirects \"my.fqdn.url\" to \"my-service.default.svc.cluster.local\"</p> <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        rewrite name my.fqdn.url my-service.default.svc.cluster.local\n        ...\n    }\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n</code></pre> <p>This needs to restart coredns deployment</p> <p>https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/</p>"},{"location":"kubernetes/hack-dns/#redirect-an-domain-to-a-custom-dns-server","title":"Redirect an domain to a custom dns server","text":"<p>We can redirect all domain resolutions to a custom dns server</p> <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        forward graphenus.internal my.dnsserver.ip\n        ...\n    }\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n</code></pre> <p>https://coredns.io/manual/setups/</p>"},{"location":"kubernetes/hack-dns/#custom-dns-entries-in-a-pod","title":"Custom dns entries in a pod","text":"<p>In pod spec.hostAliases we can set a list of entries that contains a hostname and an ip to give custom resolutions to a pod. This adds a /etc/hosts entry</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test\nspec:\n  hostAliases:\n  - ip: \"127.0.0.1\"\n    hostnames:\n    - \"foo.local\"\n    - \"bar.local\"\n  - ip: \"10.1.2.3\"\n    hostnames:\n    - \"foo.remote\"\n    - \"bar.remote\"\n  ...\n</code></pre> <p>https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/</p>"},{"location":"kubernetes/hack-dns/#core-dns-in-eks","title":"Core dns in EKS","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html</p> <p>https://repost.aws/knowledge-center/eks-conditional-forwarder-coredns</p>"},{"location":"kubernetes/autoescaling-finops/autoescaling-nodes/","title":"Autoescaling nodes","text":""},{"location":"kubernetes/autoescaling-finops/autoescaling-nodes/#solutions","title":"Solutions","text":"<ul> <li> <p>Cluster Autoescaler https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler</p> </li> <li> <p>Karpenter https://karpenter.sh/</p> </li> <li> <p>Google GKE Autopilot https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</p> </li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/","title":"Cloud Cost Solutions","text":"<p>Open-source solutions for monitoring and analyzing cloud costs in Kubernetes environments.</p>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#opencost","title":"OpenCost","text":"<p>OpenCost is a vendor-neutral open-source project for measuring and allocating infrastructure and container costs in real-time. It's a CNCF Incubating project that provides cost visibility for Kubernetes workloads.</p> <ul> <li>GitHub Stars: 6.1k</li> <li>Contributors: 140</li> <li>Company Behind: Originally developed by Kubecost (Stackwatch Inc.), now owned by IBM (since September 2024)</li> <li>CNCF Status: Incubating (accepted as Sandbox in June 2022, promoted to Incubating in October 2024)</li> <li>CNCF Member: IBM is a founding member of CNCF (2015)</li> <li>Contributing Organizations: Kubecost, RedHat, AWS, Adobe, SUSE, Armory, Google Cloud, Pixie, Mindcurv, D2IQ, New Relic</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#opencost-references","title":"OpenCost References","text":"<ul> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#kubecost","title":"Kubecost","text":"<p>Kubecost provides real-time cost visibility and insights for Kubernetes clusters. While it offers commercial enterprise features, the core product (formerly known as Kubecost Free) is available as open-source and provides comprehensive cost monitoring capabilities.</p> <ul> <li>GitHub Stars: 577</li> <li>Contributors: 150+</li> <li>Company Behind: Kubecost (Stackwatch Inc.), acquired by IBM in September 2024</li> <li>CNCF Relation: Not a CNCF project (commercial product), but created and donated OpenCost to CNCF</li> <li>CNCF Member: Kubecost was a CNCF member; now part of IBM (CNCF founding member)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#kubecost-references","title":"Kubecost References","text":"<ul> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Kubecost Blog</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#grafana-cloud-cost-exporter","title":"Grafana Cloud Cost Exporter","text":"<p>Grafana Cloud Cost Exporter is an open-source Prometheus exporter that collects cloud provider cost metrics and makes them available for visualization in Grafana dashboards. It focuses on exporting billing data as Prometheus metrics rather than providing a complete cost allocation platform.</p> <ul> <li>GitHub Stars: 102</li> <li>Contributors: 18+</li> <li>Company Behind: Grafana Labs</li> <li>CNCF Relation: Not a CNCF project, but Grafana Labs contributes to OpenCost and other CNCF projects</li> <li>CNCF Member: Grafana Labs is a CNCF Platinum member (upgraded from Silver in July 2021)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/cloud-cost-solutions/#grafana-cloud-cost-exporter-references","title":"Grafana Cloud Cost Exporter References","text":"<ul> <li>GitHub Repository</li> <li>Grafana Cloud Cost Dashboards</li> <li>AWS Cost Explorer API</li> <li>Azure Cost Management API</li> <li>GCP Billing Export</li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/","title":"Horizontal pod autoescaler","text":""},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#definir-que-queremos-escalar","title":"Definir que queremos escalar","text":"<p>Se hace mediante spec.scaleTargetRef</p> <pre><code>    scaleTargetRef:\n      apiVersion: apps/v1\n      kind: Deployment\n      name: mideployment\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#replicas-deseadas","title":"Replicas deseadas","text":"<p>Se hace mediante:</p> <ul> <li>spec.minReplicas</li> <li>spec.maxReplicas</li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#donde-buscar-la-metrica","title":"Donde buscar la metrica","text":"<p>Al definir una metrica de escalado, debemos definir mediante \"type\" donde mirar</p> <ul> <li> <p>Resource: Con Resource se mira el uso de requests de cpu o memoria de los contenedores de un pod</p> </li> <li> <p>ContainerResource Con ContainerResource, estable desde kubernetes 1.30 mirara el uso de requests de cpu o memoria de un contenedor individual dentro de un pod</p> </li> <li> <p>Pods Pods permite utilizar custom metrics</p> </li> <li> <p>External Para usar metricas externas a kubernetes</p> </li> <li> <p>Object Object mirara en un objeto de kubernetes</p> </li> </ul>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#escalados","title":"Escalados","text":""},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-al-porcentaje-de-uso-de-requests","title":"En base al porcentaje de uso de requests","text":"<p>Con target.type: Utilization y averageUtilization podemos especificar un numero que sera un porcentaje del uso de resource requests (cpu y/o memoria). HPA intentara mantener ese porcentaje mediante el escalado.</p> <p>Solo esta soportado para ContainerResource y Resource</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: memory\n      target:\n        averageUtilization: 70 # media de un 70% de los requests de memoria\n        type: Utilization\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-a-un-promedio","title":"En base a un promedio","text":"<p>Con target.type: AverageValue + averageValue podemos especificar un quantity como una media de un valor de la metrica. HPA intentara mantener ese valor promedio mediante el escalado.</p> <p>En el caso de recursos de pods, sera bytes para memoria y milicores para CPU.</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: cpu\n      target:\n        averageValue: 500 # media de 500 milicores\n        type: AverageValue\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#en-base-a-un-valor-absoluto","title":"En base a un valor absoluto","text":"<p>Con target.type: Value + value  definimos un un quantity como un valor absoluto de la metrica. HPA intentara mantener ese valor mediante el escalado.</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nombredelhpa\nspec:\n  metrics:\n  - resource:\n      name: cpu\n      target:\n        value: 2 # uso de 2 cpus sumando los pods\n        type: Value\n    type: Resource\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#behaviour","title":"Behaviour","text":"<p>Con spec.behaviour se puede controlar la frecuencia y velocidad de escalado y desescalado</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#stabilizationwindowseconds","title":"stabilizationWindowSeconds","text":"<p>Con stabilizationWindowSeconds se puede reducir la frecuencia de escalado o desescalado cuando las metricas cambian. Con este valor, se definen una ventana de valores que se tendran en cuenta a la hora del escalado o desescalado.</p> <p>El valor puede ser desde 0 (no hay estabilzacion) hasta 3600 (un dia). Por defecto, al escalar hacia arriba el valor es 0 (no hay estabilizacion) y al escalar hacia abajo 300.</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#policies","title":"Policies","text":"<p>Con las policies se pueden definir una serie de posibles politicas de escalado.</p> <pre><code>behavior:\n  scaleDown:\n    policies:\n    - type: Pods # this permits to scale down 3 replicas in 2 minutes\n      value: 3\n      periodSeconds: 120\n    - type: Percent # this permits at most 20% of the current replicas to be scaled down in 3 minutes\n      value: 20\n      periodSeconds: 180\n</code></pre> <p>Si hay varias politicas definidas, por defecto se aplica la que permite mas cambios. Con selectPolicy se puede cambiar este comportamiento y elegir que politica usar.</p>"},{"location":"kubernetes/autoescaling-finops/horizontal-pod-autoescaler/#links","title":"Links","text":"<ul> <li>Horizontal Pod Autoscaling</li> </ul> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</p> <ul> <li>HorizontalPodAutoscaler Walkthrough</li> </ul> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</p> <ul> <li>autoscaling/v2 API</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/","title":"Scale to 0","text":"<p>It can be useful to scale to 0 when desired some workloads in order to reduce resource utilizacion. In addition to this, downscaling nodes will make a cheaper deployment.</p> <p>So, scale to 0 the deployments you need and the let karpenter o another tool to consolidate and delete nodes.</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#note-about-gitops-tools","title":"Note about gitops tools","text":"<p>If you are using a gitops tool like argocd or flux, take care about the interactions with the original manifests. Argocd recommends to not specify the replicas in your manifests and let them to be managed by horizontal pod autoescaler or these tools.</p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#keda","title":"Keda","text":"<p>Keda has a scaler called \"cron\" that permits to scale to 0 the desired workloads when needed. It alson contains a lot of ways to scale our workloads.</p> <p>https://keda.sh/docs/latest/scalers/cron/</p> <p>Keda recommends not to use horizontal pod autoescaler with keda</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#kube-green","title":"Kube-green","text":"<p>Kube green permit sto \"sleep\" your pods</p> <p>https://kube-green.dev/</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#snorlax","title":"Snorlax","text":"<p>Snorlax is a tool that also permits to sleep your workloads</p> <p>https://github.com/moonbeam-nyc/snorlax</p>"},{"location":"kubernetes/autoescaling-finops/scale-to-0/#sleepcycles","title":"Sleepcycles","text":"<p>Another tool. They say the argocd self healing must be disabled</p> <p>https://github.com/rekuberate-io/sleepcycles</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/","title":"Vertical por autoescaling","text":"<p>In the context of Vertical Pod Autoscaling (VPA) in Kubernetes, the terms lower bound, target, uncapped target, and upper bound refer to the recommendations provided by the VPA for setting the resource requests for pods. These recommendations are calculated based on historical usage data, current resource requests, and limits defined in the pod's specification. Here's what each term means:</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#recommendations","title":"Recommendations","text":""},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#lower-bound","title":"Lower Bound","text":"<p>This is the minimum amount of resources the VPA recommends for the pod to function correctly based on its observed resource usage. Setting the resource requests below this value might lead to insufficient resources for the pod, potentially causing performance issues or even failures</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#target","title":"Target","text":"<p>This is the VPA's recommendation for the resource requests that should be set for the pod to ensure optimal performance under normal workload conditions. The target value is calculated based on the pod's historical resource usage, aiming to balance resource efficiency with sufficient headroom for typical workload variations</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#uncapped-target","title":"Uncapped Target","text":"<p>The uncapped target is similar to the target but without considering the resource limits set on the pod. This value represents what the VPA calculates the pod needs based on its usage, without being constrained by the current resource limits. If the uncapped target is higher than the upper bound (i.e., the current limit), it indicates that the pod's performance might be constrained by its resource limits</p>"},{"location":"kubernetes/autoescaling-finops/vertical-pod-autoescaling/#upper-bound","title":"Upper Bound","text":"<p>This is the maximum amount of resources the VPA believes the pod might need under peak conditions observed during the analysis period. Setting the resource requests or limits at or above this value should ensure that the pod has enough resources to handle spikes in workload without being throttled or running out of resources</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/","title":"Disruption","text":"<p>Karpenter's disruption is the process that makes Karpenter terminates nodes in the Kubernetes cluster.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#planning-phase-disruption-controller","title":"Planning phase (disruption controller)","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#search-candidates","title":"Search candidates","text":"<p>The disruption controller is continuously discovering nodes that can be disrupted because of this reasons:</p> <ul> <li>drift</li> <li>consolidation (empty)</li> <li>consolidation (underutilized)</li> </ul> <p>The number of candidates per reason is located in the karpenter_voluntary_disruption_eligible_nodes prometheus metric</p> <p>First, the disruption controller starts searching for candidates for a drift disruption and it gives them priorities. If a node in that list has pods that cannot be evicted from the node, the node is ignored for now.</p> <p>The disruption can be blocked here because of:</p> <ul> <li>Pod disruption budgets</li> </ul> <p>Pods in the node affected by disruption buckets with 0 ALLOWED DISRUPTIONS</p> <ul> <li>karpenter.sh/do-not-disrupt</li> </ul> <p>Pods in the node with karpenter.sh/do-not-disrupt: \"true\" annotation. If the nodeclaim has terminationGracePeriod configured, it will still be eligible for disruption via drift.</p> <p>We can see that blocked nodes with</p> <pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node | grep DisruptionBlocked\n</code></pre> <p>If no nodes cannot be disrupted, the same process will start with the consolidation disruption.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#evaluate-candidates","title":"Evaluate candidates","text":"<ul> <li>NodePool\u2019s disruption budget</li> </ul> <p>The next step is to check the if the node respect the NodePool\u2019s disruption budget, a mechanism to control the speed of the disruption process.</p> <ul> <li>Evaluate if new nodes are needed</li> </ul> <p>Then the disruption controller does a simulation to estimate if any replacement nodes are needed.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#taint-the-nodes","title":"Taint the nodes","text":"<p>Next, the chosen node(s) are tainted with karpenter.sh/disrupted:NoSchedule to prevent new pods being scheduled there.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#deploy-replacement-nodes","title":"Deploy replacement nodes","text":"<p>If new replacement nodes are needed, the disruption controller triggers their deployment and wait until they are deployed. If the deployment fails, the node(s) is(are) untainted and the whole process starts again.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#node-deletion","title":"Node deletion","text":"<p>Here the disruption controller deletes the node. All the Nodes and NodeClaims deployed via Karpenter have a kubernetes finalizer karpenter.sh/termination.  So the the deletion is blocked leaves that task to the termination controller.</p> <p>When the termination controller terminates the node, the whole process starts again.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#execution-phase-termination-controller","title":"Execution phase (termination controller)","text":"<p>The termination controller is responsible to finally delete the node. The deletion if blocked by the finalizer. This deletion can be triggered by:</p> <ul> <li>the disruption controller</li> <li>a user using manual disruption</li> <li>an external system that deletes the node resource</li> </ul> <p>The APIServer has added the DeletionTimestamp on the node</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#taint","title":"Taint","text":"<p>The chosen node(s) is(are) tainted with karpenter.sh/disrupted:NoSchedule to prevent new pods being scheduled there. Depending of the disruption method, that taint can exist.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#eviction","title":"Eviction","text":"<p>The termination controller starts evicting the pods using the Kubernetes Eviction API.</p> <ul> <li>This respects Pod disruption budgets</li> <li>Static pods, pods tolerating the karpenter.sh/disrupted:NoSchedule taint, and succeeded/failed pods are ignored</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#cleaning","title":"Cleaning","text":"<ul> <li>When the node is drained, the NodeClaim is deleted</li> <li>Finally the finalizer is removed from the node so the APIServer can remove it</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/10-disruption/#forceful-deletion","title":"Forceful deletion","text":"<p>In expiration and interruption methods the disruption controller immediately triggers tainting and draining as soon as the event is detected (interruption signal or expireAfter).</p> <ul> <li>That methods do not respect NodePool\u2019s disruption budget.</li> <li>Pod disruption budgets can be used to control the disruption speed at application level.</li> <li>That methods they do not wait for a replacement node to be healthy</li> </ul>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/","title":"Disruption methods","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#manual-deletion","title":"Manual deletion","text":"<ul> <li>Node</li> </ul> <p>A node can be manually deleted by a user or by an external system. When a node without this finalizer is deleted (via kubectl or api call), the instance will not be deleted from AWS EC2. It is only unregistered from the kubernetes cluster. All the containing pods will deleted by a gargage collection and they will start in another node. This karpenter finalizer improves the deletion process.</p> <ul> <li>Nodeclaim</li> </ul> <p>The karpenter nodes are associated to a nodeclain. Deleting the nodeclain also deletes the node.</p> <ul> <li>Nodepool</li> </ul> <p>The nodepools are the owners of the nodeclaims (ownerReferences). If a nodepool is deleted, the associated nodeclaims and nodes will be deleted.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automatic-graceful-methods-consolidation","title":"Automatic graceful methods: Consolidation","text":"<p>With this method karpenter tries to reduce the cluster cost deleting nodes or replacing them.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#consolidationpolicy","title":"consolidationPolicy","text":"<p>Karpenter can delete nodes:</p> <ul> <li>when the node is empty. This is when it has no daemonset related pods running. (deletion mechanism)</li> <li>when the workloads can run in another nodes. (deletion mechanism)</li> <li>when the nodes can be replaced with cheaper variants. (replace mechanism)</li> </ul> <p>There are 2 consolidation policies: WhenEmptyOrUnderutilized (default) and WhenEmpty and it can be specified in spec.disruption.consolidationPolicy of the nodepool.</p> <p>The order of the actions that the consolidation tries to do is:</p> <ul> <li>delete all the empty nodes in parallel</li> <li>delete 2 or more nodes and possibly creating a new one if this is a cheaper solution</li> <li>delete a single node and possibly creating a new one if this is a cheaper solution</li> </ul> <p>Nodes with fewer pods, or with upcoming expiration or with lower priority pods will be better candidates to be consolidated</p> <p>Things like the anti-affinity, pod disruption budgets or topology spreads affects the effectiveness of the consolidation</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#spot-to-spot-consolidation","title":"Spot to spot consolidation","text":"<p>The spot nodes are consolidated by default with the deletion mechanism.</p> <p>It is possible to enable the replace one through the SpotToSpotConsolidation feature flag karpenter considers another things in addition to the cheapest price. It also needs a minimum of 15 instance types to work and possibility to be interrupted is also observed.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#consolidateafter","title":"consolidateAfter","text":"<p>When a pod is added or deleted from a node, karpenter starts to calculate if the node is consolidatable when the value specified in spec.disruption.consolidateAfter is reached. With this we can tell karpenter to be more cautious or aggressive in terms of consolidation.</p> <p>We can disable the consolidation with the \"Never\" value here</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automatic-graceful-methods-drift","title":"Automatic graceful methods: Drift","text":"<p>The drift method tries to reconciliate the desired state of the nodepools and ec2nodeclasses with the actual one. In order to check if there is a drift, karpenter compares some fields in that resources. It also maintains some hashes in the resources.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automated-forceful-methods-expiration","title":"Automated forceful methods: Expiration","text":"<p>It is possible to expire nodes with the spec.template.spec.expireAfter field. The default vale is 720 hours (30 days)</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/11-disruption-methods/#automated-forceful-methods-interruption","title":"Automated forceful methods: Interruption","text":"<p>When this methods karpenter watch some events that can cause involuntary interruptions.</p> <ul> <li>AWS will reclaim an spot instance</li> <li>Maintenance tasks</li> <li>Instance deletion events</li> <li>Instance stopping events</li> </ul> <p>Then karpenter sends a drain, taint and deletion of the node.</p> <p>With the spot interruption warnings, there are 2 minutes to solve the situation. In order to get the events we need to configure an sqs queue and a some EventBridge rules. Also, by default, karpenter does not manage the Spot Rebalance Recommendations.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/","title":"Disruption speed","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/#nodepool-terminationgraceperiod","title":"Nodepool TerminationGracePeriod","text":"<p>The spec.template.spec.terminationGracePeriod in a nodepool sets the time a node can be in a draining state before being forcibly deleted. Changing this value in the nodepool will drift the nodeclaims.</p> <p>During this time, the pods are being deleted based on the terminationGracePeriodSeconds pods setting until terminationGracePeriod in the nodepool reaches, so it can be a good practice to set the terminationGracePeriod with a greater value than the higher terminationGracePeriodSeconds. With this command we can get the 5 higher terminationGracePeriod setting in all the pods of the cluster:</p> <pre><code>kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.terminationGracePeriodSeconds}{\"\\n\"}{end}' | sort -k 2,2n | tail -5\n</code></pre> <p>Warning. This command does not makes a relation between the pod and the nodepool where it is deployed</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/12-disruption-speed/#nodepool-disruption-budgets","title":"Nodepool Disruption Budgets","text":"<p>With spec.disruption.budgets we can control the speed of the disruption defining budgets. In a budget we can set some filters and settings:</p> <ul> <li>nodes:</li> </ul> <p>Limit the maximum nodes of the nodepool that can be deleted at the same time. It can be an number or a percentage. A \"0\" value disables the disruption for this budget.</p> <ul> <li>reasons:</li> </ul> <p>This budget applies to nodes that they are being disrupted for the specified reasons. The possible values are Empty, Drifted and Underutilized</p> <ul> <li>schedule:</li> </ul> <p>Cron like schedule when this budget applies</p> <ul> <li>duration:</li> </ul> <p>It is a needed setting when using schedule. It defines the time this budget is active when the schedule starts.</p> <p>By default, there is only a budget with nodes: 10%</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/","title":"Scheduling","text":"<p>In kubernetes we can define some settings to limit where a pod can be scheduled, and this affects karpenter.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#resources-requests-and-limits","title":"Resources (requests and limits)","text":"<p>It is a kubernetes best practice to define in our workload correct values for CPU/Memory requests and limits. In Karpenter scenarios this particularly important.</p> <p>In terms of karpenter scheduling, the important setting here is the CPU/Memory request value, because if a pod does not have a node where to be deployed, it will be in \"Pending\" state, triggering a new nodeclaim.</p> <p>But it is a karpenter best practice to give the memory request and limit the same value to avoid (OOM) conditions in situations where some pods can exceed their requests and the same time. For example, during a consolidation or a simply drain.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#restrict-nodes","title":"Restrict nodes","text":"<p>There are some ways to restrict the nodes where the pods can be scheduled.</p> <ul> <li> <p>nodeSelector is the simplest way, using node labels in the definition of the pods.</p> </li> <li> <p>affinity is for more complex situations we can use affinity in pods. Here we can use nodeAffinity, podAffinity and podAntiAffinity.</p> </li> <li> <p>taints and tolerations. Here we define taints in the nodes. Then we define tolerations at pod level to permit them.</p> </li> <li> <p>topologySpreadConstraints permits to control de distribution of pods in your cluster using.</p> </li> </ul> <p>Karpenter supports the following topologyKey(s):  </p> <ul> <li>topology.kubernetes.io/zone  </li> <li>kubernetes.io/hostname  </li> <li>karpenter.sh/capacity-type  </li> </ul> <p>For more information visit the folling links</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/20-scheduling/#links","title":"Links","text":"<ul> <li>Scheduling</li> </ul> <p>https://karpenter.sh/docs/concepts/scheduling/</p> <ul> <li>Karpenter Best practices</li> </ul> <p>https://docs.aws.amazon.com/eks/latest/best-practices/karpenter.html</p> <ul> <li>Pod Scheduling API</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/","title":"spotToSpotConsolidation","text":"<p>spotToSpotConsolidation is a karpenter feature (alpha since v0.34.x) that permits to replace existing Spot Instances with new Spot Instances that are more cost-effective or better suited to your workloads.</p> <p>spotToSpotConsolidation is disabled by default because increases the risk of workload interruptions because it can increase the termination and creation of spot instances. By keeping it disabled by default, Karpenter ensures a more stable and predictable environment.</p> <p>It is an aggresive way to optimize costs and be more resource effective</p> <p>In 1 node to 1 node consolidations, Karpenter requires at least 15 different instance types  a minimum instance type flexibility of 15 candidate instance types. flexibility won\u2019t lead to \u201crace to the bottom\u201d scenarios.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#recommended-environments","title":"Recommended environments","text":"<p>This situations make spotToSpotConsolidation a good option</p> <ul> <li>Clusters with high tolerance to interruptions</li> <li>Non production clusters</li> <li>Environments where Spot Instance prices fluctuate frequently</li> </ul> <p>To make a cluster more interruption tolerant, there are features like Pod Disruption Budgets or graceful termination hooks</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#non-recommended-environments","title":"Non recommended environments","text":"<ul> <li>Production clusters</li> </ul> <p>Test it in non production clusters first</p> <ul> <li>The workloads are stateful</li> </ul> <p>Stateful applications (e.g., databases, message queues) are not ideal for frequent Spot Instance replacements. Moving them to on demand nodes can be a good practice</p> <ul> <li>Ha apps</li> </ul> <p>If your workloads require consistent availability and cannot tolerate interruptions, this feature may introduce unnecessary risk.</p> <ul> <li>Spot Market Is Unstable</li> </ul> <p>In regions or zones where Spot Instance availability is highly volatile, frequent consolidations may lead to excessive disruptions.</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#how-to-enable","title":"How to enable","text":"<p>It is a feature flag / gate. To enable it via helm, we need this</p> <pre><code>settings:\n  featureGates:\n    spotToSpotConsolidation: true\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/30-spot-to-spot-consollidation/#links","title":"Links","text":"<ul> <li>Disruption</li> </ul> <p>https://karpenter.sh/docs/concepts/disruption/</p> <ul> <li>Applying Spot-to-Spot consolidation best practices with Karpenter  </li> </ul> <p>https://aws.amazon.com/blogs/compute/applying-spot-to-spot-consolidation-best-practices-with-karpenter/</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/","title":"Tips","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#show-karpenter-nodes","title":"Show karpenter nodes","text":"<pre><code>eks-node-viewer -disable-pricing -node-selector karpenter.sh/registered=true\n</code></pre> <pre><code>kubectl get node -o yaml | grep -A 1 finalizer\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#get-the-node-events","title":"Get the node events","text":"<pre><code>kubectl get events --all-namespaces --field-selector involvedObject.kind=Node\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#cannot-disrupt-node","title":"Cannot disrupt Node","text":"<pre><code>Cannot disrupt Node: state node doesn't contain both a node and a nodeclaim\n</code></pre> <p>This can tell you this node is not managed by karpenter</p>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#spottospotconsolidation-is-disabled","title":"SpotToSpotConsolidation is disabled","text":"<pre><code>SpotToSpotConsolidation is disabled, can't replace a spot node with a spot node\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#cant-replace-with-a-cheaper-node","title":"Can't replace with a cheaper node","text":""},{"location":"kubernetes/autoescaling-finops/karpenter/98-tips/#pdb-xxx-prevents-pod-evictions","title":"Pdb XXX prevents pod evictions","text":"<pre><code>Pdb XXX prevents pod evictions\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/karpenter/99-links/","title":"Links","text":"<ul> <li>Official Karpenter Docs  </li> </ul> <p>https://karpenter.sh/docs/</p> <ul> <li>Karpenter Best Practices  </li> </ul> <p>https://aws.github.io/aws-eks-best-practices/karpenter/</p> <ul> <li>Karpenter Blueprints for Amazon EKS  </li> </ul> <p>https://github.com/aws-samples/karpenter-blueprints</p> <ul> <li>Optimizing Karpenter on EKS: A Guide to Efficient NodePool Configuration Strategies</li> </ul> <p>https://builder.aws.com/content/2z6RjwBGRVcPg7IIpYf9tk0XpzQ/optimizing-karpenter-on-eks-a-guide-to-efficient-nodepool-configuration-strategies</p> <ul> <li>Escalamiento de nodos de Amazon EKS con Karpenter  </li> </ul> <p>https://aws.amazon.com/es/blogs/aws-spanish/escalamiento-de-nodos-de-amazon-eks-con-karpenter/</p> <ul> <li>Reinvent 2023</li> </ul> <p>https://youtu.be/lkg_9ETHeks?feature=shared</p> <ul> <li>Amazon EC2 Instance Selector</li> </ul> <p>https://github.com/aws/amazon-ec2-instance-selector</p> <ul> <li>Introducing the price-capacity-optimized allocation strategy for EC2 Spot Instances</li> </ul> <p>https://aws.amazon.com/blogs/compute/introducing-price-capacity-optimized-allocation-strategy-for-ec2-spot-instances/</p>"},{"location":"kubernetes/autoescaling-finops/keda/98-tips/","title":"Tips","text":""},{"location":"kubernetes/autoescaling-finops/keda/98-tips/#multiple-triggers","title":"Multiple triggers","text":"<p>KEDA allows you to use multiple triggers as part of the same ScaledObject or ScaledJob.</p> <p>By doing this, your autoscaling becomes better:</p> <ul> <li>All your autoscaling rules are in one place</li> <li>You will not have multiple ScaledObject\u2019s or ScaledJob\u2019s interfering with each other</li> </ul> <p>KEDA will start scaling as soon as when one of the triggers meets the criteria. Horizontal Pod Autoscaler (HPA) will calculate metrics for every scaler and use the highest desired replica count to scale the workload to.</p>"},{"location":"kubernetes/autoescaling-finops/keda/98-tips/#hpa-and-keda","title":"HPA and keda","text":"<p>We recommend not to combine using KEDA\u2019s ScaledObject with a Horizontal Pod Autoscaler (HPA) to scale the same workload.</p> <p>They will compete with each other resulting given KEDA uses Horizontal Pod Autoscaler (HPA) under the hood and will result in odd scaling behavior.</p> <p>If you are using a Horizontal Pod Autoscaler (HPA) to scale on CPU and/or memory, we recommend using the CPU scaler &amp; Memory scaler scalers instead.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/","title":"Scaled Object","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specscaletargetref","title":"spec.scaleTargetRef","text":"<p>pending</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#spectriggers","title":"spec.triggers","text":"<p>Here we can define one or more triggers that can activate the autoescaling</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#replicas-and-fallback","title":"Replicas and fallback","text":"<p>We can control the replicas we want with this settings</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specminreplicacount","title":"spec.minReplicaCount","text":"<p>The desired min replicas.</p> <p>If the scaledobject has only cpu/memory triggers, minReplicaCount 0 is not permitted</p> <p>The default value is 0</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specmaxreplicacount","title":"spec.maxReplicaCount","text":"<p>This will be spec.maxReplicas in the hpa and it is the maximum number of replicas of the target resource</p> <p>The default value is 100</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specidlereplicacount","title":"spec.idleReplicaCount","text":"<p>With this setting we can define the number of replicas to scale the target resource to when there is no activity detected by the triggers, but before scaling all the way down to zero.</p> <p>Allows you to keep a minimum number of \"idle\" replicas running when there is no activity, instead of scaling directly to zero.</p> <p>This is an optional setting.</p> <ul> <li>If not specified, it is ignored</li> <li>The only supported value is 0 (https://github.com/kedacore/keda/issues/2314) so it effectively behaves the same as scaling to zero.</li> <li>Must be less than minReplicaCount</li> </ul>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specfallback","title":"spec.fallback","text":"<p>This is an optional setting that permits to configure a default behaviour when an scaler fails getting metrics from the source.</p> <p>It only supports scalers whose target is an AverageValue metric (cpu/memory not supported) and ScaledObjects (not ScaledJobs)</p> <p>Here we can define:</p> <ul> <li>spec.fallback.failureThreshold</li> </ul> <p>As the number of failures needed to start the fallback</p> <ul> <li>spec.fallback.replicas</li> </ul> <p>The desired replicas in fallback</p> <ul> <li>spec.fallback.behavior</li> </ul> <p>The desired behaviour. There are some options here (static, currentReplicas, currentReplicasIfHigher and currentReplicasIfLower)</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#behaviour","title":"Behaviour","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#specpollinginterval","title":"spec.pollingInterval","text":"<p>This field defines the interval to check each trigger, in seconds, to determine if scaling actions are needed.</p> <p>The default value (if not specified) is 30</p> <p>When the deployment or other workload is at 0 replicas, KEDA will check the triggers every seconds defined in spec.pollingInterval. The polling interval is controlled by KEDA.</p> <p>When the deployment is running (with replicas), the HPA</p> <p>A lower value makes KEDA checks more frequently and react faster, but increases API calls/load A higher value makes KEDA checks less frequently with an slower reaction, but less load</p> <p>When scaling from 0 to 1, KEDA will poll for a metric value every pollingInterval seconds while the number of replicas is 0</p> <p>While scaling from 1 to N, this value is controlled by the horizontal-pod-autoscaler-sync-period parameter in the kube-controller-manager</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#cooldown","title":"Cooldown","text":"<ul> <li>spec.cooldownPeriod</li> </ul> <p>It applies when scaling to 0 and when scaling to the minReplicaCount. KEDA waits for the cooldownPeriod after the last trigger reported active before scaling the resource down.</p> <p>Prevents rapid scale-downs by introducing a delay after the last scaling activity. Useful for workloads that may have intermittent spikes, ensuring that resources are not scaled down too quickly after a burst of activity.</p> <p>It is an optional setting with default value 300 (5 minutes)</p> <ul> <li>spec.initialCooldownPeriod</li> </ul> <p>This is the time to wait to scale after the initial creation of the ScaledObject.</p> <p>Prevents immediate scale-downs right after deployment or KEDA startup, giving the workload time to stabilize and process initial events. Useful for workloads that need a warm-up period or to avoid premature scaling down due to delayed metrics or triggers.</p> <p>It is an optional setting with default value 0, so no wait.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#advanced","title":"Advanced","text":"<p>spec.advanced.scalingModifiers spec.advanced.restoreToOriginalReplicaCount spec.advanced.horizontalPodAutoscalerConfig</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#pause-an-scaled-object","title":"Pause an scaled object","text":"<p>It is possible to pause the autoescaling adding this 2 annotations to an ScaledObject.</p> <pre><code>autoscaling.keda.sh/paused: \"true\"\nautoscaling.keda.sh/paused-replicas: \"0\"\n</code></pre> <p>If paused-replicas = 0, the hpa is deleted If paused-replicas = NUMBER, the hpa has minReplicas and maxReplicas = NUMBER</p> <p>To restart autoescaling, we only need to remove the annotations.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaled-object/#links","title":"Links","text":"<ul> <li>Scaling Deployments, StatefulSets &amp; Custom Resources</li> </ul> <p>https://keda.sh/docs/2.17/concepts/scaling-deployments/</p> <ul> <li>ScaledObject specification</li> </ul> <p>https://keda.sh/docs/latest/reference/scaledobject-spec/</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/","title":"Autoscaling","text":""},{"location":"kubernetes/autoescaling-finops/keda/scaling/#what-kuberenetes-resources-can-keda-scale","title":"What kuberenetes resources can keda scale","text":"<p>Keda can scale reources like:</p> <ul> <li> <p>Deployments</p> </li> <li> <p>Statefulsets</p> </li> <li> <p>Custom Resources via ScaleTargetRef</p> </li> </ul> <p>KEDA can scale any Kubernetes resource that implements the /scale subresource such as Rollout, from Argo Rollouts.</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/#caching-metrics","title":"Caching metrics","text":"<p>pending</p>"},{"location":"kubernetes/autoescaling-finops/keda/scaling/#autoscaling-phases","title":"Autoscaling Phases","text":"<ul> <li>Activation/Desactivation phase</li> </ul> <p>Here, the KEDA operator decides if the workload need to be scaled from zero to 1 or from 1 to zero. This phase defines where the sacler is active.</p> <p>If spec.minReplicaCount is &gt;= 1, the scaler is always active and the activation value will be ignored.</p> <ul> <li>Scaling phase</li> </ul> <p>This phase scales from 1 to N or from N to 1. It defines the final Horizontal Pod Autoscaler to be created with the proper settings.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/","title":"AWS Billing","text":"<p>Cost and Usage Report (CUR) or Cost Explorer API</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#summary","title":"Summary","text":"<p>Amazon Athena is a serverless, interactive query service that allows you to analyze data directly in Amazon S3 using standard SQL. We will do this:</p> <ul> <li>Export the billing data to a s3 bucket</li> <li>Configure Athena to analyze that information</li> <li>Configure Opencost to use that Athena settings</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#buckets","title":"Buckets","text":"<p>Create 2 s3 buckets</p> <ul> <li>One for the billing data</li> <li>Another required for Athena results</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#create-data-export","title":"Create data export","text":"<p>Using the AWS Console, go to Billing and Cost Management &gt; Cost &amp; Usage Analysis &gt; Data exports and create an export</p> <p>Choose the export name and the s3 bucket and prefix. Also:</p> <pre><code>Type: Standard data export\nName: Choose one, for example opencost-myekscluster\nData table content settings: CUR 2.0\nAdditional export content: Check \"Include resource IDs\" # https://github.com/opencost/opencost/issues/3076\nTime granularity: Hourly (recommended)\nCompression type and file format: Parquet (optimized for Athena)\nS3 bucket: Choose the s3bucket created for the billing data\nS3 path prefix: Choose a prefix\n</code></pre> <p>First export can take some hours to appear in S3. Wait for initial data generation exploring the s3 bucket some hours later.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#configure-athena","title":"Configure Athena","text":"<p>Using the AWS Console, go to AWS Athena</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#workgroup","title":"Workgroup","text":"<p>Create an Athena workgroup for opencost:</p> <ul> <li>Query result configuration: customer-managed</li> <li>Location of query result: associate the created bucket for athena queries</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#create-database-and-billing-table","title":"Create database and billing table","text":"<p>Under athena query editor, select the workgroup opencost and create the database (glue)</p> <pre><code>CREATE DATABASE IF NOT EXISTS MYDATABASE;\n</code></pre> <p>Then select in the dropdown menu the created database and create the table using the following sql, changing the LOCATION</p> <ul> <li>Create table file</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#add-partitions-required","title":"Add partitions (REQUIRED)","text":"<p>After creating the table, you must add partitions for the billing periods. Without partitions, OpenCost will fail with SQL errors.</p> <p>Check your S3 bucket structure first to see available billing periods, then choose one of these approaches:</p> <p>Option 1: Add partitions manually (change the billing period and bucket path):</p> <pre><code>ALTER TABLE billing ADD IF NOT EXISTS\nPARTITION (billing_period='2025-10')\nLOCATION 's3://YOUR-BUCKET/path-to-data/BILLING_PERIOD=2025-10/';\n</code></pre> <p>Option 2: Auto-discover all partitions (recommended):</p> <pre><code>MSCK REPAIR TABLE billing;\n\n</code></pre> <p>The MSCK REPAIR TABLE command will scan your S3 bucket structure and automatically discover all the partition folders (like BILLING_PERIOD=2024-10/, BILLING_PERIOD=2024-09/, etc.) and add them to the table metadata.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#tests-change-the-billing-period","title":"Tests (change the billing period)","text":"<p>Verify partitions were added:</p> <pre><code>DESCRIBE billing;\nSHOW PARTITIONS billing;\n</code></pre> <p>Query</p> <pre><code>SELECT\n  line_item_usage_start_date,\n  line_item_product_code,\n  SUM(line_item_unblended_cost) as cost\nFROM billing\nWHERE billing_period = '2025-09'\nGROUP BY line_item_usage_start_date, line_item_product_code\nLIMIT 10;\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost","title":"Opencost","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-iam-permissions","title":"Opencost Iam permissions","text":"<p>OpenCost supports four AWS authorizer types:</p> <ol> <li>AccessKey - Direct AWS access key and secret authentication</li> <li>ServiceAccount - Kubernetes service account with pod annotations</li> <li>AssumeRole - IAM role assumption using another authorizer</li> <li>WebIdentity - Web identity token authentication (supports Google as identity provider)</li> </ol> <p>The Service Account authorizer leverages Kubernetes service account annotations and works with AWS pod identity mechanisms like:</p> <ul> <li>EKS Pod Identity</li> <li>IAM Roles for Service Accounts (IRSA)</li> </ul> <p>A very open policy can be this</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-settings","title":"Opencost settings","text":"<ul> <li>For Pod Identity Access</li> <li>Creating a secret called cloud-costs using externalsecrets operator. The target of the external-secret can be this</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#opencost-deployment","title":"Opencost deployment","text":"<p>Configure opencost using helm chart for aws billing, choosing our secret</p> <pre><code>opencost:\n  cloudIntegrationSecret: \"cloud-costs\"\n  cloudCost:\n    enabled: true\n  exporter: # this fixed a bug mounting the secret?\n    extraVolumeMounts:\n      - mountPath: \"/var/configs\"\n        name: cloud-integration\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-billing/#links","title":"Links","text":"<ul> <li>Installing on Amazon Web Services (AWS)</li> </ul> <p>https://opencost.io/docs/configuration/aws/</p> <ul> <li>Creating reports</li> </ul> <p>https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html</p> <ul> <li>IAM</li> </ul> <p>https://github.com/opencost/opencost/issues/3056 https://github.com/opencost/opencost/issues/1217 https://github.com/opencost/opencost/issues/3204 https://github.com/opencost/opencost/issues/2869</p> <p>https://medium.com/@prassonmishra330/cloud-cost-integration-aws-with-opencost-8b9557448e3a</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-in-cluster/","title":"AWS Regular OpenCost (in-cluster costs)","text":"<p>Opencost gets the following information</p> <ul> <li>Kubernetes Resource Usage Metrics (via Prometheus)</li> </ul> <p>OpenCost queries Prometheus for historical usage data so you need metrics from kubelet, node-exporter or kube-state-metrics)</p> <ul> <li>AWS Instance Information (from Kubernetes Node Metadata)</li> </ul> <p>It also get aws instance information like instance type, AWS region, Availability zone, Provider ID</p> <ul> <li>AWS Pricing Data (from Public AWS Pricing API)</li> </ul> <p>It gets EC2 on-demand instance pricing (per instance type, per region), CPU pricing (per vCPU-hour), Memory pricing (per GB-hour), EBS storage pricing, Network egress pricing (inter-AZ and internet)</p> <p>It automatically detects AWS as the cloud provider and fetches pricing from AWS public pricing endpoint:</p> <p>https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/${node_region}/index.json</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-in-cluster/#limitations-without-cur","title":"Limitations Without CUR","text":"<ul> <li>Uses list prices - not your actual negotiated AWS rates</li> <li>No reserved instance discounts - cannot track RI or Savings Plans</li> <li>Spot pricing is estimated - not actual billed spot prices</li> <li>No other AWS services - only EC2/EBS visible (no S3, RDS, etc.)</li> <li>No reconciliation - costs are estimates until CUR data is integrated</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/","title":"AWS Spot Instance Data Feed","text":"<p>It provides OpenCost accurate cost allocation and historical pricing data for spot instances.</p> <p>Spot instance prices fluctuate constantly. The Data Feed provides the actual prices paid at hourly granularity, which is more accurate than using current spot prices for historical costs.</p> <p>To make it work we must:</p> <ul> <li>Configure an Spot Instance Data Feed</li> <li>Give opencost permissions to access to the spot feed bucket via IRSA or PIA</li> </ul> <p>We can use the opencost cloud cost feature to get out-of-cluster costs</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#regional-availability","title":"Regional Availability","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#supported-regions","title":"Supported Regions","text":"<p>Spot Data Feed subscription is available in all AWS regions except:</p> <ul> <li>China (Beijing)</li> <li>China (Ningxia)</li> <li>AWS GovCloud (US)</li> <li>Regions that are disabled by default (opt-in regions)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#opt-in-regions","title":"Opt-in Regions","text":"<p>Regions introduced after March 20, 2019 are disabled by default and require manual enablement:</p> <ul> <li>Africa (Cape Town) - af-south-1</li> <li>Asia Pacific (Hong Kong) - ap-east-1</li> <li>Europe (Milan) - eu-south-1</li> <li>Europe (Spain) - eu-south-2</li> <li>Middle East (Bahrain) - me-south-1</li> <li>Middle East (UAE) - me-central-1</li> <li>Asia Pacific (Hyderabad) - ap-south-2</li> <li>Asia Pacific (Jakarta) - ap-southeast-3</li> <li>Asia Pacific (Melbourne) - ap-southeast-4</li> <li>Canada (Central) - ca-west-1</li> <li>Europe (Zurich) - eu-central-2</li> <li>Israel (Tel Aviv) - il-central-1</li> </ul> <p>Important: Even if an opt-in region is enabled, Spot Data Feed subscription may not be available. For example, eu-south-2 returns <code>UnsupportedOperation</code> error even when the region is enabled.</p>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#troubleshooting-regional-issues","title":"Troubleshooting Regional Issues","text":"<p>If you encounter this error:</p> <pre><code>An error occurred (UnsupportedOperation) when calling the DescribeSpotDatafeedSubscription operation: The functionality you requested is not available in this region.\n</code></pre> <p>Solution: Use an alternative region where the service is fully supported, such as:</p> <ul> <li>eu-west-1 (Ireland)</li> <li>eu-central-1 (Frankfurt)</li> <li>us-east-1 (N. Virginia)</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#create-spot-data-feed-subscription","title":"Create Spot Data Feed Subscription","text":""},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#aws-cli-command","title":"AWS CLI Command","text":"<pre><code>aws ec2 create-spot-datafeed-subscription \\\n    --dry-run \\\n    --bucket your-spot-datafeed-bucket \\\n    --prefix spot-datafeed/ \\\n    --region eu-south-2\n</code></pre> <pre><code>aws ec2 create-spot-datafeed-subscription \\\n    --bucket your-spot-datafeed-bucket \\\n    --prefix spot-datafeed/ \\\n    --region us-east-1\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#parameters","title":"Parameters","text":"<ul> <li><code>--bucket</code>: S3 bucket name where spot pricing data will be stored</li> <li><code>--prefix</code>: Optional prefix for the data files (recommended for organization)</li> <li><code>--region</code>: AWS region where the subscription should be created</li> </ul>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#verify-subscription","title":"Verify Subscription","text":"<pre><code>aws ec2 describe-spot-datafeed-subscription\n</code></pre>"},{"location":"kubernetes/autoescaling-finops/opencost/aws-spot-data-feed/#links","title":"Links","text":"<ul> <li>Track your Spot Instance costs using the Spot Instance data feed</li> </ul> <p>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-data-feeds.html</p>"},{"location":"kubernetes/ingress/path-pathtype/","title":"Path and pathType","text":""},{"location":"kubernetes/ingress/path-pathtype/#path","title":"path","text":"<p>pending</p>"},{"location":"kubernetes/ingress/path-pathtype/#pathtype","title":"pathType","text":"<p>pathType defines what calls will be matched by this rule and we can choose between 3 options:</p>"},{"location":"kubernetes/ingress/path-pathtype/#exact","title":"Exact","text":"<p>This matches the exact URL</p>"},{"location":"kubernetes/ingress/path-pathtype/#prefix","title":"Prefix","text":"<p>This matches an URL that starts with the configured path using / as separator of subpaths</p> <p>This example matches /path/subpath, /path/subpath/another1, /path/subpath/another2 but not /path/subpathanother3</p> <pre><code>path: /path/subpath \npathType: Prefix\n</code></pre>"},{"location":"kubernetes/ingress/path-pathtype/#implementationspecific","title":"ImplementationSpecific","text":"<p>This leaves the path matching to the ingress controller. Depending of the ingress controller, it can treat it as Prefix or Exact This path type provides flexibility for Ingress controllers to implement custom path-matching rules that may not conform to the standard Exact or Prefix types. In the other hand, the same ingress resource is not guaranteed to be consistent across different controllers.</p>"},{"location":"kubernetes/ingress/path-pathtype/#links","title":"Links","text":"<ul> <li>Ingress v1 spec</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/</p>"},{"location":"kubernetes/kubeadm/expose-etcd/","title":"Change listen address in etcd","text":"<p>In a non ha deployment, the etcd metrics listens in 127.0.0.1:2381 by default.</p> <p>If we want to change to 0.0.0.0:2381 we need to change the kubeadm-config ConfigMap</p> <pre><code>kubectl edit cm kubeadm-config -n kube-system\n</code></pre> <p>And leave the etcd section this way</p> <pre><code>apiVersion: v1\ndata:\n  ClusterConfiguration: |\n    etcd:\n      local:\n        dataDir: /var/lib/etcd\n        extraArgs: \n          listen-metrics-urls: http://0.0.0.0:2381\nkind: ConfigMap\nmetadata:\n  name: kubeadm-config\n  namespace: kube-system\n</code></pre> <p>And upgrade all the master nodes</p> <pre><code>kubeadm upgrade node --dry-run\nkubeadm upgrade node\n</code></pre> <p>Check with</p> <pre><code>kubectl describe pod -l component=etcd\n</code></pre>"},{"location":"kubernetes/kubeadm/kubeadm-tips/","title":"Kubeadm tips","text":""},{"location":"kubernetes/kubeadm/kubeadm-tips/#auto-clean-kubeadm-backups","title":"Auto clean kubeadm backups","text":"<p>This command configures tmp systemd-tmpfiles-clean to delete kubeadm backups older than 6 months</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/tmpfiles.d/kubeadm-tmp.conf\ne    /etc/kubernetes/tmp/kubeadm*        -    -    -    6M\nEOF\n</code></pre> <p>And execute it with</p> <pre><code>systemd-tmpfiles --clean /etc/tmpfiles.d/kubeadm-tmp.conf\n</code></pre> <p>Or restart the service</p> <pre><code>systemctl restart systemd-tmpfiles-clean.service\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/","title":"Static control plane pods","text":"<p>Kubeadm deploy some static control plane pods in using the /etc/kubernetes/manifests directory.</p> <ul> <li> <p>They are managed directly by the kubelet, not by the Kubernetes API server or controllers and they are not rescheduled to other nodes</p> </li> <li> <p>They are not evicted by normal Kubernetes eviction mechanisms (such as those triggered by resource pressure or node taints).</p> </li> <li> <p>If killed, kubelet always restarts them</p> </li> <li> <p>When draining a node, they are ignored by default and they will remain running unless you use the --force flag</p> </li> <li> <p>In this case, kubelet uses the requests values only for internal resource management (e.g., for eviction thresholds and reporting), not for scheduling decisions.</p> </li> <li> <p>If a control plane container exceeds its CPU or memory limit, it may be throttled (CPU) or killed (memory), just like any other pod.</p> </li> </ul> <p>Best Practice: Set appropriate requests and limits to protect both the control plane and other workloads on the node.</p>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#configure-requests-and-limits","title":"Configure requests and limits","text":"<p>We can use the patches feature</p> <pre><code>sudo mkdir /etc/kubernetes/patches\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#api-server","title":"Api server","text":"<p>sudo vi /etc/kubernetes/patches/kube-apiserver.yaml</p> <pre><code>spec:\n  containers:\n    - name: kube-apiserver\n      resources:\n        requests:\n          cpu: 1000m\n        limits:\n          memory: 8000Mi\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#controller-manager","title":"Controller manager","text":"<p>sudo vi /etc/kubernetes/patches/kube-controller-manager.yaml</p> <pre><code>spec:\n  containers:\n  - name: kube-controller-manager\n    resources:\n      requests:\n        cpu: 200m\n      limits:\n        memory: 500Mi\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#scheduler","title":"Scheduler","text":"<p>Edit the kube scheduler manifest</p> <pre><code>sudo vi /etc/kubernetes/patches/kube-scheduler.yaml\n</code></pre> <pre><code>spec:\n  containers:\n  - name: kube-scheduler\n    resources:\n      requests:\n        cpu: 100m\n      limits:\n        memory: 300Mi\n</code></pre> <pre><code>sudo kubeadm upgrade node --patches /etc/kubernetes/patches/ --dry-run\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#etcd","title":"Etcd","text":"<pre><code>sudo vi /etc/kubernetes/patches/etcd.yaml\n</code></pre> <pre><code>spec:\n  containers:\n  - name: etcd\n    resources:\n      requests:\n        cpu: 200m\n      limits:\n        memory: 500Mi\n</code></pre> <p>Remember pass this param in all kubeadm upgrades</p> <p>Other way is editing the configuration</p> <pre><code>kubectl edit cm -n kube-system kubeadm-config\n</code></pre>"},{"location":"kubernetes/kubeadm/static-control-plane-pods/#links","title":"Links","text":"<ul> <li>Reconfiguring a kubeadm cluster</li> </ul> <p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure/</p> <ul> <li>kubeadm Configuration (v1beta4)</li> </ul> <p>https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta4/</p> <p>https://serverfault.com/questions/1089688/setting-resource-limits-on-kube-apiserver</p>"},{"location":"kubernetes/kubeadm/upgrade/","title":"Upgrade","text":""},{"location":"kubernetes/kubeadm/upgrade/#read-the-kubernetes-changes","title":"Read the kubernetes changes","text":"<p>Check the changes between releases</p> <ul> <li>Releases</li> </ul> <p>https://kubernetes.io/releases/</p> <ul> <li>Blog</li> </ul> <p>https://kubernetes.io/blog/</p>"},{"location":"kubernetes/kubeadm/upgrade/#read-the-documentation-about-upgrading-process","title":"Read the documentation about upgrading process","text":"<ul> <li>Upgrade A Cluster</li> </ul> <p>https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/</p> <ul> <li>Upgrading kubeadm clusters</li> </ul> <p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p> <ul> <li>Upgrading Linux nodes</li> </ul> <p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/</p>"},{"location":"kubernetes/kubeadm/upgrade/#compatibility","title":"Compatibility","text":"<p>Check the compatibility between all your applications and the new kubernetes release. For example:</p> <ul> <li>CSI driver (cilium,...)</li> <li>CNI driver (vmware, nfs,..)</li> </ul>"},{"location":"kubernetes/kubeadm/upgrade/#check-deprecated-apis","title":"Check deprecated apis","text":"<p>Kubernetes apis can change between release</p> <p>https://kubernetes.io/docs/reference/using-api/deprecation-guide/</p> <p>Tools to check</p> <ul> <li>Silver Surfer</li> </ul> <p>https://github.com/devtron-labs/silver-surfer&gt;</p> <ul> <li>Pluto</li> </ul> <p>https://github.com/FairwindsOps/pluto</p> <ul> <li>Kubepug</li> </ul> <p>https://github.com/kubepug/kubepug</p>"},{"location":"kubernetes/kubectl/98-tips/","title":"Tips","text":""},{"location":"kubernetes/kubectl/98-tips/#delete-all-evicted-pods","title":"Delete all evicted pods","text":"<pre><code>kubectl get pods --all-namespaces | awk '/Evicted/ {print \"kubectl delete po -n \",$1,$2}'|bash -x  \n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-the-current-cluster","title":"Get the current cluster","text":"<pre><code>kubectl config view --minify -o jsonpath='{.clusters[].cluster.server}'\n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-all-images","title":"get all images","text":"<pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{.items[*].spec.containers[*].image}\" |\n    tr -s '[[:space:]]' '\\n' |\n    sort |\n    uniq -c\n</code></pre>"},{"location":"kubernetes/kubectl/98-tips/#get-etcd-status","title":"Get etcd status","text":"<pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table endpoint health\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table endpoint status\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  -w table member list\n</code></pre>"},{"location":"kubernetes/network/ipam/","title":"IP Address Management (IPAM)","text":""},{"location":"kubernetes/network/ipam/#pods","title":"Pods","text":"<p>In Kubernetes, the CNI (Container Network Interface) plugin/driver is responsible for assigning IP addresses to pods. So read your network plugin documentation to know the different options and settings it has.</p> <p>When a pod is created, the Kubelet on the node asks the configured CNI plugin to set up networking for the pod.</p>"},{"location":"kubernetes/network/ipam/#kubelet-cluster-mode","title":"Kubelet cluster mode","text":"<p>In kubelet cluster mode it is possible to enable and specify the global cidr range using the following settings:</p> <p>kube controller manager configuration file</p> <pre><code>AllocateNodeCIDRs: true\nClusterCIDR\n</code></pre> <p>kube controller manager parameter</p> <pre><code>--allocate-node-cidrs=true\n--cluster-cidr \n</code></pre> <p>The default cilium IPAM mode \"Cluster Scope\" ignores this setting and uses ipam.operator.clusterPoolIPv4PodCIDRList (default 10.0.0.0/8)</p>"},{"location":"kubernetes/network/ipam/#kubelet-standalone-mode","title":"Kubelet standalone mode","text":"<p>If we have an standalone kubelet nodes, it is possible to configure the pod cidr range using the following setting:</p> <pre><code>podCIDR # kubelet configuration file\n--pod-cidr # kubelet parameter\n</code></pre>"},{"location":"kubernetes/network/ipam/#services","title":"Services","text":"<p>The kube-apiserver is configured to assign IP addresses to Services.</p> <p>pending</p>"},{"location":"kubernetes/network/ipam/#nodes","title":"Nodes","text":"<p>pending</p>"},{"location":"kubernetes/storage/99-links/","title":"Links","text":"<ul> <li>Storage</li> </ul> <p>https://kubernetes.io/docs/concepts/storage/</p> <ul> <li>storageclass.info</li> </ul> <p>https://storageclass.info/</p> <ul> <li>CSI github</li> </ul> <p>https://github.com/kubernetes-csi</p> <ul> <li>CSI doc</li> </ul> <p>https://kubernetes-csi.github.io/docs</p>"},{"location":"kubernetes/storage/errors/","title":"Errors","text":""},{"location":"kubernetes/storage/errors/#only-dynamically-provisioned-pvc","title":"Only dynamically provisioned pvc","text":"<p>only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize</p> <p>Solution: If storageclass supports resize, add allowVolumeExpansion: true to the storageclass</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/","title":"Increase the pvc size in a statefulset","text":"<p>In order to increase the desired size of a volumeClaimTemplates in a kubernetes statefulset, if we only change the size, we wil get this error:</p> <pre><code>recreating StatefulSet because the update operation wasn't possible\n...\nForbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden\"\n</code></pre> <p>This is because that field is inmutable.</p> <p>If we can lose data, we can simply delete the statefulset and create it again with the desired size. But if we want to maintain the data the steps can be:</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#disable-autosync","title":"Disable autosync","text":"<p>If using autosync in argocd or similar tools, disable autosync</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#increase-the-size-in-the-pvc","title":"Increase the size in the PVC","text":"<p>Edit the PVC manually and change the size with</p> <pre><code>kubectl edit pvc NAME-OF-THE-PVC\n</code></pre> <p>And the wait until it has been resized.</p> <pre><code>kubectl get pvc NAME-OF-THE-PVC -w\n</code></pre> <p>This step needs to have an storageclass with allowVolumeExpansion supported by the storagebackend and enabled in our storageclass definition</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#do-an-orphan-delete-of-the-statefulset","title":"Do an orphan delete of the statefulset","text":"<p>Next we will delete the statefulset via</p> <pre><code>kubectl delete sts --cascade=orphan STATEFULSET\n</code></pre> <p>This will not delete the pods and PVC</p>"},{"location":"kubernetes/storage/increate-size-in-pvc/#reapply-the-statefulset-with-the-new-size","title":"Reapply the statefulset with the new size","text":"<p>Finally apply the manifest with the new size</p> <pre><code>kubectl apply -f STATEFULSET\n</code></pre> <p>If using argocd or similar tool, change the size in the manifest, and do a syn of the statefulset. Enable autosync again</p>"},{"location":"kubernetes/storage/node-disk-protection/","title":"Node disk protection","text":"<p>Kubernetes has 2 mechanisms to protect a node because of disk usage problems</p>"},{"location":"kubernetes/storage/node-disk-protection/#image-garbage-collector","title":"Image Garbage Collector","text":"<ul> <li>Container runtime</li> </ul> <p>Imagefs is the storage space used by the container runtime (containerd/cri-o,docker) for its operations. Containerd tracks and reports usage of its own storage directories, this is /var/lib/containerd/</p> <pre><code>/var/lib/containerd/\n\u251c\u2500\u2500 io.containerd.content.v1.content/     # Image blobs. This is what Image GC primarily targets for removal\n\u251c\u2500\u2500 io.containerd.snapshotter.v1.overlayfs/ # Layer snapshots. NOT cleaned by Image GC - requires container restart/cleanup\n\u251c\u2500\u2500 io.containerd.metadata.v1.bolt/       # Metadata database\n\u2514\u2500\u2500 other containerd directories...\n</code></pre> <ul> <li>Kubelet</li> </ul> <p>Then kubelet queries containerd for storage stats via CRI and receives imageFs data</p> <ul> <li>Image Garbage Collector</li> </ul> <p>Then kubelet removes unused images when some thresholds are reaches.</p> <p>When the imageFS usage reaches the HighThresholdPercent setting, kubelet starts to delete container images ordered by last time they were used until the LowThresholdPercent is reached</p> <p>Since kubernetes 1.30 (beta) we can configure a imageMaximumGCAge as the maximum time a local image can be unused for</p> <ul> <li>Metrics</li> </ul> <p>This metrics are exposed under /proxy/stats/summary API</p> <p>You can see this in the kubelet metrics:</p> <pre><code>kubectl get --raw /api/v1/nodes/&lt;node&gt;/proxy/stats/summary | jq '.node.runtime.imageFs'\n</code></pre>"},{"location":"kubernetes/storage/node-disk-protection/#pod-eviction","title":"Pod eviction","text":"<p>Node-pressure eviction can remove pods because a threshold has been reached, and there are 3 filesystem identifiers that can be used with eviction signals:</p> <ul> <li>nodefs</li> </ul> <p>Is the directory path defined under --root-dir kubelet setting. The default is /var/lib/kubelet</p> <p>nodefs.available is calculated via node.stats.fs.available</p> <ul> <li>imagefs</li> </ul> <p>Here we have the container images. In containerd this is located in /var/lib/containerd/images/</p> <p>imagefs.available eviction signal is calculated via node.stats.runtime.imagefs.available</p> <ul> <li>containerfs</li> </ul> <p>Here we have the writeable layers and logs. In containerd this is located in /var/lib/containerd/containers/</p> <p>containerfs.available eviction signal is calculated via node.stats.runtime.containerfs.available</p> <p>We can get this 3 data for a node with:</p> <pre><code>kubectl get --raw /api/v1/nodes/NODE/proxy/stats/summary | jq '.node.fs'\nkubectl get --raw /api/v1/nodes/NODE/proxy/stats/summary | jq '.node.runtime'\n</code></pre> <p>In BottleRocket OS or Flatcar all paths are under the same / overlay partition so the result is the same.</p>"},{"location":"kubernetes/storage/node-disk-protection/#soft-and-hard-eviction","title":"Soft and hard eviction","text":"<ul> <li>The soft eviction has a grace period until kubelet start to evict pods.</li> <li>The hard eviction has no grace period</li> <li>By default, only hard evictions are configured: imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10% This can be a good situation for spot instances, stateless workloads or environments with constant pod creation/deletion</li> <li>For production environments, define eviction soft settings with higher values and trigger some automatic and proactive cleanup during grace period</li> <li>Setup soft and hard prometheus alerts</li> </ul> <pre><code>evictionHard: # default\n    nodefs.available: \"10%\" \n    imagefs.available: \"15%\"\nevictionSoft:\n    nodefs.available: \"20%\"    # 5% buffer before hard eviction\n    imagefs.available: \"25%\"   # 10% buffer before hard eviction\nevictionSoftGracePeriod:\n    nodefs.available: \"2m\"     # Allow 2 minutes for cleanup/migration\n    imagefs.available: \"2m\"\n</code></pre>"},{"location":"kubernetes/storage/node-disk-protection/#links","title":"Links","text":"<ul> <li>Node-pressure Eviction</li> </ul> <p>https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/</p> <ul> <li>Garbage collection of unused containers and images</li> </ul> <p>https://kubernetes.io/docs/concepts/architecture/garbage-collection/#containers-images</p> <ul> <li>Local ephemeral storage</li> </ul> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/","title":"CPU requests and limits","text":""},{"location":"kubernetes/tuning/cpu-requests-limits/#units","title":"Units","text":"<p>The CPU requests and limits can be specified in some formats:</p> <ul> <li>Using a number, with or without decimals</li> <li>Using milicores / milicpu (a cpu capacity / 1000)</li> </ul> <p>For values less that 1 CPU, it is recommended to use the milicpu format and not the decimal format</p> <pre><code>resources:\n    requests:\n    cpu: 2 # 2 CPU\n</code></pre> <pre><code>resources:\n    requests:\n    cpu: 0.5 # Half CPU\n</code></pre> <pre><code>resources:\n    requests:\n    cpu: \"200m\" # 200 milicores = 0.2\n</code></pre> <p>If we define a cpu limit but not a CPU request, kubernetes gives both the same value.</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#cpu-request","title":"CPU request","text":"<p>Giving a CPU request to a kubernetes container has some implications:</p> <ul> <li>Scheduling:</li> </ul> <p>Kube scheduler will assign that pod to a node that has that CPU resources as available. If no node has that resources the pod will be in Pending state until they new resources are available.</p> <ul> <li>Guarantee:</li> </ul> <p>Once the container is up in the node, that CPU resources will be guaranteed.</p> <ul> <li>Weight in CPU contention:</li> </ul> <p>The requested CPU will be used during CPU contention situations in the node. Containers with higher CPU requests will have higher priority accesing the node CPU.</p> <p>Este maximo de cpu que puede lograr vendra dado por la capacidad del nodo o, si el container tiene cpu limit, por dicho limite.</p> <ul> <li>HPA</li> </ul> <p>That value will be used in the CPU horizontal pod autoescaler</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#cpu-limit","title":"CPU limit","text":"<p>The CPU limit in kubernetes is controlled by the linux kernel and the Completely Fair Scheduler (CFS).</p> <p>It can be defined as the maximum CPU time a container can use every in a CPU cycle interval</p> <p>The CPU limit is translated to the CFS as the cpu_quota_us setting, and the cycle interval as the cpu_period_us</p> <p>The default cpu_period_us is 100 ms</p> <p>So a container with 200m as limit, cannot use more than 200 milicpu at every 100 miliseconds.</p> <p>Using CPU limits has some implications:</p> <ul> <li>Protecting other containers</li> </ul> <p>Ensures a container cannot use too much resources and affect others</p> <ul> <li>CPU throttling</li> </ul> <p>CPU under kubernetes is a compressible resource. When a container reaches that quota, it will have to wait to the next period (100 ms) to try to access to the CPU resources. This affects the performance and latency in the container and unwanted an uncontrolled situations can appear, like readiness or liveness probe failures.</p> <ul> <li>Underutilization</li> </ul> <p>Also, the node can have free CPU resources available, but they are not accesible by the limited container (underused) Not using CPU limits permits a better use of the CPU node resources</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#some-thougts-and-best-practices","title":"Some thougts and best practices","text":"<ul> <li> <p>Always define CPU requests. This also avoids the qos class besteffort</p> </li> <li> <p>In order to define a good value it is very important to have access to metrics about cpu consumption. Utilities like Goldilocks (vpa) o Robusta KRR can give recommendations.</p> </li> <li> <p>Using CPU limits gives more importance to protect the nodes and other containers during high CPU usage and to having a controlled environment.</p> </li> <li> <p>Some people recommends not using CPU limits and only use good CPU request values. It can also be disabled at kubelet level using cpuCFSQuota=0.</p> </li> <li> <p>For pods that can have more that 1 replicas it can be useful to use horizontal pod autoescaler or Keda</p> </li> <li> <p>For pods with only 1 replica,  it can be useful to use vertical pod autoescaling</p> </li> <li> <p>In some environments can be a good practice to change the cpu_period_us</p> </li> </ul>"},{"location":"kubernetes/tuning/cpu-requests-limits/#links","title":"Links","text":"<ul> <li>Resource Management for Pods and Containers  </li> </ul> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p> <ul> <li>Assign CPU Resources to Containers and Pods  </li> </ul> <p>https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/</p> <ul> <li>Pod Overhead  </li> </ul> <p>https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/</p> <ul> <li>For the Love of God, Stop Using CPU Limits on Kubernetes  </li> </ul> <p>https://home.robusta.dev/blog/stop-using-cpu-limits</p> <ul> <li>For the love of god, learn when to use CPU limits on Kubernetes  </li> </ul> <p>https://medium.com/@eliran89c/for-the-love-of-god-learn-when-to-use-cpu-limits-on-kubernetes-2225341e9dbd</p> <ul> <li>Why You Should Keep Using CPU Limits on Kubernetes  </li> </ul> <p>https://dnastacio.medium.com/why-you-should-keep-using-cpu-limits-on-kubernetes-60c4e50dfc61</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 1  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-1/</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 2  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-2/</p> <ul> <li>Kubernetes resources under the hood \u2013 Part 3  </li> </ul> <p>https://directeam.io/blog/kubernetes-resources-under-the-hood-part-3/</p> <ul> <li>CPU Limits in Kubernetes: Why Your Pod is Idle but Still Throttled: A Deep Dive into What Really Happens from K8s to Linux Kernel and Cgroups v2</li> </ul> <p>https://www.reddit.com/r/kubernetes/comments/1k28c00/cpu_limits_in_kubernetes_why_your_pod_is_idle_but/</p>"},{"location":"kubernetes/tuning/cpu-requests-limits/#tools","title":"Tools","text":"<ul> <li> <p>Kube capacity https://github.com/robscott/kube-capacity</p> </li> <li> <p>Robusta KRR https://github.com/robusta-dev/krr</p> </li> <li> <p>Goldilocks https://github.com/FairwindsOps/goldilocks</p> </li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/","title":"Graceful terminations in pods","text":"<p>Pods are ephemeral and there a lot of reasons why they can be deleted so it is critical to make our application to shutdown gracefully. In order to do it the proper way, we must understand the pod deletion processes.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#api-call","title":"Api call","text":"<p>The deletion starts with an request to the api server. Then the api server changes the status of the pod top to \"Terminating\". This triggers 2 parallel process.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#endpoint-deletion","title":"Endpoint deletion","text":"<p>The endpoint controller starts removing the pod from EndpointSlices and Endpoints. The endpoints are not deleted immediately from the EndpointSlices. The EndpointSlices change the status from this:</p> <pre><code>conditions:\n    ready: true\n    serving: true\n    terminating: false\n</code></pre> <p>to</p> <pre><code>conditions:\n    ready: false\n    serving: true\n    terminating: true\n</code></pre> <p>The pod stops receiving new connections. At the end, the pod related entries are removed in kube-proxy, coredns,...</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#pod-termination","title":"Pod termination","text":"<p>At the same time the pod termination process starts with 2 phases: the preStop hook and the SIGTERM</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#prestop-hook","title":"preStop hook","text":"<p>First of all, a preStop hook is executed if defined in the pod. The preStop hook is a command or http request you can use to execute in the pod. Here the application does not know it will be terminated.</p> <p>In addition to a command or http requests, there is a beta feature called \"sleep\" that pauses the container for a time.</p> <p>It is not a good idea to use a typical sleep command because the time to do a proper shutdown will be different in every situation. Another bad idea is to expose the http endpoint called in the http request. It should be internal.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#sigterm","title":"SIGTERM","text":"<p>After this, kubelet sends sends the SIGTERM (terminate) signal to the process with ID 1 to all the containers in the pod and it waits for a response that tells it is safe to delete it. If there is no response, the application is deleted inmediately with a SIGKILL signal.</p> <p>Because of this, the application needs to be prepared to manage that SIGTERM signal and do a graceful shutdown.</p> <p>The default signal received by the container is the SIGTERM. In a Dockerfile you can change the signal sent to the main process with the STOPSIGNAL instruction. For example, the nginx official image changes it to SIGQUIT, but the correct interpretation of this change depends of the container runtime.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#terminationgraceperiodseconds","title":"terminationGracePeriodSeconds","text":"<p>This setting is the time kubernetes waits before using that SIGKILL signal. Lets see some explanation about this setting:</p> <ul> <li>The terminationGracePeriodSeconds includes the time dedicated to the preStop hook and the SIGTERM.</li> <li>The default value is 30 seconds and it can be modified. If the preStop hook and SIGTERM need more time, increase it.</li> <li>If the process reaches the time defined in terminationGracePeriodSeconds while the preStop hook is being executed, kubelet extends this time 2 more seconds</li> <li>If you give to it the value zero, kubernetes uses inmediatly the SIGKILL signal.</li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/#note-about-race-condition","title":"Note about race condition","text":"<p>Because the deletion from the endpoints and the pod termination are executed at the same time, we can have a race condition where the pod has been deletes but the endpoint exists. The system tries to send traffic to a pod that does not exists.</p> <p>This can suggest the preStop hook is a safer way in that cases because it is executed before the SIGTERM and endpoint removal. There are another solutions like wait some seconds in the application's code where the SIGTERM is received in order to give time to the endpoint deletion, but that needed time can vary in every situation and maybe it is not the best option.</p>"},{"location":"kubernetes/tuning/graceful-termination-pods/#best-practices","title":"Best practices","text":"<ul> <li>A good readiness probe is a must in every container, it tells kubelet when the container is ready to accept incoming connections</li> <li>Your application must manage the SIGTERM signal in order to get a gracefulshutdown and do proper tasks.</li> <li>A preStop hook prevents a race condition. Some people say you must always use it.</li> <li>It is a decision to take, what must be included in the preStop hook and what the SIGTERM must do in the application.</li> <li>Configure a proper terminationGracePeriodSeconds for every application</li> </ul>"},{"location":"kubernetes/tuning/graceful-termination-pods/#more-info","title":"More info","text":"<ul> <li>Pod Lifecycle  </li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/</p> <ul> <li>Pod Lifecycle spec  </li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle</p> <ul> <li>Dockerfile reference: STOPSIGNAL</li> </ul> <p>https://docs.docker.com/reference/dockerfile/#stopsignal</p> <ul> <li>Graceful shutdown in Kubernetes  </li> </ul> <p>https://learnk8s.io/graceful-shutdown</p> <ul> <li>How to Gracefully Shutdown Your Apps with a preStop Hook</li> </ul> <p>https://www.youtube.com/watch?v=ahCuWAsAPlc</p> <ul> <li>Decoding the pod termination lifecycle in Kubernetes: a comprehensive guide</li> </ul> <p>https://www.cncf.io/blog/2024/12/19/decoding-the-pod-termination-lifecycle-in-kubernetes-a-comprehensive-guide/</p>"},{"location":"kubernetes/tuning/memory-requests-limits/","title":"Memory requests and limits","text":""},{"location":"kubernetes/tuning/memory-requests-limits/#recommendations","title":"Recommendations","text":"<ul> <li>The values need regular observation to see the baseline memory usage and spikes</li> <li>Always use memory limits</li> <li>If you want overprovisioning, add memory requests to a level slightly above the average baseline memory usage and add memory limits to absorbe spikes.</li> <li>If not, always set your memory requests equal to your limits to a level to absorbe spikes.</li> <li>Use Horizontal pod autoescaler for workloads with replicas</li> <li>Investigate to use vertical pod autoescaling in workloads without replicas</li> </ul> <p>Example:</p> <ul> <li>Observed Peak: 2000Mi</li> <li>Observed Baseline: 1000Mi</li> <li>Request with Overprovision: 1100Mi</li> <li>Limit: 2300Mi or greater</li> </ul>"},{"location":"kubernetes/tuning/memory-requests-limits/#links","title":"Links","text":"<ul> <li>Kubernetes OOM and CPU Throttling</li> </ul> <p>https://sysdig.com/blog/troubleshoot-kubernetes-oom/</p> <ul> <li>What Everyone Should Know About Kubernetes Memory Limits, OOMKilled Pods, and Pizza Parties</li> </ul> <p>https://home.robusta.dev/blog/kubernetes-memory-limit</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/","title":"Pod disruption budget","text":"<p>Pod disruption budget, estable desde kubernetes 1.21, existe para ofrecer un mayor control sobre operaciones que suponen desalojos (disruptions) voluntarios en deployments o statefulsets principalmente.</p> <p>Podemos traducir disruption como \"interrupcion\" y eviction como \"desalojo\"</p> <p>El ejemplo mas tipico es al hacer un drain de un nodo, ya sea de forma manual o mediante herramientas como Cluster Autoescaler o Karpenter. PDB permite establecer un minimo de replicas que deben estar disponibles o bien un maximo de replicas que pueden no estar disponibles durante el proceso. De esta forma se detiene el drain hasta que no se hayan levantado las suficiente replicas en otros nodos.</p> <p>Esto es debido a que \"drain\" para funcionar utiliza la Eviction API, la cual respeta estos PDB. Borrar un pod no lo hace.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#creacion-de-un-pdb","title":"Creacion de un PDB","text":"<p>La creacion de un pdb tiene las siguientes configuraciones:  </p> <ul> <li> <p>Selector Primeramente debemos elegir a que pods aplica el pdb mediante un clasico label selector (matchLabels o matchExpressions)</p> </li> <li> <p>Definir el comportamiento Aqui podemos elegir si queremos un minimo de replicas levantadas (minAvailable) o un maximo de replicas no disponibles (maxUnavailable). Son configuraciones excluyentes.</p> </li> </ul> <p>Podemos especificar este valor mediante un numero</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: integer\nspec:\n  minAvailable: 5\n  selector:\n    matchLabels:\n      app: myapp\n</code></pre> <p>o bien un porcentaje</p> <pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: percentage\nspec:\n  maxUnavailable: 30%\n  selector:\n    matchLabels:\n      app: myapp\n</code></pre> <ul> <li>unhealthyPodEvictionPolicy Por defecto, pdb cuenta un pod como \"healthy\" cuando su status es type=\"Ready\" y status=\"True\".</li> </ul> <p>Con unhealthyPodEvictionPolicy, feature en beta desde kubernetes 1.27, se puede cambiar el criterio sobre como actuar sobre \"unhealthy\" pods.</p> <p>Con el valor IfHealthyBudget, que es el aplicado por defecto, permite que pods que esten levantados pero no healthy puedan ser desalojados solo cuando se este respetando los criterios del PDB. Este valor puede afectar negativamente a acciones voluntarias cuando tenemos aplicaciones con un mal funcionamiento (estado CrashLoopBackOff) o que reportan de forma incorrecta su estado Ready y cuentan con una proteccion via pdb.</p> <p>El valor AlwaysAllow  permite que pods que esten levantados pero no healthy puedan ser desalojados independientemente si se cumplen o no los criterios del PDB. Esta opcion es mas agresiva y se comporta mejor en los supuestos antes descritos.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#estado-de-un-pdb","title":"Estado de un pdb","text":"<p>El estado (.status) de un recurso pdb tiene varios campos</p> <ul> <li>currentHealthy es el numero de pods considerados actualmente como healthy</li> <li>desiredHealthy es el numero minimo de pods healthy que debe haber</li> <li>disruptionsAllowed es el numero de disruptions (interrupciones) permitidas</li> </ul> <p>Un pdb protege una aplicacion haciendo que su .status.currentHealthy no sea inferior a .status.desiredHealthy poniendo el disruptionsAllowed a 0</p> <ul> <li>expectedPods es el numero de pods que se esperan que esten healthy</li> <li>conditions nos muestra si nay mas pods que los requeridos por el pdb y se permiten interrupciones (SufficientPods) o no (InsufficientPods)</li> <li>disruptedPods muestra los pods marcados para ser desalojados pero que aun no han sido matados. Es basicamente un listado de los pods que van a ser desalojados. Este listado deberia estar vacio normalemente. Si se mantiene con varias entradas puede haber problemas de borrado de pods</li> </ul>"},{"location":"kubernetes/tuning/pod-disruption-budget/#algunas-recomendaciones","title":"Algunas recomendaciones","text":"<ul> <li>Tener un numero alto de replicas en una aplicacion stateless puede sugerir el uso de minAvailable (o maxUnavailable) con un porcentaje</li> <li>En aplicaciones stateless con pocas replicas se puede recurrir al uso de enteros</li> <li>En aplicaciones stateful hay que alinear la configuracion con la naturaleza de la misma. Por ejemplo en 5 replicas poner minAvailable a 3 o maxUnavailable a 2 para respetar un quorum.</li> <li>Aplicaciones con una sola replica puede ser recomendable o no usar pdb y asumir la perdida de servicio en las interrupciones voluntarias, o bien poner un pdb con maxUnavailable=0 para, de entrada, bloquear la interrupcion a la espera de una intervencion manual, como borrar el pdb.</li> <li>Utilizar unhealthyPodEvictionPolicy AlwaysAllow en aplicaciones con frecuentes CrashLoopBackOff y que queramos proteger mediante pdb</li> </ul> <p>Valores muy restrictivos perjudican las interrupciones voluntarias y valores muy agresivos pueden no proteger lo suficiente la aplicacion.</p>"},{"location":"kubernetes/tuning/pod-disruption-budget/#links","title":"Links","text":"<ul> <li> <p>Disruptions https://kubernetes.io/docs/concepts/workloads/pods/disruptions/</p> </li> <li> <p>Specifying a Disruption Budget for your Application https://kubernetes.io/docs/tasks/run-application/configure-pdb/</p> </li> <li> <p>PodDisruptionBudget Spec https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/pod-disruption-budget-v1/</p> </li> <li> <p>Safely Drain a Node https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/</p> </li> <li> <p>API-initiated Eviction https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/</p> </li> </ul>"},{"location":"kubernetes/tuning/prioridades/","title":"Prioridades entre pods","text":""},{"location":"kubernetes/tuning/prioridades/#priority-classes-scheduling","title":"Priority classes (scheduling)","text":"<p>El uso de priority classes es una herramienta para dar prioridades a pods a la hora de ser desplegados en un cluster de kubernetes. Mediante ellas, si un pod con una prioridad mas alta no puede ser desplegado, el scheduler intentara por defecto desalojar pods con prioridades mas bajas para hacerle hueco.</p> <p>Por defecto, Kubernetes viene con 2 PriorityClass listas para ser asignadas a pods</p> <ul> <li>system-node-critical, con prioridad mayor, 2000001000</li> <li>system-cluster-critical, con prioridad 2000000000</li> </ul> <p>Asi, en una priorityclass se  puede definir</p> <ul> <li>Una descripcion</li> <li>El numero que especifica la prioridad</li> <li>globalDefault: Para espeficar si es la priority class por defecto</li> <li>preemptionPolicy: Admite 2 valores: Con Never los pods con mas prioridad estaran en la cola antes que los de menos prioridad, pero no se liberaran recursos. Es decir, si no hay recursos para ellos seguiran en la cola sin ser desplegados hasta que se hayan liberado por otros metodos. Con PreemptLowerPriority, que es el valor por defecto, se liberan recursos</li> </ul> <p>To preempt significa adelantarse</p>"},{"location":"kubernetes/tuning/prioridades/#lista-de-algunas-priority-classes-y-como-se-crean","title":"Lista de algunas priority classes y como se crean","text":"Priority Class Value Deployer system-node-critical 2000001000 kubeadm system-cluster-critical 2000000000 kubeadm high-priority 100000000 custom workflow-controller 1000000 argo workflows"},{"location":"kubernetes/tuning/prioridades/#lista-de-algunos-workloads-y-sus-default-priority-classes","title":"Lista de algunos workloads y sus default priority classes","text":"Workload Default cilium Daemonset system-node-critical ebs-csi-node Daemonset system-node-critical eks-pod-identity-agent Daemonset system-node-critical argo workflow controller workflow-controller karpenter system-cluster-critical aws-load-balancer-controller system-cluster-critical cilium-operator system-cluster-critical coredns system-cluster-critical karpenter system-cluster-critical ebs-csi-controller system-cluster-critical node-exporter system-cluster-critical karpenter system-cluster-critical metrics server system-cluster-critical vsphere-csi-controller system-cluster-critical"},{"location":"kubernetes/tuning/prioridades/#listar-pods-de-una-priorityclass","title":"Listar pods de una priorityclass","text":"<pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"system-cluster-critical\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre> <pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"system-node-critical\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre> <pre><code>kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priorityClassName==\"high-priority\") | .metadata.name + \" in \" + .metadata.namespace'\n</code></pre>"},{"location":"kubernetes/tuning/prioridades/#quality-of-service-classes","title":"Quality of Service Classes","text":""},{"location":"kubernetes/tuning/prioridades/#links","title":"Links","text":"<ul> <li> <p>Pod Priority and Preemption https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/</p> </li> <li> <p>Guaranteed Scheduling For Critical Add-On Pods https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/</p> </li> <li> <p>PriorityClass spec https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/priority-class-v1/</p> </li> <li> <p>Pod Quality of Service Classes https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/ https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</p> </li> <li> <p>Node Pressure Eviction https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/</p> </li> </ul>"},{"location":"kubernetes/tuning/probes/","title":"Kubernetes probes and self healing","text":"<p>If the container has not probes defined, they will be considered as success</p>"},{"location":"kubernetes/tuning/probes/#startupprobe","title":"startupProbe","text":"<p>This was included after readinessProbe and livenessProbe. It is commonly used to setup a delay in pods with slow startup process. The readinessProbe and livenessProbe are not checked until the startupProbe is considered \"Success\".</p> <p>If your container usually starts in more than initialDelaySeconds + failureThreshold \u00d7 periodSeconds, you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.</p> <p>If the probe is not ok, kubelet will kill the container and it will apply the pod restart policy:</p> <ul> <li>Always (default)</li> </ul> <p>The container is restarted</p> <ul> <li>OnFailure  </li> </ul> <p>The container is restart if the container had an exit status different than 0</p> <ul> <li>Never</li> </ul> <p>Never restart the container</p>"},{"location":"kubernetes/tuning/probes/#readinessprobe","title":"readinessProbe","text":"<p>This probe is related with considering the container is ready to accept petitions. It is useful when what to define when to start sending traffic to the container.</p> <ul> <li>If the probe fails, the container is pulled of from the services to stop receiving traffic</li> <li>The default result is \"Failure\". It must be accomplished to be considered as \"Success\"</li> </ul>"},{"location":"kubernetes/tuning/probes/#livenessprobe","title":"livenessProbe","text":"<p>This probe is related with considering the container is alive and running.</p> <p>It is useful like a way to tell kubelet the pod crashed, it encounters an issue or becomes unhealthy. The kubelet will automatically perform the correct action in accordance with the Pod's restartPolicy.</p>"},{"location":"kubernetes/tuning/probes/#recommendations","title":"Recommendations","text":"<ul> <li>Use startupProbe for slow starting apps</li> <li>The probes must be simple and lightweight</li> <li>Ensure the probe target is independent of the main application</li> <li>They can fail in heavy loaded environments</li> <li>In general, it is a best practice to define a livenessProbe and a readinessProbe. And they must be different.</li> <li>If using the same endpoint, set a higher failureThreshold value for the livenessProbe, that is, disconnect traffic and customers earlier, and if things are really bad, then restart.</li> </ul>"},{"location":"kubernetes/workloads/cronjob/","title":"Cronjob","text":""},{"location":"kubernetes/workloads/cronjob/#specstartingdeadlineseconds","title":"spec.startingDeadlineSeconds","text":"<p>This field defines the maximum time in seconds that a job can be delayed its initialization from the initial scheduled time.</p> <p>For example, a problem in the kubernetes cluster can cause the cronjob cannot start the job in the scheduled time. spec.startingDeadlineSeconds is the delay we accept, the tolerance window. If the delay is bigger than startingDeadlineSeconds, the job will not be executed and it will be considered failed.</p>"},{"location":"kubernetes/workloads/cronjob/#specconcurrencypolicy","title":"spec.concurrencyPolicy","text":"<p>This field controls if we permit or not the execution of multiple jobs from this cronjob at the same time.</p> <ul> <li> <p>The default value is \"Allow\"</p> </li> <li> <p>The \"Forbid\" policy does not allow concurrent instances. The new one will be skipped.</p> </li> </ul> <p>If the current job finishes and the second one is in the spec.startingDeadlineSeconds time, this will make the second instance start.</p> <ul> <li>The \"Replace\" policy replaces the currently running Job run with a new Job run</li> </ul>"},{"location":"kubernetes/workloads/cronjob/#links","title":"Links","text":"<ul> <li>CronJob</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/</p> <ul> <li>CronJob spec</li> </ul> <p>https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/cron-job-v1/</p>"},{"location":"kubernetes/workloads/job/","title":"Jobs","text":""},{"location":"kubernetes/workloads/job/#configuring-the-job","title":"Configuring the job","text":"<ul> <li>restartPolicy</li> </ul> <p>OnFailure</p> <ul> <li>Retries</li> </ul> <p>With spec.backoffLimit we can define the number of retries before considering a Job as failed. The default value is 6.</p> <p>By default, a Job will run uninterrupted unless a Pod fails (restartPolicy=Never) or a Container exits in error (restartPolicy=OnFailure), at which point the Job defers to the .spec.backoffLimit described above. Once .spec.backoffLimit has been reached the Job will be marked as failed and any running Pods will be terminated.</p> <ul> <li>activeDeadlineSeconds</li> </ul> <p>When a jobs reaches the number of seconds defined in spec.activeDeadlineSeconds, the pods will be terminated and the pod will have the status \"Failed with DeadlineExceeded as reason\". It is a good way to define a time we think the job is not going well.</p>"},{"location":"kubernetes/workloads/job/#clean-finished-jobs","title":"Clean finished jobs","text":""},{"location":"kubernetes/workloads/job/#via-cronjob","title":"Via Cronjob","text":"<p>pending</p>"},{"location":"kubernetes/workloads/job/#via-ttlsecondsafterfinished","title":"Via ttlSecondsAfterFinished","text":"<p>The spec.ttlSecondsAfterFinished keys permits to define an integer that represents the number of seconds to wait until the TTL Controller deletes the job. If we set this value to 0, the job will be deleted inmediately after it finishes, this is, when it has the \"Complete\" or \"Failed\" status.</p> <p>It is a best practice to define ttlSecondsAfterFinished in jobs that they are not controlled by a cronJob.</p>"},{"location":"kubernetes/workloads/job/#jobs-in-argocd","title":"Jobs in argocd","text":""},{"location":"kubernetes/workloads/job/#links","title":"Links","text":"<ul> <li>Jobs</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/job/</p> <ul> <li>Automatic Cleanup for Finished Jobs</li> </ul> <p>https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/</p>"},{"location":"languages/comment-comment-out/","title":"Comment, comment out and uncomment","text":"<p>In computing</p> <ul> <li> <p>Comment code is the action of adding a comment to a the code</p> </li> <li> <p>Comment out is the action where we exclude a block of code from being executed</p> </li> <li> <p>Uncomment is the actions that reverts this</p> </li> </ul>"},{"location":"languages/go-templating/control-structures/","title":"Control structures","text":""},{"location":"languages/go-templating/control-structures/#if","title":"If","text":"<ul> <li>Checks if a value is non-empty (not zero, not nil, not false, not empty string/slice/map).</li> <li>If the value is present and non-empty, executes the block.</li> <li>If the value is missing and missingkey=error is set, it will error.</li> </ul> <pre><code>{{- if .ignoreDifferences }}\nignoreDifferences:\n{{ toYaml .ignoreDifferences | nindent 4 }}\n{{- end }}\n</code></pre>"},{"location":"languages/go-templating/control-structures/#with","title":"With","text":"<ul> <li> <p>Checks if a value exists and is non-empty.</p> </li> <li> <p>If it exists, enters the block and sets . (dot) to that value inside the block.</p> </li> <li> <p>If the value is missing, the block is skipped without error, even with missingkey=error.</p> </li> </ul> <pre><code>{{- with .ignoreDifferences }}\nignoreDifferences:\n{{ toYaml . | nindent 4 }}\n{{- end }}\n</code></pre>"},{"location":"languages/go-templating/control-structures/#range","title":"Range","text":"<p>Pending</p>"},{"location":"languages/go-templating/missinkey-options/","title":"Missing key options","text":"<p>Control the behavior during execution if a map is indexed with a key that is not present in the map.</p> <p>For that we can assign some values to missingkey</p>"},{"location":"languages/go-templating/missinkey-options/#default-invalid","title":"default / invalid","text":"<p>The default behavior: Do nothing and continue execution. If printed, the result of the index operation is the string  \"NO VALUE\".</p>"},{"location":"languages/go-templating/missinkey-options/#zero","title":"zero","text":"<p>The operation returns the zero value for the map type's element.</p>"},{"location":"languages/go-templating/missinkey-options/#error","title":"error","text":"<p>Execution stops immediately with an error.</p>"},{"location":"languages/yaml/99-links/","title":"Links","text":"<ul> <li>Official website</li> </ul> <p>https://yaml.org/</p> <ul> <li>YAML Multiline</li> </ul> <p>https://yaml-multiline.info/</p> <ul> <li>YAML Learning Guide</li> </ul> <p>https://dev.to/wymdev/yaml-learning-guide-complete-tutorial-3nlc</p>"},{"location":"languages/yaml/arrays/","title":"Arrays / lists","text":"<p>There are 2 ways to represent an array</p>"},{"location":"languages/yaml/arrays/#abbreviated-form","title":"Abbreviated form","text":"<pre><code>rebootDays: [mo,tu,we,th]\n</code></pre>"},{"location":"languages/yaml/arrays/#block-style-multi-line-format","title":"Block style (multi line format)","text":"<pre><code>rebootDays:\n  - mo\n  - tu\n  - we\n  - th\n</code></pre> <p>By using the block style, you can make the array more readable, especially when dealing with longer lists or more complex structures.</p>"},{"location":"misc/android/","title":"Android","text":""},{"location":"misc/android/#fix-location-in-mindgapps-and-lineageos","title":"Fix location in MindGapps and Lineageos","text":"<pre><code>adb shell pm grant com.google.android.gms android.permission.ACCESS_COARSE_LOCATION\nadb shell pm grant com.google.android.gms android.permission.ACCESS_FINE_LOCATION\n</code></pre>"},{"location":"misc/licenses-adquisitions/","title":"Cases","text":""},{"location":"misc/licenses-adquisitions/#ibm","title":"IBM","text":"<p>Centos, bought by Redhat</p> <p>Coreos, bought by Redhat</p> <p>Redhat, bought by IBM</p> <p>Hashicorp, license change and bought by IBM</p> <p>Kubecost, bought by IBM https://techcrunch.com/2024/09/17/ibm-acquires-kubernetes-cost-optimization-startup-kubecost/</p>"},{"location":"misc/licenses-adquisitions/#other","title":"Other","text":"<p>Elasticsearch, license change</p> <p>Redis, license change</p> <p>Fork: Valkey Alternatives: valkey</p> <p>Vmware, bought by Broadcom</p>"},{"location":"networking/onpremise-loadbalancers/","title":"On premise load balancers","text":"<p>On-premise Kubernetes load balancers provide LoadBalancer service type support for clusters running outside of cloud providers. While cloud providers automatically provision load balancers for LoadBalancer services, on-premise clusters need additional solutions to expose services externally. These load balancers typically work by assigning external IP addresses from a pool and using BGP, ARP, or Layer 2 protocols to advertise these IPs to the network infrastructure, making services accessible from outside the cluster.</p>"},{"location":"networking/onpremise-loadbalancers/#cilium-loadbalancer-ip-address-management-lb-ipam","title":"Cilium LoadBalancer IP Address Management (LB IPAM)","text":"<p>Cilium's LB IPAM feature provides LoadBalancer service type support for on-premise clusters by automatically managing IP address allocation and assignment for load balancer services.</p> <ul> <li>Website: https://cilium.io/</li> <li>GitHub: https://github.com/cilium/cilium</li> <li>Documentation: https://docs.cilium.io/en/stable/network/lb-ipam/</li> <li>Stats: \u2b50 22.5k stars, \ud83d\udc65 1000+ contributors</li> <li>Language: Go, eBPF</li> <li>Companies: Isovalent</li> <li>CNCF Relation: Graduated project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#metallb","title":"MetalLB","text":"<p>MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.</p> <ul> <li>Website: https://metallb.universe.tf/</li> <li>GitHub: https://github.com/metallb/metallb</li> <li>Stats: \u2b50 7.8k stars, \ud83d\udc65 210+ contributors</li> <li>Language: Go (68.6%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#kube-vip","title":"Kube-VIP","text":"<p>Kube-VIP provides Kubernetes Virtual IP and Load-Balancer for both control plane and Kubernetes services in bare-metal, edge, and virtualization environments.</p> <ul> <li>GitHub: https://github.com/kube-vip/kube-vip</li> <li>Stats: \u2b50 2.6k stars, \ud83d\udc65 98 contributors</li> <li>Language: Go (95.5%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#loxilbio","title":"loxilb.io","text":"<p>loxilb.io is a cloud-native load balancer for Kubernetes that provides high-performance layer 4 load balancing and can run in standalone mode or as a Kubernetes operator.</p> <ul> <li>Website: https://loxilb.io/</li> <li>GitHub: https://github.com/loxilb-io/loxilb</li> <li>Stats: \u2b50 1.8k stars, \ud83d\udc65 Active contributors</li> <li>Language: Go</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Sandbox project</li> </ul>"},{"location":"networking/onpremise-loadbalancers/#purelb","title":"PureLB","text":"<p>PureLB is a load-balancer orchestrator for Kubernetes clusters that uses standard Linux networking and routing protocols, inspired by MetalLB.</p> <ul> <li>Website: https://purelb.gitlab.io/purelb/</li> <li>GitHub: https://github.com/purelb/purelb</li> <li>Stats: \u2b50 111 stars, \ud83d\udc65 76 contributors</li> <li>Language: Go (96.8%)</li> <li>Companies: Community-driven</li> <li>CNCF Relation: Not a CNCF project</li> </ul>"},{"location":"networking/cilium/99-tips/","title":"Tips","text":""},{"location":"networking/cilium/99-tips/#cilium-pod-does-not-start-in-flatcar","title":"Cilium pod does not start in flatcar","text":"<p>During some months/years, the defaul cilium installation did not start with flatcar. To make it work, it was neccesary to change the user domain from spc_to to unconfined_t in the cilium installation.</p> <p>The setting in helm chart</p> <pre><code>securityContext:\n  seLinuxOptions:\n    type: unconfined_t\n</code></pre> <p>Today this problem seems to be solved</p> <p>See more here</p> <ul> <li>Cilium CNi with k8s does not work with SELinux in permissive mode #891</li> </ul> <p>https://github.com/flatcar/Flatcar/issues/891</p>"},{"location":"networking/cilium/99-tips/#no-inter-node-communication-between-pods-in-vmware-vsphere","title":"No inter node communication between pods in Vmware Vsphere","text":"<p>There is a bug in the vmxnet driver that makes the pods don't have inter node connectivity using vxlan. Cilium forward the packets to the overlay but they don't reach the destiation pod/service</p> <p>The following command in the cilium agent</p> <pre><code>cilium-health status --verbose\n</code></pre> <p>shows</p> <pre><code>HTTP to agent:   Get \"&lt;http://10.42.4.211:4240/hello&gt;\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n</code></pre> <p>One solution is to change the tunnelPort to a different than the default one (8472)</p> <p>The setting in helm chart</p> <pre><code>tunnelPort: 8223\n</code></pre> <p>More info here</p> <ul> <li>Installation on Broadcom VMware ESXi / NSX\uf0c1</li> </ul> <p>https://docs.cilium.io/en/latest/installation/k8s-install-broadcom-vmware-esxi-nsx/</p>"},{"location":"networking/cilium/gateway-api/","title":"Gateway API","text":""},{"location":"networking/cilium/gateway-api/#supported-releases","title":"Supported releases","text":"Cilium release Supported Gateway Api release 1.16 v1.1.0 1.17 v1.2.0"},{"location":"networking/cilium/gateway-api/#enable-gateway-api-support","title":"Enable gateway api support","text":"<ul> <li>Deploy the gateway api crds</li> </ul> <p>https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml</p> <ul> <li> <p>Cilium must be deployed with kubeproxy replacement or nodePort.enabled=true</p> </li> <li> <p>Cilium must have l7Proxy=true (default)</p> </li> </ul> <p>Then enable gateway api with gatewayAPI.enabled=true in the helm chart</p>"},{"location":"networking/cilium/gateway-api/#links","title":"Links","text":"<ul> <li>Gateway API Support</li> </ul> <p>https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/</p> <ul> <li>Migrating from Ingress to Gateway</li> </ul> <p>https://docs.cilium.io/en/stable/network/servicemesh/ingress-to-gateway/ingress-to-gateway/</p>"},{"location":"networking/cilium/upgrade/","title":"Upgrade cilium","text":""},{"location":"networking/cilium/upgrade/#preparation","title":"Preparation","text":""},{"location":"networking/cilium/upgrade/#if-upgrading-to-a-new-minor-version","title":"If upgrading to a new minor version","text":"<ul> <li> <p>First update to the latest patch version of the current minor version.</p> </li> <li> <p>Read the release notes documentation for the new minor version, for example</p> </li> </ul> <pre><code>https://docs.cilium.io/en/v1.16/operations/upgrade/#upgrade-notes # when upgrading to 1.16\nhttps://docs.cilium.io/en/v1.17/operations/upgrade/#upgrade-notes # when upgrading to 1.17\nhttps://docs.cilium.io/en/v1.18/operations/upgrade/#upgrade-notes # when upgrading to 1.18\n</code></pre>"},{"location":"networking/cilium/upgrade/#deploy-a-pre-flight-check","title":"Deploy a pre-flight check","text":"<p>The preflight</p> <pre><code>preflight:\n  enabled: true\nagent: false\noperator:\n  enabled: false\n</code></pre> <p>Remember to delete the pre-flight check at the end</p>"},{"location":"networking/cilium/upgrade/#upgradecompatibility","title":"upgradeCompatibility","text":"<p>The upgradeCompatibility setting minimizes \"datapath disruption during the upgrade\". Configure it with the initial cilium version installed in the cluster</p>"},{"location":"networking/cilium/upgrade/#links","title":"Links","text":"<ul> <li>Upgrade Guide</li> </ul> <p>https://docs.cilium.io/en/stable/operations/upgrade</p>"},{"location":"networking/contour/deployment/","title":"Deployment","text":""},{"location":"networking/contour/deployment/#yaml","title":"YAML","text":"<p>https://projectcontour.io/quickstart/contour.yaml&gt;</p>"},{"location":"networking/contour/deployment/#helm","title":"Helm","text":"<p>https://charts.bitnami.com/bitnami</p>"},{"location":"networking/contour/deployment/#contour-gateway-provisioner","title":"Contour Gateway Provisioner","text":"<p>https://projectcontour.io/quickstart/contour-gateway-provisioner.yaml</p> <p>This method of installation still allows you to use any of the supported APIs for defining virtual hosts and routes:</p> <ul> <li>Ingress</li> <li>HTTPProxy</li> <li>Gateway API\u2019s HTTPRoute and TLSRoute</li> </ul>"},{"location":"networking/contour/deployment/#links","title":"Links","text":"<p>https://projectcontour.io/getting-started/ https://projectcontour.io/docs/main/deploy-options/ https://projectcontour.io/docs/main/config/gateway-api/</p>"},{"location":"networking/contour/expose/","title":"Expose services","text":""},{"location":"networking/contour/expose/#ingress","title":"Ingress","text":"<p>https://projectcontour.io/docs/main/config/ingress/</p>"},{"location":"networking/contour/expose/#httpproxy","title":"HTTPProxy","text":"<p>https://projectcontour.io/docs/main/config/fundamentals/</p>"},{"location":"networking/contour/expose/#gateway-api","title":"Gateway api","text":"<p>https://projectcontour.io/docs/main/config/gateway-api/</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/","title":"Contour to Envoy Gateway Migration Path","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#overview","title":"Overview","text":"<p>VMware Contour, a CNCF Incubating project, has announced a strategic migration path toward Envoy Gateway, a new CNCF project designed to consolidate the fragmented landscape of Envoy-based ingress controllers. This document outlines the migration strategy, timeline, and resources for developers transitioning from Contour to Envoy Gateway.</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#background","title":"Background","text":"<p>In May 2022, VMware announced that Contour would join forces with community leaders including Tetrate and Ambassador to build the new Envoy Gateway project under the Envoy banner. This initiative aims to create a single, cohesive, canonical implementation of a Kubernetes Gateway API reference implementation.</p>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#current-status","title":"Current Status","text":"<ul> <li>Contour Development: Continues with current release cadence and support policy</li> <li>Gateway API Development: VMware's Gateway API development efforts have moved to Envoy Gateway</li> <li>Contour's Gateway API: Remains in experimental state</li> <li>Feature Development: VMware continues feature development on Contour for existing users</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#long-term-direction","title":"Long-term Direction","text":"<ul> <li>Evolutionary Migration: The transition is designed to be gradual, not immediate</li> <li>Feature Parity Requirement: Migration evaluation is postponed until Envoy Gateway achieves feature parity with Contour</li> <li>User Protection: Existing Contour users remain fully supported throughout the transition</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#migration-tools-and-support","title":"Migration Tools and Support","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#planned-migration-tooling","title":"Planned Migration Tooling","text":"<ul> <li>HTTPProxy to Gateway API: Conversion tools to migrate from Contour's HTTPProxy resource to Gateway API</li> <li>Configuration Migration: Tooling to help migrate existing configurations</li> <li>Feature Gap Analysis: Tools to identify features not yet supported by Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#feature-considerations","title":"Feature Considerations","text":"<p>Current HTTPProxy functionality not yet available in Gateway API:</p> <ul> <li>Rate limiting</li> <li>Authentication mechanisms</li> <li>Advanced routing features</li> <li>Custom middleware integrations</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#timeline-and-roadmap","title":"Timeline and Roadmap","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#short-term-current","title":"Short Term (Current)","text":"<ul> <li>Contour maintains full development and support</li> <li>Users can continue using Contour without disruption</li> <li>Gateway API remains experimental in Contour</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#medium-term","title":"Medium Term","text":"<ul> <li>Envoy Gateway development continues toward feature parity</li> <li>Migration tooling development</li> <li>Community feedback and testing</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#long-term","title":"Long Term","text":"<ul> <li>Evaluation of Contour's direction as Envoy Gateway matures</li> <li>Potential transition of users to Envoy Gateway</li> <li>Contour may become a wrapper around Envoy Gateway core</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#benefits-of-migration","title":"Benefits of Migration","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-developers","title":"For Developers","text":"<ul> <li>Simplified API: Easier adoption of Envoy as API gateway \"out of the box\"</li> <li>Standardization: Single canonical implementation reduces fragmentation</li> <li>Community Support: Broader community backing under CNCF</li> <li>Gateway API Native: Built from ground up for Kubernetes Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-organizations","title":"For Organizations","text":"<ul> <li>Reduced Maintenance: Consolidated project reduces operational overhead</li> <li>Better Interoperability: Standard implementation improves compatibility</li> <li>Long-term Support: CNCF governance provides sustainability</li> <li>Vendor Neutrality: Community-driven development model</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#current-recommendations","title":"Current Recommendations","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-new-projects","title":"For New Projects","text":"<ul> <li>Evaluate Envoy Gateway: Consider starting with Envoy Gateway for new deployments</li> <li>Monitor Development: Track Envoy Gateway's feature development progress</li> <li>Test Migration Tools: Participate in migration tooling beta testing</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#for-existing-contour-users","title":"For Existing Contour Users","text":"<ul> <li>Continue Current Usage: No immediate action required</li> <li>Plan for Future: Begin planning migration strategy for long-term</li> <li>Stay Informed: Monitor announcements from both projects</li> <li>Test Compatibility: Evaluate current configurations against Gateway API</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#resources-and-links","title":"Resources and Links","text":""},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#official-announcements","title":"Official Announcements","text":"<ul> <li>Contour Joins Forces With Community Leaders to Build New Envoy Gateway Project - VMware Open Source Blog</li> <li>Introducing Envoy Gateway - CNCF Blog</li> <li>VMware Hands Control of Kubernetes Ingress Controller Contour to CNCF - Data Center Knowledge</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#project-resources","title":"Project Resources","text":"<ul> <li>Project Contour - Official Contour Website</li> <li>Contour CNCF Project Page - CNCF</li> <li>Envoy CNCF Project Page - CNCF</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Mapping out the future of cluster ingress with Contour and Gateway API - CNCF Blog</li> <li>VMware Tanzu Platform Service Routing with Contour - VMware Tanzu</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#industry-analysis","title":"Industry Analysis","text":"<ul> <li>Envoy Gateway Makes Using Envoy Proxy Easier for Developers - Tetrate</li> <li>VMware's Contour becomes the CNCF's latest incubation-level project - SiliconANGLE</li> </ul>"},{"location":"networking/gateway-api/contour-to-envoy-gateway-migration/#conclusion","title":"Conclusion","text":"<p>The migration from Contour to Envoy Gateway represents a strategic consolidation in the Envoy-based ingress controller ecosystem. While the transition is evolutionary and long-term, developers should begin familiarizing themselves with Envoy Gateway and planning for eventual migration. VMware's commitment to providing migration tools and maintaining Contour support ensures a smooth transition path for existing users.</p> <p>The success of this migration depends on Envoy Gateway achieving feature parity with Contour and the development of robust migration tooling. Organizations should monitor both projects' progress and participate in community testing to ensure their specific use cases are supported in the migration path.</p>"},{"location":"networking/gateway-api/gateway-api-implementations/","title":"Gateway API Implementations","text":""},{"location":"networking/gateway-api/gateway-api-implementations/#requirements","title":"Requirements","text":"<p>For inclusion in this documentation, Gateway API implementations must meet the following criteria:</p> <ol> <li>Open Source: The implementation must be open source software</li> <li>Official Listing: Must be listed on the official Gateway API implementations page</li> </ol>"},{"location":"networking/gateway-api/gateway-api-implementations/#available-open-source-implementations","title":"Available Open Source Implementations","text":"Implementation Status Type GitHub Stars Contributors Description Contour \u2705 Envoy 3.8k 226 CNCF Incubating Envoy-based ingress controller Emissary-Ingress \ud83d\udd34 Envoy 4.5k 215 CNCF Incubating project (Alpha status) Envoy Gateway \u2705 Envoy 2.1k 251 CNCF Graduated Envoy subproject Istio \u2705 Envoy 37.3k 1,147 CNCF Graduated service mesh kgateway \u2705 Envoy 4.9k 219 CNCF Sandbox AI-powered API Gateway (formerly Gloo) Cilium \ud83d\udd34 eBPF 22.5k 955+ CNCF Graduated eBPF-based networking solution Apache APISIX \ud83d\udd34 Other 15.7k 480 Apache Foundation API Gateway Easegress \u2705 Other 5.9k 65+ CNCF Sandbox Cloud Native traffic orchestration Flomesh Service Mesh \ud83d\udd34 Other 66 9+ Community driven lightweight service mesh HAProxy Ingress \u2705 Other 791 50+ Community driven ingress controller ingate - Other 685 Multiple Kubernetes SIG Network reference implementation Kuma \u2705 Other 3.9k 114 CNCF Sandbox service mesh Linkerd - Other 11.1k 375+ CNCF Graduated service mesh LoxiLB \ud83d\udd34 Other 1.8k 20+ Open source load balancer NGINX Gateway Fabric \u2705 Other 674 56 Open source NGINX implementation Traefik Proxy \u2705 Other 56.8k 891 Open source cloud-native application proxy WSO2 APK \u2705 Other 166 30+ Open source API management solution <p>Status Legend: \u2705 = GA (Generally Available) | \ud83d\udd34 = Beta/Alpha/Experimental | - = Not specified</p> <p>Notes:</p> <ul> <li>ingate is designed as a migration path from NGINX Ingress Controller to Gateway API</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#deprecatedlegacy-implementations","title":"Deprecated/Legacy Implementations","text":"<ul> <li>Acnodal EPIC Open Source External Gateway platform</li> <li>GitHub: epic-gateway/resource-model | \u2b50 0 | \ud83d\udc65 2 contributors</li> <li>Airlock Microgateway Open source WAAP solution</li> <li>GitHub: airlock/microgateway | \u2b50 18 | \ud83d\udc65 3-5 contributors</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#commercialproprietary-implementations","title":"Commercial/Proprietary Implementations","text":"<p>The following commercial or proprietary Gateway API implementations are officially listed:</p> <ul> <li>LiteSpeed Ingress Controller Commercial web ADC controller with Gateway API support</li> <li>Tyk Gateway Cloud-native API Gateway working towards Gateway API implementation</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#cloud-provider-implementations","title":"Cloud Provider Implementations","text":"<p>Cloud providers offer managed Gateway API implementations:</p>"},{"location":"networking/gateway-api/gateway-api-implementations/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>AWS Gateway API Controller Integrates with Amazon VPC Lattice for EKS clusters</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#microsoft-azure","title":"Microsoft Azure","text":"<ul> <li>Azure Application Gateway for Containers Managed application load balancing solution</li> </ul>"},{"location":"networking/gateway-api/gateway-api-implementations/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>GKE Gateway API Controller Integrates with Google Cloud Load Balancers</li> <li>Google Cloud Service Mesh Supports Envoy-based and Proxyless-GRPC mesh implementations</li> </ul>"},{"location":"networking/gateway-api/linking-apps/","title":"Linking Apps","text":""},{"location":"networking/gateway-api/linking-apps/#external-dns","title":"External DNS","text":""},{"location":"networking/gateway-api/linking-apps/#gateways","title":"Gateways","text":"<ul> <li>Gateway sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway/</p>"},{"location":"networking/gateway-api/linking-apps/#routes","title":"Routes","text":"<ul> <li>Gateway API Route Sources</li> </ul> <p>https://kubernetes-sigs.github.io/external-dns/latest/docs/sources/gateway-api/</p>"},{"location":"networking/gateway-api/linking-apps/#cert-manager","title":"Cert-Manager","text":"<p>First we need to enable Gateway Api Support in our controller</p> <pre><code>config:\n  apiVersion: controller.config.cert-manager.io/v1alpha1\n  kind: ControllerConfiguration\n  enableGatewayAPI: true\n</code></pre> <p>Then we can annotate a gateway to generate the certificates</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: example\n  annotations:\n    cert-manager.io/issuer: foo\n</code></pre> <ul> <li>Annotated Gateway resource</li> </ul> <p>https://cert-manager.io/docs/usage/gateway/</p> <ul> <li>Envoy Proxy</li> </ul> <p>https://gateway.envoyproxy.io/docs/tasks/security/tls-cert-manager/</p>"},{"location":"networking/gateway-api/providers/","title":"Providers","text":""},{"location":"networking/gateway-api/providers/#aws","title":"AWS","text":"<p>Under AWS ew can use this solutions with gateway api</p>"},{"location":"networking/gateway-api/providers/#aws-gateway-api-controller","title":"AWS Gateway API Controller","text":"<p>The official AWS controller for Amazon VPC Lattice</p> <p>See https://www.gateway-api-controller.eks.aws.dev/latest/</p>"},{"location":"networking/gateway-api/providers/#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>In version v2.13 \"Using the LBC and Gateway API together is not suggested for production workloads\" See https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/gateway/gateway/ for updated info</p> <p>To make it work it is neccesary to enable it via feature gates</p> <p>With an AWS NLB:</p> <ul> <li>TCPRoute</li> <li>UDPRoute</li> <li>TLSRoute</li> </ul> <p>With an AWS ALB:</p> <ul> <li>HTTPRoute</li> <li>GRPCRoute</li> </ul> <p>Implementation</p> <ul> <li>kgateway</li> </ul> <p>https://kgateway.dev/docs/main/integrations/aws-elb/alb/ https://kgateway.dev/docs/main/integrations/aws-elb/nlb/</p> <ul> <li>Envoy gateway (via EnvoyProxy resource</li> </ul> <p>https://gateway.envoyproxy.io/latest/tasks/operations/customize-envoyproxy/</p>"},{"location":"networking/gateway-api/routes/00-routes/","title":"Gateway API Routes","text":"<p>Gateway API routes are fundamental resources that define how traffic flows from Gateways to backend services. They provide declarative configuration for traffic routing, load balancing, and request processing within Kubernetes clusters.</p>"},{"location":"networking/gateway-api/routes/00-routes/#route-types-overview","title":"Route Types Overview","text":"<p>Gateway API defines several route types to handle different traffic patterns and protocols:</p>"},{"location":"networking/gateway-api/routes/00-routes/#standard-routes-ga","title":"Standard Routes (GA)","text":"<ul> <li>HTTPRoute - HTTP/HTTPS traffic routing with advanced features</li> <li>GRPCRoute - gRPC-specific routing with protocol awareness</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#experimental-routes","title":"Experimental Routes","text":"<ul> <li>TCPRoute - Layer 4 TCP traffic routing</li> <li>TLSRoute - TLS passthrough and SNI-based routing  </li> <li>UDPRoute - UDP traffic routing for stateless protocols</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#core-concepts","title":"Core Concepts","text":""},{"location":"networking/gateway-api/routes/00-routes/#route-attachment","title":"Route Attachment","text":"<p>Routes attach to Gateways through <code>parentRefs</code>, creating a binding between the Gateway listener and the route configuration:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-route\nspec:\n  parentRefs:\n  - name: example-gateway\n    sectionName: http-listener\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#traffic-matching","title":"Traffic Matching","text":"<p>Routes define matching criteria to select which requests they handle:</p> <ul> <li>Hostname matching - Route based on Host header</li> <li>Path matching - Route based on URL paths (exact, prefix, regex)</li> <li>Header matching - Route based on HTTP headers</li> <li>Method matching - Route based on HTTP methods</li> <li>Query parameter matching - Route based on URL parameters</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#backend-selection","title":"Backend Selection","text":"<p>Routes forward matched traffic to backend services with configurable weights and filters:</p> <pre><code>spec:\n  rules:\n  - backendRefs:\n    - name: service-a\n      port: 80\n      weight: 90\n    - name: service-b  \n      port: 80\n      weight: 10\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#request-processing","title":"Request Processing","text":"<p>Routes can modify requests and responses through filters:</p> <ul> <li>Request header modification - Add, set, or remove headers</li> <li>Response header modification - Transform response headers</li> <li>URL rewriting - Modify paths and hostnames</li> <li>Request mirroring - Duplicate traffic to additional backends</li> <li>Request redirection - Return HTTP redirects</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#route-hierarchy","title":"Route Hierarchy","text":"<p>Routes follow a hierarchical attachment model:</p> <ol> <li>Gateway - Defines listeners and infrastructure</li> <li>Route - Defines traffic routing rules</li> <li>Backend - Target services for traffic forwarding</li> </ol>"},{"location":"networking/gateway-api/routes/00-routes/#cross-namespace-routing","title":"Cross-Namespace Routing","text":"<p>Routes can reference backends in different namespaces with proper RBAC configuration:</p> <pre><code>spec:\n  rules:\n  - backendRefs:\n    - name: backend-service\n      namespace: backend-ns\n      port: 8080\n</code></pre> <p>Requires ReferenceGrant in the target namespace:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-routes\n  namespace: backend-ns\nspec:\n  from:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    namespace: frontend-ns\n  to:\n  - group: \"\"\n    kind: Service\n</code></pre>"},{"location":"networking/gateway-api/routes/00-routes/#route-status-and-conditions","title":"Route Status and Conditions","text":"<p>Routes report their status through conditions:</p> <ul> <li>Accepted - Route configuration is valid</li> <li>ResolvedRefs - All references can be resolved</li> <li>PartiallyInvalid - Some rules are invalid but others work</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#best-practices","title":"Best Practices","text":""},{"location":"networking/gateway-api/routes/00-routes/#security","title":"Security","text":"<ul> <li>Use ReferenceGrants for cross-namespace access</li> <li>Implement proper RBAC for route management</li> <li>Validate input through admission controllers</li> <li>Use TLS termination at the Gateway level</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#performance","title":"Performance","text":"<ul> <li>Minimize regex matching in favor of exact/prefix matching</li> <li>Use appropriate backend weights for load distribution</li> <li>Configure timeouts and retry policies appropriately</li> <li>Monitor route performance and adjust configurations</li> </ul>"},{"location":"networking/gateway-api/routes/00-routes/#maintainability","title":"Maintainability","text":"<ul> <li>Use descriptive names and annotations</li> <li>Group related routes logically</li> <li>Document complex routing logic</li> <li>Version control route configurations</li> </ul>"},{"location":"networking/gateway-api/routes/grpcroute/","title":"GRPCRoute (standard)","text":""},{"location":"networking/gateway-api/routes/grpcroute/#purpose","title":"Purpose","text":"<p>GRPCRoute provides gRPC-specific routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route gRPC traffic based on service names, method names, and header matching</li> <li>Load balance gRPC calls across multiple backend services with protocol awareness</li> <li>Handle streaming RPCs for both unary and streaming gRPC communication patterns</li> <li>Apply gRPC-specific filters for request/response transformation and middleware integration</li> <li>Support service mesh integration with advanced gRPC features like retries and circuit breaking</li> <li>Enable microservices communication with type-safe, high-performance RPC calls</li> <li>Manage API versioning through service and method-level routing rules</li> </ul> <p>GRPCRoute operates at the application layer with deep understanding of gRPC protocols, providing sophisticated routing for modern microservices architectures. It's ideal for internal service-to-service communication, API gateways serving gRPC clients, and hybrid environments mixing HTTP and gRPC traffic in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/grpcroute/#reference","title":"Reference","text":"<ul> <li>Info</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/grpc-routing/ https://gateway-api.sigs.k8s.io/api-types/grpcroute/</p> <ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#grpcroute</p>"},{"location":"networking/gateway-api/routes/httproute/","title":"HTTPRoute (standard)","text":""},{"location":"networking/gateway-api/routes/httproute/#purpose","title":"Purpose","text":"<p>HTTPRoute provides HTTP-specific routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route HTTP traffic based on hostnames, paths, headers, and query parameters</li> <li>Load balance traffic across multiple backend services</li> <li>Transform requests through header manipulation, URL rewriting, and redirects</li> <li>Split traffic for A/B testing, canary deployments, and gradual rollouts</li> <li>Mirror traffic to additional backends for testing and monitoring</li> <li>Apply filters for request/response modification and middleware integration</li> </ul> <p>HTTPRoute acts as the configuration layer that defines how HTTP requests should be processed and forwarded to backend services, providing fine-grained control over HTTP traffic routing within your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/httproute/#reference","title":"Reference","text":"<ul> <li> <p>Spec https://gateway-api.sigs.k8s.io/api-types/httproute/ https://gateway-api.sigs.k8s.io/reference/spec/#httproute</p> </li> <li> <p>Info https://gateway-api.sigs.k8s.io/guides/http-routing/ https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/ https://gateway-api.sigs.k8s.io/guides/http-header-modifier/ https://gateway-api.sigs.k8s.io/guides/traffic-splitting/ https://gateway-api.sigs.k8s.io/guides/http-request-mirroring/</p> </li> </ul>"},{"location":"networking/gateway-api/routes/tcproute/","title":"TCPRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/tcproute/#purpose","title":"Purpose","text":"<p>TCPRoute provides Layer 4 TCP traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route TCP traffic based on port and destination matching</li> <li>Load balance TCP connections across multiple backend services</li> <li>Proxy TCP streams for non-HTTP protocols like databases, message queues, and custom applications</li> <li>Handle persistent connections for stateful services requiring session affinity</li> <li>Bridge network protocols for legacy applications and services that don't use HTTP</li> </ul> <p>TCPRoute operates at the transport layer, providing simple but powerful routing for any TCP-based service without requiring protocol-specific knowledge, making it ideal for databases (PostgreSQL, MySQL), message brokers (Redis, RabbitMQ), and other TCP services in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/tcproute/#reference","title":"Reference","text":"<ul> <li>Info</li> </ul> <p>https://gateway-api.sigs.k8s.io/guides/tcp/</p> <ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#tcproute</p>"},{"location":"networking/gateway-api/routes/tlsroute/","title":"TLSRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/tlsroute/#purpose","title":"Purpose","text":"<p>TLSRoute provides TLS-aware traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route TLS traffic based on SNI (Server Name Indication) without terminating encryption</li> <li>Preserve end-to-end encryption by passing through encrypted traffic to backend services</li> <li>Handle TLS passthrough for services that need to manage their own TLS termination</li> <li>Route encrypted protocols like HTTPS, secure databases, and other TLS-wrapped services</li> <li>Support SNI-based routing for multiple domains sharing the same IP address</li> <li>Maintain certificate control at the backend service level</li> </ul> <p>TLSRoute operates at the TLS layer, inspecting only the SNI information in the TLS handshake to make routing decisions while keeping the payload encrypted. This makes it ideal for scenarios where backend services need direct control over TLS termination and certificate management, or when compliance requirements mandate end-to-end encryption.</p>"},{"location":"networking/gateway-api/routes/tlsroute/#reference","title":"Reference","text":"<ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#tlsroute</p>"},{"location":"networking/gateway-api/routes/udproute/","title":"UDPRoute (experimental)","text":""},{"location":"networking/gateway-api/routes/udproute/#purpose","title":"Purpose","text":"<p>UDPRoute provides Layer 4 UDP traffic routing capabilities within the Kubernetes Gateway API. It allows you to:</p> <ul> <li>Route UDP traffic based on port and destination matching</li> <li>Load balance UDP packets across multiple backend services</li> <li>Handle stateless protocols like DNS, DHCP, and logging services</li> <li>Support connectionless communication for services that don't maintain persistent connections</li> <li>Route multimedia traffic for streaming protocols and real-time applications</li> <li>Manage fire-and-forget messaging patterns common in distributed systems</li> </ul> <p>UDPRoute operates at the transport layer for connectionless UDP traffic, providing simple packet forwarding without session tracking. This makes it ideal for services like DNS servers, syslog collectors, game servers, streaming media applications, and other UDP-based services that require fast, low-overhead packet delivery in your Kubernetes cluster.</p>"},{"location":"networking/gateway-api/routes/udproute/#reference","title":"Reference","text":"<ul> <li>Spec</li> </ul> <p>https://gateway-api.sigs.k8s.io/reference/spec/#udproute</p>"},{"location":"networking/kgateway/gateway-classes/","title":"Gateway Classes","text":""},{"location":"networking/kgateway/gateway-classes/#kgateway-vs-kgateway-waypoint-gatewayclasses","title":"kgateway vs kgateway-waypoint GatewayClasses","text":""},{"location":"networking/kgateway/gateway-classes/#kgateway","title":"kgateway","text":"<ul> <li>Purpose: Standard class for managing Gateway API ingress traffic</li> <li>Use case: Regular ingress/gateway functionality for external traffic routing</li> <li>Description: \"Standard class for managing Gateway API ingress traffic\"</li> </ul>"},{"location":"networking/kgateway/gateway-classes/#kgateway-waypoint","title":"kgateway-waypoint","text":"<ul> <li>Purpose: Specialized class for Istio ambient mesh waypoint proxies</li> <li>Use case: Service mesh integration with Istio ambient mode</li> <li>Description: \"Specialized class for Istio ambient mesh waypoint proxies\"</li> <li>Special annotation: ambient.istio.io/waypoint-inbound-binding: PROXY/15088 - indicates integration with Istio ambient mesh waypoint functionality</li> </ul>"},{"location":"networking/kgateway/gateway-classes/#comparison","title":"Comparison","text":"<p>Common characteristics</p> <ul> <li>Both use the same controller: kgateway.dev/kgateway</li> <li>Both are managed by the same kgateway installation</li> <li>Both have Accepted and SupportedVersion conditions</li> </ul> <p>The main difference is that kgateway-waypoint is specifically designed for Istio service mesh ambient mode waypoint proxy functionality, while kgateway is for standard ingress traffic routing.</p>"},{"location":"observability/architecture/","title":"Architecture","text":"<p>This page outlines a pragmatic, operations-focused view of an observability stack.</p>"},{"location":"observability/architecture/#highlevel-layers","title":"High\u2011level layers","text":"<ol> <li>Instrumentation (generation)</li> <li>Application SDKs, auto-instrumentation, exporters emit logs/metrics/traces (L/M/T)</li> <li> <p>Standard protocols: OTLP over gRPC/HTTP; legacy inputs supported via receivers</p> </li> <li> <p>Ingestion/collection</p> </li> <li>Agents/DaemonSets/sidecars and/or a gateway OpenTelemetry Collector receive telemetry</li> <li> <p>Fan\u2011in from nodes, apps, infrastructure, cloud services</p> </li> <li> <p>Processing/enrichment</p> </li> <li>Pipelines apply sampling, filtering, redaction, resource detection, attribute transforms</li> <li>Batching, retry, timeouts, queueing/backpressure control to stabilize flow</li> <li> <p>Routing by signal/tenant/team to specific backends</p> </li> <li> <p>Export/transport</p> </li> <li>OTLP/gRPC or HTTP to remote backends; TLS/mTLS and auth (API keys, OIDC)</li> <li> <p>Circuit breakers, exponential backoff, persistent queues for resilience</p> </li> <li> <p>Storage backends (by signal)</p> </li> <li>Metrics: Prometheus/Thanos/Mimir, VictoriaMetrics</li> <li>Traces: Tempo/Jaeger/Elastic APM</li> <li>Logs: Loki/Elasticsearch/OpenSearch</li> <li> <p>Sometimes a columnar data lake/warehouse for long\u2011term retention and cost control</p> </li> <li> <p>Query/visualization &amp; alerting</p> </li> <li>Grafana/Kibana/Tempo/Jaeger UIs; SLOs and alert rules</li> <li>Routing to Alertmanager, PagerDuty, Opsgenie, Slack, email</li> </ol>"},{"location":"observability/architecture/#where-the-opentelemetry-collector-fits","title":"Where the OpenTelemetry Collector fits","text":"<p>The OTel Collector spans multiple layers:</p> <ul> <li>Layer 2 (Ingestion/collection): receivers (otlp, prometheus, filelog, k8s events)</li> <li>Layer 3 (Processing/enrichment): processors (batch, memory_limiter, attributes/resource, transform, tail_sampling, routing)</li> <li>Layer 4 (Export/transport): exporters (otlp[gRPC/HTTP], prometheusremotewrite, loki, tempo/jaeger), TLS/mTLS, retries/queues</li> </ul> <p>It is not a storage backend or UI (does not cover layers 5 or 6).</p>"},{"location":"observability/architecture/#reference-view-kubernetes-opentelemetry","title":"Reference view (Kubernetes + OpenTelemetry)","text":"<ul> <li>Workloads emit OTLP \u2192 node/sidecar/agent collector (DaemonSet or sidecar)</li> <li>Optional gateway collector (centralized, stateless) \u2192 processes and routes signals</li> <li>Backends per signal; Grafana on top for dashboards, logs, traces, exemplars</li> </ul> <pre><code>Apps/SDKs \u2500\u2500OTLP\u2500\u2500&gt; Node/Sidecar Collector \u2500\u2500OTLP\u2500\u2500&gt; Gateway Collector \u2500\u2500&gt; Backends\n                                              \u2502                          \u251c\u2500&gt; Metrics TSDB\n                                              \u2502                          \u251c\u2500&gt; Traces Store\n                                              \u2502                          \u2514\u2500&gt; Logs Store\n                                              \u2514\u2500\u2500 kube/system exporters (kube-state, cAdvisor)\n</code></pre>"},{"location":"observability/architecture/#collector-deployment-patterns","title":"Collector deployment patterns","text":"<ul> <li>Sidecar: per\u2011pod isolation; simplest context propagation; higher overhead</li> <li>DaemonSet (agent): per\u2011node collector for all workloads; good default</li> <li>Gateway: centralized fan\u2011in; enforces org\u2011wide policy (sampling, PII scrubbing)</li> <li>Common to use Agent (DaemonSet) + Gateway for scale and control</li> </ul>"},{"location":"observability/architecture/#signalspecific-notes","title":"Signal\u2011specific notes","text":"<ul> <li>Metrics: prefer low\u2011cardinality labels; use histograms; remote write to long\u2011term TSDB</li> <li>Traces: sampling strategies (tail\u2011based at gateway for best value; head\u2011based at source for low overhead)</li> <li>Logs: structure at source (JSON); drop/trim noisy lines early; labels/indices budgeted</li> </ul>"},{"location":"observability/architecture/#reliability-and-cost-levers","title":"Reliability and cost levers","text":"<ul> <li>Backpressure: memory_limiter + queued_retry processors in OTel Collector</li> <li>Batching: reduce connection churn and backend CPU</li> <li>Redaction: attributes and body processors for PII/compliance</li> <li>Multi\u2011route: split traffic by environment/tenant to different clusters/backends</li> <li>Retention tiers: hot (short), warm (mid), cold (cheap/archival)</li> </ul>"},{"location":"observability/architecture/#minimal-otel-pipeline-conceptual","title":"Minimal OTel pipeline (conceptual)","text":"<p>Receivers \u2192 processors \u2192 exporters, per signal:</p> <pre><code>receivers: otlp, prometheus, filelog\nprocessors: memory_limiter, batch, attributes(resource), transform, tail_sampling\nexporters: otlphttp(tempo), prometheusremotewrite(mimir), loki\n</code></pre>"},{"location":"observability/architecture/#ops-best-practices","title":"Ops best practices","text":"<ul> <li>Start with a single protocol (OTLP) end\u2011to\u2011end</li> <li>Keep metrics cardinality in check; gate label additions</li> <li>Make sampling an explicit decision (prove value, then tune)</li> <li>Treat collectors as stateless and horizontally scalable</li> <li>Version and test pipelines as code; lint configs; add golden queries/alerts</li> </ul>"},{"location":"observability/architecture/#see-also","title":"See also","text":"<ul> <li>OpenTelemetry Collector: pipelines and processors (receivers \u2192 processors \u2192 exporters)</li> <li>OTLP protocol (gRPC/HTTP)</li> <li>Backend options: Mimir/Thanos, Tempo/Jaeger, Loki/Elasticsearch</li> </ul>"},{"location":"observability/tools/","title":"Observatility Tool","text":""},{"location":"observability/tools/#ecosystems","title":"Ecosystems","text":"Ecosystem Metrics Collector Traces Collector Logs Collector Dashboard Enterprise Opentelemetry OT Collector OT Collector OT Collector Grafana G.Mimir Alloy G.Tempo Alloy / Beyla Loki Alloy Grafana Grafana Cloud / Grafana Enterprise Prometheus Prometheus Elasticsearch Elasticsearch Kibana Elastic Cloud Jaeger Jaeger Collector Jaeger UI"},{"location":"observability/tools/#log-collectors","title":"Log collectors","text":"<ul> <li>Logstash</li> </ul> <p>It is the logs collector from the Elasticsearch ecosystem</p> <ul> <li>Opentelemetry collector</li> </ul> <p>Thel opentelemetry collector can act as a log collector</p> <ul> <li>Grafana Alloy</li> </ul> <p>It is the new name of the Grafana Agent, an Opentelemetry collector distribution from the grafana ecosystem. It also replaces Grafana Promtail.</p> <ul> <li>Fluentbit and fluentd</li> </ul> <p>Both are well known logs collector. Fluentbit is lightweight and simpler.</p> <ul> <li>Vector</li> </ul> <p>Vector is the collector from datadog</p>"},{"location":"observability/alertmanager/amc-beta/","title":"AlertmanagerConfig v1beta1","text":"<p>Does AlertmanagerConfig v1beta1 exists?</p> <ul> <li>v1beta1 was announced but not fully deployed:</li> </ul> <p>The prometheus-operator team announced v1beta1 for AlertmanagerConfig in version 0.57.0 (June 2022), but it was implemented as an opt-in feature requiring conversion webhooks.</p> <ul> <li>Default CRDs still use v1alpha1</li> </ul> <p>The standard CRD files in example/prometheus-operator-crd/ directory only contain v1alpha1. The v1beta1 version would be in example/prometheus-operator-crd-full/ but requires additional webhook setup.</p> <ul> <li> <p>Documentation vs. Reality Gap:</p> </li> <li> <p>OpenShift/OKD documentation shows v1beta1 because they have their own implementation</p> </li> <li>The official prometheus-operator CRDs in their GitHub repo still primarily use v1alpha1</li> <li>Third-party CRD catalogs (like Datree's) follow the standard CRDs, hence only v1alpha1</li> </ul> <p>Current Status: The v1beta1 API exists in the codebase but is not the default deployment method. Most users continue using v1alpha1.</p>"},{"location":"observability/alertmanager/inhibit-rules/","title":"Inhibit rules","text":"<p>A Prometheus inhibit rule is a configuration in Alertmanager that prevents certain alerts from firing when other higher-priority or related alerts are already active. It's a way to reduce alert noise by suppressing redundant or less important notifications.</p>"},{"location":"observability/alertmanager/inhibit-rules/#structure","title":"Structure","text":"<p>In an inhibit rule we must define:</p> <ul> <li>Source alerts</li> </ul> <p>They are alerts that will supress other alerts when active.</p> <ul> <li>Target alerts:</li> </ul> <p>They are the alerts that will be suppressed when source alerts match</p>"},{"location":"observability/alertmanager/inhibit-rules/#example-cases","title":"Example cases","text":"<ul> <li>Supress alerts related with diskspace, cpu usage or memory usage when the node is down</li> <li>Supress warning alerts when critical alerts are firing about the same topic</li> <li>Supress a less important alert when a more important alert. For example, don't alert an application is down when its loadbalancer is down, that causes the application is down</li> </ul> <p>In prometheus operator this can be achieved using an AlertmanagerConfig kubernetes resource spec.inhibitRules</p>"},{"location":"observability/grafana/azure-ad-auth/","title":"Autenticacion via Azure","text":""},{"location":"observability/grafana/azure-ad-auth/#creacion-de-la-aplicacion","title":"Creacion de la aplicacion","text":"<p>Creamos una nueva aplicacion desde App Registrations &gt; New Registration</p> <ul> <li>Supported Account types: Por ejemplo \"Accounts in this organizational directory only\"</li> <li>Redirect URI En el asistente inicial, ponemos https://migrafana.dominio.com</li> </ul>"},{"location":"observability/grafana/azure-ad-auth/#postconfiguracion-de-la-aplicacion","title":"Postconfiguracion de la aplicacion","text":"<ul> <li> <p>Segunda Redirect URl En Manage - Authentication - Web - Redirect URls creamos una nueva con https://migrafana.dominio.com/login/azuread</p> </li> <li> <p>Client Secret En \"Manage - Certificates &amp; secrets - Client secrets\" creamos un nuevo client secret y colocamos su valor en la variable GF_AUTH_AZUREAD_CLIENT_SECRET</p> </li> <li> <p>Groups claim En \"Manage - Token Configuration\" agregamos un groups claim seleccionando \"security groups \"y \"Groups assigned to the application\"</p> </li> <li> <p>Api permissions Deberiamos tener Microsoft Graph - User.Read</p> </li> <li> <p>App Roles Creamos 3 roles con el mismo valor Display Name, Value y Description y eligiendo \"Users/Groups\" como \"Allowed member types\". Ese valor sera Viewer, Editor y GrafanaAdmin</p> </li> <li> <p>Agregar grupos Desde Enterprise Applications entramos en nuestra aplicacion y en Manage - Users and groups elegimos los grupos y usuarios que queremos y los mapemos a los roles deseados</p> </li> </ul>"},{"location":"observability/grafana/azure-ad-auth/#lista-de-variables-de-entorno","title":"Lista de variables de entorno","text":""},{"location":"observability/grafana/azure-ad-auth/#autenticacion","title":"Autenticacion","text":"<pre><code>GF_AUTH_AZUREAD_AUTH_URL: valor que figura como \"OAuth 2.0 authorization endpoint (v2)\" en \"Endpoints\"\nGF_AUTH_AZUREAD_TOKEN_URL: valor que figura como \"OAuth 2.0 token endpoint (v2)\" en \"Endpoints\"\nGF_AUTH_AZUREAD_CLIENT_ID: valor que figura como \"Application (client) ID\" en Overview\nGF_AUTH_AZUREAD_CLIENT_SECRET: valor del secret creado anteriormente\n\n</code></pre>"},{"location":"observability/grafana/azure-ad-auth/#usuarios-y-asignacion-de-roles","title":"Usuarios y asignacion de roles","text":"<p>Si no hay definido ningun rol en la aplicacion, el valor asignado sera el indicado en GF_USERS_AUTO_ASSIGN_ORG_ROLE. Este valor por defecto es Viewer y tambien puede ser Admin, Editor y None.</p> <p>Este funcionamiento de asignar un rol por defecto puede quitarse poniendo a true GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT, lo que no permite el login si no hay un rol definido para el usuario.</p> Variable Valor raz. Funcion GF_USERS_AUTO_ASSIGN_ORG_ROLE Viewer permite definir el rol predeterminado para los usuarios de la organizacion principal GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT deshabilita la asignacion de un rol por defecto GF_AUTH_AZUREAD_SKIP_ORG_ROLE_SYNC evita traerse los roles del Azure. <p>Puede ser una buena practica habilitar GF_AUTH_AZUREAD_ROLE_ATTRIBUTE_STRICT, lo que fuerza a crear roles de aplicacion</p>"},{"location":"observability/grafana/azure-ad-auth/#misc","title":"Misc","text":"Variable Valor raz. Funcion GF_AUTH_AZUREAD_ALLOW_ASSIGN_GRAFANA_ADMIN false Deshabilita al rol GrafanaAdmin el tener privilegios de administrador GF_AUTH_AZUREAD_ALLOW_SIGN_UP true GF_AUTH_AZUREAD_AUTO_LOGIN false Habilitarlo se salta la pantalla de login GF_AUTH_AZUREAD_ENABLED true Habilita Azure ad Auth GF_AUTH_AZUREAD_NAME \"Azure AD\" Nombre de la configuracion GF_AUTH_AZUREAD_SCOPES \"openid email profile\" GF_AUTH_AZUREAD_USE_PKCE true GF_AUTH_AZUREAD_ALLOWED_ORGANIZATIONS identificador de la organizacion que queremos permitir el acceso GF_AUTH_AZUREAD_ALLOWED_GROUPS grupos que permitimos el acceso separado por comas o espacios de i GF_AUTH_AZUREAD_ALLOWED_DOMAIN dominios que permitmos el acceso separado por comas o espacios"},{"location":"observability/grafana/azure-ad-auth/#links","title":"Links","text":"<ul> <li> <p>Configure Azure AD OAuth2 authentication https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/azuread/</p> </li> <li> <p>Configure Grafana https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/</p> </li> </ul>"},{"location":"observability/kubernetes/0-metrics-api/","title":"Kubernetes Metrics Api","text":"<p>The Kubernetes Metrics API is a standardized way to access some metrics using the kubernetes api.</p> <p>It has some different components:</p>"},{"location":"observability/kubernetes/0-metrics-api/#resource-metrics-api","title":"Resource Metrics API","text":"<p>Provides basic resource usage metrics (cpu and memory) for pods and nodes. It is used for:</p> <ul> <li>Autoescaling purposes via horizontal pod autoescaler</li> <li>The kubectl top command</li> </ul> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta1.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.metrics.k8s.io\nspec:\n  group: metrics.k8s.io\n  version: v1beta1\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n  ...\n</code></pre> <p>In order to get and check the metrics provided by the resource metrics api, we can</p> <pre><code>kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\"\n</code></pre> <p>There are 2 known implementations of the resource metrics api</p> <ul> <li>Metrics server</li> </ul> <p>Metrics server is the lightest and simplest implementation. Metrics server queries some kubelet  endpoints to get that information.</p> <p>More info here: metrics-server</p> <ul> <li>Prometheus adapter</li> </ul> <p>Prometheus adapter can replace metrics server and it can also serve other Kubernetes Metrics Api components, like custom and external metrics api. It queries a prometheus instance to get that queries.</p> <p>More info here: prometheus-adapter</p>"},{"location":"observability/kubernetes/0-metrics-api/#custom-metrics-api","title":"Custom Metrics API","text":"<p>The Custom Metrics API is a Kubernetes Metrics APi component that permits to access some custom metrics via the kubernetes api.</p> <p>It permits to define and use that custom metrics for monitoring and autoescaling purposes.</p> <pre><code>kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1\n</code></pre> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta2.custom.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta2.custom.metrics.k8s.io\nspec:\n  group: custom.metrics.k8s.io\n  version: v1beta2\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n  ...\n</code></pre> <p>The most typical implementation of the custom metrics api is the prometheus adapter. For this, the prometheus adapter permits advanced autoescaling based on different metrics other than the cpu and memory.</p> <p>Again, more info here: prometheus-adapter</p> <p>Zalando has another implementation called kube-metrics-adapter</p>"},{"location":"observability/kubernetes/0-metrics-api/#external-metrics-api","title":"External Metrics API","text":"<p>The External Metrics API is a Kubernetes Metrics APi component that permits to access some external metrics via the kubernetes api. Those metrics are not necessarily tied to kubernetes objects.</p> <p>Those metrics also can be used to define and use that external metrics for monitoring and autoescaling purposes, and provides advanced autoescaling features.</p> <p>To access those metrics we need to deploy:</p> <ul> <li>An kubernetes application and service that can serve the queries</li> <li>Register an APIService called v1beta1.external.metrics.k8s.io linked to that service</li> </ul> <pre><code>apiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.external.metrics.k8s.io\nspec:\n  group: external.metrics.k8s.io\n    version: v1beta1\n  service:\n    name: NAME-OF-THE-SERVICE\n    namespace: NAMESPACE\n    port: PORT\n</code></pre>"},{"location":"observability/kubernetes/0-metrics-api/#links","title":"Links","text":"<p>More info at https://github.com/kubernetes/metrics</p>"},{"location":"observability/kubernetes/events/","title":"Kubernetes Events Visibility","text":"<p>Kubernetes events are ephemeral (stored for only 1 hour in etcd). To maintain visibility and enable historical analysis, events need to be exported to an observability backend.</p>"},{"location":"observability/kubernetes/events/#solutions-for-events-export","title":"Solutions for Events Export","text":""},{"location":"observability/kubernetes/events/#grafana-alloy-recommended-for-loki","title":"Grafana Alloy (Recommended for Loki)","text":"<p>Status: Active - Replacement for deprecated Grafana Agent (EOL Nov 2025)</p> <p>Grafana Alloy with the <code>loki.source.kubernetes_events</code> component tails events from the Kubernetes API and forwards them to Loki.</p>"},{"location":"observability/kubernetes/events/#kubernetes-event-exporter","title":"kubernetes-event-exporter","text":"<p>Status: Active - Maintained by resmoio (fork of deprecated opsgenie version)</p> <p>Exports events to multiple destinations: Loki, Elasticsearch, Kafka, Webhooks, etc.</p> <p>Bitnami Helm Chart: <code>bitnami/kubernetes-event-exporter</code></p>"},{"location":"observability/kubernetes/events/#fluent-bit","title":"Fluent Bit","text":"<p>Status: Active - Native kubernetes_events input plugin (v3.1+)</p> <p>Uses Kubernetes watch stream (not polling) to collect events.</p> <p>Important: Must be deployed as a Deployment with single replica, NOT as DaemonSet (no leader election).</p>"},{"location":"observability/kubernetes/events/#fluentd","title":"Fluentd","text":"<p>Status: Active - No native events input plugin</p> <p>Can output to Loki via <code>fluent-plugin-grafana-loki</code>, but requires another tool (like Fluent Bit or Event Exporter) to collect events first.</p> <p>Use case: Events processing pipeline (not collection)</p>"},{"location":"observability/kubernetes/events/#logstash-metricbeat","title":"Logstash + Metricbeat","text":"<p>Status: Active - Requires Metricbeat for events collection</p> <p>Metricbeat has a kubernetes event metricset that collects from the API, then forwards to Logstash for processing.</p> <p>Use case: Elastic stack processing pipeline, not for direct collection</p>"},{"location":"observability/kubernetes/events/#data-prepper","title":"Data Prepper","text":"<p>Status: Active - OpenSearch alternative to Logstash, no native events input</p> <p>Data Prepper is a server-side data collector for filtering, enriching, and transforming data for OpenSearch. Requires another tool (Fluent Bit, kubernetes-event-exporter, or OTel Collector) to collect events first.</p> <p>Use case: OpenSearch stack processing pipeline, not for direct collection</p>"},{"location":"observability/kubernetes/events/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>Status: Active - Vendor-neutral observability data collector with native Kubernetes events receivers</p> <p>Has two receivers for events collection:</p> <ul> <li>k8seventsreceiver - Dedicated Kubernetes events receiver</li> <li>k8sobjectsreceiver - General Kubernetes objects receiver (commonly used for events)</li> </ul> <p>Can export to multiple backends: Loki (otlphttp), Elasticsearch, OpenSearch, Prometheus, etc.</p> <p>Important: Deploy as single-replica Deployment (not DaemonSet)</p> <p>Helm chart preset: Use <code>kubernetesEvents</code> preset for quick setup</p>"},{"location":"observability/kubernetes/events/#kube-state-metrics-prometheus-only","title":"kube-state-metrics (Prometheus Only)","text":"<p>Status: Active</p> <p>Exposes limited event metrics for Prometheus (not full event details):</p> <ul> <li><code>kube_event_count</code> - Count of events</li> <li><code>kube_event_unique_events_total</code> - Unique events by reason</li> </ul> <p>Use case: High-level alerting, not event investigation</p>"},{"location":"observability/kubernetes/events/#deprecated-solutions","title":"Deprecated Solutions","text":"<ul> <li>EventRouter (vmware-archive) - Archived, no longer maintained</li> <li>Grafana Agent - EOL November 1, 2025, replaced by Grafana Alloy</li> <li>opsgenie/kubernetes-event-exporter - Deprecated, use resmoio fork</li> </ul>"},{"location":"observability/kubernetes/events/#best-practices","title":"Best Practices","text":"<ol> <li>Use JSON format - Faster LogQL queries than logfmt</li> <li>Filter namespaces - Reduce noise by watching specific namespaces</li> <li>Store in Loki - Events are text/structured data, not time-series metrics</li> <li>Set retention - Configure Loki retention based on compliance needs</li> <li>Use labels wisely - Namespace, type, reason are good labels; avoid pod names (high cardinality)</li> <li>Single replica deployment - Deploy Fluent Bit and OTel Collector as Deployment (not DaemonSet) to avoid duplicate event collection</li> </ol>"},{"location":"observability/kubernetes/events/#recommendation","title":"Recommendation","text":"<ul> <li>For Loki users: Use Grafana Alloy with <code>loki.source.kubernetes_events</code> (recommended) or Fluent Bit</li> <li>For vendor-neutral/multi-backend: Use OpenTelemetry Collector (supports Loki, Elasticsearch, OpenSearch, etc.)</li> <li>For multi-destination needs: Use resmoio/kubernetes-event-exporter</li> <li>For Elastic stack: Use Metricbeat + Logstash or kubernetes-event-exporter</li> <li>For OpenSearch stack: Use OpenTelemetry Collector, Fluent Bit + Data Prepper, or kubernetes-event-exporter</li> <li>For Prometheus metrics only: Use kube-state-metrics</li> </ul>"},{"location":"observability/kubernetes/events/#links","title":"Links","text":"<ul> <li>Grafana Alloy</li> <li>loki.source.kubernetes_events</li> <li>resmoio/kubernetes-event-exporter</li> <li>Fluent Bit kubernetes_events</li> <li>Fluent Bit Loki output</li> <li>Fluent Bit OpenSearch output</li> <li>OpenTelemetry Collector for Kubernetes</li> <li>OTel k8seventsreceiver</li> <li>OTel k8sobjectsreceiver</li> <li>kube-state-metrics</li> <li>Metricbeat Kubernetes event metricset</li> <li>OpenSearch Data Prepper</li> <li>Kubernetes Events API</li> </ul>"},{"location":"observability/loki/perlas/","title":"Perlas","text":""},{"location":"observability/loki/perlas/#grafana-loki-no-inicia-y-se-queda-en-containercreating","title":"Grafana loki no inicia y se queda en ContainerCreating","text":"<p>En modo singlebinary</p> <pre><code>kubectl delete pod/loki-0 persistentvolumeclaim/storage-loki-0 --grace-period=0 --force\n</code></pre>"},{"location":"observability/prometheus/ingress-certificates/","title":"Metrics for ingress and certificates","text":""},{"location":"observability/prometheus/ingress-certificates/#tools","title":"Tools","text":"<ul> <li>Kube state metrics</li> </ul> <p>It gets info about kubernetes secrets but it needs a customresourceconfig to check the expiration field</p> <p>https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md</p> <ul> <li>Blackbox exporter</li> </ul> <p>The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP, ICMP and gRPC.</p> <p>https://github.com/prometheus/blackbox_exporter</p> <ul> <li>X.509 Certificate Exporter</li> </ul> <p>https://github.com/enix/x509-certificate-exporter</p> <p>Generate metrics for certificates, with focus on expiration</p>"},{"location":"observability/prometheus/labels/","title":"Labels","text":"<p>The prometheus labels are key-value pairs attached to metrics. They offer additional context, information and metadata to the metrics and permit things like identify, filter and categorize metrics.</p> <p>Example</p> <pre><code> metric_name{label1=\"value1\", label2=\"value2\"} value\n</code></pre>"},{"location":"observability/prometheus/labels/#metric-labels","title":"Metric labels","text":"<p>This labels are generated from the application that expose the metrics:</p> <ul> <li>they are embedded in the metric data itself</li> <li>they describe the metric</li> </ul> <p>They have high cardinality risk</p>"},{"location":"observability/prometheus/labels/#target-labels","title":"Target labels","text":"<p>This labels are added during the scraping process. They describe where the metrics came from, not what they represent.</p> <p>They have low cardinality risk</p>"},{"location":"observability/prometheus/labels/#core-target-label-job","title":"Core Target Label: job","text":"<p>The job label identifies the scrape configuration.</p> <p>In prometheus operator we can give the job label the value of a kubernetes label from the associated pod. This is done using spec.jobLabel in a podmonitor or servicemonitor resource.</p>"},{"location":"observability/prometheus/labels/#core-target-label-instance","title":"Core Target Label: instance","text":"<p>Instance label is the host and port of the target</p> <p>More information about Jobs and instances here https://prometheus.io/docs/concepts/jobs_instances/</p>"},{"location":"observability/prometheus/labels/#internal-labels-__","title":"Internal labels (__)","text":"<p>There are some internal labels (prefixed with __) are temporary labels used by Prometheus during the scraping process. They're not stored with metrics but control how scraping happens.</p> <p>Examples:</p> <pre><code>__address_ # as the target's original address\n__metrics_path__ # as the endpoint path\n__scheme__=\"http\" # as the protocol to use\n__param_* # as query parameters\n__meta_* # Contains discovery metadata\n__meta_kubernetes_ # Contains kubernetes discovery metadata\n__meta_consul_* # Consul discovery metadata\n__meta_ec2_* # EC2 discovery metadata\n</code></pre> <p>With Relabeling it is possible to use internal labels to create permanent labels using relabel_configs</p> <p>There can be other informational labels, not true internal labels so they cannot be used in relabeling</p> <pre><code>__scrape_interval__ # Shows configured interval\n__scrape_timeout__ # Shows configured timeout\n</code></pre>"},{"location":"observability/prometheus/labels/#sources-of-target-labels","title":"Sources of Target Labels","text":"<p>Sources of Target Labels can be defined:</p> <ul> <li>using an static configuration</li> </ul> <pre><code>  - job_name: 'api-servers'\n    static_configs:\n    - targets: ['api1:8080', 'api2:8080']\n      labels:\n        env: 'production'\n        team: 'backend'\n</code></pre> <ul> <li>using Service Discovery (Kubernetes, Consul, etc.)</li> </ul> <pre><code> - job_name: 'kubernetes-pods'\n    kubernetes_sd_configs:\n    - role: pod\n</code></pre> <p>Auto-discovers and adds labels like:</p> <pre><code>  {\n    job=\"kubernetes-pods\",\n    instance=\"10.244.0.15:8080\",\n    __meta_kubernetes_pod_name=\"webapp-123\",\n    __meta_kubernetes_namespace=\"production\"\n  }\n</code></pre> <ul> <li>using ServiceMonitor/PodMonitor from prometheus operator</li> </ul>"},{"location":"observability/prometheus/labels/#honorlabels","title":"honorLabels","text":"<p>honorLabels controls how Prometheus handles label conflicts when scraping metrics</p> <p>Default behavior (honorLabels: false):</p> <ul> <li>Prometheus overwrites target labels with its own labels</li> <li>Target's job label gets renamed to exported_job</li> <li>Target's instance label gets renamed to exported_instance</li> <li>Prometheus uses its own job and instance values</li> </ul> <p>With honorLabels: true:</p> <ul> <li>Prometheus keeps the target's original labels</li> <li>Target's labels take precedence over Prometheus labels</li> <li>No renaming happens</li> </ul> <pre><code>  Example: Target exposes: my_metric{job=\"custom-job\", instance=\"app-1\"}\n\nhonorLabels: false (default)\nmy_metric{job=\"serviceMonitor/namespace/name\", instance=\"10.0.0.1:8080\", exported_job=\"custom-job\", exported_instance=\"app-1\"}\n\nhonorLabels: true  \nmy_metric{job=\"custom-job\", instance=\"app-1\"}\n</code></pre> <p>Use honorLabels: true:</p> <ul> <li>When targets provide meaningful job/instance labels</li> <li>For federation setups</li> <li>When you want to preserve application-defined labels</li> </ul> <p>Common use case: Kubernetes service discovery often uses honorLabels: true to preserve pod labels as metric labels.</p>"},{"location":"observability/prometheus/labels/#cardinality-risk","title":"Cardinality risk","text":"<p>Cardinality is the number of unique label combinations for a metric.</p> <p>The problem is each unique combination creates a separate time series. Prometheus stores each series individually.</p> <p>Cardinality risk is when you create too many unique time series, causing Prometheus performance and storage problems. It can crash or severely degrade your monitoring system.</p> <p>High Cardinality Risk is when too many label combinations create too many time series. This can be caused by:</p> <ul> <li>Unique Identifiers:</li> </ul> <pre><code>requests{user_id=\"abc123\"}        # millions of users\nrequests{request_id=\"xyz789\"}     # every request unique\nrequests{timestamp=\"1634567890\"}  # every second unique\n</code></pre> <ul> <li>Unbounded Values:</li> </ul> <pre><code>response_time{url=\"/user/12345/profile\"}  # infinite URLs\nerrors{error_msg=\"Connection timeout\"}    # many error messages\n</code></pre> <p>The solutions can be:</p> <ul> <li>Use Bounded Labels (limited values)</li> </ul> <pre><code>requests{user_type=\"premium\"}     # premium, basic, free\nrequests{status_class=\"4xx\"}      # 2xx, 3xx, 4xx, 5xx\n</code></pre> <ul> <li>Group/Aggregate:</li> </ul> <pre><code>requests{region=\"us-east\", user_tier=\"paid\"}\n</code></pre> <ul> <li>Use Histograms for Ranges (instead of exact response times)</li> </ul> <pre><code>http_request_duration_bucket{le=\"0.1\"}  # predefined buckets\n</code></pre>"},{"location":"observability/prometheus/labels/#links","title":"Links","text":"<p>https://github.com/prometheus-operator/prometheus-operator/issues/3246</p> <p>relabelings vs metricRelabelings</p>"},{"location":"observability/prometheus/links/","title":"Links","text":""},{"location":"observability/prometheus/links/#prometheus-operator","title":"Prometheus operator","text":"<p>https://prometheus-operator.dev/</p>"},{"location":"observability/prometheus/links/#prometheus-operator-api-reference","title":"Prometheus Operator Api reference","text":"<p>https://prometheus-operator.dev/docs/api-reference/api/</p>"},{"location":"observability/prometheus/links/#oficial-prometheus-registry","title":"Oficial prometheus registry","text":"<p>https://quay.io/prometheus/prometheus</p> <p>https://samber.github.io/awesome-prometheus-alerts/rules</p> <p>https://github.com/zalando-incubator/kube-metrics-adapter</p>"},{"location":"observability/prometheus/metric-types/","title":"Metrics types","text":"<p>Prometheus offer 4 core metric types</p>"},{"location":"observability/prometheus/metric-types/#counter","title":"Counter","text":"<p>A counter is a prometheus metric that represents a numeric value that can increase (cumulative) o be reset to zero on restart.</p>"},{"location":"observability/prometheus/metric-types/#gauge","title":"Gauge","text":"<p>A gauge is a a prometheus metric that represents a numeric value that can increase o decrease.</p>"},{"location":"observability/prometheus/metric-types/#histogram","title":"Histogram","text":"<p>An histogram is a prometheus metric that permits to see the evolution of a metric.</p>"},{"location":"observability/prometheus/metric-types/#buckets","title":"Buckets","text":"<p>Histograms count observations in predefined buckets. Each bucket represents a range of values. Histogram buckets are useful for understanding the performance and latency of web services by providing a detailed breakdown of request durations.</p> <pre><code>&lt;basename&gt;_bucket{le=\"&lt;upper inclusive bound&gt;\"}\n</code></pre> <p>This example tells asks for HTTP requests that had a duration of 0.1 seconds or less.</p> <pre><code>http_request_duration_seconds_bucket{le=\"0.1\"}\n</code></pre>"},{"location":"observability/prometheus/metric-types/#sum-of-observations","title":"Sum of Observations","text":"<p>Histograms also keep a sum of all observed values, which can be used to calculate the average</p> <pre><code>&lt;basename&gt;_sum\n</code></pre> <p>This example is tracking the total sum of the durations of all HTTP requests in seconds</p> <pre><code>http_request_duration_seconds_sum\n</code></pre>"},{"location":"observability/prometheus/metric-types/#cumulative-counts","title":"Cumulative Counts","text":"<p>Histograms maintain a cumulative count of observations for each bucket. The count of events that have been observed</p> <pre><code>&lt;basename&gt;_count\n</code></pre> <p>or</p> <pre><code>&lt;basename&gt;_bucket{le=\"+Inf\"}\n</code></pre> <p>The le stands for \"less than or equal to,\" and \"+Inf\" (positive infinity) means that this bucket includes all HTTP requests, regardless of their duration.</p> <p>Lets see this example</p> <pre><code>http_request_duration_seconds_bucket{le=\"+Inf\"}\nhttp_request_duration_seconds_count\n</code></pre> <p>The metric name http_request_duration_seconds_count indicates that it is tracking the total number of HTTP requests that have been observed.</p> <p>It is useful for calculating the average request duration when combined with the corresponding sum metric (http_request_duration_seconds_sum). For example, dividing the sum of durations by the count gives the average duration of an HTTP request.</p> <p>This gives the average duration of etcd commits called by backend</p> <pre><code>etcd_disk_backend_commit_duration_seconds_sum/etcd_disk_backend_commit_duration_seconds_count\n</code></pre> <p>Use the histogram_quantile() function to calculate quantiles from histograms or even aggregations of histograms. A histogram is also suitable to calculate an Apdex score. When operating on buckets, remember that the histogram is cumulative. See histograms and summaries for details of histogram usage and differences to summaries.</p>"},{"location":"observability/prometheus/metric-types/#know-the-metric-type","title":"Know the metric type","text":""},{"location":"observability/prometheus/metric-types/#via-prometheus-ui","title":"Via prometheus UI","text":"<p>To know the metric type in the prometheus UI, we must go to Query &gt; Explain</p>"},{"location":"observability/prometheus/metric-types/#summary","title":"Summary","text":"<p>pending</p>"},{"location":"observability/prometheus/metric-types/#links","title":"Links","text":"<ul> <li>Metrics types</li> </ul> <p>https://prometheus.io/docs/concepts/metric_types/</p> <ul> <li>Histograms and summaries</li> </ul> <p>https://prometheus.io/docs/practices/histograms/</p>"},{"location":"observability/prometheus/operator-alerting/","title":"Prometheus operator and alerting","text":""},{"location":"observability/prometheus/operator-alerting/#configure-the-prometheus-resource","title":"Configure the prometheus resource","text":"Section Explanation spec.alerting Configure the alertmanager endpoints to send the alerts <p>There are a lot of other prometheus and kubernetes settings we can configure (replicas, retention, persistent storage, resources,...)</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-alertmanager-resource","title":"Setup the alertmanager resource","text":"<p>This will deploy an alertmanager statefulset and its settings</p> <ul> <li>metadata.namespace</li> </ul> <p>pending</p> <ul> <li>metadata.labels</li> </ul> <p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#alertmanager-configuration","title":"Alertmanager configuration","text":"<p>There are 3 ways to configure alertmanager</p> <ul> <li>spec.configSecret  </li> </ul> <p>With this we can choose a Kubernetes secret that contains a native alertmanager configuration (global) in the alertmanager.yaml key.</p> <pre><code>kubectl create secret generic myalertmanagerconfig --from-file=alertmanager.yaml=myfile.yaml\n</code></pre> <p>And in the alertmanager resource</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  configSecret: myalertmanagerconfig\n</code></pre> <p>The default value is alertmanager-name_of-the_alertmanager_instance. If the alertmanager.yaml key does not exist or a secret is not defined, a default configuration will deployed dropping alert notifications.</p> <ul> <li>spec.alertmanagerConfiguration  </li> </ul> <p>Experimental feature that takes precedence over the configSecret field as a global configuration. We can choose an alertmanagerconfig kubernetes resource.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  alertmanagerConfiguration:\n    name: myalertmanagerconfig\n</code></pre> <ul> <li>spec.alertmanagerConfigSelector</li> </ul> <p>Choose what labels should have an alertmanagerconfig resource to be selected for to merge and configure Alertmanager with.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Alertmanager\nmetadata:\n  name: myalertmanager\nspec:\n  alertmanagerConfigSelector:\n    matchLabels:\n      mylabel: myvalue\n</code></pre> <ul> <li>spec.alertmanagerConfigNamespaceSelector</li> </ul> <p>Choose in what namespaces search for alertmanagerconfig resources to be selected for to merge and configure Alertmanager with.</p> <ul> <li>spec.alertmanagerConfigMatcherStrategy</li> </ul> <p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-alertmanagerconfig-resource","title":"Setup the alertmanagerconfig resource","text":"<p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#setup-the-prometheusrule-resources","title":"Setup the prometheusrule resources","text":"<p>pending</p>"},{"location":"observability/prometheus/operator-alerting/#links","title":"Links","text":"<ul> <li>Prometheus: Alerting overwiew</li> </ul> <p>https://prometheus.io/docs/alerting/latest/overview/</p> <ul> <li>Prometheus: Alerting rules</li> </ul> <p>https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/</p> <ul> <li>Prometheus Operator: Alerting Routes</li> </ul> <p>https://prometheus-operator.dev/docs/developer/alerting/</p> <ul> <li>Why does alertmanagerconfigs automatically add a namespace matcher</li> </ul> <p>https://github.com/prometheus-operator/prometheus-operator/discussions/3733</p>"},{"location":"observability/prometheus/operator-prometheus/","title":"Prometheus operator instance","text":"<p>This prometheus kubernetes resource will deploy a prometheus statefulset and its settings under \"spec\"</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\nspec:\n</code></pre> <p>There are some the settings we can configure</p>"},{"location":"observability/prometheus/operator-prometheus/#choose-what-to-discover","title":"Choose what to discover","text":"<p>The prometheus operator also permits to define some namespaced kubernetes resources like:</p> <ul> <li>PodMonitor (scrape metrics from a group of pods)</li> <li>Probe (how to scrape metrics from prober exporters such as the blackbox exporter)</li> <li>PrometheusRule (defines alerting and recording rules)</li> <li>ScrapeConfig (currently at Alpha level)</li> <li>ServiceMonitor (scrape metrics from a group of services)</li> </ul> <p>We must choose what ServiceMonitors, PodMonitors, Probes and ScrapeConfigs will be related with our prometheus instance. With the following settings we can select by labels the namespaces to discover that resources.</p> <ul> <li>spec.podMonitorNamespaceSelector</li> <li>spec.probeNamespaceSelector</li> <li>spec.ruleNamespaceSelector</li> <li>spec.scrapeConfigNamespaceSelector</li> <li>spec.serviceMonitorNamespaceSelector</li> </ul> <p>An empty label \"{}\" selector matches all namespaces</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\nspec:\n  podMonitorNamespaceSelector: {}\n  probeNamespaceSelector: {}\n  ruleNamespaceSelector: {}\n  scrapeConfigNamespaceSelector: {}\n  serviceMonitorNamespaceSelector: {}\n</code></pre> <p>A \"null\" label selector matches the namespace where the prometheus instance has been deployed</p> <p>Also we can filter by labels in those resources. Only the resources created with that labels defined here will be related with our prometheus instance.</p> <ul> <li>spec.podMonitorSelector</li> <li>spec.probeSelector</li> <li>spec.scrapeConfigSelector</li> <li>spec.serviceMonitorSelector</li> <li>spect.ruleSelector</li> </ul> <p>Again, an empty label \"{}\" selector matches all objects. A \"null\" label selector matches no objects.</p>"},{"location":"observability/prometheus/operator-prometheus/#alertmanager","title":"Alertmanager","text":"<p>With spec.alerting.alertmanagers we can define alertmanager endpoints where to send alerts</p>"},{"location":"observability/prometheus/operator-prometheus/#other-settings","title":"Other settings","text":"<ul> <li>spec.version and spec.image</li> </ul> <p>With spec.version we can choose the prometheus release we want to deploy. It is neccesary to ensure the Prometheus Operator knows which version of Prometheus is being configured. With spec.image we can configure the container image.</p> <p>The operator itself has a default release for both settings</p> <p>example:</p> <pre><code>spec:\n    image: quay.io/prometheus/prometheus:v3.1.0\n    version: v3.1.0\n</code></pre> <ul> <li>spec.replicas</li> </ul> <p>With spec.replicas we can configure the name of instances we want in our prometheus statefulset. The default number is 1</p> <ul> <li>spec.storage, spec.volumes and spec.volumeMounts</li> </ul> <p>We can configure the persistence here</p> <ul> <li>spec.retention and spec.retentionSize</li> </ul> <p>Limit the data will be stored. Retention by date (24h default) and retentionSize by size.</p> <ul> <li>spec.logLevel and spec.logFormat</li> </ul> <p>We can configure the logformat and change the verbosity of the prometheus and config-reloader containers</p> <ul> <li> <p>spec.scrapeInterval, spec.scrapeTimeout and spec.scrapeProtocols</p> </li> <li> <p>spec.externalUrl</p> </li> </ul> <p>If we want to expose prometheus, for example, via ingress, we must configure this to generate correct URLs</p> <ul> <li>spec.resources</li> </ul> <p>Tune the kubernetes requests and limits of the prometheus instance</p> <ul> <li>spec.priorityClassName</li> </ul> <p>Give a priorityclass to the prometheus pods</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/","title":"Prometheus Long-Term Storage","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#overview","title":"Overview","text":"<p>Prometheus, while excellent for real-time monitoring and short-term metric storage, has inherent limitations when it comes to long-term data retention and horizontal scalability. By default, Prometheus stores data locally and is designed for deployments lasting weeks to months rather than years.</p> <p>This document explores five major open-source solutions that extend Prometheus capabilities for long-term storage: Mimir, Cortex, Thanos, VictoriaMetrics, and GreptimeDB.</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#governance-and-company-dependency-risk","title":"Governance and Company Dependency Risk","text":"Solution Risk Level Primary Controller Governance Model Community Independence Mimir \ud83d\udfe1 Medium-High Grafana Labs (single company) Corporate-controlled Limited - Grafana Labs drives roadmap Cortex \ud83d\udfe2 Low CNCF (vendor-neutral) Community governance High - Multi-vendor collaboration Thanos \ud83d\udfe2 Low-Medium CNCF (vendor-neutral) Community governance High - No single company dominance VictoriaMetrics \ud83d\udd34 High VictoriaMetrics company Corporate-controlled Low - Single company development GreptimeDB \ud83d\udd34 High Greptime Inc. Corporate-controlled Low - Single company development"},{"location":"observability/prometheus/prometheus-long-term-storage/#solution-architectures","title":"Solution Architectures","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#grafana-mimir","title":"Grafana Mimir","text":"<p>Origin: Fork of Cortex by Grafana Labs (2022) Architecture: Microservices-based with horizontal scaling Repository: grafana/mimir \u2b50 4.1k+ stars, 500+ contributors CNCF Status: Not a CNCF project (Grafana Labs proprietary) Key Contributors: Grafana Labs, Microsoft, Red Hat, Bloomberg, GitLab</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#key-components","title":"Key Components","text":"<ul> <li>Distributor: Receives metrics and forwards to ingesters</li> <li>Ingester: Writes metrics to storage and serves recent queries</li> <li>Store Gateway: Queries historical data from object storage</li> <li>Query Frontend: Optimizes and splits queries</li> <li>Compactor: Compacts and processes blocks in object storage</li> <li>Ruler: Evaluates recording and alerting rules</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-benefits","title":"Architecture Benefits","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502 Distributor \u2502\u2500\u2500\u2500\u25b6\u2502  Ingester   \u2502\n\u2502   (Remote   \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502   Write)    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502Query Frontend\u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502             \u2502    \u2502             \u2502    \u2502 (S3/GCS/etc)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#features","title":"Features","text":"<ul> <li>Multi-Tenancy: Native support with tenant isolation</li> <li>Streaming: Real-time query results streaming</li> <li>Advanced Compaction: Intelligent block compaction strategies</li> <li>Autoscaling: Kubernetes-native autoscaling support</li> <li>Monitoring: Extensive built-in observability</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex","title":"Cortex","text":"<p>Origin: CNCF project originally developed by Weaveworks Architecture: Microservices-based, highly configurable Repository: cortexproject/cortex \u2b50 5.5k+ stars, 800+ contributors CNCF Status: Graduated project (2020) Key Contributors: Weaveworks, Grafana Labs, Red Hat, Google, AWS, Microsoft</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex-components","title":"Cortex Components","text":"<ul> <li>Distributor: Load balances incoming metrics</li> <li>Ingester: Stores recent metrics in memory and periodically flushes to storage</li> <li>Querier: Handles PromQL queries</li> <li>Query Frontend: Splits and caches queries</li> <li>Store Gateway: Reads historical data from object storage</li> <li>Compactor: Compacts blocks and handles retention</li> <li>Ruler: Evaluates rules and generates alerts</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-pattern","title":"Architecture Pattern","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502 Distributor \u2502\u2500\u2500\u2500\u25b6\u2502  Ingester   \u2502\n\u2502 (Remote     \u2502    \u2502             \u2502    \u2502 (Ring-based)\u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Query     \u2502\u25c0\u2500\u2500\u2500\u2502   Querier   \u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502  Frontend   \u2502    \u2502             \u2502    \u2502   Backend   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cortex-features","title":"Cortex Features","text":"<ul> <li>Flexible Deployment: Multiple deployment modes (single binary, microservices)</li> <li>Multi-Tenancy: Comprehensive tenant isolation</li> <li>Ring-based Architecture: Consistent hashing for load distribution</li> <li>Configurable Storage: Multiple storage backend options</li> <li>CNCF Graduated: Mature project with strong community support</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos","title":"Thanos","text":"<p>Origin: Developed by Improbable, now CNCF project Architecture: Sidecar-based approach with global querying Repository: thanos-io/thanos \u2b50 13k+ stars, 1,000+ contributors CNCF Status: Incubating project (2019) Key Contributors: Improbable, Red Hat, Google, Polar Signals, GitLab</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos-components","title":"Thanos Components","text":"<ul> <li>Sidecar: Connects to Prometheus instances and uploads blocks</li> <li>Store Gateway: Provides unified interface to historical data</li> <li>Query: Global query layer across multiple Prometheus instances</li> <li>Query Frontend: Query optimization and caching</li> <li>Compactor: Compacts and downsamples historical data</li> <li>Receiver: Ingests metrics via remote write (alternative to sidecar)</li> <li>Ruler: Evaluates rules on historical data</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#architecture-model","title":"Architecture Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus A\u2502\u25c0\u2500\u2500\u25b6\u2502Thanos Sidecar\u2502   \u2502Object Storage\u2502\n\u2502             \u2502    \u2502             \u2502\u2500\u2500\u25b6 \u2502   Backend   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502             \u2502\n                                      \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502             \u2502\n\u2502 Prometheus B\u2502\u25c0\u2500\u2500\u25b6\u2502Thanos Sidecar\u2502\u2500\u2500\u25b6\u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                            \u25b2\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502Thanos Query \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502             \u2502    \u2502 (Global)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#thanos-features","title":"Thanos Features","text":"<ul> <li>Global View: Query across multiple Prometheus instances</li> <li>Minimal Changes: Requires minimal changes to existing Prometheus setups</li> <li>Downsampling: Automatic data downsampling for long-term storage</li> <li>Deduplication: Automatic deduplication of metrics</li> <li>Multi-Cloud: Works across different cloud providers and on-premises</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics","title":"VictoriaMetrics","text":"<p>Origin: Developed by VictoriaMetrics team, focused on performance and cost efficiency Architecture: Single binary or cluster mode with emphasis on resource efficiency Repository: VictoriaMetrics/VictoriaMetrics \u2b50 12k+ stars, 400+ contributors CNCF Status: Not a CNCF project (independent open-source) Key Contributors: VictoriaMetrics, Individual contributors, Community-driven development</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-components","title":"VictoriaMetrics Components","text":"<p>Single Binary Mode:</p> <ul> <li>All-in-One: Complete solution in a single binary for smaller deployments</li> <li>Embedded Storage: Built-in time series database optimized for compression</li> <li>HTTP API: Prometheus-compatible API for seamless integration</li> </ul> <p>Cluster Mode:</p> <ul> <li>VMStorage: Storage nodes handling data persistence and queries</li> <li>VMInsert: Ingestion nodes for distributing incoming metrics</li> <li>VMSelect: Query nodes for handling PromQL queries</li> <li>VMAgent: Lightweight agent for metrics collection and remote write</li> <li>VMAlert: Alerting component compatible with Prometheus rules</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-architecture","title":"VictoriaMetrics Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502  VMAgent    \u2502\u2500\u2500\u2500\u25b6\u2502  VMInsert   \u2502\n\u2502 (Remote     \u2502    \u2502(Collection) \u2502    \u2502 (Ingestion) \u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502  VMSelect   \u2502\u25c0\u2500\u2500\u2500\u2502 VMStorage   \u2502\n\u2502             \u2502    \u2502  (Query)    \u2502    \u2502(Persistence)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#victoriametrics-features","title":"VictoriaMetrics Features","text":"<ul> <li>High Performance: Up to 20x better performance than Prometheus</li> <li>Resource Efficiency: Minimal CPU and memory usage</li> <li>Superior Compression: 10x better compression ratios than Prometheus</li> <li>Prometheus Compatibility: Drop-in replacement with full PromQL support</li> <li>Multi-Tenancy: Built-in tenant isolation in cluster mode</li> <li>Downsampling: Automatic data downsampling for long-term retention</li> <li>Backfilling: Support for importing historical data</li> <li>Stream Aggregation: Real-time metric aggregation capabilities</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb","title":"GreptimeDB","text":"<p>Origin: Developed by Greptime team, modern cloud-native time series database Architecture: SQL-based time series database with storage/compute separation Repository: GreptimeTeam/greptimedb \u2b50 4.2k+ stars, 300+ contributors CNCF Status: Not a CNCF project (independent open-source) Key Contributors: Greptime Inc., PingCAP, TiKV contributors, Community developers</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-components","title":"GreptimeDB Components","text":"<p>Core Architecture:</p> <ul> <li>Frontend: SQL query layer and protocol handlers (MySQL, PostgreSQL, PromQL)</li> <li>Datanode: Storage engine for time series data with advanced compression</li> <li>Metanode: Cluster metadata management and coordination</li> <li>Object Storage: S3-compatible storage backend for data persistence</li> </ul> <p>Deployment Modes:</p> <ul> <li>Standalone: Single binary for development and small deployments</li> <li>Cluster: Distributed mode with separate frontend, datanode, and metanode</li> <li>Cloud: Managed service with automatic scaling and optimization</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-architecture","title":"GreptimeDB Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Prometheus  \u2502\u2500\u2500\u2500\u25b6\u2502  Frontend   \u2502\u2500\u2500\u2500\u25b6\u2502  Datanode   \u2502\n\u2502 (Remote     \u2502    \u2502 (PromQL)    \u2502    \u2502 (Storage)   \u2502\n\u2502  Write)     \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n                                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Grafana   \u2502\u25c0\u2500\u2500\u2500\u2502  Frontend   \u2502\u25c0\u2500\u2500\u2500\u2502Object Storage\u2502\n\u2502             \u2502    \u2502 (Query)     \u2502    \u2502 (S3/GCS)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"observability/prometheus/prometheus-long-term-storage/#greptimedb-features","title":"GreptimeDB Features","text":"<ul> <li>SQL + PromQL: Native SQL support with &gt;90% PromQL compatibility</li> <li>Exceptional Performance: 5x more resource efficient than Mimir</li> <li>Superior Compression: Advanced compression algorithms for cost optimization</li> <li>Elastic Scaling: Independent scaling of storage and compute resources</li> <li>Multi-Protocol: MySQL, PostgreSQL, PromQL, and InfluxDB protocols</li> <li>Cloud-Native: Kubernetes-native with operator support</li> <li>Time Travel: Historical data queries with SQL temporal functions</li> <li>Real-time Analytics: Built-in stream processing capabilities</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#deployment-complexity","title":"Deployment Complexity","text":"Aspect Mimir Cortex Thanos VictoriaMetrics GreptimeDB Setup Complexity Medium High Low-Medium Low Low Operational Overhead Medium High Low Very Low Low Prometheus Changes Remote write only Remote write only Minimal (sidecar) Remote write only Remote write only Learning Curve Medium High Low-Medium Low Low-Medium"},{"location":"observability/prometheus/prometheus-long-term-storage/#scalability-performance","title":"Scalability &amp; Performance","text":"Feature Mimir Cortex Thanos VictoriaMetrics GreptimeDB Horizontal Scaling Excellent Excellent Good Excellent Excellent Query Performance High High Medium-High Very High Very High Ingestion Rate Very High High Medium Exceptional Exceptional Memory Efficiency Optimized Good Good Exceptional Exceptional Storage Efficiency High High Very High (downsampling) Exceptional Exceptional"},{"location":"observability/prometheus/prometheus-long-term-storage/#features-capabilities","title":"Features &amp; Capabilities","text":"Feature Mimir Cortex Thanos VictoriaMetrics GreptimeDB Multi-Tenancy \u2705 Advanced \u2705 Advanced \u2705 Basic \u2705 Advanced \u2705 Advanced Global Querying \u2705 \u2705 \u2705 Excellent \u2705 \u2705 Deduplication \u2705 \u2705 \u2705 Advanced \u2705 \u2705 Downsampling \u2705 \u2705 \u2705 Automatic \u2705 Automatic \u2705 Automatic Rule Evaluation \u2705 \u2705 \u2705 \u2705 \u2705 Alerting \u2705 \u2705 \u2705 \u2705 \u2705 Stream Processing \u2705 \u274c \u274c \u2705 Advanced \u2705 Native"},{"location":"observability/prometheus/prometheus-long-term-storage/#storage-retention","title":"Storage &amp; Retention","text":"Aspect Mimir Cortex Thanos VictoriaMetrics GreptimeDB Object Storage S3, GCS, Azure S3, GCS, Azure, Swift S3, GCS, Azure, Swift S3, GCS, Azure, Local S3, GCS, Azure Compression Excellent Good Excellent Exceptional Exceptional Retention Policies Flexible Flexible Flexible Very Flexible Very Flexible Block Management Advanced Standard Advanced Optimized Advanced"},{"location":"observability/prometheus/prometheus-long-term-storage/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-mimir-when","title":"Choose Mimir When","text":"<ul> <li>Modern Deployments: Starting fresh or modernizing existing setups</li> <li>High Performance Required: Need maximum query and ingestion performance</li> <li>Grafana Ecosystem: Already using Grafana and want tight integration</li> <li>Multi-Tenancy: Require advanced tenant isolation and management</li> <li>Real-time Analytics: Need streaming query capabilities</li> </ul> <p>Ideal For: SaaS platforms, multi-tenant environments, high-traffic applications</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-cortex-when","title":"Choose Cortex When","text":"<ul> <li>CNCF Compliance: Need a graduated CNCF project for governance</li> <li>Flexibility Required: Need highly configurable deployment options</li> <li>Mature Solution: Want proven technology with extensive production use</li> <li>Community Support: Prefer established community and ecosystem</li> <li>Hybrid Deployments: Need to support various deployment patterns</li> </ul> <p>Ideal For: Enterprise environments, regulated industries, complex multi-cloud setups</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-thanos-when","title":"Choose Thanos When","text":"<ul> <li>Existing Prometheus: Want to extend current Prometheus deployments with minimal changes</li> <li>Global Visibility: Need to query across multiple clusters/regions</li> <li>Cost Optimization: Storage costs are a primary concern (excellent compression/downsampling)</li> <li>Gradual Migration: Want to incrementally adopt long-term storage</li> <li>Multi-Cloud: Operating across different cloud providers</li> </ul> <p>Ideal For: Large-scale Kubernetes deployments, multi-cluster environments, cost-sensitive operations</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-victoriametrics-when","title":"Choose VictoriaMetrics When","text":"<ul> <li>Resource Efficiency: Need maximum performance with minimal resource usage</li> <li>Cost Optimization: Primary concern is reducing infrastructure costs</li> <li>Simple Operations: Want minimal operational complexity</li> <li>High Performance: Need exceptional ingestion and query performance</li> <li>Prometheus Compatibility: Require seamless migration from existing Prometheus setups</li> <li>Single Binary Deployment: Prefer simple deployment models</li> </ul> <p>Ideal For: High-scale cost-conscious environments, resource-constrained deployments, performance-critical applications</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#choose-greptimedb-when","title":"Choose GreptimeDB When","text":"<ul> <li>Modern Architecture: Want a cloud-native, SQL-based time series database</li> <li>Maximum Efficiency: Need the best resource efficiency and cost optimization</li> <li>Multi-Protocol Support: Require SQL, PromQL, and other protocol compatibility</li> <li>Advanced Analytics: Want built-in SQL capabilities for complex queries</li> <li>Elastic Scaling: Need independent storage and compute scaling</li> <li>Simplified Operations: Prefer managed service options with minimal operational overhead</li> </ul> <p>Ideal For: Modern cloud-native deployments, analytical workloads, cost-sensitive high-scale environments</p>"},{"location":"observability/prometheus/prometheus-long-term-storage/#operational-considerations","title":"Operational Considerations","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>All solutions provide comprehensive metrics for monitoring:</p> <ul> <li>Ingestion Metrics: Rate, errors, latency</li> <li>Query Metrics: Performance, cache hit rates</li> <li>Storage Metrics: Object storage operations, compaction status</li> <li>Resource Metrics: CPU, memory, disk usage per component</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#backup-disaster-recovery","title":"Backup &amp; Disaster Recovery","text":"<ul> <li>Object Storage: Primary data durability mechanism</li> <li>Multi-Region: Configure object storage for cross-region replication</li> <li>Metadata Backup: Backup configuration and metadata regularly</li> <li>Testing: Regular disaster recovery testing procedures</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#security-considerations","title":"Security Considerations","text":"<ul> <li>Authentication: Integration with existing identity providers</li> <li>Authorization: Role-based access control (RBAC)</li> <li>Encryption: Data encryption in transit and at rest</li> <li>Network Security: Proper network segmentation and policies</li> <li>Secrets Management: Secure handling of storage credentials</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#cost-analysis","title":"Cost Analysis","text":""},{"location":"observability/prometheus/prometheus-long-term-storage/#infrastructure-costs","title":"Infrastructure Costs","text":"Component Mimir Cortex Thanos VictoriaMetrics GreptimeDB Compute High (microservices) High (microservices) Medium (fewer components) Low (efficient) Very Low (optimized) Storage Medium (efficient compression) Medium Low (excellent compression) Very Low (superior compression) Very Low (advanced compression) Network Medium Medium Low (query optimization) Low (optimized protocols) Low (efficient protocols)"},{"location":"observability/prometheus/prometheus-long-term-storage/#operational-costs","title":"Operational Costs","text":"<ul> <li>Mimir: Lower operational overhead due to better defaults</li> <li>Cortex: Higher operational overhead due to configuration complexity</li> <li>Thanos: Lowest operational overhead for existing Prometheus users</li> <li>VictoriaMetrics: Minimal operational overhead with single binary option</li> <li>GreptimeDB: Very low operational overhead with cloud-native automation</li> </ul>"},{"location":"observability/prometheus/prometheus-long-term-storage/#conclusion","title":"Conclusion","text":"<p>The choice between Mimir, Cortex, Thanos, VictoriaMetrics, and GreptimeDB depends on your specific requirements:</p> <ul> <li>Mimir offers the best performance and modern features for new deployments</li> <li>Cortex provides maximum flexibility and is ideal for complex enterprise requirements</li> <li>Thanos offers the easiest migration path and excellent global querying for existing Prometheus users</li> <li>VictoriaMetrics delivers exceptional resource efficiency and cost optimization with superior performance</li> <li>GreptimeDB provides modern SQL-based architecture with maximum efficiency and cloud-native automation</li> </ul> <p>All five solutions successfully address Prometheus's long-term storage limitations, but the optimal choice depends on your organization's technical requirements, operational capabilities, performance needs, and cost constraints.</p> <p>Consider starting with a proof-of-concept deployment to evaluate which solution best fits your specific use case and operational constraints.</p>"},{"location":"observability/prometheus-adapter/5-resource-metrics/","title":"Kubernetes resource metrics","text":"<p>The most simple way to make horizontal pod autoescaler and kubectl top work is to deploy metrics adapter. But if we want, for example, to use custom metrics with HPA, we need prometheus adapter.</p> <p>In order to make prometheus adapter provide that functions, we need:</p> <ul> <li> <p>Deploy metrics adapter deployment</p> </li> <li> <p>Create an v1beta1.metrics.k8s.io apiService</p> </li> <li> <p>Configure the prometheus adapter (ConfigMap)</p> </li> </ul>"},{"location":"observability/prometheus-adapter/5-resource-metrics/#deployment","title":"Deployment","text":"<p>We have 2 ways to deploy the prometheus adapter:</p> <ul> <li>Official manifests</li> </ul> <p>They include the deployment, the v1beta1.metrics.k8s.io apiService and the ConfigMap</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/tree/master/deploy/manifests</p> <ul> <li>Helm chart</li> </ul> <p>They include the deployment. To deploy the apiService and the ConfigMap we need to enable the rules.resource section in our values.yaml file and enable the APIVersions Capability with \"apiregistration.k8s.io/v1\" as value.</p> <p>https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter</p>"},{"location":"observability/prometheus-adapter/5-resource-metrics/#configuration","title":"Configuration","text":"<p>I have found 3 different configurations of the adapter. They are similar but not exactly the same</p> <ul> <li>From prometheus-adapter github repo</li> </ul> <p>https://raw.githubusercontent.com/kubernetes-sigs/prometheus-adapter/refs/heads/master/deploy/manifests/config-map.yaml</p> <ul> <li>From kube-prometheus</li> </ul> <p>https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/refs/heads/main/manifests/prometheusAdapter-configMap.yaml</p> <ul> <li>From prometheus-community helm chart</li> </ul> <p>In the default values.yaml file from the prometheus-adapter helm chart, we have some commented lines about the resource rules.</p> <p>https://raw.githubusercontent.com/prometheus-community/helm-charts/refs/heads/main/charts/prometheus-adapter/values.yaml</p>"},{"location":"observability/prometheus-adapter/9-tips/","title":"Tips","text":""},{"location":"observability/prometheus-adapter/9-tips/#incorrect-values-in-kubectl-top","title":"Incorrect values in kubectl top","text":"<p>If we have incorrect and very high values using \"kubectl top\" we are probably getting the same values twice:</p> <ul> <li>From kubelet /metrics/resource</li> <li>From kubelet /metrics/cadvisor</li> </ul> <p>\"This is because container_cpu_usage_seconds_total and container_memory_working_set_bytes used there are exposed by both endpoints, resulting in double counting. Therefore, you will need to take measures such as assigning a metrics_path label using relabel_configs and narrowing it down to one.</p> <p>As an alternative solution to avoid using /metrics/resource, you could follow the approach mentioned here(I used it). However, in that case, you'll need Prometheus Node Exporter, and you'll also need to assign node names to node label using relabel_configs.</p> <p>More info at:</p> <p>https://github.com/kubernetes-sigs/prometheus-adapter/issues/639</p> <p>Another option is that we can have more that one service being matched by the kubelet service monitor</p>"},{"location":"operating-systems/too-many-open-files/","title":"Too Many Open Files","text":"<p>This error is related with the max open file descriptors limit.</p> <p>To see list the opened file descriptors per process (open file descriptors, pid an process name):</p> <pre><code>for pid in $(ls /proc | grep -E '^[0-9]+$'); do\n  echo \"$(ls /proc/$pid/fd 2&gt;/dev/null | wc -l) $pid $(cat /proc/$pid/comm 2&gt;/dev/null)\"\ndone | sort -nr | head\n</code></pre>"},{"location":"operating-systems/too-many-open-files/#systemd-service","title":"Systemd service","text":"<p>In order to see if the problem is related with a systemd service, we can see the limits this way</p> <p>With containerd:</p> <pre><code>systemctl show containerd | grep LimitNOFILE\ncat /proc/$(pgrep containerd | head -1)/limits | grep files\n</code></pre> <p>In order to solve it we can:</p> <ul> <li>Change the DefaultLimitNOFILE settings in the /etc/systemd/system.conf file, affecting to all systemd services.</li> </ul> <p>The default value is 1024:524288, where 1024 is the soft limit and 524288 the hard limit</p> <ul> <li>Or adding a systemd dropping in /etc/systemd/system/containerd.service.d/limits.conf with our desired LimitNOFILE</li> </ul>"},{"location":"operating-systems/too-many-open-files/#system-process","title":"System process","text":"<p>If the problem is related with a non systemd service, we must probably increase fs.inotify.max_user_instances. This is the upper limit on the number of INotify instances (file descriptors) that can be created per real user ID</p> <p>To see the current settings</p> <pre><code>sysctl fs.inotify.max_user_instances\ncat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>To increase it temporarily (sometimes the default value is 128)</p> <pre><code>sysctl fs.inotify.max_user_instances=512\n</code></pre> <p>To make it persistent</p> <pre><code>cat &lt;&lt; EOF &gt; /etc/sysctl.d/10-open-files.conf\nfs.inotify.max_user_instances = 512\nEOF\nsystemctl restart systemd-sysctl.service\n</code></pre> <p>And check again the current settings</p>"},{"location":"operating-systems/bottlerocket/98-tips/","title":"Tips","text":""},{"location":"operating-systems/bottlerocket/98-tips/#get-admin-console-access-in-eks","title":"Get admin console access in EKS","text":"<p>Connect the ec2 instance, for example, using session manager. Then</p> <pre><code>enter-admin-container\nsudo sheltie\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#get-logs","title":"Get logs","text":"<p>We can generate the logs or get kubelet logs, for example</p> <pre><code>logdog\njournalctl -u kubelet\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#get-the-kubernetes-images","title":"Get the kubernetes images","text":"<pre><code>ctr --namespace k8s.io images list\n</code></pre>"},{"location":"operating-systems/bottlerocket/98-tips/#re-push-the-kubernetes-images","title":"Re-Push the  kubernetes images","text":"<p>This will ask you for the password</p> <pre><code>ctr --namespace k8s.io image push -u MYUSER URL-OF-THE-IMAGE\n</code></pre>"},{"location":"operating-systems/bottlerocket/99-links/","title":"Links","text":"<ul> <li>AWS Bottlerocket</li> </ul> <p>https://aws.amazon.com/bottlerocket/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D</p> <ul> <li>Github</li> </ul> <p>https://github.com/bottlerocket-os/bottlerocket</p> <ul> <li>Website</li> </ul> <p>https://bottlerocket.dev/</p>"},{"location":"operating-systems/hyperv/tips/","title":"Tips","text":""},{"location":"operating-systems/hyperv/tips/#a-virtual-machine-is-stuck-powering-off","title":"A virtual machine is stuck powering off","text":"<p>Identify the Id of the vm via powershell</p> <pre><code>get-vm | ft VMName, VMId\n</code></pre> <p>Then got to task manager to see the processes. Under the \"command line\" column, end the virtual machine process (vmwp.exe) that has the VMId as parameter</p>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/","title":"Citrix workspace in Ubuntu 24.04","text":""},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#install-dependencies","title":"Install dependencies","text":"<p>As root</p> <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/apt/sources.list.d/jammy.list\ndeb &lt;http://gb.archive.ubuntu.com/ubuntu&gt; jammy main\napt update\napt install libwebkit2gtk-4.0-dev\nrm -f /etc/apt/sources.list.d/jammy.list\napt update\n</code></pre>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#install","title":"Install","text":"<p>As user, download the tarball from here</p> <ul> <li>Citrix Workspace app https://www.citrix.com/downloads/workspace-app/linux/</li> </ul> <p>Uncompress and exec the installer</p> <pre><code>./setupwfc\n</code></pre>"},{"location":"operating-systems/linux/citrix-client-ubuntu24.04/#links","title":"Links","text":"<ul> <li>Install, Uninstall, and Update https://docs.citrix.com/en-us/citrix-workspace-app-for-linux/install.html</li> </ul>"},{"location":"operating-systems/linux/gdm/","title":"GDM","text":"<p>GNOME CLASSIC VS UBUNTU vs WAYLAND VS XORG ls /usr/share/xsessions/</p> <p>https://help.ubuntu.com/stable/ubuntu-help/gnome-classic.html.en https://www.reddit.com/r/Ubuntu/comments/hby264/can_anyone_help_me_with_what_does_gnome_classic/</p> <ul> <li> <p>Gnome classic (wayland?) Exec=gnome-session-classic TryExec=gnome-session</p> </li> <li> <p>Gnome classic on Xorg Exec=env GNOME_SHELL_SESSION_MODE=classic gnome-session TryExec=gnome-session GNOME 46 has been released in coordination with the latest GTK version, 4.14</p> </li> <li> <p>Ubuntu (wayland?) Exec=env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gnome-session --session=ubuntu TryExec=/usr/bin/gnome-shell</p> </li> <li> <p>Ubuntu on Xorg Exec=env GNOME_SHELL_SESSION_MODE=ubuntu /usr/bin/gnome-session --session=ubuntu TryExec=/usr/bin/gnome-shell</p> </li> </ul> <p>Classic emulates gnome 2 The Xorg sessions are legacy and shouldn\u2019t be used unless you have a specific need.</p>"},{"location":"operating-systems/linux/gnome-oled/","title":"Gnome tips for ultrawide oled monitor","text":"<p>This is based in Ubuntu 24.04</p>"},{"location":"operating-systems/linux/gnome-oled/#prevent-burn-in-issues","title":"Prevent burn in issues","text":""},{"location":"operating-systems/linux/gnome-oled/#pure-black-theme","title":"Pure black theme","text":"<p>Here we must search for gnome-shell and gtk/3/4 themes.</p> <p>We can find some themes here. Some of them provide both gnome-shell and gtk themes</p> <p>https://www.pling.com/browse?cat=366&amp;ord=latest</p> <p>Ir order to install them we need to install the gnome-tweaks package and the User Themes extension</p> <p>gnome-extensions-app</p> <p>https://www.reddit.com/r/gnome/comments/15c16hx/anyone_have_a_matte_black_theme/</p>"},{"location":"operating-systems/linux/gnome-oled/#gnome-shell-themes","title":"Gnome shell themes","text":"<p>The gnome shell themes change the appearance of the GNOME Shell interface, including the top bar, system menus, notifications, and the overview screen.</p> <p>The system themes are installed in /usr/share/themes/NAME-OF-THEME/gnome-shell and they can be installed by the user in the ./themes/NAME-OF-THEME/gnome-shell directory</p>"},{"location":"operating-systems/linux/gnome-oled/#gtk34-themes","title":"Gtk/3/4 themes","text":"<p>They change the appearance of GTK applications, including window decorations, buttons, sliders, and other UI elements.</p> <p>The system themes are installed in</p> <ul> <li>/usr/share/themes/NAME-OF-THEME/gtk-3.0</li> <li>/usr/share/themes/NAME-OF-THEME/gtk-4.0</li> </ul> <p>And they can be installed by de user</p> <ul> <li>GTK3: ~/.themes/NAME-OF-THEME/gtk-3.0</li> <li>GTK4: ~/.themes/NAME-OF-THEME/gtk-4.0</li> </ul>"},{"location":"operating-systems/linux/gnome-oled/#full-black-desktop-background","title":"Full black desktop background","text":"<pre><code>gsettings set org.gnome.desktop.background picture-options 'none'\ngsettings set org.gnome.desktop.background primary-color '#000000'\n</code></pre> <p>Other option is to use some pure black wallpapers combined with the \"Wallpaper slideshow\" gnome-shell extension</p>"},{"location":"operating-systems/linux/gnome-oled/#auto-hide-the-dock","title":"Auto hide the dock","text":"<p>We can enable autohide in the dock via settings - Ubuntu Desktop - Dock and enable \"Auto-hide the Dock\"</p>"},{"location":"operating-systems/linux/gnome-oled/#dont-show-desktop-icons","title":"Dont show desktop icons","text":"<p>Go to settings &gt; Ubuntu Desktop and disable Desktop icons</p>"},{"location":"operating-systems/linux/gnome-oled/#auto-hide-the-top-bar","title":"Auto hide the top bar","text":"<p>We can hide the top back installing the \"Hide Top Bar\" gnome-shell extension</p>"},{"location":"operating-systems/linux/gnome-oled/#other-applications","title":"Other applications","text":"<ul> <li>Chrome and firefox</li> </ul> <p>https://darkreader.org/</p> <ul> <li>VSCODE</li> </ul> <p>Choose the dark high contrast theme</p>"},{"location":"operating-systems/linux/gnome-oled/#organize-windows","title":"Organize windows","text":"<p>We can use Ubuntu tiling asistant installing the gnome-shell-extension-ubuntu-tiling-assistant package</p> <p>https://extensions.gnome.org/extension/3733/tiling-assistant/ https://github.com/Leleat/Tiling-Assistant</p> <p>There are other similar tools like Linux Powertoys https://github.com/domferr/Linux-PowerToys</p>"},{"location":"platforms/aws/cli/98-tips/","title":"Tips","text":""},{"location":"platforms/aws/cli/98-tips/#install-as-user","title":"Install as user","text":"<pre><code>mkdir ~/apps ~/bin\nbash install --install-dir ~/apps/aws-cli --bin-dir ~/bin\n~/bin/aws --version\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#list-s3-buckets","title":"List s3 buckets","text":"<pre><code>aws s3 ls --profile myprofile\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#get-my-kubeconfig","title":"Get my kubeconfig","text":"<pre><code>export AWS_PROFILE=myprofile\naws eks update-kubeconfig --region region-code --name my-cluster --kubeconfig pathtomykubeconfig\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#login-to-ecr","title":"Login to ecr","text":"<pre><code>aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\n</code></pre>"},{"location":"platforms/aws/cli/98-tips/#other","title":"Other","text":"<pre><code>aws eks get-token  --cluster-name my-cluster\n\naws sts get-caller-identity\n\naws sts get-session-token\n</code></pre> <p>To delete a profile with the AWS CLI, you need to manually remove the profile's configuration from the AWS configuration files. The AWS CLI stores profiles in two files: ~/.aws/config and ~/.aws/credentials.</p> <p>Steps to Delete a Profile: Open the Configuration Files:</p> <p>Open ~/.aws/config and ~/.aws/credentials in a text editor. Remove the Profile from ~/.aws/config:</p> <p>Locate the profile section you want to delete. Profile sections start with [profile profile-name] for named profiles. Delete the entire section for the profile. Remove the Profile from ~/.aws/credentials:</p> <p>Locate the profile section you want to delete. Profile sections start with [profile-name]. Delete the entire section for the profile.</p>"},{"location":"platforms/aws/cli/authentication/","title":"Authentication","text":""},{"location":"platforms/aws/cli/authentication/#common","title":"Common","text":"<p>List profiles</p> <pre><code>aws configure list-profiles\n</code></pre> <p>Choose profile</p> <pre><code>export AWS_PROFILE=profile\n</code></pre> <p>Files</p> <p>~/.aws/credentials</p> <p>~/.aws/config</p>"},{"location":"platforms/aws/cli/authentication/#standard-profile","title":"Standard profile","text":"<p>Configure standard profile</p> <pre><code>aws configure\n</code></pre>"},{"location":"platforms/aws/cli/authentication/#single-sign-on-ssl","title":"Single Sign on (ssl)","text":"<p>Configure sso profile</p> <pre><code>aws configure sso\n</code></pre> <p>Login to a sso profile</p> <pre><code>aws sso login --profile=profile\n</code></pre>"},{"location":"platforms/aws/ecr/aws-cli/","title":"Aws cli and ecr","text":"<p>Get a token valid for 12 hours</p> <pre><code>aws ecr get-login-password --region MYREGION \n</code></pre> <p>Get a token valid for 12 hours and login in docker</p> <pre><code>aws ecr get-login-password --region MYREGION | docker login --username AWS --password-stdin MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com\n</code></pre> <p>Create a repository</p> <pre><code>aws ecr create-repository --repository-name REPOSITORYNAME\n</code></pre>"},{"location":"platforms/aws/ecr/policies/","title":"Iam Policies","text":"<p>Example policies:</p> <ul> <li> <p>AmazonEC2ContainerRegistryPowerUser is a predefined policy that permits an user to pull and push images</p> </li> <li> <p>Project power user as an user that can pull and push images only under MYPROJECT/* repositories</p> </li> <li> <p>AmazonEC2ContainerRegistryPullOnly is a predefined policy that can be assigned to service accounts to pull images</p> </li> <li> <p>Project puller to be assigned to service accounts to pull images only located under MYPROJECT/* repositories</p> </li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/","title":"All images from private ECR and no inet","text":""},{"location":"platforms/aws/ecr/private-ecr-no-inet/#situation","title":"Situation","text":"<ul> <li> <p>The cluster has been predeployed with kubeadm and temporary internet access. The control plane is working but that container images will not be accesible if they need to be downloaded again.</p> </li> <li> <p>We dont have internet access in our kubernetes cluster The cluster must get all the container images from an Amazon Elastic Container Registry. AWS gives a 12 hours valid token for that. We need to renew it properly. We cannot access to helm repositories</p> </li> <li> <p>All the secrets will be stored in AWS Secrets Manager and we will use external secrets operator v0.12.1</p> </li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#preparation","title":"Preparation","text":""},{"location":"platforms/aws/ecr/private-ecr-no-inet/#power-user","title":"Power user","text":"<p>We will do some tasks like create repositories, iam policies, iam users, push image or create secrets. We need permissions to do that operations. For example, to push images we ca use the predefined AmazonEC2ContainerRegistryPowerUser policy or use a more precise setup.</p> <p>Assume that user will be called \"power-user\"</p>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#iam-user-to-pull-images","title":"Iam user to pull images","text":"<p>We must create an iam user to permite kubelet and the service accounts to pull the images. We can use the generic AmazonEC2ContainerRegistryPullOnly predefined policy or do a more precise setup.</p> <p>Assume that user will be called \"image-puller\"</p> <p>Also create an access key and store the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in secrets manager:</p> <pre><code>image-puller-AWS_ACCESS_KEY_ID\nimage-puller-AWS_SECRET_ACCESS_KEY\nimage-puller-REGION\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#bootstrap-external-secrets-operator","title":"Bootstrap external-secrets operator","text":"<p>Because all the container images will be located in a private repository we need to bootstrap external secrets operator in order to provide kubelet the credentials to that private repository in a secure way.</p>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#external-secrets-repository","title":"External secrets repository","text":"<p>Go to Elastic Container Registry and create a repository called, for example, MYPROJECT/external-secrets. Take care about the resulting Resource ARN and URL of the repository.</p> <pre><code>MYREPOARN\nMYREPOURL  (MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com)\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#push-the-external-secrets-image","title":"Push the external secrets image","text":"<p>Pull the external-secrets operator official image for our release</p> <pre><code>docker pull oci.external-secrets.io/external-secrets/external-secrets:v0.12.1\ndocker tag oci.external-secrets.io/external-secrets/external-secrets:v0.12.1 MYREPOURL/external-secrets:v0.12.1\n</code></pre> <p>As power user push it to the new repository</p> <pre><code>aws ecr get-login-password --region MYREGION | docker login --username AWS --password-stdin MYACCOUNTID.dkr.ecr.MYREGION.amazonaws.com\ndocker push oci.external-secrets.io/external-secrets/external-secrets:v0.12.1 MYREPOURL/external-secrets:v0.12.1\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#create-the-kubelet-credentials","title":"Create the kubelet credentials","text":"<p>For that we need to create a kubernetes.io/dockerconfigjson kubernetes secret in the external-secrets with the credentials. We will call it \"puller\".</p> <ul> <li>The server will be MYREPOURL</li> <li>The username will be AWS</li> <li>The password is a 12h valid token</li> </ul> <pre><code>aws configure --profile image-puller # use the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and REGION\nexport AWS_PROFILE=image-puller\nkubectl create ns external-secrets\nkubectl create secret docker-registry puller -n external-secrets --docker-server=MYREPOURL --docker-username=AWS --docker-password=$(aws ecr get-login-password) --dry-run -o yaml\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#deploy-the-operator","title":"Deploy the operator","text":"<p>In order to deploy the operator we need to use the official yaml (not the helm chart), change the url of the images to our private repository repository and give the service accounts the kubelet credentials.</p> <p>See this kustomization file and the imagepullsecrets patch file</p> <p>Finally deploy external secrets operator with</p> <pre><code>kubectl apply -k .\n</code></pre>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#notes-about-pending-tasks","title":"Notes about pending tasks","text":"<p>Now we have a puller secret in the external-secrets namespace that can pull images from our container but we have several things to do:</p> <ul> <li>Create new repositories in ECR and pull all the images. It will not be treated here</li> <li>Change all the service accounts use the ecr-auth credentials as pullsecret.</li> <li>Convert the puller secret to be an managed (not manual) secret.</li> <li>Distribute the puller secret to all namespaces.</li> <li>The token lives 12 hours. We also needs to renew it.</li> </ul>"},{"location":"platforms/aws/ecr/private-ecr-no-inet/#links","title":"Links","text":"<ul> <li>Pushing a Docker image to an Amazon ECR private repository</li> </ul> <p>https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</p> <ul> <li>External secrets operator helm chart</li> </ul> <p>https://github.com/external-secrets/external-secrets/tree/main/deploy/charts/external-secrets</p> <ul> <li>Generators</li> </ul> <p>https://external-secrets.io/latest/guides/generator/</p> <ul> <li>External secrets operator and ecr generator</li> </ul> <p>https://external-secrets.io/latest/api/generator/ecr/</p> <ul> <li>External secrets operator cluster external secret</li> </ul> <p>https://external-secrets.io/latest/api/clusterexternalsecret/</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/","title":"Eks Pod identity agent","text":"<p>Eks pod identity is a feature in Amazon EKS that simplifies the process to give permissions to a kubernetes service accounts inside an eks cluster.</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#prepare-the-system","title":"Prepare the system","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/pod-id-agent-setup.html</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#policy-to-the-nodes","title":"Policy to the nodes","text":"<p>Ensure the AmazonEKSWorkerNodePolicy policy is added to the node role</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#install-the-agent-addon","title":"Install the agent addon","text":"<p>Install the Amazon EKS Pod Identity Agent addon to EKS</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#prepare-iam","title":"Prepare IAM","text":""},{"location":"platforms/aws/eks/eks-pod-identity-agent/#create-the-policy","title":"Create the Policy","text":"<p>Create a policy with the desired permissions to the kubernetes application</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#create-the-role-role","title":"Create the role Role","text":"<p>Create a role with that policy and this trust relationship</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#add-the-association-in-eks","title":"Add the association in EKS","text":"<p>https://docs.aws.amazon.com/eks/latest/userguide/pod-id-association.html</p> <p>In our eks cluster - Access tab, create a new Pod Identity association</p> <ul> <li>choose the created iam role</li> <li>choose the namespace</li> <li>choose an existing service account inside that namespace</li> </ul> <p>And that's it!!</p> <p>The service account or the application usually don't need additional settings (no arn, no annotation,..). But check the documentation or forums for every application in order to use Pod Identity Agent</p>"},{"location":"platforms/aws/eks/eks-pod-identity-agent/#links","title":"Links","text":"<ul> <li> <p>EKS Pod Identities https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html</p> </li> <li> <p>Github https://github.com/aws/eks-pod-identity-agent</p> </li> </ul>"},{"location":"platforms/aws/eks/irsa/","title":"IRSA","text":""},{"location":"platforms/aws/eks/irsa/#oidc","title":"OIDC","text":"<p>Create an IAM OIDC provider for your cluster</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html</p>"},{"location":"platforms/aws/eks/irsa/#create-the-policy","title":"Create the Policy","text":"<p>Create a policy with the desired permissions to the kubernetes application</p> <p>Generate Policy Script</p>"},{"location":"platforms/aws/eks/irsa/#create-the-role","title":"Create the role","text":"<p>Create a role with that policy and this trust relationship</p> <p>You can use the folling script</p> <p>Generate Trust Relationship Script</p>"},{"location":"platforms/aws/eks/irsa/#annotation","title":"Annotation","text":"<p>Add the following annotation to the service account that needs the permissions</p> <pre><code>eks.amazonaws.com/role-arn: THE-CREATED-ROLE-ARN\n</code></pre>"},{"location":"platforms/aws/eks/irsa/#links","title":"Links","text":"<ul> <li>Create and associate IAM Role</li> </ul> <p>https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html</p>"},{"location":"queues/nats/99-links/","title":"Links","text":""},{"location":"queues/nats/99-links/#websites","title":"Websites","text":"<ul> <li> <p>Official Nats website https://nats.io/</p> </li> <li> <p>Official Nats documentation https://docs.nats.io/</p> </li> <li> <p>Github nats org https://github.com/nats-io</p> </li> <li> <p>Awesome Nats https://github.com/synadia-io/awesome-nats</p> </li> <li> <p>Learn NATS by Example</p> </li> </ul> <p>https://natsbyexample.com/</p>"},{"location":"queues/nats/99-links/#monitoring-and-dashboards","title":"Monitoring and dashboards","text":"<ul> <li>Nats Surveyor</li> </ul> <p>https://github.com/nats-io/nats-surveyor</p> <ul> <li>Prometheus nats exporter</li> </ul> <p>https://github.com/nats-io/prometheus-nats-exporter/</p> <ul> <li> <p>Nats Dashboard https://natsdashboard.com/ https://github.com/mdawar/nats-dashboard</p> </li> <li> <p>Nats-top</p> </li> </ul> <p>https://github.com/nats-io/nats-top</p>"},{"location":"queues/nats/99-links/#videos","title":"Videos","text":"<ul> <li> <p>Nats Youtube Channel https://www.youtube.com/c/nats_messaging</p> </li> <li> <p>Synadia Communications Youtube Channel https://www.youtube.com/@SynadiaCommunications</p> </li> <li> <p>Cloud Native Live: Advanced NATS https://www.youtube.com/watch?v=3vHKKmVkb80</p> </li> </ul>"},{"location":"queues/nats/jetstream/01-stream/","title":"Stream","text":"<p>A nats stream is a messsage store with some settings like the retention. It is related with nats subjects, and any message published there will be stored in the configured storage</p> <p>Streams are responsible for storing the published messages,</p>"},{"location":"queues/nats/jetstream/01-stream/#configuration","title":"Configuration","text":"<p>We can see the settings of an stream with</p> <pre><code>nats stream info STREAM\n</code></pre>"},{"location":"queues/nats/jetstream/01-stream/#basic-settings","title":"Basic Settings","text":"<ul> <li> <p>The name of the stream</p> </li> <li> <p>Description</p> </li> <li> <p>The storage type. It can be file (default) or memory</p> </li> <li> <p>The list of subjects to bind</p> </li> <li> <p>The replicas to keep for each message</p> </li> <li> <p>Mirror stream. A stream can be configured as a mirror of another stream.</p> </li> </ul>"},{"location":"queues/nats/jetstream/01-stream/#limits-and-discard-policy","title":"Limits and discard policy","text":"<ul> <li>Maximum Messages (MaxMsgs)</li> </ul> <p>Maximum number of messages stored in the stream. Adheres to Discard Policy, removing oldest or refusing new messages if the Stream exceeds this number of messages.</p> <ul> <li>Maximum Age (MaxAge)</li> </ul> <p>Maximum age to keep any message in the Stream, expressed in nanoseconds</p> <ul> <li>Maximum Bytes (MaxBytes)</li> </ul> <p>Maximum number of bytes stored in the stream, maximum bytes to keep. Adheres to Discard Policy, removing oldest or refusing new messages if the Stream exceeds this size.</p> <ul> <li> <p>Maximum Message Size (MaxMsgSize) The largest message that will be accepted by the Stream. The size of a message is a sum of payload and headers.</p> </li> <li> <p>Maximum Consumers (MaxConsumers) Maximum number of Consumers allowed, that can be defined for a given Stream, -1 for unlimited. Cannot be edited</p> </li> <li> <p>Discard policy</p> </li> </ul> <p>DiscardOld (default) will delete the oldest messages in order to maintain the limit.</p> <p>DiscardNew will reject new messages from being appended to the stream if it would exceed one of the limits. An extension to this policy is DiscardNewPerSubject which will apply this policy on a per-subject basis within the stream. An extension to this policy is DiscardNewPerSubject which will apply this policy on a per-subject basis within the stream.</p> <ul> <li>DiscardNewPerSubject</li> </ul> <p>If true, applies discard new semantics on a per subject basis. Requires DiscardPolicy to be DiscardNew and the MaxMsgsPerSubject to be set.</p>"},{"location":"queues/rabbitmq/links/","title":"Links","text":"<ul> <li>RabbitMQ Cluster Kubernetes Operator</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/operator-overview</p> <ul> <li>RabbitMQ Messaging Topology Operator</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/install-topology-operator</p>"},{"location":"queues/rabbitmq/updates/","title":"Updates","text":"<p>We can do 2 updates if we are using the rabbitmq cluster kubernetes operator:</p> <ul> <li>Update the operator</li> <li>Update the cluster</li> </ul>"},{"location":"queues/rabbitmq/updates/#operator","title":"Operator","text":"<p>In order to update the operator we only need to update the manifest to the desired releases. All the releases are located here:</p> <p>https://github.com/rabbitmq/cluster-operator/releases</p> <p>The latest version of the operator is located here:</p> <p>https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml</p> <p>Sometimes updating the rabbitmq kubernetes operator also updates the pods of the cluster:</p> <ul> <li> <p>If we are not using spec.image in the definition of the cluster, we are using the default rabbitmq container image. The new release of the operator will probably have a newer default rabbitmq container image, but the operator update will maintain the previous one in exiting clusters. Newer clusters without spec.image will use the new default one.</p> </li> <li> <p>It is possible that updating the operator changes another fields of the podSpec so the pods will be restarted.</p> </li> </ul> <p>A good workaround can be pause the reconciliation, upgrade the operator, resume the reconciliation when you decide to do it</p> <p>To know the default rabbitmq release the operator deploys and if it will cause a restart of the pods, see the changelog</p>"},{"location":"queues/rabbitmq/updates/#pause-the-reconciliation","title":"Pause the reconciliation","text":"<p>In order to pause the reconciliation we need to add this label to the cluster</p> <pre><code>rabbitmq.com/pauseReconciliation: true\n</code></pre>"},{"location":"queues/rabbitmq/updates/#links","title":"Links","text":"<ul> <li>Upgrading the RabbitMQ Kubernetes Operators</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/upgrade-operatorhttps://www.rabbitmq.com/kubernetes/operator/upgrade-operator</p> <ul> <li>Pause Reconciliation for a RabbitMQCluster</li> </ul> <p>https://www.rabbitmq.com/kubernetes/operator/using-operator#pause</p>"},{"location":"security/cert-manager/98-tips/","title":"Perlas","text":""},{"location":"security/cert-manager/98-tips/#pod-identity-agent","title":"Pod identity agent","text":"<p>Para poder usar AWS Pod identity agent en un issuer es necesario habilitar en el controller</p> <pre><code>--issuer-ambient-credentials\n</code></pre> <p>Para cluster issuer viene habilitado por defecto</p> <p>Para poder usarlo es posible que necesites una version mas reciente de cert-manager. En 1.12 no parece funcionar</p>"},{"location":"security/cert-manager/98-tips/#http-tls-handshake-error-from-xxxx-eof","title":"http: TLS handshake error from XXXX EOF","text":"<p>Borrar a mano cert-manager-webhook-ca y cert-manager-webhook-tls si existen</p>"},{"location":"security/cert-manager/98-tips/#auto-clean-secrets","title":"Auto clean secrets","text":"<p>By default cert-manager does not remove a secret when the certificate is removed. We can enable it with the following controller option:</p> <pre><code>--enable-certificate-owner-ref\n</code></pre> <p>For example, deleting an ingress resource removes the certificate. With this setting, also the secret</p> <p>This setting makes the certificate resource as an owner of secret where the tls certificate is stored.</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/","title":"Aws Secrets Manager with PIA","text":""},{"location":"security/external-secrets/aws-secrets-manager-pia/#role-and-policies","title":"Role and policies","text":"<p>Create a role called, for example external-secrets with this trust policy (trust relationship)</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>And with this permission policy called, for example AllowPullSecrets</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"secretsmanager:GetResourcePolicy\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecretVersionIds\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre> <p>The policy can be more precise for your secret</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#aws-secrets-manager","title":"Aws Secrets Manager","text":"<p>Go to Aws Secrets Manager and Store a new secret with \"secret type\" \"Other type of secret\" and put some key/value</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#deploy-external-secrets","title":"Deploy external secrets","text":"<p>Deploy the external dns helm chart</p>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#configure-the-eks-cluster","title":"Configure the eks cluster","text":"<ul> <li> <p>Under addons, deploy the pod identity agent plugin if not using Eks Auto Mode</p> </li> <li> <p>Under access, create a pod identity association between the external-secrets role and the external-secrets service account</p> </li> </ul>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#create-a-secret-store","title":"Create a secret store","text":"<p>In order to check if it is working, create a secret store</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: mystore\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: YOURREGION\n</code></pre> <p>... and see the logs in the secret manager pod</p> <p>Probably a restart of the external-secrets deployment is needed</p> <pre><code>kubectl rollout restart deployment external-secrets -n external-secrets\n</code></pre>"},{"location":"security/external-secrets/aws-secrets-manager-pia/#links","title":"Links","text":"<ul> <li>AWS Secrets Manager</li> </ul> <p>https://external-secrets.io/latest/provider/aws-secrets-manager/</p>"},{"location":"security/external-secrets/azure-key-vault/","title":"Azure Key Vault","text":""},{"location":"security/external-secrets/azure-key-vault/#authentication-authtype","title":"Authentication (authType)","text":"<p>Defining the (Cluster)SecretStore, external Secrets Operator supports 3 authentication types defined in .spec.provider.azurekv.authType:</p> <p>In all cases you must configure \"environmentType\" and \"vaultUrl\", but there are some differences in setup between the authTypes</p>"},{"location":"security/external-secrets/azure-key-vault/#serviceprincipal","title":"ServicePrincipal","text":"<p>This Azure Service Principal is the default authType and it can be used with:</p> <ul> <li>ClientID and ClientSecret</li> <li>ClientCertificate in PEM format</li> </ul> <p>If we want to use this authentication type, we also need to configure:</p> <ul> <li> <p>authSecretRef: the secret that stores that credential</p> </li> <li> <p>tenantId: the Azure Tenant ID</p> </li> </ul> <pre><code>apiVersion: external-secrets.io/v1\nkind: SecretStore\nmetadata:\n  name: azure-backend\nspec:\n  provider:\n    azurekv:\n      authType: ServicePrincipal\n</code></pre>"},{"location":"security/external-secrets/azure-key-vault/#managedidentity-not-recommended","title":"ManagedIdentity (not recommended)","text":"<p>Uses aad-pod-identity, which was deprecated in 2022 and replaced by Azure Workload Identity</p>"},{"location":"security/external-secrets/azure-key-vault/#workloadidentity","title":"WorkloadIdentity","text":"<p>Replaces aad-pod-identity and requires configuring \"serviceAccountRef\".</p> <p>Optional settings:</p> <ul> <li>tenantId</li> <li>authSecretRef</li> </ul>"},{"location":"security/external-secrets/azure-key-vault/#settings-table","title":"Settings Table","text":"Setting ServicePrincipal ManagedIdentity WorkloadIdentity authType \u2705 Required \u2705 Required \u2705 Required vaultUrl \u2705 Required \u2705 Required \u2705 Required environmentType \u2705 Required \u2705 Required \u2705 Required tenantId \u2705 Required \u274c Not used \u26aa Optional authSecretRef \u2705 Required \u274c Not used \u26aa Optional serviceAccountRef \u274c Not used \u274c Not used \u2705 Required"},{"location":"security/external-secrets/azure-key-vault/#supported-object-types","title":"Supported Object Types","text":"<p>External Secrets Operator can manage all 3 types of objects: secrets, certificates, and keys (jwk)</p> <pre><code>  data:\n    - secretKey: database-username\n        remoteRef:\n            key: database-username # secret without prefix (default value)\n    - secretKey: database-password\n        remoteRef:\n            key: secret/database-password # secret with prefix\n    - secretKey: db-client-cert\n        remoteRef:\n            key: cert/db-client-cert # certificate with prefix\n    - secretKey: encryption-pubkey\n        remoteRef:\n            key: key/encryption-pubkey # key with prefix\n</code></pre>"},{"location":"security/external-secrets/azure-key-vault/#links","title":"Links","text":"<ul> <li>External Secrets Operator and AzureAD</li> </ul> <p>https://external-secrets.io/latest/provider/azure-key-vault/</p> <ul> <li>Api Spec</li> </ul> <p>https://external-secrets.io/latest/api/spec/#external-secrets.io/v1beta1.AzureKVProvider</p> <ul> <li>AAD Pod identity (deprecated)</li> </ul> <p>https://azure.github.io/aad-pod-identity/docs/</p> <ul> <li>Azure AD Workload Identity</li> </ul> <p>https://azure.github.io/azure-workload-identity/docs/</p> <ul> <li>Azure AD Workload Identity Federation</li> </ul> <p>https://docs.microsoft.com/en-us/azure/active-directory/develop/workload-identity-federation</p>"},{"location":"security/external-secrets/backup/","title":"Cloud secret storages and backup","text":"<p>Both have secrets versioning</p>"},{"location":"security/external-secrets/backup/#azure-key-vault","title":"Azure key vault","text":"<ul> <li>Permits manual backups</li> </ul> <p>https://learn.microsoft.com/en-us/azure/key-vault/general/backup</p> <ul> <li>You can enable purge protection at key vault level</li> </ul> <p>https://learn.microsoft.com/en-gb/azure/key-vault/general/soft-delete-overview</p>"},{"location":"security/external-secrets/backup/#aws-secrets-manager","title":"AWS secrets manager","text":"<ul> <li> <p>Permits manual backups https://docs.aws.amazon.com/cli/latest/reference/secretsmanager/</p> </li> <li> <p>If you delete a secret, it is maintained 7 days at least https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_delete-secret.html</p> </li> <li> <p>You can add a replicacion between regions https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/</p> </li> </ul>"},{"location":"security/external-secrets/datafrom/","title":"dataFrom in external secret","text":"<p>dataFrom is a way to get secrets from the secrets provider and it is used when we want to fetch all the keys from the provider with the ability to do some filters.</p> <p>It is an array with entries with \"sourceRef\" as mandatory field and, optionally, \"extract\", \"find\" and \"rewrite\".</p> <p>If multiple entries are specified, the Secret keys are merged in the specified order</p>"},{"location":"security/external-secrets/datafrom/#sourceref","title":"sourceRef","text":"<p>First of all we need to setup the secret store, cluster secret store o generator to obtain the data</p> <p>SourceRef points to a store or generator which contains secret values ready to use. Use this in combination with Extract or Find pull values out of a specific SecretStore. When sourceRef points to a generator Extract or Find is not supported. The generator returns a static map of values</p> <p>SourceRef allows you to override the source from which the secret will be pulled from. You can define at maximum one property.</p> <p>Example: Using a secret store</p> <pre><code>  dataFrom:\n  - sourceRef:\n      storeRef:\n        name: mysecretstore\n        kind: SecretStore # default\n</code></pre> <p>Example: Using a cluster secret store</p> <pre><code>  dataFrom:\n  - sourceRef:\n      storeRef:\n        name: myclustersecretstore\n        kind: ClusterSecretStore\n</code></pre> <p>Example: Using a generator</p> <p>When sourceRef points to a generator, extract or find is not supported.</p> <pre><code>  dataFrom:\n  - sourceRef:\n      generatorRef:\n        apiVersion: generators.external-secrets.io/v1alpha1\n        kind: ECRAuthorizationToken\n        name: \"my-ecr\"\n</code></pre>"},{"location":"security/external-secrets/datafrom/#extract-optional","title":"extract (optional)","text":"<p>Used to extract multiple key/value pairs from one secret</p> <p>mandatory field: key</p> <p>Key is the key used in the Provider, mandatory</p> <p>optional fields:</p> <ul> <li>metadataPolicy</li> <li>property</li> <li>version</li> <li>conversionStrategy</li> <li>decodingStrategy</li> </ul> <p>Documentation about extract</p> <p>https://external-secrets.io/latest/guides/all-keys-one-secret/</p>"},{"location":"security/external-secrets/datafrom/#find-optional","title":"find (optional)","text":"<p>Used to create a secret in kubernetes from several secrets in the provider. With find we fetch them based on certain criteria like \"path\", \"name\" and \"tags\". We can use them when needed combined in the same \"find\" field.</p> <ul> <li>path Specifies a root path to start the find operations. It is only supported in some providers.</li> </ul> <pre><code>  dataFrom:\n  - find:\n      path: path-to-filter\n</code></pre> <ul> <li>name Finds secrets based on the name and an optional regular expression</li> </ul> <pre><code>  dataFrom:\n  - find:\n      name:\n        regexp: \".*myfilter.*\"\n</code></pre> <ul> <li>tags Find secrets based on tags</li> </ul> <pre><code>  dataFrom:\n  - find:\n      tags:\n        environment: \"prod\"\n        application: \"app-name\"\n</code></pre> <p>Another optional fields:</p> <ul> <li>conversionStrategy</li> <li>decodingStrategy</li> </ul> <p>Documentation about find:</p> <p>https://external-secrets.io/latest/guides/getallsecrets/</p>"},{"location":"security/external-secrets/datafrom/#rewrite-optional","title":"rewrite (optional)","text":"<p>Used to rewrite secret Keys after getting them from the secret Provider Multiple Rewrite operations can be provided. They are applied in a layered order (first to last)</p>"},{"location":"security/external-secrets/datafrom/#regexp","title":"regexp","text":"<p>Used to rewrite with regular expressions. The resulting key will be the output of a regexp.ReplaceAll operation.</p> <ul> <li> <p>source Used to define the regular expression of a re.Compiler.</p> </li> <li> <p>target Used to define the target pattern of a ReplaceAll operation.</p> </li> </ul>"},{"location":"security/external-secrets/datafrom/#transform","title":"transform","text":"<p>Used to apply string transformation on the secrets. The resulting key will be the output of the template applied by the operation.</p> <ul> <li>template: Used to define the template to apply on the secret name. .value will specify the secret name in the template.</li> </ul> <p>Documentation about rewrite:</p> <p>https://external-secrets.io/latest/guides/datafrom-rewrite/</p>"},{"location":"security/external-secrets/datafrom/#datafrom-array","title":"dataFrom array","text":"entry sourceRef storeRef sourceRef generatorRef extract key (m) extract other metadataPolicy, property, version, conversionStrategy, decodingStrategy find path find name regexp find tags find other conversionStrategy, decodingStrategy rewrite regexp source, target rewrite transform template"},{"location":"security/external-secrets/datafrom/#links","title":"Links","text":"<ul> <li>ExternalSecretDataFromRemoteRef</li> </ul> <p>https://external-secrets.io/latest/api/spec/#external-secrets.io/v1beta1.ExternalSecretDataFromRemoteRef</p> <ul> <li>External secret</li> </ul> <p>https://external-secrets.io/latest/api/externalsecret/</p>"},{"location":"security/external-secrets/external-secret/","title":"External Secret spec","text":""},{"location":"security/external-secrets/external-secret/#elegir-el-secret-store","title":"Elegir el secret store","text":"<p>Para elegir el secret store del cual traernos los secretos se usa spec.secretStoreRef, donde spec.secretStoreRef.name sera el nombre y spec.secretStoreRef.kind el tipo de secret store (SecretStore o ClusterSecretStore)</p>"},{"location":"security/external-secrets/external-secret/#datos-a-traernos-del-proveedor","title":"Datos a traernos del proveedor","text":"<p>Para traernos los secrets podemos usar:</p> <ul> <li>spec.dataFrom para traernos todas las properties de la key</li> <li>spec.data para elegir cuales traernos</li> </ul>"},{"location":"security/external-secrets/external-secret/#data-specdata","title":"Data (spec.data)","text":"<p>Data permite especificar la relacion entre las keys del secret a crear y el dato almacenado en el proveedor. Asi, dentro de cada una de las relaciones declaradas podemos especificar</p> <p>secretKey es el nombre que le damos a esta relacion</p> <p>remoteRef especifica que dato traernos del proveedor</p> <ul> <li>key: Es valor mas importante, porque es realmente la clave a traerse</li> <li>conversionStrategy: Default | Unicode</li> <li> <p>decodingStrategy: Para elegir si la clave es codificada en el proveedor y debe ser descodificada o no. Opciones: None | Auto | Base64 | Base64URL</p> </li> <li> <p>metadataPolicy: Si queremos traernos tags o labels. Puede ser None (por defecto) o Fetch</p> </li> <li>property: Si queremos traernos una property concreta (depende del proveedor)</li> <li>version: Si queremos traernos una version concreta (depende del proveedor)</li> </ul> <p>sourceRef permite especificar un secret store diferente al de spec.secretStoreRef y es obligatorio si este ultimo no existe. Se declara mediante  storeRef.name y storeRef.kind</p>"},{"location":"security/external-secrets/external-secret/#data-specdatafrom","title":"Data (spec.dataFrom)","text":"<p>Pendiente</p>"},{"location":"security/external-secrets/external-secret/#secret-a-crear-spectarget","title":"Secret a crear (spec.target)","text":"<p>Se hace mediante spec.target elegimos que secret y como crearlo.</p>"},{"location":"security/external-secrets/external-secret/#name","title":"Name","text":"<p>spec.target.name permite elegir el nombre del secret a crear. Si no se especifica, sera el nombre del externalsecret</p>"},{"location":"security/external-secrets/external-secret/#creationpolicy","title":"creationPolicy","text":"<p>spec.target.creationPolicy sirve para elegir de forma se crea el secret</p> <ul> <li>Owner es el valor por defecto. External secrets operator le pone al secret un ownerReference y lo hace susceptible del garbage colector de Kubernetes.</li> </ul> <p>Si al intentar crear el secret se encuentra uno ya existente con otro ownerReference, se genera un conflicto y falla. Si al intentar crear el secret se encuentra uno ya existente sin ownerReference, le pone un ownerReference y lo actualiza</p> <ul> <li> <p>Orphan lo crea sin ownerReference y queda fuera del garbage colector de Kubernetes</p> </li> <li> <p>Merge no crea ningun secret, sino que espera que ya exista y hacer un merge</p> </li> <li> <p>None no hace nada</p> </li> </ul>"},{"location":"security/external-secrets/external-secret/#deletionpolicy","title":"deletionPolicy","text":"<p>spec.target.deletionPolicy permite elegir que hacer con el secret cuando se borra el secret en el proveedor de secretos.</p> <ul> <li> <p>Retain es el valor por defecto y mas cauteloso. Mantiene el secret creado y el externalsecret entra en estado SecretSyncedError</p> </li> <li> <p>Delete borra el secret y el externalsecret no se considera como fallido ni tendra el estado SecretSyncedError. Tambien ocurre al crear un nuevo external secret con esta opcion si falla al mapear con el secret del proveedor.</p> </li> <li> <p>Merge borra las entradas del secret, pero no el secret en si. Al igual que con delete, o se considera como fallido ni tendra el estado SecretSyncedError.</p> </li> </ul>"},{"location":"security/external-secrets/external-secret/#immutable","title":"immutable","text":"<p>spec.target.immutable permite hacer inmutable el secret</p>"},{"location":"security/external-secrets/external-secret/#template","title":"Template","text":"<p>Pendiente</p>"},{"location":"security/external-secrets/external-secret/#otras-configuraciones","title":"Otras configuraciones","text":"<ul> <li>spec.refreshInterval Permite especificar cada cuanto leer los valores del provider. Se puede expresar en varias unidades como \"s\", \"m\" o \"h\" y esta basado en time.ParseDuration de go</li> </ul> <p>El valor por defecto es una hora Si se configura a 0, el valor se trae solo una vez al crearse</p>"},{"location":"security/external-secrets/external-secret/#ejemplo","title":"Ejemplo","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: myexternalsecret\nspec:\n  data:\n  - remoteRef:\n      key: mipassword # key de secret store donde buscar el valor\n    secretKey: mikey # key a escribir en el secret\n  refreshInterval: 1h # cada cuanto leer el secret del proveedor\n  secretStoreRef:  # especificar el secret store del cual obtener los datos\n    kind: ClusterSecretStore\n    name: nombredelsecretstore\n  target:  # definicion del secret a crear\n    name: secret # nombre del secret a crear. si o se especifica, sera el nombre del external secret\n    creationPolicy:  # Owner (por defecto) Merge o None\n    deletionPolicy:     # Delete, Merge, Retain\n</code></pre>"},{"location":"security/external-secrets/external-secret/#links","title":"Links","text":"<ul> <li> <p>ExternalSecret https://external-secrets.io/latest/api/externalsecret/</p> </li> <li> <p>Lifecycle https://external-secrets.io/latest/guides/ownership-deletion-policy/</p> </li> <li> <p>Decoding strategy https://external-secrets.io/latest/guides/decoding-strategy/</p> </li> </ul>"},{"location":"security/external-secrets/tips/","title":"Tips","text":""},{"location":"security/external-secrets/tips/#reduce-the-secretstore-and-clustersecretstore-calls","title":"Reduce the secretstore and clustersecretstore calls","text":"<p>By default the controller validates the secretstore and clustersecretstore every 5 minutes We can change this default behaviour with the store-requeue-interval parameter</p> <p>In the helm chart</p> <pre><code>extraArgs:\n  store-requeue-interval: 1h # increase to 1 h\n</code></pre> <p>Also we can override the controller's default value in the secretstore resource</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: myss\nspec:\n  refreshInterval: 30m\n</code></pre> <p>and clustersecretstore resource</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: mycss\nspec:\n  refreshInterval: 2h\n</code></pre>"},{"location":"security/external-secrets/tips/#reduce-the-externalsecret-and-clusterexternalsecret-calls","title":"Reduce the externalsecret and clusterexternalsecret calls","text":"<p>pending</p>"},{"location":"security/external-secrets/tips/#template-bad-character-u002d-","title":"template bad character U+002D '-'","text":"<p>This is a limitation of the go template language. You can not use \"-\" in variable names (secretKey)</p> <pre><code>secretKey: my-key # this fails\n</code></pre> <pre><code>secretKey: myKey # this works\n</code></pre>"},{"location":"security/external-secrets/tls-secret/","title":"Create a tls secret","text":"<p>If we have uploaded a certificate and private key, to our secret manager, we can create a tls kubernetes secret</p>"},{"location":"security/external-secrets/tls-secret/#in-aws-secrets-manager","title":"In aws secrets manager","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: tls-aws-secrets-mamager\nspec:\n  template:\n    type: kubernetes.io/tls\n    data:\n        - remoteRef:\n            decodingStrategy: Base64 # if the value is in base64\n            key: my-aws-secret # name of the aws secret\n            property: tls.crt # key inside the aws secret\n        secretKey: tls.crt # key we want in the secret\n        - remoteRef:\n            decodingStrategy: Base64 # if the value is in base64\n            key: my-aws-secret # name of the aws secret\n            property: tls.key  # key inside the aws secret\n        secretKey: tls.key # key we want in the secret\n  secretStoreRef:\n    kind: SecretStore\n    name: mystore # it can be a ClusterSecretStore \n</code></pre>"},{"location":"security/external-secrets/updates/","title":"Updates","text":"<p>First update to the latest patch release of the current installed major version Next update to latest patch releaes of the next major version</p>"},{"location":"security/external-secrets/updates/#from-015x-to-016x","title":"From 0.15.X to 0.16.X","text":"<p>Some breaking changes</p> <ul> <li>Removal of Conversion Webhooks and SecretStore/v1alpha1, ExternalSecret/v1alpha1 and their cluster counterparts</li> <li>Promotion of ExternalSecret/v1 and SecretStore/v1 and their cluster counterparts</li> <li>Removal of v1 templating engine</li> </ul> <p>Check if you can upgrade</p> <pre><code>kubectl get crd \\\n    externalsecrets.external-secrets.io\\\n    secretstores.external-secrets.io\\\n    clustersecretstores.external-secrets.io\\\n    clusterexternalsecrets.external-secrets.io\\\n    -o jsonpath='{.items[*].status.storedVersions[?(@==\"v1alpha1\")]}' | \\\n    grep -q v1alpha1 &amp;&amp; echo \"NOT SAFE! REMOVE v1alpha1 FROM YOUR STORED VERSIONS\" || echo \"Safe to Continue\"\n</code></pre> <p>If you get this error</p> <pre><code>conversion webhook for external-secrets.io/v1, Kind=ExternalSecret failed: the server could not find the requested resource\n</code></pre> <p>Simply force a refresh in an external secret from that cluster external secret</p> <pre><code>kubectl annotate externalsecrets.external-secrets.io MYCES force-sync=$(date +%s) --overwrite\n</code></pre> <ul> <li>Promoting to 0.16 #4662</li> </ul> <p>https://github.com/external-secrets/external-secrets/issues/4662</p>"},{"location":"security/external-secrets/updates/#from-016x-to-017x","title":"From 0.16.X to 0.17.X","text":"<p>Breaking change</p> <ul> <li>v0.17.0 Stops serving v1beta1 apis</li> </ul> <p>So it is needed to update the manifests v1beta1 to v1 prior to updating from v0.16 to v0.17. We need at least v0.16.2 because it supports v1.</p> <pre><code>#!/bin/bash\n\necho \"Searching apiVersion: external-secrets.io/v1 in yaml files\"\necho \"############### Showing alpha references ###############\"\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1alpha\"\necho\n\necho \"############### Showing beta references ###############\"\necho\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1beta\"\n\necho \"############### Showing non alpha or beta references ###############\"\necho\ngrep --recursive --include=\"*.yaml\" \"apiVersion: external-secrets.io/v1\" | grep -v \"v1alpha\" | grep -v \"v1beta\"\n</code></pre> <pre><code>#!/bin/bash\necho \"Changing all v1beta1 references to v1\"\nfind . \\( -name \"*.yaml\" -o -name \"*.yml\" \\) -print0 | while IFS= read -r -d '' yaml_file; do\n    if grep \"apiVersion: external-secrets.io/v1beta1\" \"$yaml_file\" &gt;/dev/null; then\n        echo \"Changing v1beta1 to v1 in $yaml_file\"\n        sed -i 's/apiVersion: external-secrets.io\\/v1beta1/apiVersion: external-secrets.io\\/v1/g' \"$yaml_file\"\n    fi\ndone\n</code></pre>"},{"location":"security/external-secrets/recipes/dockerconfigjson/","title":"Create a dockerconfigjson","text":""},{"location":"security/external-secrets/recipes/dockerconfigjson/#using-user-password-and-url","title":"Using user, password and url","text":"<p>With this template we can create an external secret to pull images from a private repository if we have the username, password and url stored in our secret store. The important thing here is the template. The data section can be different for every secret store.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: privatepull\nspec:\n  data:\n  - remoteRef:\n      key: PULL-APPS-U\n    secretKey: username\n  - remoteRef:\n      key: PULL-APPS-P\n    secretKey: password\n  - remoteRef:\n      key: PULL-APPS-URL\n    secretKey: url\n  secretStoreRef:\n    kind: SecretStore\n    name: mystore\n  target:\n    template:\n      data:\n        .dockerconfigjson: |\n          {\n            \"auths\": {\n              \"{{ .url  }}\": {\n                \"username\": \"{{ .username }}\",\n                \"password\": \"{{ .password }}\",\n                \"email\": \"\"\n              }\n            }\n          }\n      type: kubernetes.io/dockerconfigjson\n</code></pre>"},{"location":"security/external-secrets/recipes/dockerconfigjson/#using-the-base64","title":"Using the base64","text":"<p>pending</p>"},{"location":"security/external-secrets/recipes/harbor-auth/","title":"Vmware harbor auth","text":"<p>This recipe creates the Authorization header using the username and password</p> <pre><code> -H 'Authorization: THIS_WILL_BE_CREATED'\n</code></pre> <p>The spec.data section changes depending of the secret store provider</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: harbor-credentials\nspec:\n  data:\n    - remoteRef:\n        key: secret/harbor-credentials-u\n      secretKey: username\n    - remoteRef:\n        key: secret/harbor-credentialsr-p\n      secretKey: password\n  target:\n    template:\n      data:\n        auth: '{{ printf \"Basic %s\" (printf \"%s:%s\" .username .password | b64enc) }}'\n  secretStoreRef:\n    ...\n</code></pre>"},{"location":"security/external-secrets/recipes/kured-teams/","title":"Kured notifications to","text":"<p>Microsoft changed the url of the webhook created in Teams. In order to support kured notifications, this recipe adapts that webhook url to the new format.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: kured-settings\nspec:\n ...\n  target:\n    template:\n      data:\n        KURED_NOTIFY_URL: '{{ list .msteamsWebhookUrl \"template=json&amp;messagekey=text\" | join \"?\" | replace \"https://\" \"generic://\" }}'\n</code></pre> <p>And in the kured values.yaml</p> <pre><code>extraEnvVars:\n  - name: KURED_NOTIFY_URL\n    valueFrom:\n      secretKeyRef:\n        name: kured-settings\n        key: KURED_NOTIFY_URL\n</code></pre>"},{"location":"security/external-secrets/recipes/kured-teams/#links","title":"Links","text":"<ul> <li>MS Teams notifications are currently broken with new message format</li> </ul> <p>https://github.com/kubereboot/kured/issues/1024</p> <ul> <li>Add ability to set cli flags with environment variables</li> </ul> <p>https://github.com/kubereboot/kured/issues/383</p>"},{"location":"security/infisical/metrics/","title":"Metrics","text":""},{"location":"security/infisical/metrics/#prometheus-metrics","title":"Prometheus metrics","text":"<p>In order to enable the infisical prometheus metrics we must add the following environment variables to the infisical pod</p> <pre><code>OTEL_TELEMETRY_COLLECTION_ENABLED: \"true\"\nOTEL_EXPORT_TYPE: \"prometheus\"\n</code></pre> <p>This enables the metrics in the port 9464 and /metrics path</p>"},{"location":"security/infisical/metrics/#relevant-metrics","title":"Relevant metrics","text":"<ul> <li>API latency</li> </ul> <pre><code>API_latency_count\nAPI_latency_sum\nAPI_latency_bucket\n</code></pre> <ul> <li>Http server duration:</li> </ul> <p>Measures the duration of inbound HTTP requests</p> <pre><code>http_server_duration_count\nhttp_server_duration_sum\nhttp_server_duration_bucket\n</code></pre> <pre><code>API_errors_count\n</code></pre> <p>API_errors_count tracks the total number of errors that occurred for the specified route, method, and error type.</p> <pre><code>API_errors_sum\n</code></pre> <p>API_errors_sum tracks the cumulative value of the errors.</p> <pre><code>API_errors_bucket\n</code></pre> <p>API_errors_bucket is an histogram that measures the distribution of errors over different time buckets (latency or duration). Each bucket represents the number of errors that occurred within a specific time range (e.g., less than or equal to 5ms, 10ms, etc.). Labels: route: The API route (/api/v3/secrets/raw). method: The HTTP method (GET). type: The type of error (RateLimitError). name: The specific error name (RateLimitExceeded). le: The upper bound of the bucket (e.g., 5ms, 10ms, etc.).</p>"},{"location":"security/infisical/metrics/#some-tips","title":"Some tips","text":"<ul> <li>If you have an old infisical version you must probably need to update to get the metrics</li> <li>The helm chart currently does not support adding the environment variables and pod's port. The environment variables can be provided via the infisical-secrets secret.</li> </ul> <p>https://github.com/Infisical/infisical/issues/3382</p>"},{"location":"security/infisical/perlas/","title":"Perlas","text":""},{"location":"security/infisical/perlas/#error-unsupported-state-or-unable-to-authenticate-data","title":"Error \"Unsupported state or unable to authenticate data\"","text":"<ul> <li>Link https://github.com/Infisical/infisical/issues/2005</li> </ul> <p>The only way I've been able to change the encryption keys so far is to delete the containers, images, and volumes related to infisical in Docker Desktop, and run docker compose up with the desired keys in .env to rebuild the entire system from scratch. I didn't see any other way in documentation or browsing online.</p>"},{"location":"security/infisical/update-via-argocd/","title":"How to update infisical with argocd","text":"<p>In order to update infisical self hosted via slack we must do some steps</p>"},{"location":"security/infisical/update-via-argocd/#steps","title":"Steps","text":"<ul> <li> <p>Backup the database Do a backup in the database</p> </li> <li> <p>Disable autosync Disable autosync if enabled in the argocd application</p> </li> <li> <p>Change the values.yaml Change the values.yaml with the new image release.</p> </li> </ul> <p>We can get the infisical releases here: https://github.com/Infisical/infisical/releases</p> <p>And the infisical container releases here: https://hub.docker.com/r/infisical/infisical/tags</p> <ul> <li> <p>Sync the job Sync the job with Force and Replace options and see the log of the job's pod</p> </li> <li> <p>sync the deployment Sync the infisical deployment and see the log of the deployment's pod</p> </li> <li> <p>Enable autosync Enable autosync if needed in the argocd application</p> </li> </ul>"},{"location":"security/infisical/update-via-argocd/#links","title":"Links","text":"<ul> <li> <p>Kubernetes via helm chart https://infisical.com/docs/self-hosting/deployment-options/kubernetes-helm</p> </li> <li> <p>Schema migration https://infisical.com/docs/self-hosting/configuration/schema-migrations</p> </li> </ul>"},{"location":"security/keycloak/99-links/","title":"Links","text":""},{"location":"security/keycloak/99-links/#general-documentation","title":"General documentation","text":"<ul> <li>Guides</li> </ul> <p>https://www.keycloak.org/guides</p> <ul> <li>Documentation</li> </ul> <p>https://www.keycloak.org/documentation</p> <ul> <li>Redhat Build of keycloak</li> </ul> <p>https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/</p>"},{"location":"security/keycloak/99-links/#specific-documentation","title":"Specific documentation","text":"<ul> <li>Server Administration Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/server_admin/</p> <ul> <li>Server Developer Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/server_development/</p> <ul> <li>Authorization Services Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/authorization_services/</p> <ul> <li>Keycloak Admin REST API</li> </ul> <p>https://www.keycloak.org/docs-api/latest/rest-api/index.html</p>"},{"location":"security/keycloak/99-links/#updates-and-releases","title":"Updates and releases","text":"<ul> <li>Release Notes</li> </ul> <p>https://www.keycloak.org/docs/latest/release_notes/</p> <ul> <li>Upgrading Guide</li> </ul> <p>https://www.keycloak.org/docs/latest/upgrading/index.html</p>"},{"location":"security/keycloak/container/","title":"Some links about keycloak in kubernetes and containers","text":""},{"location":"security/keycloak/container/#keycloak-operator","title":"Keycloak operator","text":"<ul> <li> <p>Installation https://www.keycloak.org/operator/installation</p> </li> <li> <p>Basic deployment https://www.keycloak.org/operator/basic-deployment</p> </li> <li> <p>Advanced configuration https://www.keycloak.org/operator/advanced-configuration</p> </li> <li> <p>Using custom Keycloak images https://www.keycloak.org/operator/customizing-keycloak</p> </li> <li> <p>Realm import https://www.keycloak.org/operator/realm-import</p> </li> </ul>"},{"location":"security/keycloak/container/#containers","title":"Containers","text":"<ul> <li> <p>Getting started: Docker https://www.keycloak.org/getting-started/getting-started-docker</p> </li> <li> <p>Getting started: Podman https://www.keycloak.org/getting-started/getting-started-podman</p> </li> <li> <p>Getting started: Kubernetes https://www.keycloak.org/getting-started/getting-started-kube</p> </li> <li> <p>Getting started: Openshift https://www.keycloak.org/getting-started/getting-started-openshift</p> </li> <li> <p>Running Keycloak in a container https://www.keycloak.org/server/containers</p> </li> </ul>"},{"location":"security/keycloak/logging/","title":"Keycloak logging","text":"<p>Two ways to setup a more verbose loggin using the keycloak operator</p>"},{"location":"security/keycloak/logging/#keycloak-logging-handler","title":"Keycloak logging handler","text":"<p>We can choose between these handlers: console (default), file, syslog and gelf</p> <p>Also we can use the environment variable KC_LOG</p> <p>We to change the fields of the template used by the logging system</p> <ul> <li>using the console handler: --log-console-format and KC_LOG_CONSOLE_FORMAT</li> <li>using the file handler: --log-file-format and KC_LOG_FILE_FORMAT</li> </ul> <p>Another options are:</p> <ul> <li>Change the output to json instead of plain</li> <li>Enable color logging using the console handler</li> </ul>"},{"location":"security/keycloak/logging/#keycloak-logging-level","title":"Keycloak logging level","text":"<p>To enable a more verbose logging we can use the \"--log-level\" parameter and choose between several log levels: FATAL, ERROR, WARN, INFO, DEBUG, TRACE, ALL, OFF. The error level can be in lowercase or uppercase.</p> <p>Also we can use the environment variable KC_LOG_LEVEL</p> <p>It is possible to setup a different log level to the default (root) per category, with this format:</p> <pre><code>--log-level=\"&lt;root-level&gt;,&lt;org.category1&gt;:&lt;org.category1-level&gt;\"\n</code></pre>"},{"location":"security/keycloak/logging/#hostname-debug","title":"Hostname debug","text":"<p>For debugging hostname related problems, we can enable a debug feature using --hostname-debug=true Also we can use the environment variable KC_HOSTNAME_DEBUG</p> <p>After enabling it, we can access to MYKEYCLOAKURL/realms/master/hostname-debug to see some hostname information</p>"},{"location":"security/keycloak/logging/#example-in-the-keycloak-operator","title":"Example in the keycloak operator","text":"<pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  additionalOptions:\n    - name: hostname-debug\n      value: \"true\"\n    - name: log-level\n      value: DEBUG\n</code></pre>"},{"location":"security/keycloak/logging/#links","title":"Links","text":"<ul> <li> <p>Configure logging https://www.keycloak.org/server/logging</p> </li> <li> <p>Logging settings https://www.keycloak.org/server/all-config#category-logging</p> </li> <li> <p>Hostname troubleshooting https://www.keycloak.org/server/hostname#_troubleshooting</p> </li> <li> <p>Admin console not loading and hostname related issues #14666 https://github.com/keycloak/keycloak/issues/14666</p> </li> </ul>"},{"location":"security/keycloak/multi-yaml-deploy/","title":"Deploy multiple operators via yaml","text":""},{"location":"security/keycloak/multi-yaml-deploy/#provided-manifests","title":"Provided manifests","text":"<p>Keycloak team offers 3 yaml files:</p> <ul> <li>The keycloak resource crd</li> <li>The keycloakrealmimports crd</li> <li>The operator depllyment and other needed resources</li> </ul>"},{"location":"security/keycloak/multi-yaml-deploy/#the-problems","title":"The problems","text":"<ul> <li> <p>The keycloak operator does not support watching the resources it manages created in all namespaces, so the operator must me deployed in every namespace you create that resources.</p> </li> <li> <p>Updating the crds must be aligned with every operator instance</p> </li> <li> <p>Most of the resources in the operator yaml file are namespaced, but this includes a ClusterRoleBinding binded to the keycloak-operator service account in the keycloak namespace.</p> </li> </ul> <pre><code>subjects:\n  - kind: ServiceAccount\n    name: keycloak-operator\n    namespace: keycloak\n</code></pre> <p>But this gives openshift related permissions, so it can be ignored if you dont use Openshift</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: keycloak-operator-clusterrole\nrules:\n  - apiGroups:\n      - config.openshift.io\n...\n</code></pre>"},{"location":"security/keycloak/multi-yaml-deploy/#the-deployment","title":"The deployment","text":"<p>So we can do this via kustomize</p> <p>The operator-crds folder and its kustomization.yaml file</p> <pre><code>resources:\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/keycloaks.k8s.keycloak.org-v1.yml\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml\n</code></pre> <p>The operator-base folder and its kustomization.yaml file</p> <pre><code>resources:\n  - https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.2.5/kubernetes/kubernetes.yml\n</code></pre> <p>Our deployment folder and its kustomization.yaml file will deploy the operator in 2 namespaces but the crds once</p> <pre><code>resources:\n  - ../operator-crds-folder-location\n  - ns-1\n  - ns-2\n</code></pre> <p>Inside every namespace folder, we need a kustomization file with a suffix (or prefix). This adds a suffix or prefix to the resource name.</p> <pre><code>namespace: ns-1\nresources:\n  - ../operator-base-folder-location\nnameSuffix: -ns-1\n</code></pre> <pre><code>namespace: ns-2\nresources:\n  - ../operator-base-folder-location\nnameSuffix: -ns-2\n</code></pre> <p>The ClusterRoleBinding created with our prefix is binded to an unexistant service account, but it is openshift related.</p>"},{"location":"security/keycloak/multi-yaml-deploy/#update-the-release","title":"Update the release","text":"<p>In order to update the release we only need to change the urls in the operator-crds and operator-base folders</p>"},{"location":"security/keycloak/start-modes/","title":"Start modes","text":"<p>There are 2 ways to start keycloak, development and production mode</p>"},{"location":"security/keycloak/start-modes/#development-mode","title":"Development mode","text":"<p>In development mode we assume by default:</p> <ul> <li>HTTP is enabled</li> <li>Strict hostname resolution is disabled</li> <li>Cache is set to local (No distributed cache mechanism used for high availability)</li> <li>Theme-caching and template-caching is disabled</li> </ul> <p>If we want to start keycloak in development mode using the binary, we must use this</p> <pre><code>bin/kc.[sh|bat] start-dev\n</code></pre> <p>If we want to start keycloak in development mode using the operator, we must use this</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  unsupported:\n    podTemplate:\n      spec:\n        containers:\n          - name: keycloak\n            args:\n              - start-dev\n</code></pre> <p>In the logs we can read:</p> <pre><code>Hostname settings: Base URL: &lt;unset&gt;, Hostname: &lt;request&gt;, Strict HTTPS: false, Path: &lt;request&gt;, Strict BackChannel: false, Admin URL: &lt;unset&gt;, Admin: &lt;request&gt;, Port: -1, Proxied: true\nRunning the server in development mode. DO NOT use this configuration in production.\n</code></pre>"},{"location":"security/keycloak/start-modes/#production-mode","title":"Production mode","text":"<p>In the production mode we assume by default:</p> <ul> <li>HTTP is disabled as transport layer security (HTTPS) is essential</li> <li>Hostname configuration is expected</li> <li>HTTPS/TLS configuration is expected</li> </ul> <p>If we want to start keycloak in production mode using the binary, we must use this</p> <pre><code>bin/kc.[sh|bat] start\n</code></pre> <p>The production mode is the default mode in the keycloak operator. The minimun setup to make it work is</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  http:\n    tlsSecret: MYSECRET_CONTAINING_CERTIFICATE\n  hostname:\n    hostname: MY.DOMAIN.COM\n</code></pre> <p>If we dont provide this values, we can get some errors</p> <pre><code>Key material not provided to setup HTTPS. Please configure your keys/certificates or start the server in development mode.\n</code></pre>"},{"location":"security/keycloak/start-modes/#production-mode-disabling-tls-and-change-cache-to-local","title":"Production mode disabling tls and change cache to local","text":"<pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  http:\n    httpEnabled: true\n  hostname:\n    strict: false\n  additionalOptions:\n    - name: cache\n      value: local\n</code></pre>"},{"location":"security/keycloak/start-modes/#links","title":"Links","text":"<ul> <li>Starting Keycloak https://www.keycloak.org/server/configuration#_starting_keycloak</li> </ul>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/","title":"Endpoints","text":"<p>Keycloak exposes some endpoints for communication between applications and for management purposes.</p> <p>We can group those endpoints in 3 main endpoint groups:</p> <ul> <li>Frontend</li> <li>Backend</li> <li>Administration</li> </ul> <p>There is another interface called the management interface</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#base-url","title":"Base URL","text":"<p>For that, we need to configure a base Url for them, that contains:</p> <ul> <li> <p>a scheme (https,...)</p> </li> <li> <p>a hostname (example.keycloak.org,...)</p> </li> <li> <p>a port (8443,...)</p> </li> <li> <p>a path (/auth,...)</p> </li> </ul> <p>The base URL for each group has an important impact on:</p> <ul> <li>how tokens are issued and validated</li> <li>how links are created for actions that require the user to be redirected to Keycloak</li> <li>how applications will discover these endpoints when fetching the OpenID Connect Discovery Document from realms/{realm-name}/.well-known/openid-configuration.</li> </ul>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#frontend-group","title":"Frontend group","text":"<p>The frontend group of Keycloak endpoints refers to the URLs and API paths that are accessed by users and applications through the frontchannel. This is, via a publicly accessible communication path, typically over the internet.</p> <p>These endpoints are designed for operations that require direct user interaction , such as browser-based authentication flows. Some examples are:</p> <ul> <li>The login page</li> </ul> <p>Where users are redirected to authenticate.</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/auth\n</code></pre> <ul> <li>Consent/Registration</li> </ul> <pre><code>https://&lt;hostname&gt;/realms/{realm}/login-actions/...\n</code></pre> <ul> <li>Account management</li> </ul> <p>User self-service account management</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/account/\n</code></pre> <ul> <li>OpenID Connect Discovery Document</li> </ul> <p>OIDC discovery for applications</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/.well-known/openid-configuration\n</code></pre> <ul> <li> <p>clicking on a link to reset a password</p> </li> <li> <p>performing actions that involve binding tokens</p> </li> </ul> <p>These activities are considered frontchannel requests because they happen over a channel that is exposed to users and external applications, rather than being restricted to internal or backend communication.</p> <p>So, the front channel is a publicly accessible communication channel, which refers to a communication path that is publicly accessible, typically over the internet.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#backend-group","title":"Backend group","text":"<p>The backend group of Keycloak endpoints refers to URLs and API paths used for direct, programmatic communication between Keycloak and client applications, typically over a secure or private network.</p> <p>These endpoints are designed for backend-to-backend interactions, such as exchanging tokens, introspecting tokens, or retrieving user information, and do not require direct user interaction.</p> <p>These endpoints handle sensitive operations like token issuance and validation.</p> <p>Some examples are:</p> <ul> <li>Token Endpoint</li> </ul> <p>Issues and refreshes tokens for clients.</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/token\n</code></pre> <ul> <li>Token Introspection Endpoint Allows clients to validate and inspect tokens.</li> </ul> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/token/introspect\n</code></pre> <ul> <li>Userinfo Endpoint</li> </ul> <p>Returns user profile information associated with an access token.</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/userinfo\n</code></pre> <ul> <li>JWKS URI Endpoint</li> </ul> <p>Provides the public keys used to verify JWT signatures.</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/certs\n</code></pre> <ul> <li>Authorization Endpoint</li> </ul> <p>Used by applications to obtain authorization from users (can be both frontend and backend, depending on flow).</p> <pre><code>https://&lt;hostname&gt;/realms/{realm}/protocol/openid-connect/auth\n</code></pre> <p>The backend endpoints are those accessible through a public domain or through a private network. They\u2019re related to direct backend communication between Keycloak and a client (an application secured by Keycloak). Such communication might be over a local network, avoiding a reverse proxy.</p>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#administration-group","title":"Administration group","text":"<p>The administration group Keycloak endpoints are URLs and API paths dedicated to managing and configuring the Keycloak server and its realms.</p> <p>These endpoints are intended for administrators and are typically not exposed to the public internet for security reasons. They provide both a web-based interface and programmatic access for automation and integration.</p> <ul> <li>Administration Console</li> </ul> <p>The web-based UI for managing realms, users, clients, roles, and other Keycloak resources.</p> <pre><code>https://&lt;hostname&gt;/admin/\n</code></pre> <ul> <li>Admin REST API</li> </ul> <p>A set of RESTful endpoints for programmatic management of Keycloak. Allows automation, scripting and integration with external systems.</p> <pre><code>https://&lt;hostname&gt;/admin/realms/{realm}/...\n</code></pre> <ul> <li>Static Resources for Admin Console</li> </ul> <p>CSS, JavaScript, images, and other static files required by the administration console.</p> <pre><code>https://&lt;hostname&gt;/resources/\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/00-endpoints/#links","title":"Links","text":"<ul> <li>Configuring the hostname (v2)</li> </ul> <p>https://www.keycloak.org/server/hostname</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/","title":"Hostname","text":"<p>In order to make Keycloak accessible via the frontend URL, we must configure the hostname option:</p> <pre><code>via cli: --hostname parameter\nvia environment variable: KC_HOSTNAME\nvia operator: spec.hostname.hostname\n</code></pre> <p>The parts of the base URL we dont specify will be resolved dynamically with the request. Some hostname examples:</p> <pre><code>my.keycloak.org  &lt; hostname only\n&lt;https://my.keycloak.org&gt;  &lt; scheme, hostname\n&lt;https://my.keycloak.org:123/auth&gt;  &lt; scheme, hostname, port, path\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#dynamic-resolution-for-frontchannel","title":"Dynamic resolution for frontchannel","text":"<p>The hostname option is mandatory by default because of security reasons and this behaviour is controlled with the following setting:</p> <pre><code>via cli: --hostname-strict parameter\nvia environment variable: KC_HOSTNAME_STRICT\nvia operator: spec.hostname.strict\n</code></pre> <p>This option is enabled by default and disables dynamically resolving the hostname from request headers.</p> <p>It should always be set to true in production, unless your reverse proxy overwrites the Host header. If enabled, the hostname option needs to be specified.</p> <p>If don't want to specify the hostname and make it fully dynamic we must change it to false.</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#dynamic-resolution-for-backchannel","title":"Dynamic resolution for backchannel","text":"<p>It is possible to permit dynamic resolution for backchannel communications, then this baseURL is dynamically resolved based on incoming headers (hostname, scheme, port and context path). This permits applications and clients using an internal URL for communication while maintaining the use of a public URL for frontchannel requests.</p> <p>By default is set to false.</p> <pre><code>via cli: --hostname-backchannel-dynamic parameter\nvia environment variable: KC_HOSTNAME_BACKCHANNEL_DYNAMIC\nvia operator: spec.hostname.backchannelDynamic\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#administration-url","title":"Administration url","text":"<p>We can also use a different base URL for the administration console. This is done with the following setting:</p> <pre><code>via cli: --hostname-admin parameter\nvia environment variable: KC_HOSTNAME_ADMIN\nvia operator: spec.hostname.admin\n</code></pre> <p>This parameter accepts a full url. Example:</p> <p>https://admin.my.keycloak.org:8443</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#administration-rest-api-endpoints","title":"Administration REST API endpoints","text":"<p>This option only applies to the administration console. The Administration REST API endpoints are accesible via the frontend URL specified by the hostname option.</p> <p>If you want to restrict access to the Administration REST API, you need to do it on the reverse proxy level. Administration Console implicitly accesses the API using the URL as specified by the hostname-admin option.</p>"},{"location":"security/keycloak/endpoint-groups/01-hostname/#troubleshooting","title":"Troubleshooting","text":"<p>It is possible to troubleshoot the hostname configuration with the following setting:</p> <pre><code>via cli: --hostname-debug paramter\nvia environment variable: KC_HOSTNAME_DEBUG\n</code></pre> <p>via operator:</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  additionalOptions:\n    - name: hostname-debug\n      value: \"true\"\n</code></pre> <p>Then the debug site will be available under /realms/&gt;/hostname-debug"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/","title":"Hostname v1 and v2 features","text":"<ul> <li>Until the 25.0.0 release (jun 2024), the only way to configure the hostname is called the hostname-v1 feature.</li> <li>In this release, the default ways was the hostname:v2 feature. If you want to use the hostname-v1 feature instead of v2 feature, you must enable it. They are mutually exclusive.</li> <li>The hostname:v1 option was removed in 26.0.0</li> </ul>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#configure-the-hostname-in-hostnamev1","title":"Configure the hostname in hostname:v1","text":"<pre><code>Using --hostname-url (KC_HOSTNAME_URL)\n\nWe can provide the full url with the --hostname-url parameter or the KC_HOSTNAME_URL environemnt variable using this format:\n\n&lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;/&lt;path&gt;\n\nThis setting is not supported directly in the keycloak operator\n</code></pre> <pre><code>Using --hostname (KC_HOSTNAME) or spec.hostname.hostname in the operator\n\nHere we provide only the hostname (my.domain.com).\n\nNotes about the scheme\n\n\nThe scheme will be https unless you set --hostname-strict-https=false (KC_HOSTNAME_STRICT_HTTPS). This is an undocumented setting\nThis setting is not supported directly in the keycloak operator\n</code></pre> <p>Notes about the port</p> <pre><code>--hostname-port parameter (KC_HOSTNAME_PORT) defines the port number that the Keycloak server is listening on for HTTP or HTTPS traffic. \nThis setting is not supported directly in the keycloak operator\n</code></pre> <p>Notes about the path</p> <pre><code>We can provide --hostname-path (KC_HOSTNAME_PATH) to specify the context path or path prefix for the Keycloak server. This option affects where Keycloak is accessible and how URLs are generated, particularly when deployed behind a reverse proxy. \nExample: If a reverse proxy forwards requests to /keycloak on your Keycloak server, you might set KC_HOSTNAME_PATH=/keycloak to ensure Keycloak's URLs are also prefixed with /keycloak.\n\nThe hostname-path also affects the admin console URL. If you set hostname-path=/keycloak, the admin console will be accessible at your-domain.com/keycloak/admin.\nIn some cases, you might use http-relative-path instead of hostname-path, which specifies the relative path of the HTTP backend without affecting the full hostname.\nThis setting is not supported directly in the keycloak operator\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#hostname-v1-backend","title":"hostname-v1 backend","text":"<p>In hostname-v1, by default, the URLs for backend endpoints are also based on the incoming request. We can change this behaviour with the hostname-strict-backchannel (KC_HOSTNAME_STRICT_BACKCHANNEL). Here the URLs for the backend endpoints are going to be exactly the same as the frontend endpoints.</p> <p>When all applications connected to Keycloak communicate through the public URL, set hostname-strict-backchannel to true. Otherwise, leave this parameter as false (default) to allow client-server communication through a private network.</p>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#hostname-v1-admin-console","title":"hostname-v1 admin console","text":"<p>In hostname-v1, by default, the URLs for administration console are also based on the incoming request. We can restrict access to the administration console using a specific URL using:</p> <pre><code>a host\n\n- parameter: --hostname-admin\n- environment variable: KC_HOSTNAME_ADMIN\n- operator: spec.hostname.admin\n\nor the full url with\n\n- parameter: --hostname-admin-url\n- environment variable: KC_HOSTNAME_ADMIN_URL\n- operator: spec.hostname.adminUrl\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#changed-settings","title":"Changed settings","text":"<p>These are how the settings changed from v1 to v2</p> <p></p>"},{"location":"security/keycloak/endpoint-groups/02-hostname-v1-v2/#links","title":"Links","text":"<ul> <li>Configuring the hostname: (v1) Redhat</li> </ul> <p>https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/24.0/html/server_guide/hostname-</p> <ul> <li>Configuring the hostname (v2)  </li> </ul> <p>https://www.keycloak.org/server/hostname</p> <ul> <li>Upgrade to 25.0.0  </li> </ul> <p>https://www.keycloak.org/docs/25.0.0/upgrading/</p> <ul> <li>Keycloak 26.0.0 released</li> </ul> <p>https://www.keycloak.org/2024/10/keycloak-2600-released</p> <ul> <li>Migrating to 25.0.0</li> </ul> <p>https://www.keycloak.org/docs/latest/upgrading/#migrating-to-25-0-0</p> <ul> <li>All configuration</li> </ul> <p>https://www.keycloak.org/server/all-config</p>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/","title":"Management interface","text":"<p>The management interface in Keycloak is a dedicated HTTP interface that exposes the health checks and metrics.</p> <p>If the health checks and metrics are disabled, the management inteface is automatically turned off.</p>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/#health-checks","title":"Health checks","text":"<p>THe health check can be controlled by this setting:</p> <pre><code>via cli: --health-enabled parameter\nvia environment variable: KC_HEALTH_ENABLED\n</code></pre> <p>The health check is enabled by default in the operator. Disabling it will make the kubernetes probes fail.</p>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/#metrics-endpoint","title":"Metrics endpoint","text":"<p>THe metrics endpoint can be controlled by this setting:</p> <pre><code>via cli: --metrics-enabled parameter\nvia environment variable: KC_METRICS_ENABLED\n</code></pre> <p>In the operator we can use the additionalOptions key.</p> <pre><code>apiVersion: k8s.keycloak.org/v2alpha1\nkind: Keycloak\nmetadata:\n  name: keycloak\nspec:\n  additionalOptions:\n    - name: metrics-enabled\n      value: \"true\"\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/#port","title":"Port","text":"<p>The default management interface port is 9000. We can change it this way:</p> <pre><code>via cli: --http-management-port\nvia environement variable: KC_HTTP_MANAGEMENT_PORT\nvia operator: spec.httpManagement.port\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/#notes","title":"Notes","text":"<ul> <li> <p>In old releases, the management interface was enabled in the default server, but this behaviour, that can be enabled, it is not recommended and it will be deprecated.</p> </li> <li> <p>The management interface has its own settings like relative path or certificates but, if they are not specified, they are inherited from the default server</p> </li> </ul>"},{"location":"security/keycloak/endpoint-groups/03-management-interface/#links","title":"Links","text":"<ul> <li>Configuring the Management Interface</li> </ul> <p>https://www.keycloak.org/server/management-interface</p> <ul> <li>Configuring the Management Interface (Redhat)</li> </ul> <p>https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.2/html/server_configuration_guide/management-interface-</p> <ul> <li>All management interface settings</li> </ul> <p>https://www.keycloak.org/server/all-config#category-management</p> <ul> <li>Configuring a reverse proxy</li> </ul> <p>https://www.keycloak.org/server/reverseproxy</p>"},{"location":"security/keycloak/endpoint-groups/04-exposition/","title":"Exposing the endpoints","text":""},{"location":"security/keycloak/endpoint-groups/04-exposition/#ports","title":"Ports","text":"<p>By default keycloak is exposed with https enabled in the port 8443. We can change this behaviour with the following setting:</p> <p>change the https port (default 8443)</p> <pre><code>via cli: --https-port\nvia environement variable: KC_HTTPS_PORT\nvia operator: spec.http.httpsPort\n</code></pre> <p>enable http (default disabled)</p> <pre><code>via cli: --http-enabled\nvia environement variable: KC_HTTP_ENABLED\nvia operator: spec.http.httpEnabled\n</code></pre> <p>change http port (defalt 8080)</p> <pre><code>via cli: --http-port\nvia environement variable: KC_HTTP_PORT\nvia operator: spec.http.httpPort\n</code></pre>"},{"location":"security/keycloak/endpoint-groups/04-exposition/#some-best-practices","title":"Some best practices","text":"<ul> <li>About ports</li> </ul> <p>Only expose the https port and do not enable the http port</p> <ul> <li>Management interface</li> </ul> <p>Dot not expose the management interface</p> <ul> <li>Move the administration REST API and admin UI</li> </ul> <p>Expose the administration REST API and admin UI (--hostname-admin) in a different hostname or context-path than the one used for the public frontend URLs that are used.</p> <p>We only need to expose /realms and /resources in the public frontend URLs</p>"},{"location":"security/keycloak/endpoint-groups/04-exposition/#links","title":"Links","text":"<ul> <li>Configuring a reverse proxy</li> </ul> <p>https://www.keycloak.org/server/reverseproxy</p> <ul> <li>Configuring Keycloak for production</li> </ul> <p>https://www.keycloak.org/server/configuration-production</p> <ul> <li>All configuration</li> </ul> <p>https://www.keycloak.org/server/all-config</p> <ul> <li>Configuring the hostname (v2)</li> </ul> <p>https://www.keycloak.org/server/hostname</p>"},{"location":"security/openssl/extract-from-pem/","title":"Extract from pem","text":""},{"location":"security/openssl/extract-from-pem/#extract-certificate-from-a-pem-file","title":"Extract certificate from a pem file","text":"<pre><code>openssl x509 -in FILE.pem -out cert1.crt\n</code></pre>"},{"location":"security/openssl/extract-from-pem/#extract-private-key-from-a-pem-file","title":"Extract private key from a pem file","text":"<pre><code>openssl rsa -in FILE.pem -out cert1.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/","title":"Openssl and web certificates","text":""},{"location":"security/openssl/openssl-certs/#web-certificate-and-rfc-6125","title":"Web certificate and RFC 6125","text":"<p>In the RFC 6125 the recommendation is to use the X509v3 Subject Alternative Name (SAN). It includes all the domains and subdomains this certificate will secure. It can also include ip addresses.</p> <pre><code>X509v3 Subject Alternative Name: \n    DNS:my.domain.com\n\nIn the Subject we can setup a Common Name. Examples:\n\n```txt\nSubject: CN = my.domain.com\nSubject: CN = www.domain.com\nSubject: CN = *.domain.com\n</code></pre> <p>As the RFC 6125 says, the SAN is checked first. If SAN does not exists, the CN will be checked. If both are specified, the CN must match an entry in the SAN. But at this point, different clients can have different behaviours.</p>"},{"location":"security/openssl/openssl-certs/#information","title":"Information","text":""},{"location":"security/openssl/openssl-certs/#certificate-information","title":"Certificate information","text":"<pre><code>openssl x509 -noout -text -in 'cerfile.crt'  # PEM format (default)\nopenssl x509 -inform pem -noout -text -in 'cerfile.cer';  # PEM format (default)\nopenssl x509 -inform der -noout -text -in 'cerfile.cer'; # DER format\n</code></pre>"},{"location":"security/openssl/openssl-certs/#checks","title":"Checks","text":""},{"location":"security/openssl/openssl-certs/#private-key-integrity","title":"Private key integrity","text":"<pre><code>openssl rsa -check -noout -in privatekey.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/#modulus-they-must-match","title":"Modulus (they must match)","text":"<pre><code>openssl x509 -noout -modulus -in privatekey.key\nopenssl rsa -noout -modulus -in certificate.key\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-from-pfx","title":"Extract from pfx","text":""},{"location":"security/openssl/openssl-certs/#extract-the-private-key","title":"Extract the private key","text":"<pre><code>openssl pkcs12 -in [yourfile.pfx] -nocerts -out [tls.key] # encrypted\nopenssl pkcs12 -in [yourfile.pfx] -nocerts -noenc -out [tls.key] # no encrypted\n</code></pre> <p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created). Also, for the \"PEM pass phrase\". This will protecdt the .key generated file. Store this \"PEM pass phrase\"</p> <ul> <li>Decrypt the private key if encrypted</li> </ul> <p>Type the \"PEM pass phrase\"</p> <pre><code>openssl rsa -in [drlive.key] -out [tls-decrypted.key]\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-the-certificate","title":"Extract the certificate","text":"<p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created).</p> <pre><code>openssl pkcs12 -in [yourfile.pfx] -clcerts -nokeys -out [tls.crt]\n</code></pre>"},{"location":"security/openssl/openssl-certs/#extract-the-ca","title":"Extract the ca","text":"<p>This asks you for the import password (the password used to protect the keypair when the .pfx file was created).</p> <pre><code>openssl pkcs12 -in [yourfile.pfx] -cacerts -nokeys -out [tls.ca]\n</code></pre>"},{"location":"security/protect-git-repos/no-secrets-git/","title":"Protect your git repos from including secrets","text":""},{"location":"security/protect-git-repos/no-secrets-git/#client-side-pre-commit-hook","title":"Client-side pre-commit hook","text":"<p>The pre-commit hook is the first client-side git hook to run when a commit is executed and the best option to check if contains any secret. Every non-zero exit aborts the commit and it can be bypassed using</p> <pre><code>git commit --no-verify\n</code></pre>"},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-implementation-pre-commit-hook","title":"Gitlab implementation (pre-commit hook)","text":"<ul> <li>Client-side secret detection</li> </ul> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/client/index.html</p>"},{"location":"security/protect-git-repos/no-secrets-git/#related-tools","title":"Related tools","text":"<ul> <li>Pre-commit</li> </ul> <p>https://pre-commit.com/</p>"},{"location":"security/protect-git-repos/no-secrets-git/#server-side-and-the-pre-receive-hook","title":"Server-Side and the pre-receive hook","text":"<p>The pre-receive hook is the first server-side git hook to run when a push is executed and the best option to check if contains any secret. Every non-zero exit rejects the push.</p>"},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-implementations-pre-receive-hook","title":"Gitlab implementations (pre-receive hook)","text":"<ul> <li>Gitlab customized pre-receive hook</li> </ul> <p>In Gitlab you can create your own pre-receive hook</p> <p>https://docs.gitlab.com/ee/administration/server_hooks.html</p> <ul> <li>Gitlab Secret push protection</li> </ul> <p>Also in Gitlab, you can use the Secret Push Protection feature, available since 17.2 release.</p> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/secret_push_protection/index.html</p>"},{"location":"security/protect-git-repos/no-secrets-git/#after-the-push","title":"After the push","text":""},{"location":"security/protect-git-repos/no-secrets-git/#gitlab-cicd-implementation","title":"Gitlab CI/CD implementation","text":"<p>You can use the detection of secrets inside a pipeline and protect branches. For example, you can:</p> <ul> <li>Protect a branch</li> </ul> <p>Located in Settings - Repository - Protected branches, prevent direct pushes to important branches and require changes to go through merge requests.</p> <ul> <li>Enable \"Pipelines must succeed\"</li> </ul> <p>Located in Settings - Merge requests - Merge checks and enabling the \"Pipelines must succeed\" option, this ensures that the pipeline must pass before a merge request can be merged.</p> <ul> <li>Scan the code</li> </ul> <p>Use a tool in the pipeline to scan the code and ensure it fails if a secret is detected.</p> <p>Another gitlab related options are:</p> <ul> <li>Pipeline secret detection</li> </ul> <p>https://docs.gitlab.com/ee/user/application_security/secret_detection/pipeline/index.html</p> <ul> <li>Gitlab Static Application Security Testing (SAST)</li> </ul> <p>This a gitlab tool to scan your code using Gitlab CI/CD. It supports some programming languages, kubernetes manifests and helm-charts</p> <p>https://docs.gitlab.com/ee/user/application_security/sast/</p>"},{"location":"security/protect-git-repos/no-secrets-git/#tools","title":"Tools","text":"<p>These tools are related with implementing scans in pre-commit hooks, pre-receive hooks and pipelines.</p> Contributors Stars Notes URL Gitleaks 212 22.500 Active https://github.com/gitleaks/gitleaks Trufflehog 163 20.000 Active https://github.com/trufflesecurity/trufflehog Trivy 466 27.600 Active https://github.com/aquasecurity/trivy Talisman 68 2.000 Active https://github.com/thoughtworks/talisman Detect-secrets 71 3.800 Active? https://github.com/Yelp/detect-secrets Git secrets 30 12.400 Awslabs. Unmaintained https://github.com/awslabs/git-secrets Git guardian Platform https://www.gitguardian.com/"},{"location":"security/protect-git-repos/no-secrets-git/#links","title":"Links","text":"<ul> <li> <p>Customizing Git - Git Hooks https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks</p> </li> <li> <p>Gitlab Secret detection https://docs.gitlab.com/ee/user/application_security/secret_detection/ P</p> </li> </ul>"},{"location":"security/protect-git-repos/pre-commit-hooks/","title":"Pre commit hooks","text":""},{"location":"security/protect-git-repos/pre-commit-hooks/#git-hooks","title":"Git hooks","text":"<p>Create the .git/hooks/pre-commit file in your local repo and make it executable</p> <pre><code>cat &lt;&lt; EOF &gt; .git/hooks/pre-commit\n#!/bin/bash\necho \"Scanning commit with trufflehog\"\ntrufflehog git file://. --since-commit HEAD --results=verified,unknown --fail\necho \"Scanning commit with gitleaks\"\ngitleaks git --redact --staged --verbose\nEOF\nchmod u+x .git/hooks/pre-commit\n</code></pre>"},{"location":"security/protect-git-repos/pre-commit-hooks/#sharing-the-git-hooks-with-hookspath","title":"Sharing the git hooks with hooksPath","text":"<p>The git hooks stores in the hooks directory are not pushed to the remote repository. Each developer's hooks remain on their local machine only. This must be executed by all users in the root of all local repositories.</p> <p>An alternative approach is to store the hooks for example in another folder and push it to the repository</p> <p>This changes the path where the hooks are located</p> <pre><code>git config core.hooksPath .githooks # the default value is .git/hooks\n</code></pre>"},{"location":"security/protect-git-repos/pre-commit-hooks/#pre-commit-framework","title":"Pre-commit framework","text":"<p>Another way to share the pre commit hooks is using the pre-commit framework.</p> <p>For this you must :</p> <ul> <li>install this framework</li> <li>configure .pre-commit-config.yaml in the root of your git repo with your pre-commit hooks</li> <li>install the pre-commit scripts in your git repo</li> </ul>"},{"location":"security/protect-git-repos/pre-commit-hooks/#links-and-integrations","title":"Links and integrations","text":"<p>Pre-commit framework</p> <p>https://pre-commit.com/</p> <ul> <li>Trufflehog</li> </ul> <p>https://docs.trufflesecurity.com/pre-commit-hooks</p> <ul> <li>Gitleaks</li> </ul> <p>https://github.com/gitleaks/gitleaks</p> <ul> <li>Talisman</li> </ul> <p>https://github.com/thoughtworks/talisman</p> <ul> <li>Trivy</li> </ul> <p>https://github.com/mxab/pre-commit-trivy</p>"},{"location":"security/protect-git-repos/pre-commit-hooks/#husky","title":"Husky","text":"<p>pending</p> <p>https://typicode.github.io/husky/</p>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/","title":"Ways to scan with trufflehog","text":"<p>If we want to scan git repositories with trufflehog we have some options:</p> <p>This article is based in trufflehog 3.82.13</p>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-filesystem-level","title":"At filesystem level","text":"<p>The following command scans for credentials in a path, and we can use a cloned repo, but probably it is not the best option to scan a git repository because it does not scans the full git history.</p> <pre><code>trufflehog filesystem OPTIONS PATH\n</code></pre>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-git-level","title":"At git level","text":"<p>The following command scans for credentials in a git repository, including all branches and commit history.</p> <pre><code>trufflehog git OPTIONS URL\n</code></pre> <ul> <li>The url can be in https://, file://, or ssh:// format</li> <li>The --branch=BRANCH permits to set an specified branch</li> <li>--max-depth=DEPTH limits the commits to be scanned</li> <li>--since-commit=COMMIT also limits the commits to be scanned</li> </ul>"},{"location":"security/protect-git-repos/trufflehog/ways-to-scan/#at-provider-level","title":"At provider level","text":"<p>We can scan for credentials in a in github or gitlab repository. It is similar to the git command, but includes additional options in every provider.</p> <pre><code>trufflehog github\n</code></pre> <pre><code>trufflehog gitlab\n</code></pre> <p>There is an experimental github scan</p> <pre><code>trufflehog github-experimental\n</code></pre> <p>In both providers we can:</p> <ul> <li>Configure the github|gitlab server with the --endpoint=https://whatever option</li> <li>Tell trufflehog what repos we want to scan, we can repeat the --repo=REPO option with all the desired repos</li> <li>The authentication can be provided with the --token=TOKEN option or via the GITHUB_TOKEN or GITLAB_TOKEN environment variable</li> </ul> <p>In github there are some specific github options that permit to tell the organization, the member repositories, if include or not forks, if include or not wikis,...</p> <p>Also the output can be in a github actions format.</p>"},{"location":"services/external-dns/98-tips/","title":"Tips","text":""},{"location":"services/external-dns/98-tips/#policy","title":"Policy","text":"<p>We can control how the dns records are syncronized between sources and providers with the policy.</p> <p>Via policy, we can control if the controller can update records when the ingress or services annotations change, or remove records when they are deleted</p> Policy Create Update Remove upsert-only Yes Yes No create-only Yes No No sync Yes Yes Yes <p>upsert-only is the default behaviour</p>"},{"location":"services/external-dns/98-tips/#share-zones-between-clusters","title":"Share zones between clusters","text":"<p>External dns can dinamically create and remove dns entries in the provider when services and ingress resources with certain configurations are added and removed (sync policy)</p> <p>We can also manage the same zone from more than one kubernetes cluster, but this have a potential problem.</p> <p>External secrets by default uses txt dns entries for tracking the ownership of DNS entries. This makes external-dns will only remove entries it manages.</p> <p>This defalt behaviour can be changed changing \"registry\" setting from \"txt\" to \"aws-sd\", \"dynamodb\" or \"noop\".</p> <p>But if we share the same --txt-owner-id between clusters, both controllers will create entries in that zones but both will think they own all records in that zone. This can cause several problems.</p> <p>For AWS Route53: The txtOwnerId should be the Hosted Zone ID (e.g., Z1D633PJN98FT9).</p> <p>For multi-cluster setups with Route53: Since txtOwnerId must be the Hosted Zone ID, sharing the same Route53 zone between multiple external-dns controllers is NOT recommended when using the default \"txt\" registry. All controllers would use the same txtOwnerId, causing ownership conflicts and unpredictable behavior.</p> <p>Recommended approaches for multi-cluster Route53 setups:</p> <ol> <li>Separate hosted zones: Give each cluster its own subdomain zone</li> </ol> <p>```yaml    # Cluster 1    domainFilters: [\"cluster1.example.com\"]    txtOwnerId: Z1D633PJN98FT9</p> <p># Cluster 2    domainFilters: [\"cluster2.example.com\"]    txtOwnerId: Z2E744QKM09GHI    ```</p> <ol> <li>Use aws-sd registry: Switch from \"txt\" to \"aws-sd\" registry</li> </ol> <p><code>yaml    registry: aws-sd</code></p> <ol> <li>Use noop registry: Disable ownership tracking (loses safety features)</li> </ol> <p><code>yaml    registry: noop</code></p>"},{"location":"services/external-dns/aws-route53-pia/","title":"Route53 and Pod identity agent","text":""},{"location":"services/external-dns/aws-route53-pia/#role-and-policies","title":"Role and policies","text":"<p>Create a role called, for example external-dns with this trust policy (trust relationship)</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowEksAuthToAssumeRoleForPodIdentity\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"pods.eks.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:TagSession\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>And with this permission policy called, for example AllowExternalDNSUpdates</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ListTagsForResources\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>The policy can be more precise for your hosted zone</p>"},{"location":"services/external-dns/aws-route53-pia/#route-53-zone","title":"Route 53 zone","text":"<p>Create a hosted zone in route 53 for your desired (sub)domain</p>"},{"location":"services/external-dns/aws-route53-pia/#deploy-external-dns","title":"Deploy external dns","text":"<p>Deploy the external dns helm chart with this values.yaml file</p> <pre><code>provider:\n  name: aws\ndomainFilters:\n  - yourhostedzone\ntxtOwnerId: Z1D633PJN98FT9  # Your Route53 Hosted Zone ID\nextraArgs: [\"--aws-zone-type=public\"]   # if it is a public zone\n</code></pre> <p>If you use a very old release, pod identity agent can fail</p>"},{"location":"services/external-dns/aws-route53-pia/#configure-the-eks-cluster","title":"Configure the eks cluster","text":"<ul> <li> <p>Under addons, deploy the pod identity agent plugin if not using Eks Auto Mode</p> </li> <li> <p>Under access, create a pod identity association between the external-dns role and the external-dns service account</p> </li> </ul>"},{"location":"services/external-dns/aws-route53-pia/#check-if-it-works","title":"Check if it works","text":"<p>Restart the external-dns deployment and see the logs of the external-dns pod. You must find something like this:</p> <pre><code>\"Applying provider record filter for domains: [yourhostedzone related info ]\"\n\"All records are already up to date\"\n</code></pre>"},{"location":"services/harbor/api/","title":"Api","text":""},{"location":"services/harbor/api/#creating-projecto-scoped-robot-account","title":"Creating projecto scoped robot account","text":"<p>The only working way to create a project scoped robot account is using an admin account and the /robots api</p> <p>A non admin user or robot account gives an access denied error and the robotv1 (/project/myproject/robots) api seems buggy</p> <p>https://github.com/goharbor/harbor/issues/20692</p>"},{"location":"services/harbor/tips/","title":"Tips","text":""},{"location":"services/harbor/tips/#default-credentials","title":"Default credentials","text":"<p>We can log in with the following credentials: Username: admin Password: Harbor12345</p>"},{"location":"services/ingress-nginx/custom-headers/","title":"Add custom headers","text":"<ul> <li> <p>proxy-set-headers for passing a custom list of headers to the upstream server</p> </li> <li> <p>add-headers To pass the custom headers before sending response traffic to the client</p> </li> </ul>"},{"location":"services/ingress-nginx/custom-headers/#easier-option","title":"Easier option","text":""},{"location":"services/ingress-nginx/custom-headers/#harder-option","title":"Harder option","text":"<p>Changes to the custom header config maps do not force a reload of the ingress-nginx-controllers.</p>"},{"location":"services/ingress-nginx/custom-headers/#configure-in-the-controller","title":"Configure in the controller","text":"<p>In the ingress-nginx-controller configmap or similar, we indicate to the controller which is our configmap that will contain our custom headers. This can be done using 2 keys</p>"},{"location":"services/ingress-nginx/custom-headers/#create-the-configmap","title":"Create the configmap","text":""},{"location":"services/ingress-nginx/custom-headers/#restart-the-controller","title":"Restart the controller","text":""},{"location":"services/ingress-nginx/custom-headers/#links","title":"Links","text":"<ul> <li>Custom headers: https://kubernetes.github.io/ingress-nginx/examples/customization/custom-headers/</li> </ul>"},{"location":"services/ingress-nginx/multiple-controllers/","title":"Multiple controllers","text":"<p>You can have more than one ingress controller and ingress class in the same Kubernetes cluster using the nginx ingress controller.</p> <p>To do this, we need to install a new controller, preferably in another namespace. To prevent it from overwriting the previous one, we must add this configuration to the helm chart</p>"},{"location":"services/ingress-nginx/multiple-controllers/#default-values-of-a-helm-chart","title":"Default values of a helm chart","text":"<pre><code>controller:\n  ingressClass: minuevovalor\n  ingressClassResource:\n    name: minuevovalor\n    controllerValue: \"k8s.io/minuevovalor\"\n</code></pre> <p>The default installation uses</p> <pre><code>controller:\n  ingressClass: nginx\n  ingressClassResource:\n    name: nginx\n    controllerValue: \"k8s.io/ingress-nginx\"\n</code></pre> <p>controller.electionID is also a configurable value, which by default takes the controller name and adds the suffix \"-leader\"</p>"},{"location":"services/ingress-nginx/multiple-controllers/#links","title":"Links","text":"<ul> <li>Multiple Ingress controllers https://kubernetes.github.io/ingress-nginx/user-guide/multiple-ingress/</li> </ul>"},{"location":"services/kubeflow/components/","title":"Kubeflow components","text":""},{"location":"services/kubeflow/components/#kubeflow-pipelines","title":"Kubeflow Pipelines","text":"<p>Platform to create machine learning workflows /schedules with containers ML Workflows and Schedules</p> <p>https://www.kubeflow.org/docs/components/pipelines/ https://github.com/kubeflow/pipelines</p>"},{"location":"services/kubeflow/components/#model-registry","title":"Model registry","text":"<p>Model metadata</p> <p>https://www.kubeflow.org/docs/components/model-registry/</p>"},{"location":"services/kubeflow/components/#training-operator","title":"Training Operator","text":"<p>Model Training and Fine-Tuning</p> <p>https://www.kubeflow.org/docs/components/training/ https://github.com/kubeflow/training-operator</p>"},{"location":"services/kubeflow/components/#katib","title":"Katib","text":"<p>Model tuning. Model Optimization and AutoML automated machine learning (AutoML)</p> <p>https://www.kubeflow.org/docs/components/katib/</p>"},{"location":"services/kubeflow/components/#kserve","title":"KServe","text":"<p>Model Serving</p> <p>https://www.kubeflow.org/docs/external-add-ons/kserve/ https://github.com/kserve/kserve https://kserve.github.io/website</p>"},{"location":"services/kubeflow/components/#kubeflow-mpi-operator","title":"Kubeflow MPI Operator","text":"<p>All-Reduce Model Training</p> <p>https://www.kubeflow.org/docs/components/training/user-guides/mpi/ https://github.com/kubeflow/mpi-operator</p>"},{"location":"services/kubeflow/components/#spark-operator","title":"Spark Operator","text":"<p>Data Preparation (run spark applications)</p> <p>https://www.kubeflow.org/docs/scomponents/spark-operator/ https://github.com/kubeflow/spark-operator https://kubeflow.github.io/spark-operator/</p>"},{"location":"services/kubeflow/components/#other","title":"Other","text":""},{"location":"services/kubeflow/components/#kubeflow-notebooks-web-based-development-environments","title":"Kubeflow Notebooks (web-based development environments)","text":"<p>For interactive data exploration and model development, provides a way to run web-based development environments inside your Kubernetes cluster by running them inside Pods.</p> <ul> <li>https://www.kubeflow.org/docs/components/notebooks/</li> </ul>"},{"location":"services/kubeflow/components/#central-dashboard-to-the-kubeflow-ecosystem","title":"Central Dashboard (to the kubeflow ecosystem)","text":"<p>For easy navigation and management with Kubeflow Profiles for access control.</p> <ul> <li>https://www.kubeflow.org/docs/components/central-dash/</li> <li>https://github.com/kubeflow/kubeflow/tree/master/components/centraldashboard</li> </ul>"},{"location":"services/kubeflow/components/#mpi-operator-allreduce-distributed-training-strategy","title":"MPI Operator (allreduce distributed training strategy)","text":"<ul> <li>https://github.com/kubeflow/mpi-operator</li> <li>https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc</li> </ul>"},{"location":"services/kubeflow/components/#istio-service-mesh","title":"Istio (service mesh)","text":"<ul> <li>https://www.kubeflow.org/docs/concepts/multi-tenancy/istio/</li> </ul> <p>with cilium https://docs.cilium.io/en/latest/network/servicemesh/istio/ https://github.com/kubeflow/manifests/issues/2729</p>"},{"location":"services/kubeflow/components/#feast","title":"Feast","text":"<p>https://www.kubeflow.org/docs/external-add-ons/feast/ https://github.com/feast-dev/feast https://feast.dev/</p>"},{"location":"services/kubeflow/components/#elyra","title":"Elyra","text":"<p>https://www.kubeflow.org/docs/external-add-ons/elyra/ https://elyra.readthedocs.io/</p>"},{"location":"services/kubeflow/components/#bento-ml","title":"Bento ML","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/bentoml</p>"},{"location":"services/kubeflow/components/#seldon","title":"Seldon","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/seldon</p>"},{"location":"services/kubeflow/components/#ray","title":"Ray","text":"<p>https://github.com/kubeflow/manifests/tree/master/contrib/ray</p>"},{"location":"services/kubeflow/components/#additional-tooling","title":"Additional Tooling","text":"<p>Additional tooling for data management (PVC Viewer), visualization (TensorBoards), and more.</p>"},{"location":"services/kubeflow/components/#links","title":"Links","text":"<p>https://www.kubeflow.org/docs/started/introduction/ https://www.kubeflow.org/docs/started/architecture/ https://www.kubeflow.org/docs/releases/kubeflow-1.9/ https://blog.kubeflow.org/ https://github.com/kubeflow/manifests/</p>"},{"location":"services/kubeflow/installation/","title":"How to install kubeflow","text":""},{"location":"services/kubeflow/installation/#install-standalone-components","title":"Install standalone components","text":"<p>There are some kubeflow components that can be deployed as standalone services. They have their own git repository and documentation about how to install it.</p> <p>List:</p> <ul> <li>kserve</li> <li>katib</li> <li>model registry</li> <li>mpi operator</li> <li>pipelines</li> <li>spark operator</li> <li>training operator</li> </ul>"},{"location":"services/kubeflow/installation/#install-the-full-platform-using-distribution","title":"Install the full platform using distribution","text":"<p>https://www.kubeflow.org/docs/distributions/</p>"},{"location":"services/kubeflow/installation/#install-the-full-platform-using-manifests","title":"Install the full platform using manifests","text":"<p>The following repository permits to install the full platfrom using kuberntes manifests</p> <ul> <li> <p>1.9 release https://github.com/kubeflow/manifests/tree/v1.9.0#installation</p> </li> <li> <p>1.8 release https://github.com/kubeflow/manifests/tree/v1.8.1#installation</p> </li> </ul>"},{"location":"services/kubeflow/installation/#links","title":"Links","text":"<ul> <li>Installing Kubeflow https://www.kubeflow.org/docs/started/installing-kubeflow/</li> </ul>"},{"location":"services/kyverno/annotations/","title":"Annotations","text":""},{"location":"services/kyverno/annotations/#policieskyvernoiotitle","title":"policies.kyverno.io/title","text":"<p>Gives a title to the policy</p> <pre><code>policies.kyverno.io/title: Disallow Capabilities\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiodescription","title":"policies.kyverno.io/description","text":"<p>Provides a brief description of what the policy does.</p> <pre><code>policies.kyverno.io/description: &gt;-\n      An ingress resource needs to define an actual host name\n      in order to be valid. This policy ensures that there is a\n      hostname for each rule defined.\n</code></pre> <pre><code>policies.kyverno.io/description: \"Ensure all Pods have resource limits defined.\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioseverity","title":"policies.kyverno.io/severity","text":"<p>Defines the severity level of the policy. Possible values: low, medium, high</p> <pre><code>policies.kyverno.io/severity: medium\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiocategory","title":"policies.kyverno.io/category","text":"<p>Categorizes the policy into a specific group or type. Possible Values: security, compliance, best-practices, etc.</p> <pre><code>policies.kyverno.io/category: security\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiominversion","title":"policies.kyverno.io/minversion","text":""},{"location":"services/kyverno/annotations/#policieskyvernoiosubject","title":"policies.kyverno.io/subject","text":"<p>Specifies the subject of the policy (e.g., Pod, Namespace, Deployment).</p> <pre><code>policies.kyverno.io/subject: Ingress\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoiocontrols","title":"policies.kyverno.io/controls","text":"<p>Maps the policy to specific compliance controls (e.g., CIS benchmarks, NIST standards).</p> <pre><code>policies.kyverno.io/controls: \"CIS-1.3.2, NIST-800-53\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioowner","title":"policies.kyverno.io/owner","text":"<p>Identifies the owner or team responsible for the policy.</p> <pre><code>policies.kyverno.io/owner: \"DevSecOps Team\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioscorecard","title":"policies.kyverno.io/scorecard","text":"<p>Indicates whether the policy is part of a compliance scorecard. While primarily used for compliance tracking, this annotation can be integrated into external tools or workflows to enforce compliance checks. Useful in environments where compliance policies are tracked and enforced as part of governance.</p> <pre><code>policies.kyverno.io/scorecard: \"true\"\n</code></pre>"},{"location":"services/kyverno/annotations/#policieskyvernoioautogen-controllers","title":"policies.kyverno.io/autogen-controllers","text":"<p>This is not an informative annotation. Specifies which controllers the policy applies to when auto-generating rules.</p> <pre><code>policies.kyverno.io/autogen-controllers: \"Deployment\"\n</code></pre> <pre><code>policies.kyverno.io/autogen-controllers: \"none\"\n</code></pre> <p>See more here</p> <ul> <li>Auto-Gen Rules</li> </ul> <p>https://main.kyverno.io/docs/policy-types/cluster-policy/autogen/</p>"},{"location":"services/kyverno/annotations/#kyvernoiokyverno-version","title":"kyverno.io/kyverno-version","text":"<p>Indicates the version of Kyverno that was used to create or apply the policy.</p> <pre><code>kyverno.io/kyverno-version: 1.6.0\n</code></pre>"},{"location":"services/kyverno/annotations/#kyvernoiokubernetes-version","title":"kyverno.io/kubernetes-version","text":"<p>Indicates the version of Kubernetes that was in use when the policy was created or applied.</p> <pre><code>kyverno.io/kubernetes-version: \"1.22-1.23\"\n</code></pre>"},{"location":"services/rabbitmq/settings/","title":"Settings","text":""},{"location":"services/rabbitmq/settings/#memory","title":"Memory","text":"<p>https://www.rabbitmq.com/docs/memory</p> <p>example:</p> <pre><code>vm_memory_high_watermark.absolute = 450Mi\n</code></pre>"},{"location":"services/rabbitmq/settings/#disk","title":"Disk","text":"<p>https://www.rabbitmq.com/docs/disk-alarms</p> <p>example:</p> <pre><code>disk_free_limit.absolute = 512MB\n</code></pre>"},{"location":"services/rabbitmq/settings/#partition-handling","title":"Partition handling","text":"<p>Strategies:</p> <ul> <li>pause-minority</li> <li>pause-if-all-down mode</li> <li>autoheal</li> <li>ignore</li> </ul> <p>https://www.rabbitmq.com/docs/partitions</p> <pre><code>cluster_partition_handling = autoheal\n</code></pre>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/filestash/info/","title":"Tips","text":""},{"location":"storage/filestash/info/#environment-variables","title":"Environment variables","text":"<p>We can setup 2 environment variables:</p> <ul> <li>ADMIN_PASSWORD stores the admin password credentials in a bcrypt format</li> <li>APPLICATION_URL stores the fqdn (myhost.domain.com)</li> <li>CONFIG_JSON stores the config as a base64 string with the plg_config_env plugin</li> </ul>"},{"location":"storage/filestash/info/#some-web-endpoints","title":"Some web endpoints","text":"<ul> <li>http://domain.com/about</li> <li>http://domain.com/admin/</li> <li>http://domain.com/admin/setup</li> <li>http://domain.com/admin/backend</li> <li>http://domain.com/admin/api/simple-user-management</li> <li>http://domain.com/admin/tty</li> </ul>"},{"location":"storage/filestash/info/#links","title":"Links","text":"<ul> <li>Website</li> </ul> <p>https://www.filestash.app/</p> <ul> <li>Github</li> </ul> <p>https://github.com/mickael-kerjean/filestash</p> <ul> <li>Hardening Guide</li> </ul> <p>https://downloads.filestash.app/upload/hardening-guide.pdf</p>"},{"location":"storage/vmware-csi/tips/","title":"Tips","text":""},{"location":"storage/vmware-csi/tips/#failed-to-attach-disk-the-resource-volume-is-in-use","title":"failed to attach disk. The resource volume is in use","text":"<p>Because of some solved network errors, a container cannot start and keeps in containercreating state</p> <p>We can inspect the pvc and the VolumeAttachment</p> <pre><code>kubectl get pvc\nkubectl get VolumeAttachment | grep MYPVC\nkubectl describe VolumeAttachment OURVOLUMEATTACHMENT\n</code></pre> <p>Here we get that error</p> <pre><code>... failed to attach disk .. The resource 'volume' is in use...\n</code></pre> <p>A solution can be to delete the VolumeAttachment related with our pvc and the finalizer in that VolumeAttachment but that didn't work.</p> <p>Finally I found something was not working in the node. In the Vcenter Server interface in one node where that pod lived, the \"See all disks\" link under \"VM Hardware\" was missing. But in all the other nodes there is a link with all the csi disks attached. The solution was drain that node (the pod started ok in other node), restart the node and uncordon it.</p>"},{"location":"tools/Buildkit/output/","title":"Buildkit outputs","text":"<p>By default, the build result and intermediate cache will only remain internally in BuildKit. An output needs to be specified to retrieve the result.</p> <p>Multiple outputs are supported</p>"},{"location":"tools/Buildkit/output/#imageregistry","title":"Image/Registry","text":"<p>pending</p>"},{"location":"tools/Buildkit/output/#authentication","title":"Authentication","text":"<p>If we want pass credentials to push the image to a repository, the DOCKER_CONFIG controls the directory where the credentials are defined via the config.json file.</p> <p>The default value of this variable is ~/.docker</p>"},{"location":"tools/Buildkit/output/#local-directory","title":"Local directory","text":"<p>Buildkit permit to export the build result (files, directories, binaries or artifacts) directly to a directory on your local filesystem instead of the image itself.</p> <p>For that, Buildkit copies the output files from the build context or build stages into a specified local directory on your machine.</p> <p>This command will place the build output in the ./output directory on your local filesystem.</p> <pre><code>--output type=local,dest=./output\n</code></pre>"},{"location":"tools/Buildkit/output/#docker-tarball","title":"Docker tarball","text":"<p>We can also export it to a tarball.</p> <pre><code>--output type=tar,dest=out.tar\n</code></pre>"},{"location":"tools/Buildkit/output/#oci-tarball","title":"OCI tarball","text":"<p>We can also export it to an OCI tarball</p> <pre><code>--output type=oci,dest=path/to/output.tar\n</code></pre>"},{"location":"tools/Buildkit/output/#containerd-image-store","title":"containerd image store","text":"<p>pending</p>"},{"location":"tools/container-images/98-tips/","title":"Tips","text":""},{"location":"tools/container-images/98-tips/#add-maintainer-to-dockerfile","title":"Add Maintainer to Dockerfile","text":"<p>Use the LABEL instruction</p> <pre><code>LABEL maintainer=\"&lt;your-email@example.com&gt;\"\n</code></pre> <p>Or with more detail:</p> <pre><code>LABEL maintainer=\"Your Name &lt;your-email@example.com&gt;\"\n</code></pre> <p>Note: The old MAINTAINER instruction is deprecated.</p>"},{"location":"tools/container-images/alpine-best-practices/","title":"Alpine Linux Best Practices for Dockerfiles","text":"<p>Alpine Linux is a security-oriented, lightweight Linux distribution that has become the de facto standard for minimal container base images. This guide covers best practices for using Alpine's package manager (apk) in Dockerfiles.</p>"},{"location":"tools/container-images/alpine-best-practices/#key-principles","title":"Key Principles","text":""},{"location":"tools/container-images/alpine-best-practices/#1-always-use-no-cache","title":"1. Always Use <code>--no-cache</code>","text":"<p>The most important flag when installing packages in Alpine.</p> <pre><code>RUN apk add --no-cache curl git nginx\n</code></pre> <p>Why: The <code>--no-cache</code> flag prevents apk from storing the package index locally, reducing image size by approximately 1-3MB. This is essential for keeping container images lean.</p> <p>Without <code>--no-cache</code>:</p> <pre><code># Bad - leaves cache files in the image\nRUN apk add curl\n# Results in ~2-3MB of unnecessary cache data\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-combine-update-and-install-in-a-single-run-command","title":"2. Combine Update and Install in a Single RUN Command","text":"<p>Always update the package index and install packages in the same <code>RUN</code> instruction.</p> <pre><code>RUN apk update &amp;&amp; apk add --no-cache package1 package2\n</code></pre> <p>Why: Each <code>RUN</code> instruction creates a new image layer. Combining operations prevents creating unnecessary layers with outdated package indexes.</p> <p>Anti-pattern:</p> <pre><code># Bad - creates separate layers\nRUN apk update\nRUN apk add --no-cache curl\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-use-virtual-packages-for-build-dependencies","title":"3. Use Virtual Packages for Build Dependencies","text":"<p>When you need build tools that aren't required in the final image, use virtual packages to group and remove them efficiently.</p> <pre><code>RUN apk add --no-cache --virtual .build-deps \\\n    gcc \\\n    musl-dev \\\n    python3-dev \\\n    libffi-dev &amp;&amp; \\\n    pip install --no-cache-dir cryptography &amp;&amp; \\\n    apk del .build-deps\n</code></pre> <p>Why: Virtual packages allow you to install and remove multiple packages as a single unit, ensuring build dependencies don't bloat your final image.</p> <p>Benefits:</p> <ul> <li>Removes all build dependencies in one command</li> <li>Reduces final image size significantly (often 100-300MB savings)</li> <li>Keeps the image clean and minimal</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#4-pin-package-versions-for-reproducibility","title":"4. Pin Package Versions for Reproducibility","text":"<p>Specify exact package versions to ensure consistent builds across time and environments.</p> <pre><code>RUN apk add --no-cache \\\n    nginx=1.24.0-r15 \\\n    curl=8.5.0-r0 \\\n    ca-certificates=20240705-r0\n</code></pre> <p>Why: Without version pinning, your builds may pull different package versions over time, leading to inconsistent behavior or unexpected bugs.</p> <p>Finding versions:</p> <pre><code># Search for available versions\ndocker run --rm alpine:3.19 apk search -e nginx\n\n# Check installed version\ndocker run --rm alpine:3.19 apk info nginx\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#5-combine-related-operations-in-single-layers","title":"5. Combine Related Operations in Single Layers","text":"<p>Group logically related commands together to minimize layers and improve build performance.</p> <pre><code>RUN apk add --no-cache \\\n    python3 \\\n    py3-pip &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt &amp;&amp; \\\n    adduser -D appuser &amp;&amp; \\\n    chown -R appuser:appuser /app\n</code></pre> <p>Why: Fewer layers mean smaller images, faster builds, and better cache utilization.</p>"},{"location":"tools/container-images/alpine-best-practices/#6-clean-up-in-the-same-layer","title":"6. Clean Up in the Same Layer","text":"<p>If not using <code>--no-cache</code>, ensure cleanup happens in the same <code>RUN</code> command.</p> <pre><code>RUN apk update &amp;&amp; \\\n    apk add package &amp;&amp; \\\n    rm -rf /var/cache/apk/*\n</code></pre> <p>Important: However, using <code>--no-cache</code> is preferred as it's cleaner and more reliable.</p>"},{"location":"tools/container-images/alpine-best-practices/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"tools/container-images/alpine-best-practices/#multi-stage-builds-with-alpine","title":"Multi-Stage Builds with Alpine","text":"<p>Use Alpine in build stages to keep final images minimal.</p> <pre><code># Build stage\nFROM alpine:3.19 AS builder\nRUN apk add --no-cache --virtual .build-deps \\\n    go \\\n    git \\\n    musl-dev &amp;&amp; \\\n    go build -o /app main.go &amp;&amp; \\\n    apk del .build-deps\n\n# Final stage\nFROM alpine:3.19\nRUN apk add --no-cache ca-certificates\nCOPY --from=builder /app /app\nENTRYPOINT [\"/app\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#installing-from-edge-repository","title":"Installing from Edge Repository","text":"<p>Sometimes you need newer packages from Alpine's edge repository.</p> <pre><code>RUN apk add --no-cache \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/edge/community \\\n    package-name\n</code></pre> <p>Caution: Edge packages are less stable. Pin versions when using edge repositories.</p>"},{"location":"tools/container-images/alpine-best-practices/#adding-multiple-repositories","title":"Adding Multiple Repositories","text":"<pre><code>RUN apk add --no-cache \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/v3.19/main \\\n    --repository=http://dl-cdn.alpinelinux.org/alpine/v3.19/community \\\n    package1 package2\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":""},{"location":"tools/container-images/alpine-best-practices/#1-dont-run-update-in-a-separate-layer","title":"1. Don't Run Update in a Separate Layer","text":"<pre><code># Bad\nRUN apk update\nRUN apk add --no-cache curl\n\n# Good\nRUN apk update &amp;&amp; apk add --no-cache curl\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-dont-use-apk-upgrade-in-production-images","title":"2. Don't Use <code>apk upgrade</code> in Production Images","text":"<pre><code># Bad - unpredictable and breaks reproducibility\nRUN apk upgrade\n\n# Good - pin specific versions instead\nRUN apk add --no-cache nginx=1.24.0-r15\n</code></pre> <p>Why: Using <code>apk upgrade</code> makes builds non-reproducible and can introduce breaking changes unexpectedly.</p>"},{"location":"tools/container-images/alpine-best-practices/#3-dont-install-unnecessary-packages","title":"3. Don't Install Unnecessary Packages","text":"<pre><code># Bad - bash adds ~10MB and increases attack surface\nRUN apk add --no-cache bash curl wget git vim nano\n\n# Good - minimal installation\nRUN apk add --no-cache curl\n</code></pre> <p>Common unnecessary packages:</p> <ul> <li><code>bash</code> (use Alpine's default <code>sh</code> when possible)</li> <li><code>vim</code>, <code>nano</code> (not needed in production containers)</li> <li><code>wget</code> (if you already have <code>curl</code>)</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#4-dont-forget-to-remove-build-dependencies","title":"4. Don't Forget to Remove Build Dependencies","text":"<pre><code># Bad - leaves 200MB+ of build tools\nRUN apk add --no-cache gcc musl-dev &amp;&amp; \\\n    pip install package\n\n# Good - removes build tools\nRUN apk add --no-cache --virtual .build-deps gcc musl-dev &amp;&amp; \\\n    pip install package &amp;&amp; \\\n    apk del .build-deps\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#package-equivalence-guide","title":"Package Equivalence Guide","text":"<p>When migrating from Debian/Ubuntu to Alpine, package names often differ:</p> Debian/Ubuntu Alpine Notes <code>build-essential</code> <code>build-base</code> Includes gcc, make, libc-dev <code>python3-dev</code> <code>python3-dev</code> Same name <code>libssl-dev</code> <code>openssl-dev</code> Different prefix <code>libpq-dev</code> <code>postgresql-dev</code> Different name <code>default-libmysqlclient-dev</code> <code>mariadb-dev</code> MySQL client <code>libffi-dev</code> <code>libffi-dev</code> Same name <code>libjpeg-dev</code> <code>jpeg-dev</code> Simplified name <code>libpng-dev</code> <code>libpng-dev</code> Same name <p>Finding packages:</p> <pre><code># Search for packages\ndocker run --rm alpine:3.19 apk search keyword\n\n# Get package info\ndocker run --rm alpine:3.19 apk info package-name\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#real-world-examples","title":"Real-World Examples","text":""},{"location":"tools/container-images/alpine-best-practices/#python-application","title":"Python Application","text":"<pre><code>FROM alpine:3.19\n\n# Install runtime dependencies\nRUN apk add --no-cache \\\n    python3 \\\n    py3-pip \\\n    ca-certificates\n\n# Install build dependencies, build, then remove them\nCOPY requirements.txt .\nRUN apk add --no-cache --virtual .build-deps \\\n    gcc \\\n    musl-dev \\\n    python3-dev \\\n    libffi-dev \\\n    openssl-dev &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt &amp;&amp; \\\n    apk del .build-deps\n\nCOPY . /app\nWORKDIR /app\n\nCMD [\"python3\", \"app.py\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#nodejs-application","title":"Node.js Application","text":"<pre><code>FROM alpine:3.19\n\n# Install Node.js and npm\nRUN apk add --no-cache \\\n    nodejs=20.11.0-r0 \\\n    npm=10.2.5-r0\n\nWORKDIR /app\n\n# Install dependencies\nCOPY package*.json ./\nRUN npm ci --only=production &amp;&amp; \\\n    npm cache clean --force\n\nCOPY . .\n\nUSER node\nCMD [\"node\", \"server.js\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#go-application-multi-stage","title":"Go Application (Multi-stage)","text":"<pre><code># Build stage\nFROM alpine:3.19 AS builder\nRUN apk add --no-cache go git\nWORKDIR /build\nCOPY . .\nRUN go build -ldflags=\"-w -s\" -o app .\n\n# Final stage\nFROM alpine:3.19\nRUN apk add --no-cache ca-certificates\nCOPY --from=builder /build/app /usr/local/bin/app\nUSER nobody\nENTRYPOINT [\"/usr/local/bin/app\"]\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#security-considerations","title":"Security Considerations","text":""},{"location":"tools/container-images/alpine-best-practices/#1-keep-base-image-updated","title":"1. Keep Base Image Updated","text":"<p>Regularly update your base image version:</p> <pre><code># Pin to specific version for reproducibility\nFROM alpine:3.19\n\n# Or use latest minor version (receives security updates)\nFROM alpine:3\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-run-as-non-root-user","title":"2. Run as Non-Root User","text":"<pre><code>RUN adduser -D -u 1000 appuser\nUSER appuser\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-install-only-required-packages","title":"3. Install Only Required Packages","text":"<p>Each package increases the attack surface. Audit your dependencies regularly.</p>"},{"location":"tools/container-images/alpine-best-practices/#4-use-specific-package-versions","title":"4. Use Specific Package Versions","text":"<p>Pin versions to avoid supply chain attacks through malicious package updates.</p>"},{"location":"tools/container-images/alpine-best-practices/#performance-tips","title":"Performance Tips","text":""},{"location":"tools/container-images/alpine-best-practices/#1-order-layers-by-change-frequency","title":"1. Order Layers by Change Frequency","text":"<p>Put frequently changing instructions last to maximize cache hits:</p> <pre><code>FROM alpine:3.19\n\n# Rarely changes - cached\nRUN apk add --no-cache python3 py3-pip\n\n# Changes occasionally - cached if requirements unchanged\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Changes frequently - rebuilt often\nCOPY . /app\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#2-use-buildkit-for-better-caching","title":"2. Use BuildKit for Better Caching","text":"<pre><code>DOCKER_BUILDKIT=1 docker build -t myapp .\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#3-leverage-multi-stage-builds","title":"3. Leverage Multi-Stage Builds","text":"<p>Keep build tools in separate stages to minimize final image size.</p>"},{"location":"tools/container-images/alpine-best-practices/#useful-apk-commands","title":"Useful apk Commands","text":"<pre><code># Update package index\napk update\n\n# Install package\napk add package-name\n\n# Install without caching\napk add --no-cache package-name\n\n# Install specific version\napk add package-name=1.2.3-r0\n\n# Search for packages\napk search keyword\n\n# Show package info\napk info package-name\n\n# List installed packages\napk list --installed\n\n# Remove package\napk del package-name\n\n# Remove virtual package group\napk del .build-deps\n</code></pre>"},{"location":"tools/container-images/alpine-best-practices/#links-and-resources","title":"Links and Resources","text":"<ul> <li>Alpine Linux Official Website</li> <li>Alpine Linux Docker Hub</li> <li>Alpine Linux Packages</li> <li>Alpine Linux Wiki</li> <li>apk-tools Documentation</li> </ul>"},{"location":"tools/container-images/alpine-best-practices/#when-not-to-use-alpine","title":"When Not to Use Alpine","text":"<p>While Alpine is excellent for most use cases, consider alternatives when:</p> <ol> <li>Binary compatibility issues - Your application requires glibc (Alpine uses musl libc)</li> <li>Performance critical - Some applications show performance degradation with musl (2x slowdowns reported in edge cases)</li> <li>Limited package availability - The package you need isn't available in Alpine repositories</li> <li>Team familiarity - Your team is more comfortable with Debian/Ubuntu ecosystems</li> </ol> <p>In these cases, consider Debian Slim, Ubuntu, or distroless images instead.</p>"},{"location":"tools/container-images/build-container-images/","title":"Build container images","text":""},{"location":"tools/container-images/build-container-images/#docker-build","title":"docker build","text":"<p>What: The classic Docker CLI command to build images. Engine: Uses the legacy Docker image builder by default, but can use BuildKit if enabled. Features:</p> <ul> <li>Basic Dockerfile support</li> <li>single-platform builds</li> <li>limited caching.</li> </ul> <pre><code>docker build -t myimage .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/using_docker_build/</p>"},{"location":"tools/container-images/build-container-images/#docker-buildx","title":"docker buildx","text":"<p>What: An advanced Docker CLI plugin for building images. Engine: Uses BuildKit under the hood. Features:</p> <ul> <li>Multi-platform builds (e.g., build for amd64 and arm64 in one command)</li> <li>Advanced caching (local, registry, inline)</li> <li>Output to multiple formats (Docker, OCI, tar, etc.)</li> <li>Build secrets, better performance</li> </ul> <pre><code>docker buildx build --platform linux/amd64,linux/arm64 -t myimage .\n</code></pre>"},{"location":"tools/container-images/build-container-images/#buildkit","title":"buildkit","text":"<p>What: The next-generation image builder for Docker, designed for speed and flexibility. Engine: Can be used standalone or as the backend for docker build and docker buildx. Features:</p> <ul> <li>Parallel build steps</li> <li>Advanced caching</li> <li>Build secrets</li> <li>Improved performance</li> </ul> <p>Usage: Enabled in Docker with DOCKER_BUILDKIT=1 docker build ... Used natively by docker buildx</p> <pre><code>DOCKER_BUILDKIT=1 docker build .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/using_buildkit/</p>"},{"location":"tools/container-images/build-container-images/#buildah","title":"buildah","text":"<p>What: A Red Hat-sponsored, daemonless tool for building OCI and Docker images. Engine: Standalone, does not require the Docker daemon. Features:</p> <ul> <li>Scriptable, fine-grained control over image layers</li> <li>Rootless builds</li> <li>Integrates well with Podman and OpenShift</li> <li>No need for a running Docker daemon</li> </ul> <pre><code>buildah bud -t myimage .\n</code></pre> <p>https://docs.gitlab.com/ci/docker/buildah_rootless_multi_arch/</p>"},{"location":"tools/container-images/build-container-images/#podman","title":"podman","text":""},{"location":"tools/container-images/build-container-images/#kaniko","title":"kaniko","text":"<p>The project has no mantainers</p>"},{"location":"tools/container-images/distroless-images/","title":"Distroless images","text":"<p>Distroless images are minimal container images that contain only the application and its runtime dependencies, without including package managers, shells, or other standard Linux distribution utilities.</p>"},{"location":"tools/container-images/distroless-images/#key-benefits","title":"Key Benefits","text":"<ul> <li>Reduced Attack Surface: No shell or package manager means fewer vulnerabilities to exploit</li> <li>Smaller Image Size: Only essential runtime components are included</li> <li>Improved Security: Minimizes the potential for supply chain attacks</li> <li>Better Compliance: Easier to audit and maintain due to fewer components</li> </ul>"},{"location":"tools/container-images/distroless-images/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Production deployments where security is critical</li> <li>Microservices and cloud-native applications</li> <li>Containerized applications that don't require debugging tools in production</li> </ul>"},{"location":"tools/container-images/distroless-images/#popular-distroless-and-minimal-base-images","title":"Popular Distroless and Minimal Base Images","text":""},{"location":"tools/container-images/distroless-images/#true-distroless-images-no-package-manager","title":"True Distroless Images (No Package Manager)","text":"<p>These images do not include package managers in the final image, providing maximum security by eliminating the ability to install additional packages at runtime.</p>"},{"location":"tools/container-images/distroless-images/#google-distroless","title":"Google Distroless","text":"<p>Primarily maintained by Google and available at <code>gcr.io/distroless/</code>.</p> <p>Key Features:</p> <ul> <li>No shell or package manager</li> <li>Supports multiple runtimes: static binaries, Java, Python, Node.js, .NET</li> <li>Based on Debian</li> </ul> <p>Size: ~110MB for multi-stage builds with distroless (vs 848MB without optimization)</p> <p>Links:</p> <ul> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#chainguard-images-wolfi-based","title":"Chainguard Images (Wolfi-based)","text":"<p>Built on Wolfi, a Linux distribution designed for cloud workloads with zero known CVEs.</p> <p>Key Features:</p> <ul> <li>Uses glibc (not musl like Alpine)</li> <li>Constantly rebuilt with latest sources</li> <li>Includes SBOMs for every image</li> <li>No kernel (container runtime only)</li> <li>No package manager in runtime image (apk only during build)</li> </ul> <p>Available at: <code>cgr.dev/chainguard/</code></p> <p>Links:</p> <ul> <li>Chainguard Images GitHub</li> <li>Wolfi GitHub</li> <li>Official Website</li> <li>Image Directory</li> <li>Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#ubuntu-chiseled","title":"Ubuntu Chiseled","text":"<p>Distroless images built from Ubuntu packages using Chisel.</p> <p>Key Features:</p> <ul> <li>Uses glibc</li> <li>No shell or package manager in final image</li> <li>Only minimal required dependencies included</li> <li>Carefully sliced packages</li> </ul> <p>Available at: <code>ubuntu/</code> on Docker Hub</p> <p>Links:</p> <ul> <li>Official Page</li> <li>Chisel Documentation</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#busybox","title":"BusyBox","text":"<p>Single compact executable with simplified Linux tools.</p> <p>Key Features:</p> <ul> <li>Multiple libc flavors: musl, glibc, uclibc</li> <li>Includes basic utilities (file archiving, process manipulation, etc.)</li> <li>No package manager</li> <li>Extremely minimal</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#scratch","title":"Scratch","text":"<p>Docker's most minimal base - literally empty.</p> <p>Key Features:</p> <ul> <li>Contains nothing at all</li> <li>Only suitable for static binaries</li> <li>Smallest possible image</li> </ul> <p>Links:</p> <ul> <li>Docker Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#minimal-images-with-package-managers","title":"Minimal Images with Package Managers","text":"<p>These images include package managers, allowing installation of additional packages at runtime. They offer more flexibility but a larger attack surface compared to true distroless images.</p>"},{"location":"tools/container-images/distroless-images/#alpine-linux","title":"Alpine Linux","text":"<p>Popular minimal Linux distribution (~5MB base image).</p> <p>Key Features:</p> <ul> <li>Uses musl libc (not glibc)</li> <li>BusyBox tool suite</li> <li>apk package manager included</li> <li>Very small size</li> </ul> <p>Considerations:</p> <ul> <li>Some applications require glibc and won't work with musl</li> <li>Performance degradation possible (2x slowdown not uncommon)</li> <li>Limited package availability compared to Debian/Ubuntu</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#debian-slim","title":"Debian Slim","text":"<p>Pared-down Debian with commonly needed tools removed.</p> <p>Key Features:</p> <ul> <li>Uses glibc</li> <li>apt package manager included</li> <li>Good balance between size and functionality</li> <li>~74MB (vs 118MB for full Debian)</li> </ul> <p>Links:</p> <ul> <li>Official Website</li> <li>Docker Hub</li> </ul>"},{"location":"tools/container-images/distroless-images/#red-hat-ubi-minimal","title":"Red Hat UBI Minimal","text":"<p>RHEL-based minimal image for enterprise environments.</p> <p>Key Features:</p> <ul> <li>Based on RHEL packages</li> <li>microdnf package manager (scaled-down DNF)</li> <li>Strong security focus and timely updates</li> <li>~92MB on disk, 32MB compressed</li> </ul> <p>Considerations:</p> <ul> <li>Limited to curated Red Hat packages</li> <li>Best for Red Hat ecosystem (OpenShift, RHEL)</li> <li>Subscription required for non-UBI packages</li> </ul> <p>Links:</p> <ul> <li>Red Hat Catalog</li> <li>UBI FAQ</li> <li>Documentation</li> </ul>"},{"location":"tools/container-images/distroless-images/#comparison-summary","title":"Comparison Summary","text":""},{"location":"tools/container-images/distroless-images/#true-distroless-images-no-runtime-package-manager","title":"True Distroless Images (No Runtime Package Manager)","text":"Image Maintainer Size Package Manager libc Use Case Scratch Docker 0 MB None None Static binaries only BusyBox BusyBox Project ~5 MB None Various Basic utilities needed Chainguard Chainguard ~20-80 MB None (apk build only) glibc Supply chain security Ubuntu Chiseled Canonical ~20-50 MB None glibc Ubuntu ecosystem Distroless Google ~25-100 MB None glibc Production, high security"},{"location":"tools/container-images/distroless-images/#images-with-package-managers","title":"Images with Package Managers","text":"Image Maintainer Size Package Manager libc Use Case Alpine Alpine Linux ~5 MB apk musl General purpose, size-critical Debian Slim Debian Project ~74 MB apt glibc More tooling, Debian packages UBI Minimal Red Hat ~92 MB microdnf glibc Enterprise/Red Hat ecosystem"},{"location":"tools/container-images/distroless-images/#use-cases-for-distroless-images","title":"Use Cases for Distroless Images","text":"<ul> <li>Pure static binaries - Use scratch (most minimal, zero attack surface)</li> <li>Static binaries needing ca-certificates - Using distroless/static variant</li> <li>Precompiled Java JAR/WAR files - Using Java runtime variants</li> <li>Precompiled Python applications - Using Python runtime variant</li> <li>Precompiled Node.js applications - Using Node.js runtime variants</li> <li>Precompiled .NET applications - Using .NET runtime variants</li> <li>Dynamic binaries requiring glibc - Using base or glibc-dynamic variants</li> <li>C/C++ applications - Using cc variant (includes libgcc, libstdc++)</li> <li>Go binaries with CGO enabled - Using base variant (includes glibc and SSL libraries)</li> </ul>"},{"location":"tools/container-images/distroless-images/#specific-distroless-implementations","title":"Specific Distroless Implementations","text":""},{"location":"tools/container-images/distroless-images/#liberica-runtime-container-bellsoft","title":"Liberica Runtime Container (BellSoft)","text":"<p>Optimized Java runtime containers from BellSoft, available as both JDK and JRE variants.</p> <p>Available at: <code>bellsoft/liberica-runtime-container</code></p> <p>Links:</p> <ul> <li>Docker Hub</li> <li>BellSoft Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#amazon-corretto","title":"Amazon Corretto","text":"<p>No-cost, production-ready distribution of OpenJDK from Amazon Web Services.</p> <p>Available at: <code>amazoncorretto</code></p> <p>Links:</p> <ul> <li>Docker Hub</li> <li>ECR Public Gallery</li> <li>Official Website</li> <li>GitHub Repository</li> </ul>"},{"location":"tools/container-images/distroless-images/#multi-stage-build-pattern","title":"Multi-Stage Build Pattern","text":"<p>Distroless images are typically used in multi-stage builds where dependencies are installed in a full-featured image, then only the application and runtime are copied to the distroless final image:</p> <pre><code># Build stage\nFROM golang:1.21 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\n# Final stage\nFROM gcr.io/distroless/static-debian12\nCOPY --from=builder /app/myapp /myapp\nENTRYPOINT [\"/myapp\"]\n</code></pre>"},{"location":"tools/container-images/distroless-images/#important-note-on-entrypoint","title":"Important Note on ENTRYPOINT","text":"<p>Distroless images without a shell require ENTRYPOINT to be specified in exec form (JSON array), not shell form:</p> <pre><code># Correct - exec form\nENTRYPOINT [\"/myapp\", \"--flag\"]\n\n# Wrong - shell form (requires /bin/sh)\nENTRYPOINT /myapp --flag\n</code></pre>"},{"location":"tools/git/98-tips/","title":"Tips","text":""},{"location":"tools/git/98-tips/#pull-all-remote-branches","title":"Pull all remote branches","text":"<pre><code>git branch -r | grep -v '\\-&gt;' | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\"; done\ngit fetch --all\ngit pull --all\n</code></pre>"},{"location":"tools/git/98-tips/#how-to-delete-the-git-history","title":"How to delete the git history","text":"<p>Checkout/create orphan branch (this branch won't show in git branch command):</p> <pre><code>git checkout --orphan latest_branch\n</code></pre> <p>See what you need to include</p> <pre><code>git status\ngit add -A # if you want to add all\ngit commit -am \"clean history\"\n</code></pre> <p>Delete main branch</p> <pre><code>git branch -D main\n</code></pre> <p>Rename the current branch to main</p> <pre><code>git branch -m main\n</code></pre> <p>See the remotes and push changes This ensures you don't overwrite other people's work</p> <pre><code>git remote  -v\ngit push --force-with-lease REMOTE main\n</code></pre>"},{"location":"tools/git/98-tips/#gitlab-no-allowed-to-force-push","title":"Gitlab: no allowed to force push","text":"<p>If your are pushing to gitlab and get this error \"You are not allowed to force push code to a protected branch on this project\"</p> <p>Enable force push in the repository under Settings  &gt; Repository &gt; Protected branches</p> <p>See the branch and enable \"Allowed to force push\" and repeat the push. After the push, disable it</p>"},{"location":"tools/git/99-links/","title":"Links","text":"<ul> <li>Git Reference</li> </ul> <p>https://git-scm.com/docs</p> <ul> <li>Git book</li> </ul> <p>https://git-scm.com/book/en/v2</p> <ul> <li>Git External Links</li> </ul> <p>https://git-scm.com/doc/ext</p>"},{"location":"tools/git/move-between-places/","title":"Move between places","text":""},{"location":"tools/git/move-between-places/#working-directory","title":"Working directory","text":"<p>The working directory or working tree is the directory that containes the files we are working with.</p> <p>Inside the working directory we can find tracked an untracked files.</p>"},{"location":"tools/git/move-between-places/#tracked-files","title":"Tracked files","text":"<p>Tracked files are files that git knows about, they were in the last snapshot. They can be:</p> <ul> <li>unmodified</li> </ul> <p>No changed made from the last snapshot</p> <ul> <li>modified</li> </ul> <p>With pending changes from the last snapshot</p> <ul> <li>staged</li> </ul> <p>Added to the staging area</p> <p>the git status command show the different status of a file.</p>"},{"location":"tools/git/move-between-places/#untracked-files","title":"Untracked files","text":"<p>Untracked files are files in the working directory not included in the last snapshot and they are not in the staging area.</p>"},{"location":"tools/git/move-between-places/#staging-area-or-index","title":"Staging area or index","text":""},{"location":"tools/git/move-between-places/#head","title":"HEAD","text":""},{"location":"tools/git/move-between-places/#movements","title":"Movements","text":"<p>We can move files from one state to another</p> From to Command untracked or modified file staging area git add staging area next snapshot git commit modified file unmodified file git restore (--worktree) staging area untracked or modified file git restore --staged or git reset HEAD last snapshot untracked git rm --cached <p>checkout</p>"},{"location":"tools/git/move-between-places/#links","title":"Links","text":"<ul> <li>Recording Changes to the Repository</li> </ul> <p>https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository</p> <ul> <li>Undoing Things</li> </ul> <p>https://git-scm.com/book/en/v2/Git-Basics-Undoing-Things</p> <ul> <li>Reset Demystified https://git-scm.com/book/en/v2/Git-Tools-Reset-Demystified</li> </ul> <p>https://git-scm.com/docs/git-add https://git-scm.com/docs/git-reset https://git-scm.com/docs/git-checkout https://git-scm.com/docs/git-restore</p>"},{"location":"tools/helm/98-tips/","title":"Tips","text":""},{"location":"tools/helm/98-tips/#keep-a-resource","title":"Keep a resource","text":"<p>Add the following annotation to keep a resource when a helm uninstall, upgrade or rollback operation is done</p> <pre><code>  annotations:\n    helm.sh/resource-policy: keep\n</code></pre>"},{"location":"tools/helm/98-tips/#nil-pointer-evaluating-interface-with-range-loop-no-accede-al-values","title":"nil pointer evaluating interface with range loop, no accede al values","text":"<p>https://stackoverflow.com/questions/57475521/ingress-yaml-template-returns-error-in-renderring-nil-pointer-evaluating-int</p>"},{"location":"tools/helm/98-tips/#iterate-over-range","title":"Iterate over range","text":"<p>https://stackoverflow.com/questions/56224527/helm-iterate-over-range</p>"},{"location":"tools/helm/98-tips/#links","title":"Links","text":"<ul> <li>Chart Development Tips and Tricks</li> </ul> <p>https://helm.sh/docs/howto/charts_tips_and_tricks/</p>"},{"location":"tools/helm/syntax/","title":"Syntax","text":"<p>https://helm.sh/docs/chart_template_guide/function_list/ https://helm.sh/docs/chart_template_guide/control_structures/ if/else with range define template block</p> <p>https://helm.sh/docs/chart_template_guide/yaml_techniques/ https://helm.sh/docs/chart_template_guide/data_types/</p> <p>https://helm.sh/docs/chart_best_practices/templates/</p>"},{"location":"tools/helm/syntax/#-and-","title":"{{-  and -}}","text":"<p>{{- (with the dash and space added) indicates that whitespace should be chomped left -}} means whitespace to the right should be consumed. Be careful! Newlines are whitespace!</p>"},{"location":"tools/helm/syntax/#_1","title":":=","text":"<p>In Helm charts, the := operator is used in the Go templating language to assign a value to a variable. {{- $myVar := .Values.myValue -}} you can use $myVar in your template to refer to the value of .Values.myValue.</p> <p>Please note that the scope of variables in Helm templates can be complex, and the := operator creates a variable that is only available in the scope where it is defined.</p>"},{"location":"tools/helm/variables/","title":"Helm variables","text":"<p>Helm provides a set of built-in variables that you can use in your Helm templates. These variables are automatically populated by Helm and can be used to customize your Kubernetes manifests. Here is a list of some common Helm variables:</p>"},{"location":"tools/helm/variables/#release-information","title":"Release Information","text":"<pre><code>{{ .Release.Name }}: The name of the release.\n{{ .Release.Namespace }}: The namespace to which the release is deployed.\n{{ .Release.IsUpgrade }}: True if the current operation is an upgrade.\n{{ .Release.IsInstall }}: True if the current operation is an install.\n{{ .Release.Revision }}: The revision number of the release.\n{{ .Release.Service }}: The service rendering the template. (usually helm)\n</code></pre>"},{"location":"tools/helm/variables/#chart-information","title":"Chart Information","text":"<pre><code>{{ .Chart.Name }}: The name of the chart.\n{{ .Chart.Version }}: The version of the chart.\n{{ .Chart.AppVersion }}: The app version of the chart.\n{{ .Chart.Description }}: The description of the chart.\n</code></pre>"},{"location":"tools/helm/variables/#values","title":"Values","text":"<pre><code>{{ .Values }}: The values passed into the chart.\n{{ .Values.&lt;key&gt; }}: Access a specific value from the values file.\n</code></pre>"},{"location":"tools/helm/variables/#files","title":"Files","text":"<pre><code>{{ .Files }}: Access non-template files in the chart.\n{{ .Files.Get \"&lt;file&gt;\" }}: Get the contents of a file.\n{{ .Files.GetBytes \"&lt;file&gt;\" }}: Get the contents of a file as bytes.\n</code></pre>"},{"location":"tools/helm/variables/#capabilities","title":"Capabilities","text":"<pre><code>{{ .Capabilities.APIVersions.Has \"batch/v1\" }}: Check if a specific API version is available.\n{{ .Capabilities.KubeVersion.GitVersion }}: The Kubernetes version.\n{{ .Capabilities.KubeVersion.Major }}: The major version of Kubernetes.\n{{ .Capabilities.KubeVersion.Minor }}: The minor version of Kubernetes.\n</code></pre>"},{"location":"tools/helm/variables/#template-information","title":"Template Information","text":"<pre><code>\n{{ .Template.Name }}: The name of the template file being rendered.\n</code></pre>"},{"location":"tools/helm/variables/#links","title":"Links","text":"<ul> <li>Built-in Objects</li> </ul> <p>https://helm.sh/docs/chart_template_guide/builtin_objects/</p>"},{"location":"tools/jq/tips/","title":"Tips","text":""},{"location":"tools/jq/tips/#remove-a-prefix-from-an-string","title":"remove a prefix from an string","text":"<p>Using the ltrimstr function</p> <pre><code>export KRO_VERSION=$(curl -sL \\\n    https://api.github.com/repos/kro-run/kro/releases/latest | \\\n    jq -r '.tag_name | ltrimstr(\"v\")'\n  )\n</code></pre>"},{"location":"tools/kustomize/10-patches/","title":"Patches (overlays)","text":"<p>Patches is a kustomize feature that can add, modify or override fields on resources. It is the new way to modify existing and declared manifests</p> <p>Since kustomize 5.0.0, it makes obsolete the following features:</p> <ul> <li>patchesJson6902</li> <li>patchesStrategicMerge</li> </ul>"},{"location":"tools/kustomize/10-patches/#ways-to-patch","title":"Ways to patch","text":"<p>There are 2 ways to patch a resource using kustomize:</p> <ul> <li>using JSON6902</li> <li>using Strategic Merge</li> </ul> <p>The way to declare the patch can be:</p> <ul> <li> <p>Using a file as reference (with the \"path\" key) In a strategic merge patch, the file will be a yaml file. In a json6902 will be a json file</p> </li> <li> <p>With an inline patch, writing the content in the kustomization file</p> </li> <li> <p>The target resource can be a single resource or multiple resources</p> </li> </ul>"},{"location":"tools/kustomize/10-patches/#notes-about-patching-specific-array-elements","title":"Notes about patching specific array elements","text":"<p>Kustomize doesn't support fine-grained patching of specific array elements. The standard way to modify an array element while maintaining the others, is to overwrite the whole resource.</p>"},{"location":"tools/kustomize/10-patches/#links","title":"Links","text":"<ul> <li>Kustomize patches</li> </ul> <p>https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/patches/</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/","title":"Jsonpatch (RFC 6902)","text":"<p>A jsonpatch is a json file or json file content that includes the desired changes to apply to a kubernetes resource</p> <p>A jsonpatch can do almost everything a patchStrategicMerge can do, but with a briefer syntax</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#using-a-file-path","title":"Using a file (path)","text":"<p>ingress_patch.json</p> <pre><code>[\n  {\"op\": \"replace\",\n   \"path\": \"/spec/rules/0/host\",\n   \"value\": \"foo.bar.io\"},\n\n  {\"op\": \"replace\",\n   \"path\": \"/spec/rules/0/http/paths/0/backend/servicePort\",\n   \"value\": 80},\n\n  {\"op\": \"add\",\n   \"path\": \"/spec/rules/0/http/paths/1\",\n   \"value\": { \"path\": \"/healthz\", \"backend\": {\"servicePort\":7700} }}\n]\n</code></pre> <p>kustomization.yaml</p> <pre><code>patches:\n- path: ingress_patch.json\n  target:\n    group: networking.k8s.io\n    version: v1beta1\n    kind: Ingress\n    name: my-ingress\n</code></pre>"},{"location":"tools/kustomize/11-patches-jsonpatch/#using-inline-patch","title":"Using inline patch","text":"<p>If we want to use the inline option in a json6902 patch, we must provide it this way.</p> <p>\"op\" can be \"add\", \"replace\", or \"remove\".</p> <pre><code>patches:\n  - patch: |-\n      - op: replace\n        path: /spec/template/spec/containers/0/image\n        value: nginx:1.21.0      \n</code></pre>"},{"location":"tools/kustomize/11-patches-jsonpatch/#special-characters","title":"Special characters","text":"<p>The ~ character is used as an escape character</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#0","title":"~0","text":"<p>In Kustomize, \"~0\" typically refers to a special path indicating the current directory where the kustomization.yaml file is located. This is a shorthand for referencing files within the same directory as the Kustomization configuration. It's commonly used for defining paths to base Kubernetes manifests or patches that Kustomize will use to customize and deploy resources</p> <p>~0 represents a ~ character itself.</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#1","title":"~1","text":"<p>In Kustomize, \"~1\" represents a backslash, which is used to escape characters in a string. When used in a path, it means to include a forward slash (\"/\"). So, \"~1\" translates to \"/\" in Kustomize paths.</p> <pre><code>patches:\n  - patch: |-\n      - op: add\n        path: /metadata/labels/app.kubernetes.io~1version\n        value: 1.21.0    \n</code></pre> <p>https://github.com/kubernetes-sigs/kustomize/issues/1256 https://github.com/kubernetes-sigs/kustomize/issues/907</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#notes","title":"Notes","text":"<p>In the standard JSON merge patch, JSON objects are always merged but lists are always replaced</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#links","title":"Links","text":"<ul> <li>jsonpatch.com</li> </ul> <p>https://jsonpatch.com/</p> <ul> <li>JavaScript Object Notation (JSON) Patch  RFC 6902</li> </ul> <p>https://datatracker.ietf.org/doc/html/rfc6902</p>"},{"location":"tools/kustomize/11-patches-jsonpatch/#json6902","title":"JSON6902","text":""},{"location":"tools/kustomize/12-patches-sm/","title":"Strategic Merge Patch (SMP)","text":"<p>An strategic merge patch is a yaml file or yaml file content that includes the group/version/kind/name of the resource to patch and the desired values</p> <p>An strategic merge patch is a kubernetes k8s resource that needs to include:</p> <ul> <li>the group</li> <li>the version</li> <li>the kind</li> <li>the metadata name</li> </ul>"},{"location":"tools/kustomize/12-patches-sm/#using-a-file-path","title":"Using a file (path)","text":"<p>add-label.patch.yaml</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dummy-app\n  labels:\n    app.kubernetes.io/version: 1.21.0\n</code></pre> <p>kustomization.yaml</p> <pre><code>patches:\n  - path: add-label.patch.yaml\n\n</code></pre>"},{"location":"tools/kustomize/12-patches-sm/#path-without-target","title":"path without target","text":"<pre><code>resources:\n  - original.yaml\npatches:\n  - path: mypatch.yaml\n</code></pre> <p>Using patches without target section, 4 fields must match between the original manifiest and the patch</p> <ul> <li>apiVersion</li> <li>kind</li> <li>metadata.name</li> <li>metadata.namespace, if it is specified in the original manifiest</li> </ul> <p>If this not matches, we will get the find unique target for patch error</p>"},{"location":"tools/kustomize/12-patches-sm/#using-inline-patch","title":"Using inline patch","text":"<p>If we want to use the inline option in a SMP, we must provide the content as is</p> <pre><code>...\npatches:\n  - patch: |-\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: dummy-app\n        labels:\n          app.kubernetes.io/version: 1.21.0\n...\n</code></pre>"},{"location":"tools/kustomize/12-patches-sm/#links","title":"Links","text":"<ul> <li>patchStrategicMerge</li> </ul> <p>https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md</p>"},{"location":"tools/kustomize/98-tips/","title":"Tips","text":""},{"location":"tools/kustomize/98-tips/#extract-crds","title":"Extract crds","text":"<p>This tool extracts all CRDs from a kubernetes cluster and it writes the discovered json schemas in a local directory</p> <p>https://github.com/datreeio/CRDs-catalog/releases/latest/download/crd-extractor.zip</p>"},{"location":"tools/kustomize/98-tips/#tools-to-validate-manifests","title":"Tools to validate manifests","text":"<ul> <li>kubeconform</li> </ul> <pre><code>kubectl kustomize . | kubeconform -schema-location default -schema-location '&lt;https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json&gt;' -verbose\n</code></pre> <ul> <li>datree</li> </ul> <pre><code>datree test /path/to/file\n</code></pre> <ul> <li>kubeval</li> </ul> <pre><code>kubeval --additional-schema-locations file:\"/home/jorge.phiguera@bosonit.local/.datree/crdSchemas\" /path/to/file\n</code></pre>"},{"location":"tools/kustomize/99-links/","title":"Links","text":"<ul> <li>Declarative Management of Kubernetes Objects Using Kustomize</li> </ul> <p>https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/</p> <ul> <li>Kustomize reference</li> </ul> <p>https://kubectl.docs.kubernetes.io/references/kustomize/</p> <ul> <li>Kustomize sig</li> </ul> <p>https://github.com/kubernetes-sigs/kustomize/</p>"},{"location":"tools/runtimes/dockerfile-reference/","title":"Dockerfile reference","text":""},{"location":"tools/runtimes/dockerfile-reference/#onbuild","title":"ONBUILD","text":"<p>With the ONBUILD instruction we can setup actions that will not be executed when building the current Dockerfile. It will only be executed when the image is used as the base for another build</p>"},{"location":"tools/terraform/97-errors/","title":"Errors","text":""},{"location":"tools/terraform/97-errors/#index-value-required","title":"Index value required","text":"<pre><code>Error: Index value required \u2502 \u2502 on line 1: \u2502 (source code not available) \u2502 \u2502 Index brackets must contain either a literal number or a literal string.\n</code></pre> <p>We must use single quotes in the resource 'RESOURCE'</p>"},{"location":"tools/terraform/init/","title":"Init","text":""},{"location":"tools/terraform/init/#working-with-different-environments-and-backend","title":"Working with different environments and backend","text":"<p>If we share the code between different environments and backends, we need to run this:</p> <pre><code>terraform init -backend-config=path/to/backend-config.tfvars\n</code></pre> <ul> <li>When switching between environments with different backends</li> <li>When changing backend settings (e.g., switching S3 buckets, Azure storage accounts, or vSphere datastores).</li> <li>When initializing a new environment with a different backend configuration.</li> <li>Whenever you want to reconfigure the backend (for example, after editing backend config files).</li> </ul> <p>You do not need to run -backend-config if only changing variables in your .tfvars files that are unrelated to the backend. Use it only for backend-specific changes.</p>"},{"location":"tools/vscode/features/","title":"Features","text":""},{"location":"tools/vscode/features/#linting","title":"Linting","text":"<p>Linting refers to the process of analyzing your code for potential errors, bugs, stylistic errors, and other issues. It helps improve code quality and maintain consistency across your codebase.</p> <p>Key Features of Linting in VS Code:</p> <ul> <li>Error Detection: Linters can detect syntax errors, runtime errors, and logical errors in your code.</li> <li>Code Style Enforcement: Linters enforce coding standards and style guidelines, ensuring that your code adheres to best practices.</li> <li>Real-time Feedback: As you write code, linters provide immediate feedback, highlighting issues directly in the editor.</li> <li>Fix Suggestions: Many linters offer suggestions for fixing detected issues, and some can even automatically apply fixes.</li> </ul> <p>In order to enable linting in VS Code we need to install an extension via the VS Code Marketplace that provides a linter tool (linter) for our programming language.</p> <p>Here we have a list of linters</p> <p>https://marketplace.visualstudio.com/search?target=VSCode&amp;category=Linters</p>"},{"location":"tools/vscode/features/#formatting","title":"Formatting","text":""},{"location":"tools/vscode/features/#snippet","title":"Snippet","text":"<p>https://code.visualstudio.com/docs/editor/userdefinedsnippets</p>"},{"location":"tools/vscode/features/#intellisense-and-code-completion","title":"Intellisense and Code completion","text":"<p>editor.quickSuggestions editor.tabCompletion</p> <p>https://code.visualstudio.com/docs/editor/intellisense</p> <p>Code completion is an vscode feature that provides suggestions and auto-completions to our code.</p> <p>Code completion is part of a wider feature called intellisense.</p> <p>Key Features of Code Completion in VS Code:</p> <ul> <li> <p>Auto-Suggestions: As you type, VS Code suggests possible completions for your code based on the context. This includes variable names, function names, class names, and more.</p> </li> <li> <p>Snippet Insertion: VS Code can insert code snippets, which are predefined templates for common code structures. This helps speed up coding by reducing repetitive typing.</p> </li> <li> <p>Parameter Info: When you type a function call, VS Code shows the function's parameters and their types, helping you understand what arguments are required.</p> </li> <li> <p>Quick Info: Hovering over a symbol provides additional information, such as documentation comments, type information, and more.</p> </li> <li> <p>Member Lists: When you type a dot after an object, VS Code shows a list of available members (methods, properties, etc.) for that object.</p> </li> </ul> <p>How to Use Code Completion in VS Code:</p> <ul> <li>Typing: Simply start typing, and VS Code will automatically show suggestions based on the context.</li> <li>Trigger Suggestions: You can manually trigger suggestions by pressing Ctrl + Space.</li> <li>Accepting Suggestions: Use the Tab or Enter key to accept a suggestion from the list.</li> </ul>"},{"location":"tools/vscode/kubernetes-resources/","title":"Kubernetes syntax","text":"<p>How to enable systax for kubernetes resources (and other yaml files)</p>"},{"location":"tools/vscode/kubernetes-resources/#install-and-enable-the-yaml-extension","title":"Install and enable the yaml extension","text":"<p>Install the redhat yaml extension</p> <p>https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml</p> <p>And associate the yaml files to it in the settings.json vscode file (at user or workspace level)</p> <pre><code>    \"files.associations\": {\n        \"*.yaml\": \"yaml\"\n    },\n</code></pre>"},{"location":"tools/vscode/kubernetes-resources/#yaml-schemas","title":"Yaml schemas","text":"<p>We can associate an url or local file containing a json schema to files with the yaml.schemas setting.</p> <pre><code>\"yaml.schemas\": {\n    \"kubernetes\": \"*.yaml\"\n},\n</code></pre> <p>But this will not recognize CRDs. For that we need to get the json schema for that CRDs and associate them with files using, for example, patterns.</p>"},{"location":"tools/vscode/kubernetes-resources/#where-to-get-the-crds","title":"Where to get the CRDs","text":"<p>A typical place to search some CRDs are</p> <ul> <li>JSON schema store (very limited)</li> </ul> <p>https://www.schemastore.org/</p> <ul> <li>yannh kubernetes schemas</li> </ul> <p>This repository contains updated schemas for vanilla kubernetes resources and it is related with the kuberconform utility</p> <p>https://github.com/yannh/kubernetes-json-schema</p> <ul> <li>Datree's CRDs catalog</li> </ul> <p>Big online CRDs catalog</p> <p>https://github.com/datreeio/CRDs-catalog/</p> <ul> <li>CRDs extractor</li> </ul> <p>Datree offers a very interesting script called crd extractor that it search all the crds in a kubernetes cluster and it stores them as json under ~/.datree/crdSchemas/</p> <p>https://github.com/datreeio/CRDs-catalog/releases/latest/download/crd-extractor.zip</p> <ul> <li>Official schema</li> </ul> <p>Sometimes the developer offers a public json schema of their CRDs</p>"},{"location":"tools/vscode/kubernetes-resources/#examples","title":"Examples","text":"<p>Here we can see how to associate public or downloaded json schemas with file names.</p> <pre><code>    \"yaml.schemas\": {\n        // Gitlab ci\n        \"https://gitlab.com/gitlab-org/gitlab/-/raw/master/app/assets/javascripts/editor/schema/ci.json\": [\".gitlab-ci.yml\"],\n        // MKdocs\n        \"https://json.schemastore.org/mkdocs-1.6.json\": [\"mkdocs.yml\"],\n        // Helm\n        \"https://json.schemastore.org/chart-lock.json\" : \"Chart.lock\",\n        \"https://json.schemastore.org/chart.json\": \"Chart.yaml\",\n        // Argocd\n        \"https://raw.githubusercontent.com/datreeio/CRDs-catalog/refs/heads/main/argoproj.io/application_v1alpha1.json\": [\"argocd-app-*.yaml\"],\n        \"file:///home/myuser/.datree/crdSchemas/argoproj.io/applicationset_v1alpha1.json\": [\"argocd-appset-*.yaml\"],\n        \"file:///home/myuser/.datree/crdSchemas/argoproj.io/appproject_v1alpha1.json\": [\"argocd-proj-*.yaml\"],\n    },\n</code></pre> <p>I prefer this way over the kubernetes microsoft extension</p>"},{"location":"tools/vscode/shortcuts/","title":"Some shortcuts","text":""},{"location":"tools/vscode/shortcuts/#in-markdown","title":"In markdown","text":""},{"location":"tools/vscode/shortcuts/#ctrlshifto","title":"Ctrl+Shift+O","text":"<p>Fast access to sections</p>"},{"location":"tools/vscode/shortcuts/#ctrlshiftv","title":"Ctrl+Shift+V","text":"<p>Document preview</p>"},{"location":"tools/vscode/shortcuts/#ctrlspace","title":"Ctrl+Space","text":"<p>Show suggestions</p>"},{"location":"tools/vscode/yaml/","title":"Yaml","text":"<p>https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml</p>"},{"location":"tools/vscode/yaml/#schemas","title":"schemas","text":"<p>yaml.schemas: Helps you associate schemas with files in a glob pattern yaml.schemaStore.enable: When set to true, the YAML language server will pull in all available schemas from JSON Schema Store yaml.schemaStore.url: URL of a schema store catalog to use when downloading schemas.</p>"},{"location":"tools/vscode/yaml/#completion","title":"completion","text":"<p>yaml.completion: Enable/disable autocompletion</p>"},{"location":"tools/yq/99-links/","title":"Links","text":""},{"location":"tools/yq/99-links/#mike-farah","title":"Mike Farah","text":"<ul> <li>Github</li> </ul> <p>https://github.com/mikefarah/yq</p> <ul> <li>Documentation</li> </ul> <p>https://mikefarah.gitbook.io/yq</p> <ul> <li>Containers</li> </ul> <p>https://hub.docker.com/r/mikefarah/yq</p>"},{"location":"tools/yq/99-links/#kislyuk","title":"Kislyuk","text":"<ul> <li>Github</li> </ul> <p>https://github.com/kislyuk/yq</p> <ul> <li>Documentation</li> </ul> <p>https://kislyuk.github.io/yq/</p>"},{"location":"tools/yq/styles/","title":"Tips","text":""},{"location":"tools/yq/styles/#style-writting-keys","title":"Style writting keys","text":"<p>If the written key does not exists, i do not uses double quotes If it exists, maintains the existing style</p>"},{"location":"tools/ytt/00-intro/","title":"Intro","text":""},{"location":"tools/ytt/00-intro/#how-it-works","title":"How it works","text":"<p>Ytt is a command line tool useful for template an patch yaml files. For that purpose, it uses some diferent document types</p>"},{"location":"tools/ytt/00-intro/#templated","title":"Templated","text":"<p>A templated document in ytt is any YAML file that uses ytt's templating syntax (#@) to create dynamic, configurable Kubernetes manifests or other YAML output. Here is where we define the final yaml file.</p> <p>They do not start with an specific anotation, but they have template logic inside.</p>"},{"location":"tools/ytt/00-intro/#data-value","title":"Data value","text":"<p>A data value a ytt document that contains the values that the templates can use.</p> <p>It is very similar to a values.yaml files in helm but in this case can be validated against a schema</p> <ul> <li>https://carvel.dev/ytt/docs/latest/how-to-use-data-values/</li> <li>https://carvel.dev/ytt/docs/latest/ytt-data-values/</li> </ul>"},{"location":"tools/ytt/00-intro/#data-value-schema","title":"Data Value Schema","text":"<p>A data value schema is a ytt document where we define things for the data values documents:</p> <ul> <li>the expected structure. the shape of complex data structures</li> <li>the default values for optional fields</li> <li>types (string, int, bool, etc)</li> <li>validation rules</li> </ul> <p>When providing data values, ytt validates them against this schema to see if the values provided are correct.</p> <ul> <li>https://carvel.dev/ytt/docs/latest/lang-ref-ytt-schema/</li> <li>https://carvel.dev/ytt/docs/latest/how-to-write-validations/</li> <li>https://carvel.dev/ytt/docs/latest/schema-validations-cheat-sheet/</li> </ul>"},{"location":"tools/ytt/00-intro/#overlay","title":"Overlay","text":"<p>An Overlay Document in ytt is a document that modifies or patches existing YAML documents by applying transformations, additions, deletions, or replacements. It's marked with the #@overlay/match annotation and allows you to make targeted changes to other documents.</p> <ul> <li>https://carvel.dev/ytt/docs/latest/ytt-overlays/</li> <li>https://carvel.dev/ytt/docs/latest/lang-ref-ytt-overlay/</li> </ul>"},{"location":"tools/ytt/00-intro/#table-and-annotations","title":"Table and annotations","text":"Type Annotation Purpose Templated contains lines with #@ Data Values begins with #@data/values provide values Data Value Schema begins with #@data/values-schema data value validation Overlay begins with @overlay/match Plain No annotations"},{"location":"tools/ytt/00-intro/#conclusions","title":"Conclusions","text":"<p>In ytt we define template documents with the estructure and logic of the final yaml files we want.</p> <p>We give them values using data value documents, that can be validated against data value schema documents.</p> <p>We can use overlay documents to modify final output without modifying original templates, for example, adding or removing sections.</p>"},{"location":"tools/ytt/00-intro/#links","title":"Links","text":"<ul> <li>Data Values vs Overlays</li> </ul> <p>https://carvel.dev/ytt/docs/latest/data-values-vs-overlays/</p>"},{"location":"tools/ytt/99-links/","title":"Links","text":""},{"location":"tools/ytt/99-links/#ytt-site","title":"YTT site","text":"<p>https://carvel.dev/ytt/</p>"},{"location":"tools/ytt/99-links/#examples","title":"Examples","text":"<ul> <li> <p>https://github.com/carvel-dev/ytt/tree/develop/examples</p> </li> <li> <p>https://github.com/topics/ytt</p> </li> </ul>"},{"location":"tools/ytt/data-values/","title":"Data values","text":"<p>If we want to create a template or schema that can be called via data values, we can start our schema file this</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n</code></pre> <p>This imports the the \"data\" struct from the @ytt:data module</p>"},{"location":"tools/ytt/data-values/#declaring-data-values","title":"Declaring data values","text":"<p>A data value declaration has 3 parts: a name, a default value, and a type.</p> <pre><code>this-is-the-name: this-is-the-default-value\nmap:\n  integer: 45323\n  boolean: true\narray:\n- default-empty-string: \"\"\n  boolean: false\n</code></pre> <p>Or it can be called via data values</p> <pre><code>name: #@ data.values.name\n</code></pre>"},{"location":"tools/ytt/data-values/#schema-validations","title":"Schema validations","text":""},{"location":"tools/ytt/overlay/","title":"Overlay","text":"<p>An Overlay Document in ytt is a document that modifies or patches existing YAML documents by applying transformations, additions, deletions, or replacements. It's marked with the #@overlay/match annotation and allows you to make targeted changes to other documents.</p> <p>Purpose:</p> <ul> <li>Modify existing documents: Change specific fields or sections</li> <li>Add new content: Insert additional fields, containers, or resources</li> <li>Remove content: Delete unwanted fields or sections</li> <li>Replace values: Override specific values in targeted documents</li> <li>Conditional modifications: Apply changes based on conditions</li> </ul> <pre><code>#@overlay/match by=overlay.subset({\"kind\": \"Deployment\"})\n---\nspec:\n  #@overlay/match missing_ok=True\n  template:\n    spec:\n      containers:\n      #@overlay/match by=\"name\"\n      - name: app\n        #@overlay/replace\n        image: nginx:1.22\n        #@overlay/insert\n        env:\n        - name: DEBUG\n          value: \"true\"\n</code></pre> <p>Common overlay operations:</p> <ul> <li>@overlay/match: Select which documents/fields to modify</li> <li>@overlay/replace: Replace the entire value</li> <li>@overlay/insert: Add new fields or array items</li> <li>@overlay/remove: Delete fields or array items</li> <li>@overlay/append: Add to the end of arrays</li> <li>@overlay/merge: Merge objects together</li> </ul> <p>Use cases:</p> <ul> <li>Environment-specific changes: Different configs for dev/staging/prod</li> <li>Feature toggles: Enable/disable features conditionally</li> <li>Security patches: Add security contexts or policies</li> <li>Resource adjustments: Modify CPU/memory limits per environment</li> <li>Multi-tenancy: Customize base templates per tenant</li> </ul> <p>How it works:</p> <ul> <li>Target selection: Uses match criteria to find documents to modify</li> <li>Transformation: Applies the specified changes (replace, insert, remove, etc.)</li> <li>Output: Produces the modified YAML with changes applied</li> </ul> <p>In summary: An overlay document is ytt's way of applying targeted patches or modifications to existing YAML documents, enabling flexible customization without duplicating entire templates.</p>"},{"location":"vibe-sreing/","title":"Intro to vibe sreing","text":""},{"location":"vibe-sreing/#ai-agents-assistants","title":"Ai Agents / Assistants","text":"<p>List of AI agents</p> Agent Company URL Claude Code Terminal based Anthropic https://www.anthropic.com/claude-code Gemini CLI Terminal based Google Codex Terminal based OpenAi https://openai.com/codex/ Github copilot cli Terminal based Microsoft https://github.com/features/copilot/cli Windsurf (formerly codeium) IDE https://windsurf.com/ Vscode + Github copilot agent mode IDE Microsoft https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode Cursor IDE https://cursor.com/ Vscodium N/A https://vscodium.com/"},{"location":"vibe-sreing/#web-interfaces","title":"Web interfaces","text":"Company URL Claude Anthropic ChatGPT OpenAi https://chatgpt.com/"},{"location":"vibe-sreing/#large-language-models","title":"Large language models","text":"Family Name Latest Release Company Website Sonnet 4.5 Anthropic https://www.anthropic.com Opus 4.1 Anthropic https://www.anthropic.com Haiku 3.5 Anthropic https://www.anthropic.com GPT GPT-5 OpenAI https://openai.com o-series o4-mini OpenAI https://openai.com Gemini 2.5 Google https://ai.google.dev Llama 4 Meta https://ai.meta.com Magistral Medium Mistral AI https://mistral.ai Medium 3 Mistral AI https://mistral.ai Small 3.1 Mistral AI https://mistral.ai Qwen 3 Alibaba Cloud https://qwenlm.github.io V-series V3.2 DeepSeek https://www.deepseek.com R-series R1-0528 DeepSeek https://www.deepseek.com Grok 4 Fast xAI https://x.ai"},{"location":"vibe-sreing/vscode-extensions/","title":"VS code Extensions","text":"<ul> <li>Claude code</li> </ul> <p>Official VS Code extension bringing Claude's AI capabilities directly into the editor.</p> <p>https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code</p> <ul> <li>Github Copilot</li> </ul> <p>AI pair programmer providing code suggestions and completions based on context and comments.</p> <p>https://code.visualstudio.com/docs/copilot/overview https://marketplace.visualstudio.com/items?itemName=GitHub.copilot</p> <ul> <li>Kilo code</li> </ul> <p>AI coding assistant focused on enterprise security and compliance requirements.</p> <p>https://kilocode.ai/</p> <ul> <li>Windsurf plugin</li> </ul> <p>AI-powered coding assistant with advanced context understanding and multi-file operations.</p> <p>https://marketplace.visualstudio.com/items?itemName=Codeium.codeium</p> <ul> <li>Amazon Q for VScode</li> </ul> <p>AWS's AI assistant for developers with deep integration into AWS services and documentation.</p> <p>https://aws.amazon.com/q/developer/ https://marketplace.visualstudio.com/items?itemName=AmazonWebServices.amazon-q-vscode</p> <ul> <li>Tabnine</li> </ul> <p>AI code completion tool trained on open source code with privacy-focused options.</p> <p>https://www.tabnine.com/</p> <ul> <li>Augment</li> </ul> <p>AI coding assistant with focus on code quality and team collaboration features.</p> <p>https://marketplace.visualstudio.com/items?itemName=augment.vscode-augment</p>"},{"location":"vibe-sreing/claude-code/00-index/","title":"Intro","text":""},{"location":"vibe-sreing/claude-code/01-context/","title":"Context and memory","text":"<p>Giving a good context to claude code is one of the most important things to accomplish</p> <p>We can give context to claude using different ways.</p>"},{"location":"vibe-sreing/claude-code/01-context/#context-via-claudemd","title":"Context via CLAUDE.md","text":"<p>When you run Claude in a directory Claude uses that directory as the current working context. It can access files within that directory (and subdirectories) and it looks for any CLAUDE.md files to gain additional context, but these are optional.</p> <p>That files are pulled when starting a conversation and they can include any relevant information we can give to claude code:</p> <ul> <li>What is our project about, including environments, releases</li> <li>How we want organize the code, branches,...</li> <li>What utilities and commands we use</li> <li>What we want to test</li> <li>Code style guidelines</li> <li>Testing instructions</li> <li>and so on</li> </ul>"},{"location":"vibe-sreing/claude-code/01-context/#locations","title":"Locations","text":"<p>When we launch claude code in a directory, it search CLAUDE.md files in some locations:</p> <ul> <li>Project memory (./CLAUDE.md)</li> </ul> <p>In the directory where claude was launched</p> <ul> <li>User memory (~/.claude/CLAUDE.md)</li> </ul> <p>We can include here some personal preferences</p> <ul> <li>Parent (git)</li> </ul> <p>If the directory is part of a git repository, in parent directories up to the root of the repo</p> <ul> <li>Subdirectories</li> </ul> <p>If there are CLAUDE.md files in subdirectories, they are only included when Claude reads files in those subtrees, not at claude launch</p> <p>Using CLAUDE.local.md files is deprecated. Use imports instead</p>"},{"location":"vibe-sreing/claude-code/01-context/#tips","title":"Tips","text":"<p>The /init command reads the current and create a CLAUDE.md file The /memory command permits to edit the user and current directory CLAUDE.md files</p>"},{"location":"vibe-sreing/claude-code/01-context/#other-ways-to-add-context","title":"Other ways to add context","text":"<ul> <li> <p>The # key adds context to memory and it will incorporated to the CLAUDE.md file</p> </li> <li> <p>The current selection/tab in the IDE is automatically shared with Claude Code context</p> </li> <li> <p>Diagnostic sharing</p> </li> </ul> <p>Diagnostic errors (lint, syntax, etc.) from the IDE are automatically shared with Claude as you work</p>"},{"location":"vibe-sreing/claude-code/01-context/#links","title":"Links","text":"<ul> <li>Manage Claude's memory</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/memory</p> <ul> <li>Outdated (warning)</li> </ul> <p>https://www.anthropic.com/engineering/claude-code-best-practices</p>"},{"location":"vibe-sreing/claude-code/02-project/","title":"Project","text":""},{"location":"vibe-sreing/claude-code/02-project/#what-is-a-claude-code-project","title":"What is a Claude Code Project?","text":"<p>A Claude Code project is simply any directory where you've launched the <code>claude</code> command or are interacting with Claude Code. There's no special \"project file\" or configuration required to designate a directory as a Claude Code project.</p> <p>When you run Claude in a directory:</p> <ul> <li>Claude establishes that directory as the current working context</li> <li>It can access files within that directory and subdirectories</li> <li>It looks for CLAUDE.md files to gain additional context (optional)</li> <li>Claude becomes aware of your entire project structure</li> </ul> <p>The project concept in Claude Code is more about providing a working context than a formal technical structure with required configuration files.</p>"},{"location":"vibe-sreing/claude-code/02-project/#getting-started","title":"Getting Started","text":"<p>To start using Claude Code in your project, simply navigate to your project directory and run the <code>claude</code> command. Claude Code will then be able to help you:</p> <ul> <li>Build features from descriptions</li> <li>Debug issues</li> <li>Navigate your codebase</li> <li>Automate various development tasks</li> <li>Find up-to-date information about your project</li> </ul>"},{"location":"vibe-sreing/claude-code/02-project/#memory-and-context-management","title":"Memory and Context Management","text":"<p>Claude Code manages memory and context through a hierarchical structure with three levels:</p>"},{"location":"vibe-sreing/claude-code/02-project/#memory-types","title":"Memory Types","text":"<ol> <li>Enterprise Policy (Organization-wide)</li> <li>Stored in system-level CLAUDE.md files</li> <li>Shared with all users in the organization</li> <li> <p>Highest precedence</p> </li> <li> <p>Project Memory (Team-shared)</p> </li> <li>Stored in <code>./CLAUDE.md</code> or <code>./.claude/CLAUDE.md</code></li> <li>Contains project-specific instructions: architecture, coding standards, common workflows</li> <li> <p>Shared with team members via source control</p> </li> <li> <p>User Memory (Personal)</p> </li> <li>Stored in <code>~/.claude/CLAUDE.md</code></li> <li>Contains personal preferences</li> <li>Applies across all projects for an individual</li> </ol>"},{"location":"vibe-sreing/claude-code/02-project/#memory-loading-behavior","title":"Memory Loading Behavior","text":"<ul> <li>Memory files are automatically loaded when Claude Code launches</li> <li>Higher-level memories take precedence over lower-level ones</li> <li>The system recursively discovers memories from the current working directory upwards</li> <li>You can import additional memory files using <code>@path/to/import</code> syntax</li> </ul>"},{"location":"vibe-sreing/claude-code/02-project/#project-configuration","title":"Project Configuration","text":""},{"location":"vibe-sreing/claude-code/02-project/#project-specific-settings","title":"Project-Specific Settings","text":"<p>Projects can have their own Claude Code settings and configurations stored in <code>.claude/</code> directories:</p> <ul> <li>Project-specific subagents: <code>.claude/agents/</code></li> <li>Project memory files: <code>.claude/CLAUDE.md</code></li> <li>Custom slash commands</li> <li>Hooks and integrations</li> </ul>"},{"location":"vibe-sreing/claude-code/02-project/#directory-structure-notes","title":"Directory Structure Notes","text":"<ul> <li>The folders under <code>~/.claude/projects/</code> can be safely deleted if you no longer need those projects</li> <li>This directory is a custom organization structure, not required by Claude Code</li> <li>You can organize your projects however works best for your workflow</li> </ul>"},{"location":"vibe-sreing/claude-code/02-project/#team-collaboration","title":"Team Collaboration","text":"<p>For Team and Enterprise plans, Claude Code projects can be shared among team members with premium seats, enabling:</p> <ul> <li>Collaborative development workflows</li> <li>Shared project memory and context</li> <li>Team-wide coding standards and practices</li> <li>Consistent development patterns across the team</li> </ul>"},{"location":"vibe-sreing/claude-code/03-slash-commands/","title":"Available commands","text":""},{"location":"vibe-sreing/claude-code/03-slash-commands/#slash-commands","title":"Slash commands","text":"<p>Slash commands are a powerful feature that allows you to control Claude's behavior during an interactive session. They serve as quick, flexible shortcuts for various actions and workflows.</p> <p>Key Features:</p> <ul> <li>Built-in commands: Like <code>/clear</code>, <code>/help</code>, <code>/model</code> for core functionality</li> <li>Custom commands: User-defined, project-specific, or personal commands</li> <li>MCP commands: Commands provided by Model Context Protocol servers</li> <li>Customization: Support arguments, file references, bash command execution, and frontmatter configuration</li> <li>Flexibility: Can be simple single-line prompts or complex multi-step workflows</li> </ul> <p>Slash commands provide a streamlined, programmable interface to enhance productivity and interaction with Claude. They're easily shareable across projects and teams.</p>"},{"location":"vibe-sreing/claude-code/03-slash-commands/#builtin-commands","title":"Builtin commands","text":""},{"location":"vibe-sreing/claude-code/03-slash-commands/#context-management","title":"Context Management","text":"Command Description <code>/add-dir</code> Add additional working directories to your project context <code>/clear</code> (reset, new) Clear the entire conversation history and free up context <code>/compact</code> Condense conversation history keeping only a summary. Optional: <code>/compact [instructions]</code> for focused summary <code>/context</code> Visualize current context usage as a colored grid showing token distribution <code>/init</code> Initialize project with a CLAUDE.md guide file containing codebase documentation <code>/memory</code> Edit CLAUDE.md memory files (user and project-specific)"},{"location":"vibe-sreing/claude-code/03-slash-commands/#conversation","title":"Conversation","text":"Command Description <code>/bashes</code> List and manage background bash processes and tasks <code>/export</code> Export the current conversation to a file or clipboard <code>/resume</code> Resume a previous conversation session <code>/rewind</code> (checkpoint) Rewind the conversation and/or code to a previous checkpoint or state <code>/todos</code> List current todo items and task tracking status"},{"location":"vibe-sreing/claude-code/03-slash-commands/#configuration","title":"Configuration","text":"Command Description <code>/agents</code> Manage custom AI subagent configurations for specialized tasks <code>/config</code> (theme) Open Settings interface (Config tab) for configuration management <code>/hooks</code> Manage hook configurations for tool events and command execution <code>/ide</code> Manage IDE integrations and show connection status <code>/mcp</code> Manage MCP server connections and OAuth authentication <code>/migrate-installer</code> Migrate from global npm installation to local installation <code>/model</code> Select or change the AI model being used <code>/output-style</code> Set the output style directly or select from a menu <code>/output-style:new</code> Create a new custom output style <code>/permissions</code> (allowed-tools) View and update allow &amp; deny tool permission rules <code>/statusline</code> Set up and configure Claude Code's status line UI <code>/terminal-setup</code> Install Shift+Enter key binding for newlines (iTerm2 and VSCode) <code>/upgrade</code> Upgrade to Max plan for higher rate limits and more Opus access <code>/vim</code> Toggle between Vim mode (insert/command) and Normal editing modes"},{"location":"vibe-sreing/claude-code/03-slash-commands/#status-commands","title":"Status Commands","text":"Command Description <code>/cost</code> Show token usage statistics and subscription-specific cost details for the current session <code>/doctor</code> Check the health of your Claude Code installation and verify settings <code>/status</code> Open Settings (Status tab) showing version, model, account, API connectivity, and tool statuses <code>/usage</code> Show plan usage limits and rate limit status for your subscription"},{"location":"vibe-sreing/claude-code/03-slash-commands/#git-related","title":"Git Related","text":"Command Description <code>/install-github-app</code> Set up Claude GitHub Actions integration for a repository <code>/pr-comments</code> View and retrieve comments from a GitHub pull request <code>/review</code> Request a code review for a pull request <code>/security-review</code> Perform a comprehensive security review of pending changes on the current branch"},{"location":"vibe-sreing/claude-code/03-slash-commands/#misc","title":"Misc","text":"Command Description <code>/exit</code> (quit) Exit the Claude Code REPL <code>/feedback</code> (bug) Report bugs or submit feedback about Claude Code (sends to Anthropic) <code>/help</code> Show usage help and list all available commands <code>/login</code> Sign in or switch between Anthropic accounts <code>/logout</code> Sign out from your current Anthropic account <code>/privacy-settings</code> View and update your privacy settings and data sharing preferences <code>/release-notes</code> View release notes for Claude Code updates"},{"location":"vibe-sreing/claude-code/03-slash-commands/#custom-slash-commands","title":"Custom slash commands","text":"<p>It possible to define custom slash commands via md files</p> <ul> <li>Per user</li> </ul> <pre><code>~/.claude/commands/\n\n</code></pre> <ul> <li>Per project</li> </ul> <pre><code>.claude/commands/\n</code></pre>"},{"location":"vibe-sreing/claude-code/03-slash-commands/#links","title":"Links","text":"<ul> <li>Slash commands</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/slash-commands</p>"},{"location":"vibe-sreing/claude-code/04-subagents/","title":"Subagents","text":"<p>Subagents (also called agents) are custom AI assistants specialized for specific tasks. They can be managed through the <code>/agents</code> command and configured to handle particular workflows or domain-specific operations.</p> <p>Subagents allow you to:</p> <ul> <li>Delegate specialized tasks to focused AI assistants</li> <li>Create reusable task-specific workflows</li> <li>Maintain consistency in how certain tasks are performed</li> <li>Scale complex operations across multiple focused agents</li> </ul>"},{"location":"vibe-sreing/claude-code/05-agent-skills/","title":"Agent Skills","text":"<p>Agent Skills are modular capabilities that extend Claude's functionality for domain-specific tasks. They operate through a progressive, filesystem-based loading mechanism that optimizes token usage and performance.</p> <p>Architecture - Three Content Levels:</p> <ol> <li>Metadata (Always Loaded ~100 tokens)</li> <li>Lightweight system prompt inclusion</li> <li>Describes the skill's purpose and when to use it</li> <li> <p>Minimal token consumption</p> </li> <li> <p>Instructions (Loaded When Triggered &lt;5,000 tokens)</p> </li> <li>Procedural knowledge and workflows</li> <li>Loaded dynamically when relevant</li> <li> <p>Step-by-step task guidance</p> </li> <li> <p>Resources and Code (Loaded as Needed)</p> </li> <li>Optional executable scripts</li> <li>Reference materials</li> <li>Executed via bash without consuming context window</li> </ol> <p>Key Benefits:</p> <ul> <li>Specialize Claude for domain-specific tasks</li> <li>Reduce repetitive instructions</li> <li>Enable complex workflow composition</li> <li>Efficient token usage through progressive loading</li> </ul> <p>Structure:</p> <ul> <li>Requires a <code>SKILL.md</code> file</li> <li>Must include name (64 character max)</li> <li>Must include description (1024 character max)</li> </ul> <p>Availability:</p> <ul> <li>Claude API</li> <li>Claude Code</li> <li>Claude.ai (with limitations)</li> </ul> <p>Pre-built Skills:</p> <ul> <li>PowerPoint processing</li> <li>Excel data manipulation</li> <li>Word document editing</li> <li>PDF document handling</li> </ul> <p>Security:</p> <ul> <li>No network access by default</li> <li>Pre-configured dependencies</li> <li>Sandboxed execution environment</li> </ul>"},{"location":"vibe-sreing/claude-code/05-agent-skills/#links","title":"LInks","text":"<ul> <li>Agent Skills</li> </ul> <p>https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview</p> <p>Equipping agents for the real world with Agent Skills</p> <p>https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills</p>"},{"location":"vibe-sreing/claude-code/06-hooks/","title":"Hooks","text":"<p>Hooks are automated scripts that intercept and modify various stages of Claude's interaction with tools and the development environment. They're configured in settings files like <code>~/.claude/settings.json</code>.</p> <p>Hook Events:</p> <ul> <li>PreToolUse: Before a tool is used</li> <li>PostToolUse: After a tool completes</li> <li>UserPromptSubmit: When a user submits a prompt</li> <li>SessionStart/End: At session initialization or conclusion</li> </ul> <p>Capabilities:</p> <ul> <li>Validate tool usage and control permissions</li> <li>Add context to interactions automatically</li> <li>Log operations for auditing</li> <li>Perform security checks</li> </ul> <p>Important Notes:</p> <ul> <li>Hooks execute shell commands automatically (60-second default timeout)</li> <li>Users are solely responsible for configured commands</li> <li>Hooks run in parallel</li> <li>Environment variables like <code>CLAUDE_PROJECT_DIR</code> are available</li> </ul>"},{"location":"vibe-sreing/claude-code/08-mcp-servers/","title":"MCP servers","text":"<p>Model Context Protocol (MCP) is a standardized protocol that allows Claude Code to connect with external tools, services, and data sources.</p> <p>Purpose:</p> <ul> <li>Connect Claude to hundreds of tools and data sources</li> <li>Enable complex interactions across different platforms</li> <li>Extend Claude Code's capabilities beyond built-in tools</li> </ul> <p>Server Types:</p> <ol> <li>Remote HTTP servers</li> <li>Remote SSE servers</li> <li>Local stdio servers</li> </ol> <p>Installation Scopes:</p> <ul> <li>Local: Project-specific, private configuration</li> <li>Project: Shared team configuration</li> <li>User: Available across multiple projects</li> </ul> <p>Key Features:</p> <ul> <li>OAuth 2.0 authentication support</li> <li>Environment variable expansion</li> <li>Resource referencing via @ mentions</li> <li>Slash commands for quick actions</li> </ul> <p>Example Use Cases:</p> <ul> <li>Fetch Sentry error logs</li> <li>Review GitHub pull requests</li> <li>Query PostgreSQL databases</li> <li>Create Jira tickets</li> <li>Integrate with monitoring tools</li> </ul>"},{"location":"vibe-sreing/claude-code/09-files/","title":"Files","text":""},{"location":"vibe-sreing/claude-code/09-files/#settings","title":"Settings","text":"File Scope Use ~/.claude/settings.json User, all projects User defined settings .claude/settings.json Project .claude/settings.local.json Project .claude/settings.local.json Project"},{"location":"vibe-sreing/claude-code/09-files/#more-info","title":"More info","text":"<p>https://docs.anthropic.com/en/docs/claude-code/settings</p>"},{"location":"vibe-sreing/claude-code/97-best-practices/","title":"Best practices","text":""},{"location":"vibe-sreing/claude-code/97-best-practices/#clear-context","title":"Clear context","text":"<p>Use the /clear command frequently between tasks and conversations to reset the context window.</p>"},{"location":"vibe-sreing/claude-code/97-best-practices/#share-context","title":"Share context","text":"<p>Share CLAUDE.md context pushing them to the git repository</p>"},{"location":"vibe-sreing/claude-code/97-best-practices/#vs-code-problems","title":"VS code problems","text":"<p>Use mcp__ide__getDiagnostics to fix problems detected in VScode problems tab</p>"},{"location":"vibe-sreing/claude-code/97-best-practices/#use-commands","title":"Use commands","text":"<p>Use commands for common tasks</p>"},{"location":"vibe-sreing/claude-code/99-links/","title":"Links","text":""},{"location":"vibe-sreing/claude-code/99-links/#official","title":"Official","text":"<ul> <li>Claude code official doc</li> </ul> <p>https://docs.anthropic.com/en/docs/claude-code/overview</p>"},{"location":"vibe-sreing/claude-code/99-links/#community","title":"Community","text":"<ul> <li>Claudelog</li> </ul> <p>https://claudelog.com/</p> <ul> <li>Awesome claude code</li> </ul> <p>https://github.com/hesreallyhim/awesome-claude-code</p> <ul> <li>Awesome claude</li> </ul> <p>https://github.com/alvinunreal/awesome-claude</p> <ul> <li>Claude Code Commands</li> </ul> <p>https://claudecodecommands.directory/</p> <ul> <li>Claudepro directory</li> </ul> <p>https://claudepro.directory/</p>"},{"location":"vibe-sreing/claude-code/shortcuts/","title":"Shortcuts","text":""},{"location":"vibe-sreing/claude-code/shortcuts/#keyboard-shortcuts","title":"Keyboard shortcuts","text":"Shortcuts Description ! Shortcut to run bash commands directly # Shortcut to add to memory @ Mention a file \\ + ENTER Multiline input (quick escape, all terminals) ALT+CTRL+K Mention a focused file Attach a file CTRL+B Move bash command to background CTRL+C Cancel current input or generation CTRL+D Exit Claude Code session CTRL+J Line feed character (multiline input) CTRL+L Clear terminal screen CTRL+R Reverse search command history ESC + ESC Rewind code/conversation SHIFT+ENTER Multiline input (after /terminal-setup) SHIFT+TAB Cycle through modes: Default \u2192 Auto-Accept (\u23f5\u23f5) \u2192 Plan Mode (\u23f8) \u2192 Default... TAB Toggle extended thinking Up/Down arrows Navigate command history"},{"location":"vibe-sreing/claude-code/shortcuts/#links","title":"Links","text":"<ul> <li>Interactive mode</li> </ul> <p>https://docs.claude.com/en/docs/claude-code/interactive-mode</p>"},{"location":"vibe-sreing/claude-code/vscode/","title":"Visual Studio Code","text":"<p>There is an official Claude Code VSCode extension available in the marketplace.</p>"},{"location":"vibe-sreing/claude-code/vscode/#tips","title":"Tips","text":""},{"location":"vibe-sreing/claude-code/vscode/#mcp-settings-file","title":"MCP settings file","text":"<ul> <li> <p>The VS Code Extension uses ~/.config/Code/User/mcp.json file to configure the mcp servers</p> </li> <li> <p>The Claude Code CLI users ~/.claude.json (the large history file)</p> </li> <li> <p>Enabling enableAllProjectMcpServers at claude code settings, automatically approves all MCP servers defined in project .mcp.json</p> </li> </ul>"},{"location":"vibe-sreing/claude-code/vscode/#auto-installation","title":"Auto installation","text":"<p>The /config command permits to enable|disable the autoinstallation of the claude code IDE extension, vscode in this case. When you run claude under a vscode terminal, will will install the extension if it is enabled</p>"},{"location":"vibe-sreing/claude-code/vscode/#check-the-integration-with-vscode","title":"Check the integration with vscode","text":"<p>With the extension installed, this command manages the integration into vscode a show the status</p> <pre><code>/ide\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#open-the-extension","title":"Open the extension","text":"<p>Executing claude under a vscode terminal does not opens the extension. In order to open the extension you can:</p> <ul> <li> <p>Click the Claude Code icon</p> </li> <li> <p>Use the Ctrl+Esc shortcut</p> </li> </ul> <p>There is a vscode setting that changes this behaviour and make this icon open the vscode terminal, not the extension tab</p> <pre><code>\"claude-code.useTerminal\": true\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#add-multiline-input","title":"Add multiline input","text":"<p>We can configure the vscode terminal to support multiline prompts, this is, when in the prompt, inserts a \"\\\" and jumps to a new line.</p> <p>This is done with the following builtin command:</p> <pre><code>/terminal-setup\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#add-opened-tab-to-context","title":"Add opened tab to context","text":"<p>We can add an opened tab (file) to a claude code prompt as context (using #), focusing the tab and pressing</p> <pre><code>ALT + CTRL + k\n</code></pre>"},{"location":"vibe-sreing/claude-code/vscode/#links","title":"Links","text":"<ul> <li>Visual Studio Code</li> </ul> <p>https://docs.claude.com/en/docs/claude-code/vs-code</p> <ul> <li>VS Code Extension (Beta)</li> </ul> <p>https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code</p>"},{"location":"vibe-sreing/mcp/99-links/","title":"Links","text":"<ul> <li>Model Context Protocol</li> </ul> <p>https://modelcontextprotocol.io/</p> <ul> <li>Pulse MCP</li> </ul> <p>https://www.pulsemcp.com/</p> <ul> <li>Visual studio code MCP servers</li> </ul> <p>Model Context Protocol servers enabling AI assistants to securely access external tools and data.</p> <p>https://code.visualstudio.com/mcp</p> <ul> <li>MCP List from modelcontextprotocol.io</li> </ul> <p>https://github.com/modelcontextprotocol/servers</p> <ul> <li>mcpservers.org</li> </ul> <p>https://mcpservers.org/</p>"},{"location":"vibe-sreing/mcp/aws/","title":"AWS Official MCP servers","text":""},{"location":"vibe-sreing/mcp/aws/#overview","title":"Overview","text":"<p>AWS MCP (Model Context Protocol) Servers are specialized servers developed by AWS Labs that enhance AI applications' interactions with AWS services. These servers provide AI assistants like Claude with contextual access to AWS documentation, guidance, and best practices, enabling more accurate and efficient cloud-native development through a standardized client-server architecture.</p> <p>https://awslabs.github.io/mcp/</p>"},{"location":"vibe-sreing/mcp/aws/#key-benefits","title":"Key Benefits","text":"<ul> <li>Improved Output Quality: Provides relevant, up-to-date AWS information to AI assistants</li> <li>Latest Documentation: Access to current AWS documentation and service capabilities</li> <li>Workflow Automation: Enables automation of complex AWS-related workflows</li> <li>Domain Expertise: Delivers specialized knowledge about AWS services and best practices</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#server-categories","title":"Server Categories","text":"<p>AWS Labs provides MCP servers organized into the following categories:</p> <ol> <li>Documentation: Access to AWS documentation and knowledge bases</li> <li>Infrastructure &amp; Deployment: Tools for infrastructure management and deployment</li> <li>AI &amp; Machine Learning: Integration with AWS AI/ML services</li> <li>Data &amp; Analytics: Data processing and analytics capabilities</li> <li>Developer Tools &amp; Support: Development and debugging support</li> <li>Integration &amp; Messaging: Event-driven and messaging services</li> <li>Cost &amp; Operations: Cost management and operational tools</li> <li>Healthcare &amp; Lifesciences: Industry-specific services</li> </ol>"},{"location":"vibe-sreing/mcp/aws/#notable-mcp-servers","title":"Notable MCP Servers","text":"<ul> <li>AWS API MCP Server: Direct AWS API interaction</li> <li>AWS Documentation MCP Server: AWS documentation access</li> <li>AWS Knowledge MCP Server: AWS best practices and guidance</li> <li>Bedrock Knowledge Bases Retrieval MCP Server: Knowledge base integration</li> <li>Service-Specific Servers: DynamoDB, EKS, Lambda, S3, and more</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#deployment-options","title":"Deployment Options","text":"<ul> <li>Local Servers: Best for development, testing, and offline work</li> <li>Remote Servers: Ideal for team collaboration, scalability, and automatic updates</li> </ul>"},{"location":"vibe-sreing/mcp/aws/#use-cases","title":"Use Cases","text":"<ul> <li>Cloud-native application development with AI assistance</li> <li>Infrastructure as Code (IaC) with intelligent AWS service recommendations</li> <li>Automated AWS resource management and optimization</li> <li>Context-aware troubleshooting and debugging</li> <li>Best practices guidance for AWS architectures</li> </ul> <p>These servers enable AI assistants to provide intelligent, context-aware support for AWS-related tasks, improving development velocity and code quality.</p>"},{"location":"vibe-sreing/mcp/context7/","title":"Context7","text":""},{"location":"vibe-sreing/mcp/context7/#overview","title":"Overview","text":"<p>Context7 is an MCP (Model Context Protocol) server developed by Upstash that provides up-to-date, version-specific code documentation directly into AI coding assistants' context. It solves the common problem of AI models using outdated or generic library information by fetching real-time, accurate code examples and documentation.</p>"},{"location":"vibe-sreing/mcp/context7/#key-features","title":"Key Features","text":"<ul> <li>Version-Specific Documentation: Provides current documentation for specific library versions</li> <li>Real-Time Context: Fetches up-to-date code examples and API references</li> <li>Eliminates Hallucinations: Prevents AI-generated outdated or incorrect APIs</li> <li>Multi-Platform Support: Works with Cursor, VS Code, Claude Code, and other AI coding platforms</li> <li>Flexible Deployment: Supports both remote and local server connections</li> <li>Rate Limiting: Optional API key for higher rate limits and private repository access</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#how-it-works","title":"How It Works","text":"<ol> <li>User adds \"use context7\" to their prompt in supported AI coding platforms</li> <li>The system fetches current, version-specific documentation for the requested library or framework</li> <li>AI assistant receives accurate, up-to-date information for code generation</li> </ol>"},{"location":"vibe-sreing/mcp/context7/#benefits","title":"Benefits","text":"<ul> <li>More accurate AI-assisted coding with current documentation</li> <li>Reduces debugging time from outdated API usage</li> <li>Access to latest library features and best practices</li> <li>Eliminates need to manually search for documentation</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#requirements","title":"Requirements","text":"<ul> <li>Node.js v18.0.0 or higher</li> </ul>"},{"location":"vibe-sreing/mcp/context7/#claude-code","title":"Claude code","text":"<p>Claude Code Remote Server Connection</p> <pre><code>claude mcp add --transport http context7 &lt;https://mcp.context7.com/mcp&gt; --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n</code></pre> <p>Claude Code Local Server Connection</p> <pre><code>claude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n</code></pre>"},{"location":"vibe-sreing/mcp/context7/#vscode","title":"VSCODE","text":"<p>VS Code Remote Server Connection</p> <pre><code>\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n</code></pre> <p>VS Code Local Server Connection</p> <pre><code>\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n</code></pre>"},{"location":"vibe-sreing/mcp/context7/#links","title":"Links","text":"<ul> <li>Context7 website</li> </ul> <p>https://context7.com/</p> <ul> <li>Context7 github</li> </ul> <p>https://github.com/upstash/context7</p>"},{"location":"vibe-sreing/mcp/taskmaster/","title":"Taskmaster","text":""},{"location":"vibe-sreing/mcp/taskmaster/#overview","title":"Overview","text":"<p>Taskmaster is an open-source MCP (Model Context Protocol) server that functions as a project manager for AI assistants like Claude. It helps AI agents manage complex projects by breaking them down into manageable, trackable tasks.</p>"},{"location":"vibe-sreing/mcp/taskmaster/#key-features","title":"Key Features","text":"<ul> <li>Task Decomposition: Automatically breaks down complex projects into smaller, actionable tasks</li> <li>One-shot Completion: Helps AI agents complete tasks efficiently in single iterations</li> <li>Context Management: Prevents context overload by maintaining focus on specific tasks</li> <li>Code Protection: Avoids disrupting existing code through structured task execution</li> <li>Progress Tracking: Keeps AI agents on track throughout multi-step projects</li> </ul>"},{"location":"vibe-sreing/mcp/taskmaster/#use-cases","title":"Use Cases","text":"<ul> <li>Managing ambitious, multi-component projects</li> <li>Coordinating AI-driven development workflows</li> <li>Preventing scope creep during implementation</li> <li>Maintaining project structure across long conversations</li> </ul>"},{"location":"vibe-sreing/mcp/taskmaster/#availability","title":"Availability","text":"<ul> <li>Completely free and open source</li> <li>Requires user to provide their own API keys</li> <li>Available at: https://www.task-master.dev/</li> </ul> <p>Taskmaster MCP enables more systematic and organized collaboration between users and AI assistants, particularly valuable for complex software engineering projects that require careful planning and execution.</p>"}]}